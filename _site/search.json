[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a data scientist in the Pittsburgh area with an interest in data visualization, statistical programming, birdwatching, and civic data."
  },
  {
    "objectID": "posts/spatial-clustering-pittsburgh/index.html",
    "href": "posts/spatial-clustering-pittsburgh/index.html",
    "title": "What region is Pittsburgh in?",
    "section": "",
    "text": "“What region is Pittsburgh in?” is a question that comes up frequently. Is it in Appalachia? The Midwest? Great Lakes? Mid-Atlantic? It really depends on who you ask, and what variables you have at hand.\nIn this post, I use spatially constrained clustering with US Census data to build contiguous clusters of US counties. I analyze these clusters and identify which cluster the greater Pittsburgh region (AKA Allegheny County) is in. This post by Kyle Walker is the inspiration for this analysis: https://walker-data.com/posts/census-regions/\nI use the {rgeoda} package for the clustering analysis. It was pulled off of CRAN for compatability reasons, but it is still available on Github: https://github.com/geodacenter/rgeoda\n\nSetup\nSet up the session and load relevant R packages:\n\n\nCode\n#devtools::install_github(\"geodacenter/rgeoda\")\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tidycensus)\nlibrary(rgeoda)\nlibrary(rmapshaper)\nlibrary(leaflet)\nlibrary(leaflet.extras)\nlibrary(janitor)\nlibrary(hrbrthemes)\nlibrary(tictoc)\nlibrary(patchwork)\nlibrary(mapview)\nlibrary(leafpop)\nlibrary(broom)\nlibrary(scales)\nlibrary(GGally)\n\noptions(tigris_use_cache = TRUE,\n        scipen = 999,\n        digits = 4)\n\nset.seed(1234)\n\n\nI will use broad measures of ethnicity, population, and income to cluster the US counties. This code uses {tidycensus} to get the county-level population of various ethnicities, total population, and income. I focus on states east of the Mississippi River because that is a natural cultural and geographic threshold. I don’t think many people would consider Pittsburgh to be similar to areas west of the Mississippi.\n\n\nCode\nacs_vars &lt;- load_variables(2022, \"acs5\", cache = TRUE)\n\n# acs_vars |&gt; \n#   view()\n\nstates_vec &lt;- c(\"PA\", \"OH\", \"WV\", \"MD\", \"NY\", \"NJ\", \"VA\", \"KY\", \"DC\", \"DE\", \"CT\", \"RI\", \"MA\", \"VT\", \"NH\", \"ME\", \"MS\", \"IL\", \"IN\", \"WI\", \"MI\", \"TN\", \"AL\", \"GA\", \"NC\", \"SC\", \"FL\")\n\nvars_acs &lt;- c(income = \"B19013_001\",\n              total_population = \"B01003_001\",\n              #variables from Kyle Walker https://walker-data.com/census-r/wrangling-census-data-with-tidyverse-tools.html?q=race_vars#using-summary-variables-and-calculating-new-columns\n              dem_white = \"B03002_003\",\n              dem_black = \"B03002_004\",\n              dem_native = \"B03002_005\",\n              dem_asian = \"B03002_006\",\n              #dem_hipi = \"B03002_007\",\n              dem_hispanic = \"B03002_012\",\n              dem_other_race = \"B03002_008\")\n\n# acs_vars |&gt; \n#   filter(name %in% vars_acs)\n\ncensus_data_acs &lt;- get_acs(year = 2022, \n                           state = states_vec,\n                           variables = vars_acs, \n                           geography = \"county\", \n                           geometry = TRUE)\n\ncensus_data &lt;- census_data_acs\n\n\n\n\nExploratory data analysis\nThis code graphs the Census data for each county on a map.\n\n\nCode\ncensus_vars &lt;- census_data_acs |&gt; \n  select(-c(GEOID, moe)) |&gt; \n  pivot_wider(names_from = variable, values_from = estimate) |&gt; \n  select(NAME, starts_with(\"dem\"), total_population, income) |&gt; \n  mutate(across(-c(NAME, geometry, total_population, income), ~.x / total_population, .names = \"pct_{.col}\")) |&gt; \n  rowwise() |&gt; \n  mutate(pct_total = sum(c_across(contains(\"pct\")))) |&gt; \n  ungroup() |&gt; \n  select(NAME, total_population, income, starts_with(\"pct_dem\")) |&gt; \n  rename_with(~str_remove(.x, \"^pct_dem_\")) |&gt; \n  pivot_longer(-c(NAME, geometry))\n\nmap_census_data &lt;- function(x, var){\n  \n  x |&gt; \n    filter(name == var) |&gt; \n    ggplot(aes(fill = value)) +\n    geom_sf(lwd = 0) +\n    facet_wrap(vars(name)) +\n    scale_fill_viridis_c() +\n    guides(fill = \"none\") +\n    theme_void()\n  \n}\n\nvar_vec &lt;- census_vars |&gt; \n  st_drop_geometry() |&gt; \n  distinct(name) |&gt; \n  pull()\n\nmap_list &lt;- map(var_vec, ~map_census_data(census_vars, .x))\n\nwrap_plots(map_list)\n\n\n\n\n\n\n\n\n\nThis code combines “isolate” (AKA island) counties with their nearest comparable county. The spatially constrained clustering algorithm I use later will fail if these “island” counties are not merged because it cannot find any neighbors for them.\n\n\nCode\ncensus_tracts &lt;- census_data |&gt; \n  mutate(NAME = case_when(str_detect(NAME, \"Barnstable|Dukes|Nantucket\") ~ \"Barnstable + Dukes + Nantucket Counties, Massachusetts\",\n                           TRUE ~ NAME)) |&gt; \n  mutate(NAME = case_when(str_detect(NAME, \"Richmond County, New York|Kings County, New York\") ~ \"Richmond + Kings Counties, New York\",\n         TRUE ~ NAME)) |&gt;  #don't @ me, NY\n  group_by(NAME, variable) |&gt; \n  summarize(estimate = sum(estimate)) |&gt; \n  ungroup()\n\n#check if any counties are isolates \ncensus_tracts |&gt; \n  rook_weights() |&gt; \n  has_isolates() == FALSE\n\n\n[1] TRUE\n\n\nThis code calculates the county-level % of each ethnicity.\n\n\nCode\ncensus_tracts_wide &lt;- census_tracts |&gt; \n  pivot_wider(names_from = variable, values_from = estimate) |&gt; \n  rename_with(str_to_lower, -c(NAME, geometry)) |&gt; \n  mutate(across(-c(NAME, geometry, total_population, income), ~.x / total_population, .names = \"pct_{.col}\")) |&gt; \n  rowwise() |&gt; \n  mutate(pct_total = sum(c_across(contains(\"pct\")))) |&gt; \n  ungroup()\n\nglimpse(census_tracts_wide)\n\n\nRows: 1,604\nColumns: 17\n$ NAME               &lt;chr&gt; \"Abbeville County, South Carolina\", \"Accomack Count…\n$ geometry           &lt;GEOMETRY [°]&gt; POLYGON ((-82.74 34.21, -82..., MULTIPOLYG…\n$ dem_asian          &lt;dbl&gt; 58, 268, 59, 487, 51, 264, 127, 844, 138, 702, 1696…\n$ dem_black          &lt;dbl&gt; 6372, 9446, 307, 2565, 213, 15707, 97, 1415, 507, 3…\n$ dem_hispanic       &lt;dbl&gt; 441, 3084, 468, 1198, 1673, 2051, 303, 7688, 931, 9…\n$ dem_native         &lt;dbl&gt; 28, 56, 3, 40, 9, 57, 38, 53, 123, 42, 201, 429, 18…\n$ dem_other_race     &lt;dbl&gt; 138, 55, 6, 96, 39, 158, 36, 328, 118, 55, 500, 140…\n$ dem_white          &lt;dbl&gt; 16658, 19813, 17308, 59490, 33254, 10834, 26354, 92…\n$ income             &lt;dbl&gt; 49759, 52694, 49690, 63767, 61731, 37271, 46234, 78…\n$ total_population   &lt;dbl&gt; 24368, 33367, 18887, 65583, 35827, 29425, 27509, 10…\n$ pct_dem_asian      &lt;dbl&gt; 0.0023802, 0.0080319, 0.0031238, 0.0074257, 0.00142…\n$ pct_dem_black      &lt;dbl&gt; 0.261490, 0.283094, 0.016255, 0.039111, 0.005945, 0…\n$ pct_dem_hispanic   &lt;dbl&gt; 0.018098, 0.092427, 0.024779, 0.018267, 0.046697, 0…\n$ pct_dem_native     &lt;dbl&gt; 0.0011490, 0.0016783, 0.0001588, 0.0006099, 0.00025…\n$ pct_dem_other_race &lt;dbl&gt; 0.0056632, 0.0016483, 0.0003177, 0.0014638, 0.00108…\n$ pct_dem_white      &lt;dbl&gt; 0.6836, 0.5938, 0.9164, 0.9071, 0.9282, 0.3682, 0.9…\n$ pct_total          &lt;dbl&gt; 0.9724, 0.9807, 0.9610, 0.9740, 0.9836, 0.9880, 0.9…\n\n\nThis code checks that I am capturing most ethnicities in most counties.\n\n\nCode\n#check that I am capturing most ethnicities in most counties.\ncensus_tracts_wide |&gt; \n  ggplot(aes(pct_total)) +\n  geom_histogram() +\n  scale_x_percent() +\n  coord_cartesian(xlim = c(0, 1)) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nThis plot shows that many of the variables are correlated with each other. This could cause the clustering algorithm to “double count” a signal it has already seen.\n\n\nCode\n#https://stackoverflow.com/questions/44984822/how-to-create-lower-density-plot-using-your-own-density-function-in-ggally\nmy_fn &lt;- function(data, mapping, ...){\n      # Using default ggplot density function\n\n      p &lt;- ggplot(data = data, mapping = mapping) + \n        geom_bin_2d() +\n        scale_fill_viridis_c() +\n        scale_x_continuous(labels = label_number(scale_cut = cut_short_scale())) +\n        scale_y_continuous(labels = label_number(scale_cut = cut_short_scale()))\n      p\n}\n\ncensus_tracts_wide |&gt;\n  st_drop_geometry() |&gt; \n  select(total_population, income, contains(\"pct_dem\")) |&gt; \n  rename_with(~str_remove(.x, \"^pct_dem_\")) |&gt; \n  ggpairs(lower = list(continuous = my_fn)) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nI use PCA to de-correlate the variables while retaining the information they hold. This process creates new variables called principal components. I will use these to cluster the counties.\n\n\nCode\n#https://clauswilke.com/blog/2020/09/07/pca-tidyverse-style/\n\ncensus_pca &lt;- census_tracts_wide |&gt; \n  select(NAME, total_population, income, contains(\"pct\"), -pct_total) |&gt; \n  st_drop_geometry() |&gt; \n  rename_with(~str_remove(.x, \"^pct_dem_\"))\n\npca_fit &lt;- census_pca |&gt; \n  select(where(is.numeric)) |&gt; \n  prcomp(scale = TRUE)\n\n# define arrow style for plotting\narrow_style &lt;- arrow(\n  angle = 20, ends = \"first\", type = \"closed\", length = grid::unit(8, \"pt\")\n)\n\n# plot rotation matrix\npca_fit %&gt;%\n  tidy(matrix = \"rotation\") %&gt;%\n  pivot_wider(names_from = \"PC\", names_prefix = \"PC\", values_from = \"value\") %&gt;%\n  ggplot(aes(PC1, PC2)) +\n  geom_segment(aes(color = column), xend = 0, yend = 0, arrow = arrow_style) +\n  ggrepel::geom_label_repel(aes(label = column, fill = column)) +\n  coord_fixed() + # fix aspect ratio to 1:1\n  guides(fill = \"none\",\n         color = \"none\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nThis graph shows the following:\n\nCounties that have higher % White population are likely to have low % Black population, and vice versa. This is a statistical artifact of slavery, long-standing segregation, and other associated government policies.\nCounties that have a high % of White or Black population are more likely to have low total population.\nCounties that have higher total population are more likely to be more ethnically diverse and have higher income.\n\nAs expected, the first few components capture most of the signal.\n\n\nCode\npca_fit %&gt;%\n  tidy(matrix = \"eigenvalues\") %&gt;%\n  ggplot(aes(PC, percent)) +\n  geom_col(fill = \"#56B4E9\", alpha = 0.8) +\n  scale_x_continuous(breaks = 0:9) +\n  scale_y_continuous(\n    labels = percent_format(),\n    expand = expansion(mult = c(0, 0.01))\n  ) +\n  labs(x = \"PC\",\n       y = \"% of variance explained\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nThe 8th component contains no signal, so I will exclude it from the clustering algorithm.\nThe components are not correlated with each other.\n\n\nCode\ncensus_pca_augment &lt;- augment(pca_fit, census_pca) |&gt; \n  select(-c(.rownames, .fittedPC8))\n\ncensus_pca_augment |&gt; \n  select(contains(\".fitted\")) |&gt; \n  GGally::ggpairs(lower = list(continuous = my_fn)) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nThis maps the values of the first 4 components onto the counties.\n\n\nCode\ncensus_tracts_pca &lt;- census_tracts |&gt; \n  distinct(NAME, geometry) |&gt; \n  left_join(census_pca_augment, by = join_by(NAME))\n\ncensus_tracts_pca_long &lt;- census_tracts_pca |&gt; \n  select(NAME, .fittedPC1:.fittedPC4) |&gt; \n  pivot_longer(contains(\".fitted\"))\n\npc_vars &lt;- census_tracts_pca_long |&gt; \n  distinct(name) |&gt; \n  pull()\n\nmap_pca &lt;- function(x, var){\n  \n  x |&gt; \n    filter(name == var) |&gt; \n    ggplot() +\n    geom_sf(aes(fill = value), lwd = 0) +\n    facet_wrap(vars(name)) +\n    scale_fill_viridis_c() +\n    scale_x_continuous(labels = label_number(scale_cut = cut_short_scale())) +\n    scale_y_continuous(labels = label_number(scale_cut = cut_short_scale())) +\n    theme_void()\n  \n}\n\nmap_pca_list &lt;- map(pc_vars, ~map_pca(census_tracts_pca_long, .x))\n\nwrap_plots(map_pca_list)\n\n\n\n\n\n\n\n\n\n\n\nClustering\nThis calculates the rook contiguity weights between the counties that is used in the clustering algorithm.\n\n\nCode\n#https://geodacenter.github.io/rgeoda/articles/rgeoda_tutorial.html#spatial-clustering\n#census_tracts_wide_geo &lt;- geoda_open(\"posts/geospatial-clustering-pittsburgh/post_data/census_tracts/census_tracts_wide.shp\")\n\nw_rook &lt;- rook_weights(census_tracts_pca)\n\nsummary(w_rook)\n\n\n                     name               value\n1 number of observations:                1604\n2          is symmetric:                 TRUE\n3               sparsity: 0.00341804466390134\n4        # min neighbors:                   1\n5        # max neighbors:                  10\n6       # mean neighbors:    5.48254364089776\n7     # median neighbors:                   6\n8           has isolates:               FALSE\n\n\nHere I finalize the variables to use to cluster the counties and set a minimum population size for each of the generated clusters. The minimum is set to 5% of the total population. Note that I do not set a target number of clusters for the algorithm to create (unlike algorithms like k-means). The algorithm iteratively searches through combinations of contiguous counties until it finds an optimal number of clusters.\n\n\nCode\ncluster_df &lt;- census_tracts_pca |&gt; \n  select(contains(\".fitted\")) |&gt; \n  st_drop_geometry()\n\nbound_vals &lt;- census_tracts_wide['total_population']\n\n#minimum group population is 5% of total population\nmin_bound &lt;- census_tracts_wide |&gt; \n  st_drop_geometry() |&gt; \n  summarize(total_population = sum(total_population)) |&gt; \n  mutate(min_bound = total_population * .05) |&gt; \n  pull(min_bound)\n\ncomma(min_bound)\n\n\n[1] \"9,509,822\"\n\n\nCode\ntic()\nmaxp_clusters_greedy &lt;- maxp_greedy(w_rook, cluster_df, bound_vals, min_bound, scale_method = \"standardize\")\nmaxp_clusters_greedy[2:5]\n\n\n$`Total sum of squares`\n[1] 11221\n\n$`Within-cluster sum of squares`\n[1]  930.8  764.1 1468.0 1416.2 1256.5 1364.6 1487.3\n\n$`Total within-cluster sum of squares`\n[1] 2534\n\n$`The ratio of between to total sum of squares`\n[1] 0.2258\n\n\nCode\ntoc()\n\n\n2.235 sec elapsed\n\n\nThe ratio of between to total sum of squares (BSS to TSS) shows that the clusters explain 22% of the variance in the data. This is kind of low for a clustering analysis, but the contiguous spatial constraint creates a higher level of difficulty.\nThis scatterplot shows the total population and number of counties included in each cluster. This shows that some clusters have many more counties than others, but all clusters have at least 5% of the toal population (9.5 million people).\n\n\nCode\ntract_clusters &lt;- census_tracts_wide |&gt; \n  mutate(cluster = as.character(maxp_clusters_greedy$Clusters),\n         cluster = fct_reorder(cluster, total_population, sum, .desc = TRUE))\n\ntract_clusters |&gt; \n  st_drop_geometry() |&gt; \n  summarize(counties = n(),\n            total_population = sum(total_population),\n            .by = cluster) |&gt; \n  ggplot(aes(counties, total_population, label = cluster)) +\n  geom_label() +\n  scale_y_continuous(labels = label_number(scale_cut = cut_short_scale())) +\n  labs(x = \"Number of counties\",\n       y = \"Population\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nThis creates a custom color palette for the cluster map. This took some trial and error because it is difficult to create a reasonable discrete palette with so many options\n\n\nCode\ncluster_palette &lt;- c(RColorBrewer::brewer.pal(name = \"Paired\", n = 12), \"black\", \"hotpink\")\n\ncolor_order &lt;- farver::decode_colour(cluster_palette, \"rgb\", \"hcl\") |&gt;\n  as_tibble() |&gt;\n  mutate(color = cluster_palette) |&gt; \n  arrange(desc(c))\n\nshow_col(color_order$color)\n\n\n\n\n\n\n\n\n\nCode\ncluster_palette &lt;- color_order |&gt; pull(color)\n\nnclust &lt;- tract_clusters |&gt; \n  distinct(cluster) |&gt; \n  nrow()\n\n\nThis uses {ggplot2} to map the clusters onto the county map.\nAt first glance the algorithm made clusters that generally align with my thinking on clusters of American demographics, but also makes some interesting distinctions that I wouldn’t have thought of.\n\n\nCode\nmap_greedy &lt;- tract_clusters |&gt;  \n  group_by(cluster) |&gt; \n  summarize() |&gt; \n  ggplot() +\n  geom_sf(data = summarize(census_tracts), fill = NA) +\n  geom_sf(aes(fill = cluster), color = NA) +\n  guides(fill = \"none\") +\n  scale_fill_manual(values = cluster_palette)\n\nmap_greedy +\n  theme_void()\n\n\n\n\n\n\n\n\n\nThis shows each cluster separately.\n\n\nCode\ntract_clusters |&gt;  \n  group_by(cluster) |&gt; \n  summarize() |&gt; \n  ggplot() +\n  geom_sf(data = summarize(census_tracts)) +\n  geom_sf(aes(fill = cluster), color = NA) +\n  guides(fill = \"none\") +\n  scale_fill_manual(values = cluster_palette) +\n  facet_wrap(vars(cluster)) +\n  theme_void() +\n  theme(strip.text = element_text(size = 22))\n\n\n\n\n\n\n\n\n\nThis is an interactive Leaflet map of the clusters.\n\n\nCode\nclusters_leaflet &lt;- tract_clusters |&gt; \n  mutate(across(contains(\"pct\"), ~.x * 100)) |&gt; \n  mutate(across(contains(\"pct\"), round, 1))\n\nfill_pal &lt;- colorFactor(palette = cluster_palette, domain = clusters_leaflet$cluster)\n\nlabels &lt;- sprintf(\n  \"&lt;strong&gt;%s&lt;/strong&gt;&lt;br/&gt;Cluster: %s&lt;br/&gt;White: %#.2f%%&lt;br/&gt;Black: %#.2f%%&lt;br/&gt;Asian: %#.2f%%&lt;br/&gt;Hispanic: %#.2f%%&lt;br/&gt;Other Race: %#.2f%%&lt;br/&gt;Total population: %f&lt;br/&gt;Income: %f\",\n  clusters_leaflet$NAME, clusters_leaflet$cluster, clusters_leaflet$pct_dem_white, clusters_leaflet$pct_dem_black, clusters_leaflet$pct_dem_asian, clusters_leaflet$pct_dem_hispanic, clusters_leaflet$pct_dem_other_race, clusters_leaflet$total_population, clusters_leaflet$income\n) %&gt;% lapply(htmltools::HTML)\n\nlabels &lt;- sprintf(\n  \"&lt;strong&gt;%s&lt;/strong&gt;&lt;br/&gt;Cluster: %s&lt;br/&gt;White: %s&lt;br/&gt;Black: %s&lt;br/&gt;Asian: %s&lt;br/&gt;Hispanic: %s&lt;br/&gt;Native American: %s&lt;br/&gt;Other Race: %s&lt;br/&gt;Total population: %s&lt;br/&gt;Income: %s\",\n  tract_clusters$NAME,\n  tract_clusters$cluster,\n  percent(tract_clusters$pct_dem_white, accuracy = .1),\n  percent(tract_clusters$pct_dem_black, accuracy = .1),\n  percent(tract_clusters$pct_dem_asian, accuracy = .1),\n  percent(tract_clusters$pct_dem_hispanic, accuracy = .1),\n  percent(tract_clusters$pct_dem_native, accuracy = .1),\n  percent(tract_clusters$pct_dem_other_race, accuracy = .1),\n  comma(tract_clusters$total_population, accuracy = .1),\n  comma(tract_clusters$income, prefix = \"$\")\n  ) |&gt; \n  lapply(htmltools::HTML)\n\nclusters_leaflet |&gt; \n  leaflet() |&gt; \n  setView(lng = -81.6326, lat = 38.3498, zoom = 4) |&gt; \n  addProviderTiles(providers$CartoDB.Positron) |&gt; \n  addPolygons(fillColor = ~fill_pal(cluster),\n              fillOpacity = .5,\n              weight = .1,\n              stroke = FALSE,\n              label = labels,\n              labelOptions = labelOptions(\n                style = list(\"font-weight\" = \"normal\", padding = \"3px 8px\"),\n                textsize = \"15px\",\n                direction = \"auto\")) |&gt; \n  addFullscreenControl()\n\n\n\n\n\n\n\n\nCluster discussion\nI think you can definitely dispute some of the outcomes of the clustering algorithm on the margins, but as a whole it does make some sense. This reflects the fact that the algorithm is not trying to find the optimal cluster for each county. It is trying to find the optimal number of clusters given the variables it is dealing with.\n\nCluster 8 is New England and counties bordering Lake Ontario and Canada.\nCluster 12 is the northern Acela Corridor (Philadelphia, New Jersey, Hartford).\nCluster 14 is New York City and Long Island.\nCluster 11 is the DC/Baltimore/Philly suburb section of the Acela Corridor. Southern Delaware is most likely included because it is not populated enough to be its own cluster, and it would be cut off from its more similar counties otherwise.\nCluster 7 is the stretch of rural counties from Ohio to New Hampshire.\nCluster 10 is the Rust Belt. It includes Pittsburgh, Johnstown, Erie, Cleveland, Toledo, and Detroit.\nCluster 5 is rural Appalachia. This cluster includes Charlotte NC, probably because it wouldn’t meet the 5% population threshhold otherwise.\nCluster 4 is one of the less geographically cohesive clusters. It includes Michigan’s Lower Penninsula and then dives south. On average the counties are higher % White, lower population, with lower income.\nCluster 1 is demographically similar to Cluster 4. It includes rural western rural Michigan, central Wisconsin, and Illinois. It is also not as cohesive, and reaches into Ohio, Kentucky, and Tennessee.\nCluster 9 contains cities along the southern and western shores of Lake Michigan and northern counties of Wisconsin with high % population of Native Americans. Chicago and its southern suburbs probably explain why Clusters 1 and 4 are separate.\nClusters 6, 3, and 2\n\nCluster 6 contains the population centers of the eastern Sun Belt (excluding Florida). It is similar to other clusters in the area, but is on average more populated with higher income and lower % Black population. It contains cities such as Atlanta and Nashville.\nCluster 3 contains wide swaths of rural Mississippi, Alabama, and Georgia. It also reaches into upper Florida and the interior of South Carolina. On average it has the highest Black %, lowest % White, and the lowest income.\nCluster 2 includes Virginia, the Carolinas, and reaches into the suburbs of Atlanta. It is most similar to Cluster 3, but on average has lower Black %, higher White %, and higher income.\n\nCluster 13 is the middle and lower parts of Florida. It has the highest % Hispanic.\n\n\n\nWhere is Pittsburgh?\nThis shows that Pittsburgh (Allegheny County) is in the “Rust belt” cluster along with Detroit, Toledo, and Cleveland.\n\n\nCode\n#how to zoom\n#https://datascience.blog.wzb.eu/2019/04/30/zooming-in-on-maps-with-sf-and-ggplot2/\n\nstates_geo &lt;- tract_clusters |&gt;  \n  separate(NAME, into = c(\"county\", \"state\"), sep = \",\") |&gt; \n  mutate(across(c(county, state), str_squish)) |&gt;\n  group_by(state) |&gt; \n  summarize()\n\nac_geo &lt;- census_tracts |&gt; \n  filter(NAME == \"Allegheny County, Pennsylvania\") |&gt; \n  distinct(NAME, geometry)\n\nac_geo_centroid &lt;- ac_geo |&gt; \n  st_centroid() |&gt; \n  mutate(lon = map_dbl(geometry, 1),\n         lat = map_dbl(geometry, 2)) |&gt; \n  st_drop_geometry() |&gt; \n  select(lon, lat)\n\nzoom_to &lt;- c(ac_geo_centroid$lon, ac_geo_centroid$lat)\n\nzoom_level &lt;- 5\n\nlon_span &lt;- 360 / 2^zoom_level\nlat_span &lt;- 180 / 2^zoom_level\n\nlon_bounds &lt;- c(zoom_to[1] - lon_span / 2, zoom_to[1] + lon_span / 2)\nlat_bounds &lt;- c(zoom_to[2] - lat_span / 2, zoom_to[2] + lat_span / 2)\n\nmap_greedy +\n  geom_sf(data = states_geo, fill = NA) +\n  geom_sf(data = ac_geo,  lwd = .5, color = \"black\", fill = NA) +\n  coord_sf(xlim = lon_bounds, ylim = lat_bounds) +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n\nPossible extensions\nI think there are multiple ways of extending this analysis. The obvious one is to include the rest of the contiguous lower 48 states (Alaska and Hawaii are “isolates” in this context, and are very different demographically). I could also include other variables:\n\nReligion\nLanguage\nEducation\nPopulation density\nTrends in total population over time\n\nThe algorithm is sensitive to the population threshold and the variables it is fed, so a more optimal solution could be found by using those as tuning parameters to find a combination that maximizes BSS/TSS.\n\n\nFull map with Pittsburgh highlighted\n\n\nCode\nmap_greedy +\n  geom_sf(data = states_geo, fill = NA) +\n  geom_sf(data = ac_geo,  lwd = .5, color = \"black\", fill = NA) +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n\nCluster characteristics\nThis series of scatterplots show the mean characteristics of each cluster.\n\n\nCode\ntract_clusters |&gt; \n  select(total_population, income, contains(\"pct_dem\"), cluster) |&gt; \n  st_drop_geometry() |&gt; \n  rename_with(~str_remove(.x, \"^pct_dem_\")) |&gt; \n  pivot_longer(-c(cluster, total_population)) |&gt; \n  summarize(total_population = mean(total_population),\n            value = mean(value),\n            .by = c(cluster, name)) |&gt; \n  ggplot(aes(total_population, value, fill = cluster, label = cluster)) +\n  geom_point(aes(color = cluster)) +\n  ggrepel::geom_label_repel() +\n  facet_wrap(vars(name), scales = \"free_y\", ncol = 2) +\n  scale_x_log10(#expand = expansion(mult = c(.2, .2))#,\n                #labels = label_number(scale_cut = cut_short_scale())\n                ) +\n  scale_y_continuous(#expand = expansion(mult = c(.2, .2)),\n                     labels = label_number(scale_cut = cut_short_scale())) +\n  guides(fill = \"none\", color = \"none\") +\n  labs(x = \"Total population (log10)\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nThis shows the distribution of each variable within each cluster. Note that the X axis for each variable will be different for each cluster.\n\n\nCode\ncluster_dist &lt;- tract_clusters |&gt; \n  select(total_population, income, contains(\"pct_dem\"), cluster) |&gt; \n  st_drop_geometry() |&gt; \n  rename_with(~str_remove(.x, \"^pct_dem_\")) |&gt; \n  pivot_longer(-c(cluster))\n\nplot_cluster_dist &lt;- function(x, cluster_num){\n  \n  x |&gt; \n    filter(cluster == cluster_num) |&gt; \n    ggplot(aes(value)) +\n    geom_histogram() +\n    scale_x_continuous(labels = label_number(scale_cut = cut_short_scale())) +\n    facet_wrap(vars(name), scales = \"free\", ncol = 4) +\n    guides(fill = \"none\") +\n    theme_bw()\n  \n}\n\nplot_list &lt;- map(as.character(1:nclust), ~plot_cluster_dist(cluster_dist, .x))\n\nplot_list\n\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n\n\n\n\n[[4]]\n\n\n\n\n\n\n\n\n\n\n[[5]]\n\n\n\n\n\n\n\n\n\n\n[[6]]\n\n\n\n\n\n\n\n\n\n\n[[7]]\n\n\n\n\n\n\n\n\n\n\n[[8]]\n\n\n\n\n\n\n\n\n\n\n[[9]]\n\n\n\n\n\n\n\n\n\n\n[[10]]\n\n\n\n\n\n\n\n\n\n\n[[11]]\n\n\n\n\n\n\n\n\n\n\n[[12]]\n\n\n\n\n\n\n\n\n\n\n[[13]]\n\n\n\n\n\n\n\n\n\n\n[[14]]"
  },
  {
    "objectID": "posts/model-results-leaflet-map/index.html",
    "href": "posts/model-results-leaflet-map/index.html",
    "title": "Pittsburgh City Boundary Model Leaflet Map",
    "section": "",
    "text": "Last Friday I posted a classification model that attempted to identify which census tracts in Allegheny County fall within the border of the City of Pittsburgh. I used a variety of data from the 2010 US Census via {tidycensus}, and I intentionally did not train the model with any geographic data. I am more interested in which census tracts are more “city-like”, regardless of their distance from the geographic center of the city.\nAs with most models, I think you can learn the most when you investigate the cases where the model failed. I made an interactive Leaflet map to help me to interrogate the model’s results. For each tract it includes the average city classification % and the model’s correct classification %, as well as the census data that was used to train the model. Make sure to click the “View Fullscreen” button in the top left corner to see more of the map."
  },
  {
    "objectID": "posts/suburbanization-of-allegheny-county/index.html",
    "href": "posts/suburbanization-of-allegheny-county/index.html",
    "title": "Suburbanization of Allegheny County",
    "section": "",
    "text": "This March, researchers at the University of Georgia and Florida State University released the HHUUD10 dataset, which contains estimates of the number of housing units for decennial census years 1940-2010 and 2019. A “housing unit” could be a studio apartment or 5 bedroom single-family home. The data uses 2010 census tracts, which allows for historical comparison of housing trends across constant geometry. The full paper explains the approach.\nThis paper and the dataset can be used for a wide variety of socioeconomic issues. I will focus on suburbanization trends in the Pittsburgh area."
  },
  {
    "objectID": "posts/suburbanization-of-allegheny-county/index.html#overall-trend",
    "href": "posts/suburbanization-of-allegheny-county/index.html#overall-trend",
    "title": "Suburbanization of Allegheny County",
    "section": "Overall trend",
    "text": "Overall trend\n\nFix date formatting\nSince the data comes in a wide format, I pivot it long and fix up the year column to make it easy to graph with.\n\nac_housing_hu &lt;- ac_housing |&gt; \n  select(GEOID10, starts_with(\"hu\")) |&gt; \n  pivot_longer(cols = starts_with(\"hu\"), names_to = \"year\", values_to = \"housing_units\")\n\nyear_lookup &lt;- ac_housing_hu |&gt; \n  st_drop_geometry() |&gt; \n  distinct(year) |&gt; \n  mutate(year_fixed = c(1940, 1950, 1960, 1970, 1980, 1990, 2000, 2010, 2019))\n\nac_housing_hu &lt;- ac_housing_hu |&gt; \n  left_join(year_lookup) |&gt; \n  select(-year) |&gt; \n  rename(year = year_fixed)\n\nglimpse(ac_housing_hu)\n\nRows: 3,618\nColumns: 4\n$ GEOID10       &lt;chr&gt; \"42003560500\", \"42003560500\", \"42003560500\", \"4200356050…\n$ geometry      &lt;POLYGON [US_survey_foot]&gt; POLYGON ((1373906 410182, 1..., POL…\n$ housing_units &lt;dbl&gt; 1349, 1509, 1515, 1441, 1424, 1433, 1381, 1349, 1487, 13…\n$ year          &lt;dbl&gt; 1940, 1950, 1960, 1970, 1980, 1990, 2000, 2010, 2019, 19…\n\n\nThe number of housing units in the county stagnated after 1960, which is expected given the collapse of the steel industry.\n\nac_housing_hu |&gt; \n  st_drop_geometry() |&gt; \n  mutate(year = as.character(year) |&gt; fct_inorder()) |&gt; \n  group_by(year) |&gt; \n  summarize(housing_units = sum(housing_units)) |&gt; \n  ungroup() |&gt; \n  ggplot(aes(year, housing_units, group = 1)) +\n  geom_line() +\n  geom_point() +\n  scale_y_comma() +\n  labs(x = \"Year\",\n       y  = \"Housing units\")\n\n\n\n\n\n\n\n\nThe decennial difference in “gross” housing units also shows that growth stagnated after 1960.\n\nac_housing_hu |&gt; \n  st_drop_geometry() |&gt; \n  mutate(year = as.character(year) |&gt; fct_inorder()) |&gt; \n  group_by(year) |&gt; \n  summarize(housing_units = sum(housing_units)) |&gt; \n  ungroup() |&gt; \n  mutate(diff = housing_units - lag(housing_units)) |&gt; \n  ggplot(aes(year, diff, group = 1)) +\n  geom_line() +\n  geom_point() +\n  scale_y_comma(prefix = \"+ \") +\n  coord_cartesian(ylim = c(0, 90000)) +\n  labs(title = \"Growth stagnated after 1960\",\n       x = \"Year\",\n       y  = \"Change in housing units\")"
  },
  {
    "objectID": "posts/suburbanization-of-allegheny-county/index.html#change-from-1940-to-2019",
    "href": "posts/suburbanization-of-allegheny-county/index.html#change-from-1940-to-2019",
    "title": "Suburbanization of Allegheny County",
    "section": "Change from 1940 to 2019",
    "text": "Change from 1940 to 2019\nThis interactive map shows the areas that gained or lost the most housing units from 1940-2019. Dense housing around industrial areas along the Allegheny and Monongahela Rivers was erased. Homestead and Braddock stand out.\n\nhu_diff &lt;- ac_housing_hu |&gt; \n  group_by(GEOID10) |&gt; \n  filter(year == min(year) | year == max(year)) |&gt; \n  ungroup() |&gt; \n  select(GEOID10, year, housing_units) |&gt; \n  as_tibble() |&gt; \n  pivot_wider(names_from = year, names_prefix = \"units_\", values_from = housing_units) |&gt; \n  mutate(diff = units_2019 - units_1940) |&gt; \n  st_as_sf()\n\npal &lt;- colorNumeric(\n  palette = \"viridis\",\n  domain = hu_diff$diff)\n\nleaflet_map &lt;- hu_diff |&gt; \n  mutate(diff_formatted = comma(diff, accuracy = 1),\n         diff_label = str_c(\"Census tract: \", GEOID10, \"&lt;br/&gt;\", \"Difference: \", diff_formatted)) |&gt; \n  st_transform(crs = 4326) |&gt; \n  leaflet() |&gt; \n  setView(lat = 40.441606, lng = -80.010957, zoom = 10) |&gt; \n  addProviderTiles(providers$Stamen.TonerLite,\n                   options = providerTileOptions(noWrap = TRUE,\n                                                 minZoom = 9),\n                   group = \"Base map\") |&gt; \n  addPolygons(popup = ~ diff_label,\n              fillColor = ~pal(diff),\n              fillOpacity = .7,\n              color = \"black\",\n              weight = 1,\n              group = \"Housing\") |&gt; \n  addLegend(\"bottomright\", pal = pal, values = ~diff,\n            title = \"Difference\",\n            opacity = 1) |&gt; \n  addLayersControl(overlayGroups = c(\"Base map\", \"Housing\"),\n                   options = layersControlOptions(collapsed = FALSE)) |&gt; \n  addFullscreenControl()\n\nleaflet_map\n\n\n\n\n#frameWidget(leaflet_map, options=frameOptions(allowfullscreen = TRUE))\n\nThe North Side and the Hill were targets of “urban renewal” in the middle of the century. Dense housing in heavily African-American communities were demolished to make way for an opera house, the 279 and 579 highways, and parking lots. The highways are directly related to the white flight exodus to the suburbs, especially in the west and north. Those highways made it easy for the new suburbanites to commute longer distances in single passenger vehicles.\nThese graphs shows that the areas with the most housing in 1940 lost thousands of units, while outlying areas gained thousands of units.\n\nslope_graph_anim &lt;- hu_diff |&gt; \n  as_tibble() |&gt; \n  select(-geometry) |&gt;\n  arrange(desc(units_1940)) |&gt; \n  pivot_longer(cols = c(units_1940, units_2019), names_to = \"year\", values_to = \"housing_units\") |&gt; \n  mutate(year = str_remove(year, \"^units_\")) |&gt; \n  mutate(order = row_number()) |&gt; \n  ggplot(aes(year, housing_units)) +\n  geom_line(aes(group = GEOID10), alpha = .1) +\n  geom_point(aes(group = str_c(year, GEOID10)), alpha = .05) +\n  scale_y_comma() +\n  transition_reveal(order) +\n  labs(title = \"Housing unit change from 1940-2019\",\n       subtitle = \"From areas with the most units in 1940 to the least\",\n       x = \"Year\",\n       y = \"Housing units\") +\n  theme(panel.grid.minor.y = element_blank(),\n        panel.grid.major.y = element_blank(),\n        panel.grid.major.x = element_blank(),\n        panel.border = element_blank(),\n        axis.title.x = element_blank())\n\nslope_graph_anim &lt;- animate(slope_graph_anim, duration = 10, fps = 40, end_pause = 60)\n\nslope_graph_anim\n\n\n\n\n\n\n\n\n\nhu_diff |&gt; \n  ggplot(aes(units_1940, units_2019)) +\n  geom_abline(lty = 2) +\n  geom_point(alpha = .2) +\n  annotate(\"text\", x = 3500, y = 3800, label = \"No change\", angle = 45) +\n  annotate(\"text\", x = 300, y = 4500, label = \"Gain\") +\n  annotate(\"text\", x = 4300, y = 100, label = \"Loss\") +\n  tune::coord_obs_pred() +\n  scale_x_comma() +\n  scale_y_comma() +\n  labs(title = \"Change in housing units\",\n       x = \"Units in 1940\",\n       y = \"Units in 2019\")\n\n\n\n\n\n\n\n\n\nMoving north and west\nThese maps show the estimates of housing units for each decennial period. Outlying areas in the north and west, directly served by the new highway system, gained thousands of housing units.\n\nac_housing_hu |&gt; \n  ggplot() +\n  geom_sf(aes(fill = housing_units), color = NA) +\n  scale_fill_viridis_c(\"Housing units\", labels = comma) +\n  facet_wrap(~year) +\n  theme_bw() +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        axis.text.x = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks = element_blank())\n\n\n\n\n\n\n\n\nGeographically larger Census tracts gained more of the % of total housing over time.\n\nac_sqmi &lt;- ac_housing |&gt; \n  select(GEOID10, starts_with(\"sqmi\")) |&gt; \n  st_drop_geometry() |&gt; \n  as_tibble() |&gt; \n  pivot_longer(starts_with(\"sqmi\"), names_to = \"year\", values_to = \"sqmi\")\n\nac_sqmi_year &lt;- ac_sqmi |&gt; \n  distinct(year) |&gt; \n  mutate(year_fixed = c(1940, 1950, 1960, 1970, 1980, 1990, 2000, 2010, 2019))\n\nac_sqmi &lt;- ac_sqmi |&gt; \n  left_join(ac_sqmi_year) |&gt; \n  select(-year) |&gt; \n  rename(year = year_fixed)\n\nac_density &lt;- ac_housing_hu |&gt; \n  select(GEOID10, year, housing_units) |&gt; \n  left_join(ac_sqmi) |&gt; \n  mutate(density = housing_units / sqmi)\n\ncurve_anim &lt;- ac_density |&gt; \n  st_drop_geometry() |&gt; \n  select(GEOID10, year, housing_units, sqmi) |&gt; \n  mutate(year = as.character(year) |&gt; fct_inorder()) |&gt; \n  arrange(year, sqmi) |&gt; \n  group_by(year) |&gt; \n  mutate(housing_units_cumsum = cumsum(housing_units),\n         pct_units = housing_units_cumsum / sum(housing_units)) |&gt; \n  ungroup() |&gt; \n  ggplot(aes(sqmi, pct_units, color = year)) +\n  geom_line() +\n  scale_y_percent() +\n  labs(title = \"Housing moves to outlying areas over time\",\n       subtitle = \"Year: {closest_state}\",\n       x = \"Square miles\",\n       y = \"Cumulative percent of units\",\n       color = \"Year\") +\n  transition_states(year) +\n  shadow_mark()\n\ncurve_anim &lt;- animate(curve_anim, duration = 10, fps = 20)\n\ncurve_anim\n\n\n\n\n\n\n\n\n\n\nHousing peaks\nThis shows the year that each census tract peaked in terms of housing units. The areas that attracted heavy industry in the late 19th/early 20th century (and built housing nearby to support it) were crushed by the collapse of that industry. The single census tract that makes up “Downtown” has clawed back some housing recently.\n\nac_housing_hu |&gt; \n  group_by(GEOID10) |&gt; \n  filter(housing_units == max(housing_units)) |&gt; \n  ungroup() |&gt; \n  rename(max_year = year) |&gt; \n  ggplot() +\n  geom_sf(aes(fill = max_year), color = NA) +\n  scale_fill_viridis_c(direction = -1) +\n  labs(title = \"Year of peak housing\",\n       fill = \"Peak\") +\n  theme(panel.grid.major = element_blank(),\n        axis.text.x = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks = element_blank(),\n        panel.border = element_blank())"
  },
  {
    "objectID": "posts/suburbanization-of-allegheny-county/index.html#housing-moves-away-from-the-center",
    "href": "posts/suburbanization-of-allegheny-county/index.html#housing-moves-away-from-the-center",
    "title": "Suburbanization of Allegheny County",
    "section": "Housing moves away from the center",
    "text": "Housing moves away from the center\nA major trend from 1940-2019 is the significant shift in housing from around the core to outlying suburbs. This code calculates the distance between each tract and the “Downtown” tract (42003020100), and plots the number of units compared to that distance.\n\ndowntown_tract &lt;- ac_housing_hu |&gt; \n  filter(GEOID10 == \"42003020100\") |&gt; \n  distinct(GEOID10, geometry) |&gt; \n  mutate(centroid = st_point_on_surface(geometry)) |&gt; \n  st_set_geometry(\"centroid\") |&gt; \n  select(-geometry)\n\ndistance_anim &lt;- ac_housing_hu |&gt; \n  select(GEOID10, year, housing_units) |&gt; \n  mutate(centroid = st_point_on_surface(geometry),\n         geoid = str_c(GEOID10, year, sep = \"_\"),\n         year = as.integer(year)) |&gt; \n  mutate(distance_to_downtown = st_distance(centroid, downtown_tract) |&gt; as.numeric() / 5280) |&gt; \n  ggplot(aes(distance_to_downtown, housing_units)) +\n  geom_point(aes(group = GEOID10), alpha = .3) +\n  geom_smooth(aes(group = year)) +\n  scale_x_continuous() +\n  scale_y_comma() +\n  transition_states(year, \n                    state_length = 10) +\n  labs(title = \"Housing has moved farther away from downtown\",\n       subtitle = \"{closest_state}\",\n       x = \"Miles from downtown\",\n       y = \"Housing units\") +\n  theme(panel.grid.minor = element_blank())\n\ndistance_anim &lt;- animate(distance_anim)\n\ndistance_anim"
  },
  {
    "objectID": "posts/suburbanization-of-allegheny-county/index.html#land-use",
    "href": "posts/suburbanization-of-allegheny-county/index.html#land-use",
    "title": "Suburbanization of Allegheny County",
    "section": "Land use",
    "text": "Land use\nThe HHUUD10 data also contains estimates for the percentage of land in a tract that is “developed” for the years 1992, 2001, and 2011. “Developed” in this context means “covered by an urban land use”.\n\nac_dev &lt;- ac_housing |&gt; \n  select(GEOID10, starts_with(\"pdev\")) |&gt; \n  pivot_longer(cols = starts_with(\"pdev\"), names_to = \"year\", values_to = \"pct_dev\") \n\ndev_years &lt;- ac_dev |&gt; \n  st_drop_geometry() |&gt; \n  distinct(year) |&gt; \n  mutate(year_fixed = c(1992, 2001, 2011))\n\nac_dev &lt;- ac_dev |&gt; \n  left_join(dev_years) |&gt; \n  select(-year) |&gt; \n  rename(year = year_fixed)\n\nac_dev |&gt; \n  ggplot() +\n  geom_sf(aes(fill = pct_dev), color = NA) +\n  facet_wrap(~year) +\n  scale_fill_viridis_c(labels = percent) +\n  labs(title = \"Percent of land that is developed\",\n       fill = NULL) +\n  theme_bw() +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        axis.text.x = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks = element_blank())\n\n\n\n\n\n\n\n\nI find it interesting that more of the South Hills is developed than the North Hills. I would have expected more development in the North Hills due to the McKnight Road area and Wexford. My guess is that the tracts in the North Hills cover more land area, which decreases the % that is developed. Conversely, the tracts in the South Hills cover less land area, and less of the South Hills is useful for development because of steep hills and creeks. This concentrates development in a smaller area."
  },
  {
    "objectID": "posts/suburbanization-of-allegheny-county/index.html#conclusion",
    "href": "posts/suburbanization-of-allegheny-county/index.html#conclusion",
    "title": "Suburbanization of Allegheny County",
    "section": "Conclusion",
    "text": "Conclusion\nOver the past 80 years, Allegheny County has lost a significant amount of housing in its core urban area. Much of this is directly related to the collapse of the steel industry and “urban renewal”. At the same time, new housing development has been pushed out to the suburbs. This is a loss in terms of housing density, which has become a major discussion point in urban planning over the past 20 years.\nHigher density areas have a lower per capita carbon footprint due to non-car commute modes and agglomeration effects. Higher density also does not expand the wildland-urban interface. This leaves more land for the natural environment, moves humans away from dangers such as wildfires, and lowers the frequency of interaction between wild animals and humans, which can transfer disease (coronavirus, ebola). It will be interesting to see whether the suburbanization trend continues after the initial shocks of COVID-19 pandemic subside."
  },
  {
    "objectID": "posts/mayor-billpeduto-tweets/index.html",
    "href": "posts/mayor-billpeduto-tweets/index.html",
    "title": "Mayor Bill Peduto Tweets",
    "section": "",
    "text": "Pittsburgh Mayor Bill Peduto uses Twitter to communicate with his constituents, express his political opinions, and comment about Pittsburgh sports. I will use various R packages (mainly the Tidyverse, Tidytext, and rtweet) to analyze how the Mayor uses Twitter.\nBefore getting started, we need to load the packages required for this analysis:\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(ggraph)\nlibrary(igraph)\nlibrary(widyr)\nlibrary(SnowballC)\nlibrary(lubridate)\nlibrary(viridis)\nlibrary(scales)\nlibrary(knitr)\nlibrary(kableExtra)\nFirst, we will download @billpeduto’s recent tweets using rtweet\nrtweet::tweets_bill &lt;- get_timelines(\"BillPeduto\", n = 3200)\nI have already downloaded the data, so I will load it from my GitHub repo. I will also do some data munging to make the data easier to work with.\nurl &lt;- \"https://raw.githubusercontent.com/conorotompkins/pittsburgh_twitter/master/data/df_billpeduto.csv\"\n\nread_csv(url) %&gt;% \n  mutate(created_at = with_tz(created_at, \"US/Eastern\"),\n         date = ymd(str_sub(created_at, 1, 10)),\n         year = year(date),\n         month = month(date, label = TRUE),\n         week = week(date),\n         wday = wday(date, label = TRUE),\n         hour = hour(created_at),\n         month_year = str_c(month, year, sep = \"-\"),\n         month_year = factor(month_year, levels = unique(month_year)),\n         wday = factor(wday, levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"))) -&gt; df_bill\n\ndf_bill[1:5, 1:5] %&gt;% \n  kable(\"html\") %&gt;% \n  kable_styling()\n\n\n\n\nstatus_id\ncreated_at\nsource\nis_quote\nis_retweet\n\n\n\n\nx892785891055468548\n2017-08-02 12:35:18\nTwitter for iPhone\nFALSE\nTRUE\n\n\nx892804872390795264\n2017-08-02 13:50:43\nTwitter for iPhone\nFALSE\nTRUE\n\n\nx892820785823404032\n2017-08-02 14:53:57\nTwitter for iPhone\nFALSE\nTRUE\n\n\nx892926762933506048\n2017-08-02 21:55:04\nTwitter for iPhone\nFALSE\nTRUE\n\n\nx892953788679692288\n2017-08-02 23:42:28\nTwitter for iPhone\nFALSE\nFALSE"
  },
  {
    "objectID": "posts/mayor-billpeduto-tweets/index.html#exploratory-analysis",
    "href": "posts/mayor-billpeduto-tweets/index.html#exploratory-analysis",
    "title": "Mayor Bill Peduto Tweets",
    "section": "Exploratory Analysis",
    "text": "Exploratory Analysis\nThis plot shows that the Mayor tweets about 15 times a day, though there is considerable daily variation.\n\ndf_bill %&gt;% \n  count(date) %&gt;%\n  ggplot(aes(date, n)) +\n  geom_jitter(alpha = .5) +\n  geom_smooth(size = 2) +\n  scale_x_date(date_breaks = \"month\",\n               date_labels = \"%b-%Y\") +\n  labs(title = title, \n       x = \"\",\n       y = \"Number of tweets\",\n       caption = caption)\n\n\n\n\n\n\n\n\nThe Mayor tweets the most on weekdays; Tuesday has the highest median number of tweets. The weekends see fewer tweets, and Saturday and Sunday are very similar.\n\ndf_bill %&gt;% \n  count(wday, date) %&gt;% \n  group_by(wday) %&gt;% \n  mutate(median = median(n)) %&gt;% \n  ggplot(aes(wday, n, color = wday)) +\n  geom_jitter(alpha = .3, width = .2) +\n  geom_point(aes(wday, median), size = 5) +\n  scale_color_viridis(name = \"Day of the week\", discrete = TRUE) +\n  labs(title = title,\n       subtitle = \"One small dot = one day, large dot = median for weekday\",\n       x = \"\",\n       y = \"Number of tweets in a given day\",\n       caption = caption)\n\n\n\n\n\n\n\n\nThe Mayor does most of his tweeting around lunchtime and in the late evening.\n\ndf_bill %&gt;% \n  count(hour) %&gt;% \n  ggplot(aes(hour, y = 1, fill = n)) +\n  geom_tile() +\n  coord_equal() +\n  scale_y_continuous(expand = c(0,0)) +\n  scale_x_continuous(expand = c(0,0),\n                     breaks = seq(0, 23, by = 3)) +\n  scale_fill_viridis(name = \"Number of tweets\",\n                     option = 3) +\n  guides(fill = guide_colorbar(title.position = \"top\")) +\n  labs(title = title,\n       x = \"Hour\",\n       y = \"\",\n       caption = caption) +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        legend.position = \"bottom\",\n        legend.text = element_text(size = 7))\n\n\n\n\n\n\n\n\nThis view combines the day of the week and hour variables, and confirms what we observed when looking at the variables independently.\n\ndf_bill %&gt;% \n  mutate(wday = factor(wday, levels = rev(levels(df_bill$wday)))) %&gt;% \n  count(wday, hour) %&gt;% \n  complete(wday, hour = 0:23) %&gt;% \n  replace_na(list(n = 0)) %&gt;% \n  ggplot(aes(hour, wday, fill = n)) +\n  geom_tile() +\n  coord_equal() +\n  scale_x_continuous(expand = c(0,0),\n                  breaks = seq(0, 24, by = 3)) +\n  scale_y_discrete(expand = c(0,0)) +\n  scale_fill_viridis(option = 3) +\n  labs(title = title,\n       x = \"Hour\",\n       y = \"\",\n       caption = caption) +\n  guides(fill = guide_colorbar(\"Number of tweets\")) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nThis plot shows the percentage of the Mayor’s tweets that are regular tweets, retweets, quotes, and replies.\n\ndf_bill %&gt;% \n  select(date, month_year, month, week, is_retweet, is_quote, reply_to_screen_name) %&gt;% \n  mutate(tweet_type = case_when(is.na(reply_to_screen_name) == FALSE ~ \"Reply\",\n                                is_retweet == FALSE & is_quote == FALSE ~ \"Regular tweet\",\n                                is_retweet == TRUE ~ \"Retweet\",\n                                is_quote == TRUE ~ \"Quote\")) -&gt; df_bill_tweet_types\n\ndf_bill_tweet_types %&gt;% \n  count(month_year, tweet_type) %&gt;% \n  mutate(tweet_type = factor(tweet_type, levels = rev(c(\"Retweet\", \"Regular tweet\", \"Quote\", \"Reply\")))) -&gt; df_bill_tweet_types_month_year\n\ndf_bill_tweet_types_month_year %&gt;%  \n  ggplot(aes(month_year, n, fill = tweet_type, group = tweet_type)) +\n  geom_area(position = \"fill\") +\n  scale_fill_viridis(name = \"Tweet type\", discrete = TRUE) +\n  scale_x_discrete(expand = c(0,0)) +\n  scale_y_continuous(expand = c(0,0), labels = scales::percent) +\n  labs(title = \"Types of @billpeduto tweets\",\n       x = \"\",\n       y = \"Percentage of tweets\",\n       caption = caption) +\n  theme(axis.text.x = element_text(size = 10))\n\n\n\n\n\n\n\n\nThis heatmap shows that the Mayor replies to tweets most often in the late evening.\n\ndf_bill %&gt;% \n  filter(!is.na(reply_to_screen_name), is_quote == FALSE, is_retweet == FALSE) %&gt;% \n  count(wday, hour) %&gt;% \n  complete(wday, hour = 0:23) %&gt;% \n  replace_na(list(n = 0)) %&gt;% \n  ggplot(aes(hour, wday, fill = n)) +\n  geom_tile() +\n  coord_equal() +\n  scale_x_continuous(expand = c(0,0),\n                  breaks = seq(0, 24, by = 3)) +\n  scale_y_discrete(expand = c(0,0)) +\n  scale_fill_viridis(option = 3) +\n  labs(x = \"Hour\",\n       y = \"Day of the week\") +\n  guides(fill = guide_colorbar(\"Number of tweets\")) +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "posts/mayor-billpeduto-tweets/index.html#text-analysis",
    "href": "posts/mayor-billpeduto-tweets/index.html#text-analysis",
    "title": "Mayor Bill Peduto Tweets",
    "section": "Text Analysis",
    "text": "Text Analysis\nThe R package Tidytext makes it very easy to parse text data and pull insights from it. It interfaces very easily with ggraph, which creates network graphs.\n\nset.seed(1234)\n\ndf_bill &lt;- read_csv(\"https://raw.githubusercontent.com/conorotompkins/pittsburgh_twitter/master/data/bill_peduto_tweets.tweets.csv\")\n\nBy separating the Mayor’s tweets into bigrams (two-word chunks), we can identify which words are used together. This code cuts the tweets into bigrams and counts their occurences. I exclude retweets and quotes from this analysis.\n\ncount_twitter_bigrams &lt;- function(dataset, custom_stopwords) {\n  replace_reg &lt;- \"https://t.co/[A-Za-z\\\\d]+|http://[A-Za-z\\\\d]+|&amp;|&lt;|&gt;|RT|https|'s\"\n  \n  dataset %&gt;%\n    filter(is_quote == FALSE, is_retweet == FALSE) %&gt;% \n    mutate(text = str_replace_all(text, replace_reg, \"\")) %&gt;%\n    unnest_tokens(bigram, text, token = \"ngrams\", n = 2) %&gt;%\n    separate(bigram, c(\"word1\", \"word2\"), sep = \" \") %&gt;%\n    filter(!word1 %in% stop_words$word,\n           !word2 %in% stop_words$word,\n           !word1 %in% custom_stopwords,\n           !word2 %in% custom_stopwords) %&gt;%\n    count(word1, word2, sort = TRUE)\n}\n\nbill_stop_words &lt;- c(\"t.co\", \"https\", \"amp\")\n\ntweets_bill &lt;- count_twitter_bigrams(df_bill, bill_stop_words)\n\ntweets_bill %&gt;% \n  rename(count = n) %&gt;% \n  head() %&gt;% \n  kable(\"html\") %&gt;% \n  kable_styling()\n\n\n\n\nword1\nword2\ncount\n\n\n\n\npublic\nsafety\n12\n\n\naffordable\nhousing\n10\n\n\namazon\nhq2\n8\n\n\nclimate\nchange\n6\n\n\ncommon\nsense\n6\n\n\nu.s\ncities\n6\n\n\n\n\n\n\n\nThis network graph shows which words the Mayor uses together frequently. “public” + “safety”, and “affordable” + “housing” are the main standouts. I think this plot pretty accurately captures the topics the Mayor tweets about.\n\nvisualize_bigrams &lt;- function(bigrams, minimum, text_size = 3, title = NULL, subtitle = NULL, caption = NULL) {\n  set.seed(2016)\n  a &lt;- grid::arrow(type = \"closed\", \n                   length = unit(.1, \"inches\"))\n  \n  bigrams %&gt;%\n    filter(n &gt;= minimum) %&gt;% \n    graph_from_data_frame() %&gt;%\n    ggraph(layout = \"fr\") +\n    geom_node_point(color = \"lightblue\", size = 3) +\n    geom_node_text(aes(label = name), size = text_size, vjust = 1, hjust = 1) +\n    geom_edge_link(aes(edge_alpha = n, edge_width = n), show.legend = TRUE, arrow = a, end_cap = circle(.25, 'inches')) +\n    scale_edge_width_continuous(\"Count\", range = c(.5, 1.5)) +\n    scale_edge_alpha_continuous(\"Count\", range = c(.3, .7)) +\n    labs(title = title,\n         subtitle = subtitle,\n         caption = caption) +\n    theme_void(base_size = 18)\n}\n\nvisualize_bigrams(tweets_bill, \n                  minimum = 4,\n                  text_size = 5,\n                  title = \"@billpeduto tweets\",\n                  subtitle = \"Bigram network\",\n                  caption = caption)\n\n\n\n\n\n\n\n\nThis code determines which words are correlated with each other:\n\nword_correlations &lt;- function(dataframe, minimum, custom_stopwords){\n  replace_reg &lt;- \"https://t.co/[A-Za-z\\\\d]+|http://[A-Za-z\\\\d]+|&amp;|&lt;|&gt;|RT|https\"\n  unnest_reg &lt;- \"([^A-Za-z_\\\\d#@']|'(?![A-Za-z_\\\\d#@]))\"\n  dataframe %&gt;% \n  filter(is_quote == FALSE, is_retweet == FALSE) %&gt;% \n  select(status_id, text) %&gt;% \n  mutate(section = row_number() %/% 10) %&gt;%\n  filter(section &gt; 0) %&gt;%\n  mutate(text = str_replace_all(text, replace_reg, \"\")) %&gt;%\n  unnest_tokens(word, text, token = \"regex\", pattern = unnest_reg) %&gt;%\n  filter(!word %in% stop_words$word,\n         !word %in% custom_stopwords,\n         str_detect(word, \"[a-z]\")) %&gt;% \n  mutate(word = str_replace(word, \"'\", \"\"),\n         word = str_replace(word, \"'\", \"\"),\n         word = SnowballC::wordStem(word)) %&gt;% \n  group_by(word) %&gt;% \n  filter(n() &gt;= minimum) %&gt;%\n  pairwise_cor(word, section, sort = TRUE)\n}\n\nbill_stopwords &lt;- c(\"t.co\", \"https\", \"amp\")\n\nbill_words &lt;- word_correlations(df_bill, minimum = 10, custom_stopwords = bill_stopwords)\n\nThis plot shows which words are correlated highly with each other. Some words are “stemmed” (shortened and/or modified), to capture very similar words. For example, “business” and “businesses” could be stemmed to “busine”.\n“amazon” and “hq2” are used together overwhelmingly. The “pwsa” + “water” + “decad” combination indicate the ongoing water quality problems, and the plans to reinvest in the infrastructure. This plot shows how some of the policy issues the Mayor focuses on are connected.\n\nvisualize_word_correlations &lt;- function(dataframe, title, subtitle, caption){\n  dataframe %&gt;% \n  filter(correlation &gt; .3) %&gt;%\n  graph_from_data_frame() %&gt;%\n  ggraph(layout = \"fr\") +\n  geom_edge_link(aes(edge_alpha = correlation, edge_width = correlation), show.legend = FALSE) +\n  geom_node_point(color = \"lightblue\", size = 5) +\n  geom_node_label(aes(label = name), size = 5, repel = TRUE) +\n  scale_edge_alpha_continuous(range = c(.1, .5)) +\n  theme_void(base_size = 18) +\n  labs(title = title,\n       subtitle = subtitle,\n       caption = caption)\n}\n\nvisualize_word_correlations(bill_words, \n                            title = \"@billpeduto tweets\",\n                            subtitle = \"Word correlation network\",\n                            caption = \"@conor_tompkins\")"
  },
  {
    "objectID": "posts/mayor-billpeduto-tweets/index.html#references",
    "href": "posts/mayor-billpeduto-tweets/index.html#references",
    "title": "Mayor Bill Peduto Tweets",
    "section": "References",
    "text": "References\n\nhttps://github.com/mkearney/rtweet\nhttp://rtweet.info/\nhttp://rtweet.info/articles/intro.html\nhttps://stackoverflow.com/questions/47681690/no-twitter-authorization-prompt-when-using-rtweet-package/48275078#48275078\nhttps://stackoverflow.com/questions/47910979/setting-up-rtweet-for-r-in-aws-ubuntu-server\nhttps://github.com/r-lib/httr/issues/156\nhttps://github.com/mkearney/rtweet/issues/75\nhttps://github.com/geoffjentry/twitteR/issues/65\nhttps://github.com/r-lib/httr/blob/master/demo/oauth1-twitter.r"
  },
  {
    "objectID": "posts/shiny-venn-diagram/index.html",
    "href": "posts/shiny-venn-diagram/index.html",
    "title": "Making a Venn diagram in Shiny",
    "section": "",
    "text": "Introduction\nThis blog post is about making Venn diagrams work in Shiny, and the issues I ran into with shiny::nearPoints(). I show how this impacted my initial approach, and discuss the underlying issue.\nTLDR; shiny::nearPoints() doesn’t work with dataframes containing list-columns the way I expected\n\n\nBackground\nI have been working on a Shiny app that I will use to plan birdwatching trips. It uses the {ebirdst} package to pull abundance data for hundreds of species of birds in 27x27km tiles in North America. A major feature of the app will be the ability to compare how similar two areas (tiles) are. This compares the abundance for a species in a given tile in a given month. I wanted to include a Venn diagram that shows which species are exclusive to each tile. The user can click on the Venn diagram to see the species associated with each segment of the Venn diagram.\nThis involves making a venn diagram in ggplot2 and extracting the segment that the user clicks on with nearPoints(). This was more challenging than I had anticipated.\n\n\nVenn diagram data\nnearPoints() requires:\n\ndf: a data frame with x and y coordinates it can interpret\ncoordinfo: the user click coordinates as captured from the ui\n\nI use the ggVennDiagram package to make the venn diagram plot. This package uses ggplot2, but does a lot of pre-processing of the data beforehand. This made it difficult to get access to the df for nearPoints().\nThis is an example of a ggVennDiagram plot. It takes a list object, turns that into a dataframe, and then uses sf to draw the circles.\n\nlibrary(tidyverse)\nlibrary(ggVennDiagram)\n\ngenes &lt;- paste(\"gene\",1:100,sep=\"\")\nset.seed(20210419)\nx &lt;- list(A=sample(genes,30),\n          B=sample(genes,50))\n\nggVennDiagram(x)\n\n\n\n\n\n\n\n\nLooking under the hood of ggVennDiagram() shows the pre-processing steps:\n\nvenn &lt;- Venn(x)\ndata &lt;- process_data(venn)\n\nVenn() creates an object with slots representing the two sets A and B\n\nVenn(x)\n\nAn object of class \"Venn\"\nSlot \"sets\":\n$A\n [1] \"gene27\" \"gene76\" \"gene57\" \"gene33\" \"gene78\" \"gene39\" \"gene63\" \"gene41\"\n [9] \"gene66\" \"gene17\" \"gene16\" \"gene69\" \"gene75\" \"gene9\"  \"gene68\" \"gene3\" \n[17] \"gene34\" \"gene54\" \"gene19\" \"gene83\" \"gene2\"  \"gene40\" \"gene87\" \"gene60\"\n[25] \"gene61\" \"gene24\" \"gene44\" \"gene93\" \"gene53\" \"gene7\" \n\n$B\n [1] \"gene84\"  \"gene36\"  \"gene37\"  \"gene47\"  \"gene91\"  \"gene46\"  \"gene92\" \n [8] \"gene33\"  \"gene67\"  \"gene73\"  \"gene25\"  \"gene5\"   \"gene63\"  \"gene2\"  \n[15] \"gene83\"  \"gene56\"  \"gene77\"  \"gene10\"  \"gene12\"  \"gene95\"  \"gene76\" \n[22] \"gene53\"  \"gene99\"  \"gene19\"  \"gene31\"  \"gene86\"  \"gene80\"  \"gene65\" \n[29] \"gene48\"  \"gene100\" \"gene89\"  \"gene58\"  \"gene35\"  \"gene30\"  \"gene21\" \n[36] \"gene44\"  \"gene72\"  \"gene18\"  \"gene45\"  \"gene42\"  \"gene1\"   \"gene27\" \n[43] \"gene90\"  \"gene14\"  \"gene43\"  \"gene26\"  \"gene96\"  \"gene17\"  \"gene16\" \n[50] \"gene29\" \n\n\nSlot \"names\":\n[1] \"A\" \"B\"\n\n\nprocess_data() turns those slots into dataframes with sf columns representing the segment polygons.\n\nvenn &lt;- Venn(x)\nprocess_data(venn)\n\nAn object of class \"VennPlotData\"\nSlot \"setEdge\":\nSimple feature collection with 2 features and 5 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 125 ymin: 250 xmax: 875 ymax: 750\nCRS:           NA\n# A tibble: 2 × 6\n  id                                        geometry component item  count name \n  &lt;chr&gt;                                 &lt;LINESTRING&gt; &lt;chr&gt;     &lt;nam&gt; &lt;int&gt; &lt;chr&gt;\n1 1     (500 716, 493.065 720.007, 485.954 723.777,… setEdge   &lt;chr&gt;    30 A    \n2 2     (500 284, 506.935 279.998, 514.046 276.243,… setEdge   &lt;chr&gt;    50 B    \n\nSlot \"setLabel\":\nSimple feature collection with 2 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 250 ymin: 780 xmax: 750 ymax: 780\nCRS:           NA\n# A tibble: 2 × 4\n  id     geometry component name \n  &lt;chr&gt;   &lt;POINT&gt; &lt;chr&gt;     &lt;chr&gt;\n1 1     (250 780) setLabel  A    \n2 2     (750 780) setLabel  B    \n\nSlot \"region\":\nSimple feature collection with 3 features and 5 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 125 ymin: 250 xmax: 875 ymax: 750\nCRS:           NA\n# A tibble: 3 × 6\n  id                                        geometry component item  count name \n  &lt;chr&gt;                                    &lt;POLYGON&gt; &lt;chr&gt;     &lt;lis&gt; &lt;int&gt; &lt;chr&gt;\n1 1     ((500 716, 492.317 711.329, 484.878 706.459… region    &lt;chr&gt;    19 A    \n2 2     ((500 284, 507.683 288.649, 515.122 293.497… region    &lt;chr&gt;    39 B    \n3 12    ((507.683 711.328, 515.122 706.458, 522.317… region    &lt;chr&gt;    11 A..B \n\n\nThe region slot is most important for my purposes. It contains the sf polygons for the segments and the distinct counts exclusive to each segment.\n\nprocess_data(venn) %&gt;% \n  .@region\n\nSimple feature collection with 3 features and 5 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 125 ymin: 250 xmax: 875 ymax: 750\nCRS:           NA\n# A tibble: 3 × 6\n  id                                        geometry component item  count name \n  &lt;chr&gt;                                    &lt;POLYGON&gt; &lt;chr&gt;     &lt;lis&gt; &lt;int&gt; &lt;chr&gt;\n1 1     ((500 716, 492.317 711.329, 484.878 706.459… region    &lt;chr&gt;    19 A    \n2 2     ((500 284, 507.683 288.649, 515.122 293.497… region    &lt;chr&gt;    39 B    \n3 12    ((507.683 711.328, 515.122 706.458, 522.317… region    &lt;chr&gt;    11 A..B \n\nprocess_data(venn) %&gt;% \n  .@region %&gt;% \n  ggplot(aes(fill = name)) +\n  geom_sf()\n\n\n\n\n\n\n\n\nI thought using nearPoints() would be pretty easy once I intercepted the region object from the preprocessing steps. I was wrong.\n\n\nShiny app error\nThis basic Shiny app will reproduce the error that nearPoints() generates:\n\nlibrary(shiny)\nlibrary(tidyverse)\nlibrary(ggVennDiagram)\nlibrary(sf)\n\n#ui\nui &lt;- fluidPage(\n  \n  titlePanel(\"Shiny Venn Diagram\"),\n  \n  mainPanel(\n    plotOutput(\"venn_diagram\", click = \"plot_click\"),\n    tableOutput(\"venn_table\")\n  )\n)\n\ngenes &lt;- paste(\"gene\",1:1000,sep=\"\")\nset.seed(20210419)\nx &lt;- list(A=sample(genes,300),\n          B=sample(genes,525))\n\nvenn &lt;- Venn(x)\nvenn_data &lt;- process_data(venn)@region %&gt;% \n  mutate(centroid = st_point_on_surface(geometry),\n         x = map_dbl(centroid, 1),\n         y = map_dbl(centroid, 2)) %&gt;% \n  select(x, y, name, geometry)\n\n#server\nserver &lt;- function(input, output){\n  \n  output$venn_diagram &lt;- renderPlot({\n    \n    venn_data %&gt;% \n      ggplot(aes(x, y, fill = name, label = name)) +\n      geom_sf() +\n      geom_label()\n    \n  })\n  \n  output$venn_table &lt;- renderTable({\n    \n    req(input$plot_click)\n    \n    nearPoints(venn_data, #this is the issue\n               input$plot_click,\n               threshold = 100)\n    \n  })\n  \n}\n\nThis is the error:\n\nWarning: Error in &lt;-: number of items to replace is not a multiple of replacement length\n104: print.xtable\n98: transform\n97: func\n95: f\n94: Reduce\n85: do\n84: hybrid_chain\n83: renderFunc\n82: output$venn_table\n1: shiny::runApp\n\n\n\nThe fix\nWrapping the venn_data object in st_drop_geometry() drops the sf list-column and turns it back into a regular dataframe.\n\nlibrary(shiny)\nlibrary(tidyverse)\nlibrary(ggVennDiagram)\nlibrary(sf)\n\n#ui\nui &lt;- fluidPage(\n  \n  titlePanel(\"Shiny Venn Diagram\"),\n  \n  mainPanel(\n    plotOutput(\"venn_diagram\", click = \"plot_click\"),\n    tableOutput(\"venn_table\")\n  )\n)\n\ngenes &lt;- paste(\"gene\",1:1000,sep=\"\")\nset.seed(20210419)\nx &lt;- list(A=sample(genes,300),\n          B=sample(genes,525))\n\nvenn &lt;- Venn(x)\nvenn_data &lt;- process_data(venn)@region %&gt;% \n  mutate(centroid = st_point_on_surface(geometry),\n         x = map_dbl(centroid, 1),\n         y = map_dbl(centroid, 2)) %&gt;% \n  select(x, y, name, geometry)\n\n#server\nserver &lt;- function(input, output){\n  \n  output$venn_diagram &lt;- renderPlot({\n    \n    venn_data %&gt;% \n      ggplot(aes(x, y, fill = name, label = name)) +\n      geom_sf() +\n      geom_label()\n    \n  })\n  \n  output$venn_table &lt;- renderTable({\n    \n    req(input$plot_click)\n    \n    nearPoints(st_drop_geometry(venn_data), #the fix\n               input$plot_click,\n               threshold = 100)\n    \n  })\n  \n}\n\n\n\nWorking Shiny App\nThis is a working example of a Venn diagram in Shiny. input$plot_click captures the coordinates of the click and nearPoints() returns a dataframe of the information about the segment the user clicked on. The ID of the segment is in the name column."
  },
  {
    "objectID": "posts/pittsburgh-parking-covid-change/index.html",
    "href": "posts/pittsburgh-parking-covid-change/index.html",
    "title": "Effect of COVID-19 on Pittsburgh parking transactions",
    "section": "",
    "text": "The COVID-19 pandemic’s affect on commerce and mobility habits is well documented. For example, Apple publishes Mobility Trends reports about utilization of various transportation modes.\nApple’s data shows that utilization of driving in Pittsburgh dropped significantly in late March, but has rebounded above pre-COVID-19 levels since then.\nThe WPDRC publishes parking meter transactions for 60 parking zones in Pittsburgh. In this post I will use the frequency of parking transactions over time as a proxy for commercial activity in the city. This information only represents commerce that people use vehicles to perform, so it does not include mass transit or drivers that use private parking areas or meters that are not captured in this dataset. I will be interested to see if the parking meter data matches Apple’s report about driving."
  },
  {
    "objectID": "posts/pittsburgh-parking-covid-change/index.html#top-level-analysis",
    "href": "posts/pittsburgh-parking-covid-change/index.html#top-level-analysis",
    "title": "Effect of COVID-19 on Pittsburgh parking transactions",
    "section": "Top-level analysis",
    "text": "Top-level analysis\n\nRead in data\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(vroom)\nlibrary(hrbrthemes)\nlibrary(scales)\nlibrary(plotly)\nlibrary(broom)\nlibrary(heatwaveR)\nlibrary(gt)\n\noptions(scipen = 999,\n        digits = 4)\n\ntheme_set(theme_ipsum())\n\nAs of September 28th there are ~5 million rows in the dataset. Each row consists of a 10-minute period in a given zone with the aggregated number of transactions and the amount paid.\n\ndata &lt;- vroom(\"post_data/1ad5394f-d158-46c1-9af7-90a9ef4e0ce1.csv\")\n\nglimpse(data)\n\nRows: 7,354,373\nColumns: 9\n$ `_id`               &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,…\n$ zone                &lt;chr&gt; \"421 - NorthSide\", \"403 - Uptown\", \"412 - East Lib…\n$ start               &lt;dttm&gt; 2018-01-01 00:20:00, 2018-01-01 01:10:00, 2018-01…\n$ end                 &lt;dttm&gt; 2018-01-01 00:30:00, 2018-01-01 01:20:00, 2018-01…\n$ utc_start           &lt;dttm&gt; 2018-01-01 05:20:00, 2018-01-01 06:10:00, 2018-01…\n$ meter_transactions  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ meter_payments      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ mobile_transactions &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 2, 5, 1, 1, 1, 3, 1, 1, 1,…\n$ mobile_payments     &lt;dbl&gt; 4.00, 3.00, 3.00, 4.00, 16.25, 4.00, 3.00, 1.00, 2…\n\n\nThere are 60 distinct parking zones in the dataset.\n\ndata %&gt;% \n  distinct(zone) %&gt;% \n  arrange(zone)\n\n# A tibble: 62 × 1\n   zone                       \n   &lt;chr&gt;                      \n 1 209 - Mon Wharf            \n 2 213 - Second Avenue Plaza  \n 3 301 - Sheridan Harvard Lot \n 4 302 - Sheridan Kirkwood Lot\n 5 304 - Tamello Beatty Lot   \n 6 307 - Eva Beatty Lot       \n 7 308 - Harvard Beatty Lot   \n 8 311 - Ansley Beatty Lot    \n 9 314 - Penn Circle NW Lot   \n10 321 - Beacon Bartlett Lot  \n# ℹ 52 more rows\n\n\nThis code chunk performs most of the aggregation and manipulation. It separates the start column into start_date and start_time, calculates the number of transactions per day, and creates some date columns that I use later.\n\ndf_ts &lt;- data %&gt;%\n  select(start, meter_transactions, mobile_transactions) %&gt;%\n  separate(start, into = c(\"start_date\", \"start_time\"), remove = TRUE, sep = \" \") %&gt;%\n  mutate(start_date = ymd(start_date)) %&gt;%\n  filter(start_date &lt;= \"2020-10-05\") |&gt; \n  group_by(start_date) %&gt;%\n  summarize(meter_transactions = sum(meter_transactions),\n            mobile_transactions = sum(mobile_transactions)) %&gt;%\n  ungroup() %&gt;%\n  rowwise() %&gt;%\n  mutate(total_parking_transactions = meter_transactions + mobile_transactions) %&gt;%\n  ungroup() %&gt;%\n  mutate(year = year(start_date),\n         day_of_year = yday(start_date),\n         week_of_year = week(start_date),\n         weekday = wday(start_date, label = TRUE)) %&gt;%\n  group_by(year, week_of_year) %&gt;%\n  mutate(first_date_of_week = min(start_date)) %&gt;% \n  ungroup() %&gt;% \n  select(start_date, year, week_of_year, day_of_year, weekday, everything())"
  },
  {
    "objectID": "posts/pittsburgh-parking-covid-change/index.html#overall-timeline",
    "href": "posts/pittsburgh-parking-covid-change/index.html#overall-timeline",
    "title": "Effect of COVID-19 on Pittsburgh parking transactions",
    "section": "Overall timeline",
    "text": "Overall timeline\nThis view of the daily transactions shows that parking transactions dropped off steeply in late March 2020.\n\ndf_ts %&gt;% \n  ggplot(aes(first_date_of_week, total_parking_transactions)) +\n  geom_point(alpha = .2, size = .5) +\n  labs(title = \"Daily parking transactions\",\n       subtitle = \"2014-2020\",\n       x = \"Year\",\n       y = \"Total parking transactions\") +\n  scale_y_comma() +\n  scale_x_date(date_labels = \"%Y\")\n\n\n\n\n\n\n\n\n\n2020 vs. previous years\nStarting in March, parking transactions in 2020 fell way below the historical norm. At the most extreme, weekly transactions fell below 10,000.\n\ncompare_2020_before &lt;- df_ts %&gt;% \n  select(year, week_of_year, total_parking_transactions) %&gt;% \n  group_by(year, week_of_year) %&gt;% \n  summarize(total_parking_transactions = sum(total_parking_transactions)) %&gt;% \n  group_by(week_of_year) %&gt;% \n  mutate(week_median_parking_events = median(total_parking_transactions)) %&gt;% \n  ungroup() %&gt;% \n  mutate(period = case_when(year == 2020 ~ \"2020\",\n                               year &lt; 2020 ~ \"Before times\"))\n\ncompare_2020_before %&gt;% \n  ggplot(aes(x = week_of_year, y = total_parking_transactions, color = period, group = year)) +\n  geom_hline(yintercept = 0) +\n  geom_line(data = compare_2020_before %&gt;% filter(period == \"Before times\"),\n            size = 1.5, alpha = .7) +\n  geom_line(data = compare_2020_before %&gt;% filter(period == \"2020\"),\n            size = 1.5) +\n  scale_x_continuous(breaks = seq(0, 54, by = 4)) +\n  scale_color_manual(values = c(\"red\", \"grey\")) +\n  scale_y_comma(breaks = seq(0, 200000, by = 20000)) +\n  labs(title = \"Weekly parking transactions\",\n       x = \"Week of year\",\n       y = \"Total parking events\",\n       color = \"Period\")\n\n\n\n\n\n\n\n\n\n\n2020 vs. historical average\nThis code calculates the % difference between the number of parking transactions in 2020 and the historical average for a given week.\n\ndata_historical &lt;- df_ts %&gt;% \n  filter(start_date &lt; \"2020-01-01\") %&gt;% \n  select(year, week_of_year, total_parking_transactions) %&gt;% \n  group_by(year, week_of_year) %&gt;% \n  summarize(total_parking_transactions = sum(total_parking_transactions)) %&gt;% \n  group_by(week_of_year) %&gt;% \n  summarize(median_historical_transactions = median(total_parking_transactions),\n            day_count = n()) %&gt;% \n  ungroup()\n\n\ndata_2020 &lt;- df_ts %&gt;% \n  select(start_date, first_date_of_week, week_of_year, total_parking_transactions) %&gt;% \n  filter(start_date &gt;= \"2020-01-01\") %&gt;% \n  group_by(first_date_of_week, week_of_year) %&gt;% \n  summarize(total_parking_transactions = sum(total_parking_transactions)) %&gt;% \n  ungroup()\n\n\ndf &lt;- data_2020 %&gt;% \n  left_join(data_historical)\n\n\nsmoothed_line_df &lt;- df %&gt;% \n  mutate(pct_difference = (total_parking_transactions - median_historical_transactions) / median_historical_transactions) %&gt;% \n  select(week_of_year, first_date_of_week, pct_difference) %&gt;% \n  nest(parking_data = everything()) %&gt;% \n  mutate(model = map(parking_data, ~loess(pct_difference ~ week_of_year, data = .x, span = .3)),\n         coeff = map(model, augment))\n\nsmoothed_line_df &lt;- smoothed_line_df %&gt;% \n  unnest(parking_data) %&gt;% \n  left_join(unnest(smoothed_line_df, coeff)) %&gt;% \n  select(first_date_of_week, .fitted) %&gt;% \n  mutate(sign = .fitted &gt; 0,\n         population = \"total\")\n\nThis shows that after starting 2020 slightly above average, parking transactions fell to almost -100% in early April.\n\nsmoothed_line_df %&gt;% \n  ggplot(aes(x = first_date_of_week)) +\n  heatwaveR::geom_flame(aes(y = 0, y2 = .fitted)) +\n  geom_line(aes(y = .fitted), size = 1.5) +\n  geom_hline(yintercept = 0, lty = 2) +\n  scale_y_percent() +\n  labs(title = \"2020 vs. historical average\",\n       x = \"Date\",\n       y = \"Percent difference\")\n\n\n\n\n\n\n\n\nWhile the number of transactions recovered from the depths of March and April, it has not matched the increase that Apple’s mobility report showed for driving after May. Parking transactions are still 50% below their historical average.\n\n\nWeekday vs weekend difference, 2020 vs. historical\nThe difference between the number of parking transactions on weekdays vs. weekends did not change significantly after March 2020.\n\nweekday_weekend_df &lt;- df_ts %&gt;% \n  select(start_date, week_of_year, weekday, total_parking_transactions) %&gt;% \n  mutate(period = case_when(start_date &gt;= \"2020-01-01\" ~ \"2020\",\n                            start_date &lt; \"2020-01-01\" ~ \"Before times\"),\n         is_weekend = case_when(weekday %in% c(\"Sat\", \"Sun\") ~ \"weekend\",\n                                !(weekday %in% c(\"Sat\", \"Sun\")) ~ \"weekday\")) %&gt;% \n  mutate(period = fct_relevel(period, \"Before times\"),\n         is_weekend = fct_relevel(is_weekend, \"weekday\")) %&gt;% \n  group_by(period, is_weekend) %&gt;% \n  summarize(total_parking_transactions = sum(total_parking_transactions)) %&gt;% \n  mutate(pct_of_parking_transactions = total_parking_transactions / sum(total_parking_transactions))\n\nweekday_weekend_df %&gt;% \n  ggplot(aes(x = is_weekend, y =  pct_of_parking_transactions, fill = period)) +\n  geom_col(position = position_dodge(width = 1), color = \"black\", alpha = .8) +\n  scale_y_percent() +\n  scale_fill_viridis_d() +\n  labs(title = \"Weekday vs. weekend parking transactions\",\n       x = NULL,\n       y = \"Percent of transactions\",\n       fill = \"Period\")"
  },
  {
    "objectID": "posts/pittsburgh-parking-covid-change/index.html#neighborhood-level-analysis",
    "href": "posts/pittsburgh-parking-covid-change/index.html#neighborhood-level-analysis",
    "title": "Effect of COVID-19 on Pittsburgh parking transactions",
    "section": "Neighborhood-level analysis",
    "text": "Neighborhood-level analysis\nNext I perform the same analysis at the neighborhood level to see if any areas in the city were particularly affected. I manually aggregated the parking zones up to the neighborhood level. This code reads in that data.\n\ngeocoded_parking_locations &lt;- read_csv(\"post_data/geocoded_parking_locations.csv\")\n\ngeocoded_parking_locations %&gt;%\n  arrange(zone_region, zone)\n\n# A tibble: 62 × 3\n   zone                                 n zone_region\n   &lt;chr&gt;                            &lt;dbl&gt; &lt;chr&gt;      \n 1 354 - Walter/Warrington Lot      11735 Allentown  \n 2 355 - Asteroid Warrington Lot    48240 Allentown  \n 3 417 - Allentown                  35577 Allentown  \n 4 363 - Beechview Lot              13890 Beechview  \n 5 418 - Beechview                  68416 Beechview  \n 6 334 - Taylor Street Lot          95186 Bloomfield \n 7 335 - Friendship Cedarville Lot 188572 Bloomfield \n 8 406 - Bloomfield (On-street)    220290 Bloomfield \n 9 361 - Brookline Lot               6473 Brookline  \n10 419 - Brookline                 173071 Brookline  \n# ℹ 52 more rows\n\n\nThis code does the same aggregation as before, but adds neighborhood in the group_by function.\n\ndf_ts_neighborhood &lt;- data %&gt;%\n  left_join(geocoded_parking_locations) %&gt;%\n  select(zone_region, start, meter_transactions, mobile_transactions) %&gt;%\n  separate(start, into = c(\"start_date\", \"start_time\"), remove = TRUE, sep = \" \") %&gt;%\n  mutate(start_date = ymd(start_date)) %&gt;%\n  group_by(zone_region, start_date) %&gt;%\n  summarize(meter_transactions = sum(meter_transactions),\n            mobile_transactions = sum(mobile_transactions)) %&gt;%\n  ungroup() %&gt;%\n  rowwise() %&gt;%\n  mutate(total_parking_events = meter_transactions + mobile_transactions) %&gt;%\n  ungroup() %&gt;%\n  mutate(year = year(start_date),\n         day_of_year = yday(start_date),\n         week_of_year = week(start_date),\n         weekday = wday(start_date, label = TRUE)) %&gt;%\n  group_by(year, week_of_year) %&gt;% \n  mutate(first_date_of_week = min(start_date)) %&gt;% \n  ungroup() %&gt;% \n  select(zone_region, start_date, day_of_year, week_of_year, weekday, everything())\n\nMost of the parking transactions occur in ~13 neighborhoods, so I will focus on those.\n\nzone_fct &lt;- df_ts_neighborhood %&gt;% \n  group_by(zone_region) %&gt;% \n  summarize(total_parking_events = sum(total_parking_events)) %&gt;% \n  arrange(total_parking_events) %&gt;% \n  pull(zone_region)\n\ndf_ts_neighborhood %&gt;% \n  group_by(zone_region) %&gt;% \n  summarize(total_parking_events = sum(total_parking_events)) %&gt;% \n  mutate(zone_region = factor(zone_region, levels = zone_fct)) %&gt;% \n  ggplot(aes(total_parking_events, zone_region)) +\n  geom_col() +\n  scale_x_comma() +\n  labs(x = \"Total parking transactions\",\n       y = \"Neighborhood\")\n\n\n\n\n\n\n\n\n\ntop_zone_regions &lt;- df_ts_neighborhood %&gt;% \n  group_by(zone_region) %&gt;% \n  summarize(total_parking_events = sum(total_parking_events)) %&gt;% \n  arrange(desc(total_parking_events)) %&gt;% \n  select(zone_region) %&gt;% \n  slice(1:13)\n\n\n2020 vs. historical average\nThis code calculates the weekly % difference in parking transactions between 2020 and the previous years, by neighborhood.\n\ndf_historical &lt;- df_ts_neighborhood %&gt;% \n  arrange(zone_region, start_date) %&gt;% \n  filter(start_date &lt; \"2020-01-01\") %&gt;% \n  group_by(zone_region, year, week_of_year) %&gt;% \n  summarize(total_parking_events = sum(total_parking_events)) %&gt;% \n  ungroup()\n\ndf_historical &lt;- df_historical %&gt;% \n  group_by(zone_region, week_of_year) %&gt;% \n  summarize(median_parking_events_historical = median(total_parking_events)) %&gt;% \n  ungroup()\n\ndf_2020 &lt;- df_ts_neighborhood %&gt;% \n  filter(start_date &gt;= \"2020-01-01\", start_date &lt;= \"2020-10-05\") %&gt;% \n  complete(zone_region, week_of_year, fill = list(total_parking_events = 0)) %&gt;% \n  group_by(zone_region, week_of_year, first_date_of_week) %&gt;% \n  summarize(total_parking_events = sum(total_parking_events)) %&gt;% \n  ungroup()\n\ndf_combined &lt;- df_2020 %&gt;% \n  left_join(df_historical, by = c(\"zone_region\", \"week_of_year\")) %&gt;%\n  mutate(pct_difference = (total_parking_events - median_parking_events_historical) / median_parking_events_historical)\n\nThis shows that all the neighborhoods experienced severe drops in parking transactions. Only the North Shore returned to regular levels, and even then only temporarily.\n\nline_chart &lt;- df_combined %&gt;%\n  semi_join(top_zone_regions) %&gt;% \n  rename(neighborhood = zone_region) %&gt;% \n  mutate(pct_difference = round(pct_difference, 2)) %&gt;% \n  ggplot(aes(first_date_of_week, pct_difference, group = neighborhood)) +\n  geom_hline(yintercept = 0, lty = 2, alpha = .5) +\n  geom_line(alpha = .3) +\n  scale_y_percent() +\n  labs(title = \"2020 vs. historical average in top neighborhoods\",\n       x = \"Date\",\n       y = \"Percent difference\")\n\nline_chart %&gt;% \n  ggplotly(tooltip = c(\"neighborhood\", \"pct_difference\"))\n\n\n\n\n\nThis tile chart shows a similar pattern.\n\ntile_chart &lt;- df_combined %&gt;% \n  semi_join(top_zone_regions) %&gt;% \n  mutate(zone_region = factor(zone_region, levels = zone_fct),\n         ) %&gt;% \n  mutate(pct_difference = pct_difference %&gt;% round(2),\n         pct_difference_tooltip = pct_difference %&gt;% round(2) %&gt;% percent(accuracy = 1)) %&gt;% \n  ggplot(aes(week_of_year, zone_region, fill = pct_difference)) +\n  geom_tile() +\n  scale_fill_viridis_c(labels = percent) +\n  scale_x_continuous(expand = c(0,0)) +\n  scale_y_discrete(expand = c(0,0)) +\n  labs(title = \"2020 vs. historical average in top neighborhoods\",\n       x = \"Week of year\",\n       y = NULL,\n       fill = \"Percent difference\") +\n  theme(panel.grid = element_blank(),\n        legend.position = \"bottom\")\n\nggplotly(tile_chart, tooltip = c(\"zone_region\", \"week_of_year\", \"pct_difference\")) %&gt;% \n  layout(xaxis = list(showgrid = F),\n         yaxis = list(showgrid = F))\n\n\n\n\n\nAggregating the neighborhoods into boxplots shows that the drop in transactions mirrors the overall trend.\n\ndf_combined %&gt;% \n  semi_join(top_zone_regions) %&gt;% \n  ggplot(aes(first_date_of_week, pct_difference, group = week_of_year)) +\n  geom_boxplot(outlier.alpha = .3, outlier.size = 1) +\n  geom_hline(yintercept = 0, lty = 2, alpha = .5) +\n  scale_y_percent() +\n  labs(title = \"2020 vs. historical average\",\n       subtitle = \"Top 13 neighborhoods\",\n       x = \"Date\",\n       y = \"Percent difference\")"
  },
  {
    "objectID": "posts/pittsburgh-parking-covid-change/index.html#week-to-week-difference",
    "href": "posts/pittsburgh-parking-covid-change/index.html#week-to-week-difference",
    "title": "Effect of COVID-19 on Pittsburgh parking transactions",
    "section": "2020 week-to-week difference",
    "text": "2020 week-to-week difference\nIn terms of week-to-week difference in parking transactions, the week starting March 18th was the worst, with a -84% drop from the week before.\n\nweekly_pct_difference_df &lt;- data_2020 %&gt;% \n  mutate(weekly_difference = total_parking_transactions - lag(total_parking_transactions),\n         weekly_pct_difference = weekly_difference / lag(total_parking_transactions))\n\nweekly_pct_difference_df %&gt;% \n  mutate(max_drop_flag = weekly_pct_difference == min(weekly_pct_difference, na.rm = TRUE),\n         max_drop = case_when(max_drop_flag == TRUE ~ weekly_pct_difference,\n                              max_drop_flag == FALSE ~ NA_real_)) %&gt;% \n  ggplot(aes(first_date_of_week, weekly_pct_difference)) +\n  geom_line() +\n  geom_point() +\n  geom_point(aes(y = max_drop), color = \"red\", size = 3) +\n  ggrepel::geom_label_repel(aes(y = max_drop, label = scales::percent(max_drop)),\n                            direction = \"x\") +\n  scale_y_percent() +\n  coord_cartesian(ylim = c(-1, 1)) +\n  labs(title = \"Week-to-week difference\",\n       x = \"Date\",\n       y = \"Percent difference\")"
  },
  {
    "objectID": "posts/cumulative-ebird-sightings-in-allegheny-county/index.html",
    "href": "posts/cumulative-ebird-sightings-in-allegheny-county/index.html",
    "title": "Cumulative eBird Sightings in Allegheny County",
    "section": "",
    "text": "This will be a quick post on cumulative bird observations in Allegheny County. Cumulative graphs show overall trends, seasonality, and quirks in how the data was recorded. They are also fun to turn into animated gifs with gganimate.\nLoad the relevant libraries:\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(tidyquant)\nlibrary(janitor)\nlibrary(hrbrthemes)\nlibrary(vroom)\nlibrary(ggrepel)\nlibrary(gganimate)\n\nset.seed(1234)\n\ntheme_set(theme_bw(base_size = 16))\n\nThis reads in data from the eBird data portal:\n\ndf &lt;- vroom(\"post_data/ebd_US-PA-003_201001_202003_relFeb-2020.zip\", delim = \"\\t\") %&gt;% \n  clean_names() %&gt;% \n  mutate_at(vars(observer_id, locality, observation_date, time_observations_started, protocol_type), str_replace_na, \"NA\") %&gt;% \n  mutate(observation_count = as.numeric(str_replace(observation_count, \"X\", as.character(NA))),\n         observation_event_id = str_c(observer_id, locality, observation_date, time_observations_started, sep = \"-\"),\n         observation_date = ymd(observation_date)) %&gt;%\n  filter(all_species_reported == 1)\n\nglimpse(df)\n\nRows: 908,622\nColumns: 48\n$ global_unique_identifier     &lt;chr&gt; \"URN:CornellLabOfOrnithology:EBIRD:OBS815…\n$ last_edited_date             &lt;dttm&gt; 2018-08-03 11:44:16, 2018-08-03 11:44:34…\n$ taxonomic_order              &lt;dbl&gt; 493, 20638, 20638, 20638, 20638, 20638, 2…\n$ category                     &lt;chr&gt; \"species\", \"species\", \"species\", \"species…\n$ common_name                  &lt;chr&gt; \"American Black Duck\", \"American Crow\", \"…\n$ scientific_name              &lt;chr&gt; \"Anas rubripes\", \"Corvus brachyrhynchos\",…\n$ subspecies_common_name       &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ subspecies_scientific_name   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ observation_count            &lt;dbl&gt; 1, 7, 3, 2, 4, 3, NA, NA, 25, 2, 7, 2, 3,…\n$ breeding_bird_atlas_code     &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ breeding_bird_atlas_category &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ age_sex                      &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ country                      &lt;chr&gt; \"United States\", \"United States\", \"United…\n$ country_code                 &lt;chr&gt; \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\",…\n$ state                        &lt;chr&gt; \"Pennsylvania\", \"Pennsylvania\", \"Pennsylv…\n$ state_code                   &lt;chr&gt; \"US-PA\", \"US-PA\", \"US-PA\", \"US-PA\", \"US-P…\n$ county                       &lt;chr&gt; \"Allegheny\", \"Allegheny\", \"Allegheny\", \"A…\n$ county_code                  &lt;chr&gt; \"US-PA-003\", \"US-PA-003\", \"US-PA-003\", \"U…\n$ iba_code                     &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ bcr_code                     &lt;dbl&gt; 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 2…\n$ usfws_code                   &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ atlas_block                  &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ locality                     &lt;chr&gt; \"Rachel Carson Riverfront Park\", \"Southsi…\n$ locality_id                  &lt;chr&gt; \"L2640334\", \"L841018\", \"L329082\", \"L69207…\n$ locality_type                &lt;chr&gt; \"H\", \"H\", \"H\", \"P\", \"P\", \"H\", \"H\", \"H\", \"…\n$ latitude                     &lt;dbl&gt; 40.53731, 40.43124, 40.54348, 40.65688, 4…\n$ longitude                    &lt;dbl&gt; -79.79531, -79.97032, -79.90623, -80.1138…\n$ observation_date             &lt;date&gt; 2010-01-14, 2010-01-31, 2010-01-23, 2010…\n$ time_observations_started    &lt;chr&gt; \"11:05:00\", \"16:45:00\", \"13:15:00\", \"14:4…\n$ observer_id                  &lt;chr&gt; \"obsr39944\", \"obsr197993\", \"obsr197993\", …\n$ sampling_event_identifier    &lt;chr&gt; \"S5760087\", \"S5839167\", \"S5798726\", \"S580…\n$ protocol_type                &lt;chr&gt; \"Traveling\", \"Traveling\", \"Area\", \"Travel…\n$ protocol_code                &lt;chr&gt; \"P22\", \"P22\", \"P23\", \"P22\", \"P21\", \"P23\",…\n$ project_code                 &lt;chr&gt; \"EBIRD\", \"EBIRD\", \"EBIRD\", \"EBIRD\", \"EBIR…\n$ duration_minutes             &lt;dbl&gt; 25, 30, 90, 90, 120, 30, 35, 30, 70, 60, …\n$ effort_distance_km           &lt;dbl&gt; 0.483, 0.483, NA, 8.047, NA, NA, NA, NA, …\n$ effort_area_ha               &lt;dbl&gt; NA, NA, 24.2811, NA, NA, 4.0469, 4.0469, …\n$ number_observers             &lt;dbl&gt; 1, 2, 2, 1, NA, 2, 2, 2, 4, 2, 1, 1, 1, 4…\n$ all_species_reported         &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ group_identifier             &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ has_media                    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ approved                     &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ reviewed                     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ reason                       &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ trip_comments                &lt;chr&gt; NA, NA, NA, NA, NA, \"Temperature 8F Winds…\n$ species_comments             &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ x47                          &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ observation_event_id         &lt;chr&gt; \"obsr39944-Rachel Carson Riverfront Park-…\n\n\nI focus on the two main ways people use the eBird app: traveling and stationary. I also filter to only observations from 2016 onwards, since that is when eBird usage became stable in the county.\n\ndf_top_protocols &lt;- df %&gt;% \n  count(protocol_type, sort = TRUE) %&gt;% \n  slice(1:2)\n\ndf &lt;- df %&gt;% \n  semi_join(df_top_protocols) %&gt;% \n  filter(year(observation_date) &gt;= 2016)\n\nThis identifies the top 10 birds in terms of total observations:\n\ndf_species_count &lt;- df %&gt;% \n  group_by(common_name) %&gt;% \n  summarize(observation_count = sum(observation_count, na.rm = TRUE)) %&gt;% \n  arrange(desc(observation_count)) %&gt;% \n  slice(1:10)\n\nThis code filters on the top 10 birds and caculates the cumulative number of sightings and the rolling 21 day average of sightings.\n\ndf_cumulative &lt;- df %&gt;% \n  semi_join(df_species_count, by = c(\"common_name\")) %&gt;% \n  group_by(common_name, observation_date) %&gt;% \n  summarize(observation_count = sum(observation_count, na.rm = TRUE)) %&gt;% \n  ungroup() %&gt;% \n  arrange(common_name, observation_date) %&gt;% \n  group_by(common_name) %&gt;% \n  mutate(observation_count_cumulative = cumsum(observation_count)) %&gt;% \n  tq_mutate(\n    # tq_mutate args\n    select     = observation_count,\n    mutate_fun = rollapply, \n    # rollapply args\n    width      = 21,\n    align      = \"right\",\n    FUN        = mean,\n    # mean args\n    na.rm      = TRUE,\n    # tq_mutate args\n    col_rename = \"mean_21\"\n  )\n\nThis plots the cumulative observations by bird and creates an animation with gganimate:\n\nplot &lt;- df_cumulative %&gt;% \n  ggplot(aes(observation_date, observation_count_cumulative, group = common_name)) +\n  geom_line(alpha = .5) +\n  geom_segment(aes(xend = last(df_cumulative$observation_date) + 240, yend = observation_count_cumulative), linetype = 2, colour = 'grey') +\n  geom_point(aes(size = mean_21)) +\n  geom_label(aes(x = last(df_cumulative$observation_date) + 210, label = common_name), size = 6) +\n  scale_y_comma() +\n  scale_size_continuous(\"21 day rolling average of observation count\", range = c(2, 10), labels = scales::comma) +\n  scale_x_date(limits = c(first(df_cumulative$observation_date), last(df_cumulative$observation_date) + 250)) +\n  labs(x = NULL,\n       y = \"Cumulative observations\",\n       title = \"eBird observations in Allegheny County\",\n       subtitle = \"Top 10 birds 2016 through January 2020\",\n       caption = \"@conor_tompkins\") +\n  coord_cartesian(clip = 'off') +\n  transition_reveal(observation_date)\n\nplot"
  },
  {
    "objectID": "posts/forecasting-pittsburgh-potholes-with-fable/index.html",
    "href": "posts/forecasting-pittsburgh-potholes-with-fable/index.html",
    "title": "Forecasting Pittsburgh Potholes with {fable}",
    "section": "",
    "text": "Potholes are the bane of Pittsburgh drivers’ existence. You can either weave around the minefield of holes in the road (some of alarming size) or risk damage to your vehicle. Drastic swings in weather also exacerbate the natural freeze-thaw cycle. The winter of 2017/2018 was a particularly bad year for potholes in the region.\nIn this post I will use {fable} and related {tidyverts} packages to model the number of reports about potholes to Pittsburgh’s 311 service. The report data is available here."
  },
  {
    "objectID": "posts/forecasting-pittsburgh-potholes-with-fable/index.html#intro",
    "href": "posts/forecasting-pittsburgh-potholes-with-fable/index.html#intro",
    "title": "Forecasting Pittsburgh Potholes with {fable}",
    "section": "",
    "text": "Potholes are the bane of Pittsburgh drivers’ existence. You can either weave around the minefield of holes in the road (some of alarming size) or risk damage to your vehicle. Drastic swings in weather also exacerbate the natural freeze-thaw cycle. The winter of 2017/2018 was a particularly bad year for potholes in the region.\nIn this post I will use {fable} and related {tidyverts} packages to model the number of reports about potholes to Pittsburgh’s 311 service. The report data is available here."
  },
  {
    "objectID": "posts/forecasting-pittsburgh-potholes-with-fable/index.html#eda",
    "href": "posts/forecasting-pittsburgh-potholes-with-fable/index.html#eda",
    "title": "Forecasting Pittsburgh Potholes with {fable}",
    "section": "EDA",
    "text": "EDA\n\nPothole data from 311\nThis code loads the relevant packages:\n\nlibrary(fpp3)\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(future)\nlibrary(hrbrthemes)\n\ntheme_set(theme_ipsum())\n\nplan(multisession)\n\noptions(scipen = 999, digits = 4)\n\nThis code reads in CSV containing the 311 data and filters to only the pothole complaints.\n\n#read in pothole data\npothole_data &lt;- read_csv(\"post_data/wprdc_311.csv\") |&gt; \n  clean_names() |&gt; \n  filter(request_type == \"Potholes\") |&gt; \n  mutate(created_yearmonth = yearmonth(created_on))\n\nNext, summarize the data by year and month, and convert the data into a time series tsibble.\n\n#create basic tsibble\npothole_df &lt;- pothole_data |&gt; \n  group_by(created_yearmonth, request_type) |&gt; \n  summarize(report_count = n()) |&gt; \n  ungroup() |&gt;\n  as_tsibble()\n\npothole_df\n\n# A tsibble: 93 x 3 [1M]\n   created_yearmonth request_type report_count\n               &lt;mth&gt; &lt;chr&gt;               &lt;int&gt;\n 1          2015 Apr Potholes              906\n 2          2015 May Potholes             1493\n 3          2015 Jun Potholes             1236\n 4          2015 Jul Potholes             1288\n 5          2015 Aug Potholes              734\n 6          2015 Sep Potholes              526\n 7          2015 Oct Potholes              516\n 8          2015 Nov Potholes              890\n 9          2015 Dec Potholes              309\n10          2016 Jan Potholes              222\n# ℹ 83 more rows\n\n\n{tidyverts} provides some out-of-the-box functions to visualize the time series data. This is an important step to understand the dynamics of the data.\n\nautoplot(pothole_df)\n\n\n\n\n\n\n\n\n\ngg_season(pothole_df)\n\n\n\n\n\n\n\n\n\ngg_subseries(pothole_df) +\n  facet_wrap(vars(month(created_yearmonth)))\n\n\n\n\n\n\n\n\nDecomposing a time series into components (trend, seasonality, remainder) gives a more detailed view into how the series behaves.\n\ndcmp &lt;- pothole_df |&gt;\n  model(stl = STL(report_count, robust = TRUE))\n\ndcmp_components &lt;- components(dcmp)\n\ndcmp_components\n\n# A dable: 93 x 7 [1M]\n# Key:     .model [1]\n# :        report_count = trend + season_year + remainder\n   .model created_yearmonth report_count trend season_year remainder\n   &lt;chr&gt;              &lt;mth&gt;        &lt;int&gt; &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n 1 stl             2015 Apr          906 1179.        369.   -641.  \n 2 stl             2015 May         1493 1128.        353.     11.9 \n 3 stl             2015 Jun         1236 1077.        123.     36.7 \n 4 stl             2015 Jul         1288 1026.        117.    145.  \n 5 stl             2015 Aug          734  978.       -160.    -83.1 \n 6 stl             2015 Sep          526  929.       -353.    -50.7 \n 7 stl             2015 Oct          516  881.       -374.      8.42\n 8 stl             2015 Nov          890  836.       -480.    534.  \n 9 stl             2015 Dec          309  791.       -503.     21.5 \n10 stl             2016 Jan          222  746.       -402.   -122.  \n# ℹ 83 more rows\n# ℹ 1 more variable: season_adjust &lt;dbl&gt;\n\n\n\ndcmp_components |&gt; \n  autoplot()\n\n\n\n\n\n\n\n\nYou can use the remainders to look for outliers in the data.\n\noutliers &lt;- dcmp_components |&gt;\n  filter(\n    remainder &lt; quantile(remainder, 0.25) - 3*IQR(remainder) |\n    remainder &gt; quantile(remainder, 0.75) + 3*IQR(remainder)\n  )\n\noutliers |&gt; \n  select(created_yearmonth, remainder)\n\n# A tsibble: 10 x 2 [1M]\n   created_yearmonth remainder\n               &lt;mth&gt;     &lt;dbl&gt;\n 1          2016 Feb    -1011.\n 2          2017 Feb     -939.\n 3          2018 Jan     1572.\n 4          2018 Feb     2640.\n 5          2018 Apr     1641.\n 6          2018 May      982.\n 7          2018 Jun      726.\n 8          2020 Feb     -863.\n 9          2020 Apr     -772.\n10          2021 Feb    -1011.\n\n\nThe winter of 2017/2018 clearly had many outliers.\n\npothole_df |&gt;\n  ggplot(aes(created_yearmonth, report_count)) +\n  geom_line() +\n  geom_point(data = outliers, color = \"red\")"
  },
  {
    "objectID": "posts/forecasting-pittsburgh-potholes-with-fable/index.html#traintest-approach",
    "href": "posts/forecasting-pittsburgh-potholes-with-fable/index.html#traintest-approach",
    "title": "Forecasting Pittsburgh Potholes with {fable}",
    "section": "Train/test approach",
    "text": "Train/test approach\nThe classic method for determining the accuracy of any model is to train the model on a subset of the data and test the model against another subset. This code splits the time series into 80% training and 20% testing sets.\n\n#split into train/test and forecast\ndata_test &lt;- pothole_df |&gt; \n  slice_tail(prop = .2)\n\ndata_train &lt;- pothole_df |&gt; \n  anti_join(data_test, by = \"created_yearmonth\")\n\nI fit 3 models against the training set:\n\nARIMA\nExponential smoothing\nLinear model with seasonal effects\n\nI transform the data with log() and add 1 to the result to guarantee that the forecasts are positive. This is necessary because many of the observations are close to zero, and the models would not know otherwise that the count of pothole complaints cannot be negative. {fable} automatically back-transforms the forecast onto the original scale of the data.\n\nmodel_df &lt;- data_train |&gt; \n    model(arima = ARIMA(log(report_count + 1)),\n          ets = ETS(log(report_count + 1)),\n          lm_seasonal = TSLM(log(report_count + 1) ~ trend() + season()))\n\nThe forecast() function returns the full (transformed) distribution of the forecast and the mean of that distribution.\n\npothole_fc &lt;- model_df |&gt; \n  forecast(data_test)\n\npothole_fc\n\n# A fable: 54 x 5 [1M]\n# Key:     .model [3]\n   .model created_yearmonth    report_count .mean request_type\n   &lt;chr&gt;              &lt;mth&gt;          &lt;dist&gt; &lt;dbl&gt; &lt;chr&gt;       \n 1 arima           2021 Jul t(N(5.4, 0.25)) 237.  Potholes    \n 2 arima           2021 Aug t(N(5.1, 0.32)) 195.  Potholes    \n 3 arima           2021 Sep t(N(4.9, 0.34)) 151.  Potholes    \n 4 arima           2021 Oct t(N(4.9, 0.36)) 162.  Potholes    \n 5 arima           2021 Nov t(N(4.4, 0.38))  98.0 Potholes    \n 6 arima           2021 Dec t(N(4.2, 0.41))  77.0 Potholes    \n 7 arima           2022 Jan t(N(4.7, 0.43)) 129.  Potholes    \n 8 arima           2022 Feb t(N(5.6, 0.45)) 335.  Potholes    \n 9 arima           2022 Mar t(N(5.8, 0.47)) 423.  Potholes    \n10 arima           2022 Apr t(N(6.4, 0.49)) 764.  Potholes    \n# ℹ 44 more rows\n\n\n{fabletools} provides many measures of forecast accuracy. I focus on the following:\n\nCPRS (skill score): CPRS measures how well the forecast distribution fits the test data. The skill_score function compares this to the CPRS of a naive model. This results in a measure how much accuracy the model is adding over a naive model.\nRMSE: Root Mean Squared Error\n\n\nfc_acc &lt;- pothole_fc |&gt; \n  accuracy(pothole_df,\n           measures = list(point_accuracy_measures, distribution_accuracy_measures, skill_cprs = skill_score(CRPS))) |&gt; \n  select(.model, .type, skill_cprs, RMSE) |&gt; \n  arrange(desc(skill_cprs))\n\nfc_acc\n\n# A tibble: 3 × 4\n  .model      .type skill_cprs  RMSE\n  &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;\n1 lm_seasonal Test       0.616  183.\n2 ets         Test       0.585  200.\n3 arima       Test       0.486  246.\n\n\nThe lm_seasonal model provides the most accurate distribution and average forecast.\nThe autoplot function automatically extracts the 80% and 95% prediction intervals from the forecast distribution. You can see that the 80% interval of the lm_seasonal model fully contains the actual observations.\n\npothole_fc |&gt; \n  autoplot(pothole_df |&gt; \n             filter(year(created_yearmonth) &gt;= 2021)) +\n  facet_wrap(vars(.model), scales = \"free_y\", ncol = 1)\n\n\n\n\n\n\n\n\nThe report function provides the details of the specified model:\n\nmodel_df |&gt; \n  select(lm_seasonal) |&gt; \n  report()\n\nSeries: report_count \nModel: TSLM \nTransformation: log(report_count + 1) \n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1790 -0.4089 -0.0561  0.3659  1.5638 \n\nCoefficients:\n               Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)     6.71860    0.28196   23.83 &lt; 0.0000000000000002 ***\ntrend()        -0.01324    0.00328   -4.04              0.00015 ***\nseason()year2   0.55293    0.35301    1.57              0.12237    \nseason()year3   0.87219    0.35306    2.47              0.01626 *  \nseason()year4   0.62630    0.34030    1.84              0.07049 .  \nseason()year5   0.67473    0.34022    1.98              0.05178 .  \nseason()year6   0.47684    0.34017    1.40              0.16597    \nseason()year7   0.37048    0.35355    1.05              0.29876    \nseason()year8   0.05145    0.35338    0.15              0.88472    \nseason()year9  -0.31460    0.35324   -0.89              0.37659    \nseason()year10 -0.37161    0.35314   -1.05              0.29674    \nseason()year11 -0.54787    0.35306   -1.55              0.12581    \nseason()year12 -0.92564    0.35301   -2.62              0.01098 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.611 on 62 degrees of freedom\nMultiple R-squared: 0.54,   Adjusted R-squared: 0.451\nF-statistic: 6.08 on 12 and 62 DF, p-value: 0.00000068\n\n\nThis code refits the lm_seasonal model against the entire pothole_df dataset and produces a true forecast with a 12 month horizon. The distribution reflects the uncertainty from the variation in previous years. The model forecasts that the overall downward trend will continue.\n\nfinal_model &lt;- model_df |&gt; \n  select(lm_seasonal) |&gt; \n  refit(pothole_df, reestimate = TRUE)\n\nfinal_model |&gt; \n  forecast(h = 12) |&gt; \n  autoplot(pothole_df)"
  },
  {
    "objectID": "posts/forecasting-pittsburgh-potholes-with-fable/index.html#cross-validation-approach",
    "href": "posts/forecasting-pittsburgh-potholes-with-fable/index.html#cross-validation-approach",
    "title": "Forecasting Pittsburgh Potholes with {fable}",
    "section": "Cross-validation approach",
    "text": "Cross-validation approach\nCross-validation is the more robust way to measure the accuracy of a model. Instead of splitting the data into train/test sets, I create multiple subsets of the data with increasing origin points. This code creates the CV set by starting with the first 36 observations and adding 1 observation at a time to the rolling origin.\n\npothole_cv &lt;- stretch_tsibble(pothole_df, .step = 1, .init = 36) |&gt; \n  relocate(created_yearmonth, .id)\n\npothole_cv\n\n# A tsibble: 3,741 x 4 [1M]\n# Key:       .id [58]\n   created_yearmonth   .id request_type report_count\n               &lt;mth&gt; &lt;int&gt; &lt;chr&gt;               &lt;int&gt;\n 1          2015 Apr     1 Potholes              906\n 2          2015 May     1 Potholes             1493\n 3          2015 Jun     1 Potholes             1236\n 4          2015 Jul     1 Potholes             1288\n 5          2015 Aug     1 Potholes              734\n 6          2015 Sep     1 Potholes              526\n 7          2015 Oct     1 Potholes              516\n 8          2015 Nov     1 Potholes              890\n 9          2015 Dec     1 Potholes              309\n10          2016 Jan     1 Potholes              222\n# ℹ 3,731 more rows\n\n\nEach CV .id contains one more observation than the previous .id.\n\npothole_cv |&gt; \n  count(.id)\n\n# A tibble: 58 × 2\n     .id     n\n   &lt;int&gt; &lt;int&gt;\n 1     1    36\n 2     2    37\n 3     3    38\n 4     4    39\n 5     5    40\n 6     6    41\n 7     7    42\n 8     8    43\n 9     9    44\n10    10    45\n# ℹ 48 more rows\n\n\nThis code refits the models against the cross-validation set. The {fable} package automatically finds the appropriate model parameters for ARIMA and ETS models. Since each .id has a different subset of the data, the model parameters can be different for each .id.\n\nmodels_cv &lt;- pothole_cv |&gt; \n    model(arima = ARIMA(log(report_count + 1)),\n          ets = ETS(log(report_count + 1)),\n          lm_seasonal = TSLM(log(report_count + 1) ~ trend() + season()))\n\nmodels_cv\n\n# A mable: 58 x 4\n# Key:     .id [58]\n     .id                     arima          ets lm_seasonal\n   &lt;int&gt;                   &lt;model&gt;      &lt;model&gt;     &lt;model&gt;\n 1     1 &lt;ARIMA(0,0,1)(0,1,0)[12]&gt; &lt;ETS(A,N,N)&gt;      &lt;TSLM&gt;\n 2     2 &lt;ARIMA(0,1,1)(0,1,0)[12]&gt; &lt;ETS(A,N,N)&gt;      &lt;TSLM&gt;\n 3     3 &lt;ARIMA(0,1,1)(0,1,0)[12]&gt; &lt;ETS(A,N,N)&gt;      &lt;TSLM&gt;\n 4     4 &lt;ARIMA(0,1,1)(0,1,0)[12]&gt; &lt;ETS(A,N,N)&gt;      &lt;TSLM&gt;\n 5     5 &lt;ARIMA(0,1,1)(0,1,0)[12]&gt; &lt;ETS(A,N,N)&gt;      &lt;TSLM&gt;\n 6     6 &lt;ARIMA(0,1,1)(0,1,0)[12]&gt; &lt;ETS(A,N,N)&gt;      &lt;TSLM&gt;\n 7     7 &lt;ARIMA(0,1,1)(1,1,0)[12]&gt; &lt;ETS(A,N,N)&gt;      &lt;TSLM&gt;\n 8     8 &lt;ARIMA(0,1,1)(1,1,0)[12]&gt; &lt;ETS(A,N,N)&gt;      &lt;TSLM&gt;\n 9     9 &lt;ARIMA(0,1,1)(1,1,0)[12]&gt; &lt;ETS(A,N,N)&gt;      &lt;TSLM&gt;\n10    10 &lt;ARIMA(0,1,1)(0,1,0)[12]&gt; &lt;ETS(A,N,N)&gt;      &lt;TSLM&gt;\n# ℹ 48 more rows\n\n\nNext we forecast for each model and .id with a 12 month horizon.\n\nforecast_cv &lt;- models_cv |&gt; \n    forecast(h = 12)\n\nforecast_cv\n\n# A fable: 2,088 x 5 [1M]\n# Key:     .id, .model [174]\n     .id .model created_yearmonth    report_count .mean\n   &lt;int&gt; &lt;chr&gt;              &lt;mth&gt;          &lt;dist&gt; &lt;dbl&gt;\n 1     1 arima           2018 Apr  t(N(6.7, 0.4))  955.\n 2     1 arima           2018 May   t(N(7, 0.47)) 1347.\n 3     1 arima           2018 Jun t(N(6.8, 0.47)) 1089.\n 4     1 arima           2018 Jul t(N(6.7, 0.47))  991.\n 5     1 arima           2018 Aug t(N(6.4, 0.47))  767.\n 6     1 arima           2018 Sep t(N(5.9, 0.47))  457.\n 7     1 arima           2018 Oct t(N(5.6, 0.47))  344.\n 8     1 arima           2018 Nov t(N(5.3, 0.47))  256.\n 9     1 arima           2018 Dec t(N(4.9, 0.47))  162.\n10     1 arima           2019 Jan t(N(7.6, 0.47)) 2469.\n# ℹ 2,078 more rows\n\n\nYou can see that each .id gains one observation, and the model forecasts reflect that difference. This code graphs every 10th .id.\n\nforecast_cv |&gt; \n  filter(.id %in% seq(min(.id), max(.id), 10)) |&gt; \n  autoplot(pothole_cv) +\n  facet_wrap(vars(.id), ncol = 2, scales = \"free_y\")\n\n\n\n\n\n\n\n\nThe forecast accuracy for each model is averaged across all the .ids. This gives a more robust estimation of accuracy.\n\ncv_acc &lt;- forecast_cv |&gt; \n    accuracy(pothole_df, measures = list(point_accuracy_measures, distribution_accuracy_measures, skill_cprs = skill_score(CRPS))) |&gt; \n    select(.model, .type, skill_cprs, RMSE) |&gt; \n    arrange(desc(skill_cprs))\n\ncv_acc |&gt; \n  arrange(desc(skill_cprs))\n\n# A tibble: 3 × 4\n  .model      .type skill_cprs  RMSE\n  &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;\n1 lm_seasonal Test       0.716  592.\n2 ets         Test       0.622 1347.\n3 arima       Test       0.465 1958.\n\n\nOn average, the lm_seasonal model provides more accurate forecasts.\nThe basic models have higher CV accuracy than ARIMA, which probably shows that the more complicated ARIMA model over-fits the training data.\n\nfc_acc |&gt; \n  mutate(type = \"train_test\") |&gt; \n  bind_rows(cv_acc |&gt; \n              mutate(type = \"cv\")) |&gt; \n  select(.model, type, skill_cprs) |&gt; \n  pivot_wider(names_from = type, values_from = skill_cprs)\n\n# A tibble: 3 × 3\n  .model      train_test    cv\n  &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt;\n1 lm_seasonal      0.616 0.716\n2 ets              0.585 0.622\n3 arima            0.486 0.465\n\n\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: x86_64-apple-darwin20\nRunning under: macOS 15.1.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-x86_64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-x86_64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n [1] hrbrthemes_0.8.7  future_1.34.0     janitor_2.2.0     forcats_1.0.0    \n [5] stringr_1.5.1     purrr_1.0.2       readr_2.1.5       tidyverse_2.0.0  \n [9] fable_0.4.1       feasts_0.4.1      fabletools_0.5.0  tsibbledata_0.4.1\n[13] tsibble_1.1.5     ggplot2_3.5.1     lubridate_1.9.3   tidyr_1.3.1      \n[17] dplyr_1.1.4       tibble_3.2.1      fpp3_1.0.1       \n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.5            anytime_0.3.9           xfun_0.49              \n [4] htmlwidgets_1.6.4       numDeriv_2016.8-1.1     tzdb_0.4.0             \n [7] vctrs_0.6.5             tools_4.4.1             generics_0.1.3         \n[10] parallel_4.4.1          fansi_1.0.6             pkgconfig_2.0.3        \n[13] distributional_0.5.0    lifecycle_1.0.4         farver_2.1.2           \n[16] compiler_4.4.1          munsell_0.5.1           codetools_0.2-20       \n[19] snakecase_0.11.1        fontLiberation_0.1.0    fontquiver_0.2.1       \n[22] htmltools_0.5.8.1       yaml_2.3.10             Rttf2pt1_1.3.12        \n[25] pillar_1.9.0            crayon_1.5.3            extrafontdb_1.0        \n[28] ellipsis_0.3.2          fontBitstreamVera_0.1.1 parallelly_1.38.0      \n[31] tidyselect_1.2.1        digest_0.6.37           stringi_1.8.4          \n[34] listenv_0.9.1           labeling_0.4.3          extrafont_0.19         \n[37] fastmap_1.2.0           grid_4.4.1              colorspace_2.1-1       \n[40] cli_3.6.3               magrittr_2.0.3          utf8_1.2.4             \n[43] future.apply_1.11.2     withr_3.0.1             gdtools_0.4.0          \n[46] scales_1.3.0            rappdirs_0.3.3          bit64_4.0.5            \n[49] timechange_0.3.0        rmarkdown_2.28          globals_0.16.3         \n[52] bit_4.0.5               progressr_0.14.0        hms_1.1.3              \n[55] evaluate_0.24.0         knitr_1.48              ggdist_3.3.2           \n[58] rlang_1.1.4             Rcpp_1.0.13             glue_1.8.0             \n[61] renv_1.0.11             vroom_1.6.5             rstudioapi_0.16.0      \n[64] jsonlite_1.8.8          R6_2.5.1                systemfonts_1.1.0"
  },
  {
    "objectID": "posts/analyzing-commuter-patterns-in-allegheny-county/index.html",
    "href": "posts/analyzing-commuter-patterns-in-allegheny-county/index.html",
    "title": "Analyzing Commuter Patterns in Allegheny County",
    "section": "",
    "text": "Note: high-res images of the main graphs from this post are available here, here, here, and here."
  },
  {
    "objectID": "posts/analyzing-commuter-patterns-in-allegheny-county/index.html#eda",
    "href": "posts/analyzing-commuter-patterns-in-allegheny-county/index.html#eda",
    "title": "Analyzing Commuter Patterns in Allegheny County",
    "section": "EDA",
    "text": "EDA\nFirst, I do some exploratory analysis of the number of commuters in/out per tract. Since we cannot join the census geography dataframe to the LODES data directly, I reverse engineer the process by splitting the data into separate “home” and “work” dataframes, and then joining the census geography dataframe to those dataframes.\n\ndf_home &lt;- df_tracts_summarized %&gt;% \n  rename(tract = h_tract,\n         commuters_out = commuters) %&gt;% \n  select(-w_tract) %&gt;% \n  group_by(tract) %&gt;% \n  summarize(commuters_out = sum(commuters_out))\n\ndf_work &lt;- df_tracts_summarized %&gt;% \n  rename(tract = w_tract,\n         commuters_in = commuters) %&gt;% \n  select(-h_tract) %&gt;% \n  group_by(tract) %&gt;% \n  summarize(commuters_in = sum(commuters_in))\n\nThis checks that there are no duplicate tracts in either dataframe:\n\ndf_home %&gt;% \n  count(tract, sort = TRUE) %&gt;% \n  filter(n &gt; 1)\n\n# A tibble: 0 × 2\n# ℹ 2 variables: tract &lt;chr&gt;, n &lt;int&gt;\n\ndf_home %&gt;% \n  count(tract, sort = TRUE) %&gt;% \n  filter(n &gt; 1)\n\n# A tibble: 0 × 2\n# ℹ 2 variables: tract &lt;chr&gt;, n &lt;int&gt;\n\n\nThis joins the separate dataframes back to the census geography dataframe:\n\nallegheny_tracts &lt;- allegheny_tracts %&gt;% \n  left_join(df_home, by = c(\"GEOID\" = \"tract\")) %&gt;% \n  left_join(df_work, by = c(\"GEOID\" = \"tract\")) %&gt;% \n  replace_na(list(commuters_in = 0))\n\nThe tract with many more commuters “in” is downtown Pittsburgh.\n\nallegheny_tracts %&gt;% \n  ggplot(aes(commuters_out, commuters_in, label = NAME)) +\n  geom_point() +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nallegheny_tracts %&gt;% \n  ggplot() +\n  geom_sf(aes(fill = commuters_in), color = NA) +\n  scale_fill_viridis_c() +\n  theme_graph()\n\n\n\n\n\n\n\n\nThis shows which tracts have the most outflow of commuters:\n\nallegheny_tracts %&gt;% \n  ggplot() +\n  geom_sf(aes(fill = commuters_out), color = NA) +\n  scale_fill_viridis_c() +\n  theme_graph()\n\n\n\n\n\n\n\n\nThe outflow of commuters tracks with the population of the tract, with some exceptions.\n\nallegheny_tracts %&gt;% \n  ggplot(aes(commuters_out, total_pop)) +\n  geom_point() +\n  theme_bw()\n\n\n\n\n\n\n\n\nDowntown Pittsburgh dominates in terms of inflow vs. outflow differential.\n\nallegheny_tracts %&gt;% \n  mutate(diff = commuters_in - commuters_out) %&gt;% \n  ggplot() +\n  geom_sf(aes(fill = diff), color = NA) +\n  scale_fill_viridis_c(\"Commuters in minus commuters out\", direction = 1) +\n  theme_graph()"
  },
  {
    "objectID": "posts/analyzing-commuter-patterns-in-allegheny-county/index.html#set-up-main-graph",
    "href": "posts/analyzing-commuter-patterns-in-allegheny-county/index.html#set-up-main-graph",
    "title": "Analyzing Commuter Patterns in Allegheny County",
    "section": "Set up main graph",
    "text": "Set up main graph\nWith the exploratory analysis done, I move on to the main task of making a geographic network plot where the nodes are the center of each census tract.\nFirst, I set a minimum threshhold for the number of commuters flowing between two tracts. Then, I transform the df_tracts_summarized data into a tbl_graph object, which lets it be used for network analysis.\n\nminimum_commuters &lt;- 100\n\ng &lt;- df_tracts_summarized %&gt;% \n  as_tbl_graph(directed = TRUE)\n\ng\n\n# A tbl_graph: 402 nodes and 74239 edges\n#\n# A directed simple graph with 1 component\n#\n# Node Data: 402 × 1 (active)\n   name       \n   &lt;chr&gt;      \n 1 42003010300\n 2 42003020100\n 3 42003020300\n 4 42003030500\n 5 42003040200\n 6 42003040400\n 7 42003040500\n 8 42003040600\n 9 42003040900\n10 42003050100\n# ℹ 392 more rows\n#\n# Edge Data: 74,239 × 3\n   from    to commuters\n  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;\n1     1     2        73\n2     1     5        22\n3     1    64        15\n# ℹ 74,236 more rows\n\n\nThe result is a list of two dataframes; one for the nodes, one for the edges.\nThis filters out the edges that do not meet the minimum threshhold.\n\ng_main &lt;- g %&gt;% \n  activate(edges) %&gt;% \n  filter(commuters &gt; minimum_commuters)\n\ng_main\n\n# A tbl_graph: 402 nodes and 360 edges\n#\n# A directed simple graph with 136 components\n#\n# Edge Data: 360 × 3 (active)\n    from    to commuters\n   &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;\n 1     4     2       143\n 2    10     2       103\n 3    11     2       142\n 4    15     2       178\n 5    16     2       101\n 6    17     2       115\n 7    18     2       284\n 8    18   402       158\n 9    19     2       136\n10    19   402       117\n# ℹ 350 more rows\n#\n# Node Data: 402 × 1\n  name       \n  &lt;chr&gt;      \n1 42003010300\n2 42003020100\n3 42003020300\n# ℹ 399 more rows\n\n\nThis shows the edges with the most commuters:\n\ng_main %&gt;% \n  activate(edges) %&gt;% \n  arrange(desc(commuters))\n\n# A tbl_graph: 402 nodes and 360 edges\n#\n# A directed simple graph with 136 components\n#\n# Edge Data: 360 × 3 (active)\n    from    to commuters\n   &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;\n 1   122     2       552\n 2    75     2       484\n 3   125     2       440\n 4    65     2       407\n 5    79     2       393\n 6   127     2       389\n 7   253     2       385\n 8   249     2       366\n 9    62     2       361\n10   228     2       361\n# ℹ 350 more rows\n#\n# Node Data: 402 × 1\n  name       \n  &lt;chr&gt;      \n1 42003010300\n2 42003020100\n3 42003020300\n# ℹ 399 more rows\n\n\nThis code sets the nodes for the graph at the center of each census tract and creates a manual layout that the network plot will use.\n\nnode_pos &lt;- allegheny_tracts_centroids\n\nmanual_layout &lt;- create_layout(graph = g_main,\n                               layout = node_pos)\n\n\n\n# A tibble: 402 × 7\n   GEOID           x     y name        .ggraph.orig_index .ggraph.index circular\n   &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;                    &lt;int&gt;         &lt;int&gt; &lt;lgl&gt;   \n 1 42003010300 -80.0  40.4 42003010300                  1             1 FALSE   \n 2 42003020100 -80.0  40.4 42003020100                  2             2 FALSE   \n 3 42003020300 -80.0  40.5 42003020300                  3             3 FALSE   \n 4 42003030500 -80.0  40.4 42003030500                  4             4 FALSE   \n 5 42003040200 -80.0  40.4 42003040200                  5             5 FALSE   \n 6 42003040400 -79.9  40.4 42003040400                  6             6 FALSE   \n 7 42003040500 -80.0  40.4 42003040500                  7             7 FALSE   \n 8 42003040600 -80.0  40.4 42003040600                  8             8 FALSE   \n 9 42003040900 -80.0  40.4 42003040900                  9             9 FALSE   \n10 42003050100 -80.0  40.4 42003050100                 10            10 FALSE   \n# ℹ 392 more rows\n\n\nFinally, this sets the parameters for the plot and makes the network graph:\n\nlegend_title &lt;- str_c(\"Minimum: \", minimum_commuters, \" commuters\")\n\nggraph(manual_layout) +\n  geom_sf(data = allegheny_tracts, color = \"dark grey\", fill = NA) +\n  geom_sf(data = rivers, fill = \"white\", color = NA, alpha = .5) +\n  geom_node_point(alpha = 0) +\n  geom_edge_fan(aes(edge_width = commuters, \n                    edge_alpha = commuters),\n                arrow = arrow(length = unit(.5, 'lines')), \n                start_cap = circle(.1, 'lines'),\n                end_cap = circle(.2, 'lines'),\n                color = \"white\",\n                strength = .5) +\n  scale_edge_width_continuous(legend_title, range = c(.1, 1.5)) +\n  scale_edge_alpha_continuous(legend_title, range = c(.1, .8)) +\n  scale_fill_viridis_c() +\n  labs(x = NULL,\n       y = NULL,\n       title = \"Where do people commute from/to for work?\",\n       subtitle = \"Excludes within-tract commuters\",\n       caption = \"Based on 2015 US Census LODES dataset | @conor_tompkins\") +\n  theme_graph() +\n  theme(legend.background = element_rect(fill = \"light grey\"),\n        legend.text = element_text(color = \"black\"),\n        legend.title = element_text(color = \"black\"),\n        panel.background = element_rect(fill = \"black\"))\n\n\n\n\n\n\n\n\nClearly downtown Pittsburgh attracts the most commuters. This is not surprising, but it obscures other job centers that attract employees."
  },
  {
    "objectID": "posts/analyzing-commuter-patterns-in-allegheny-county/index.html#set-up-graph-without-downtown-tract",
    "href": "posts/analyzing-commuter-patterns-in-allegheny-county/index.html#set-up-graph-without-downtown-tract",
    "title": "Analyzing Commuter Patterns in Allegheny County",
    "section": "Set up graph without downtown tract",
    "text": "Set up graph without downtown tract\nThis code filters out commuters that work downtown. Again, set a threshhold and filter on it.\n\nminimum_commuters &lt;- 100\n\ng_filtered &lt;- g %&gt;% \n  activate(edges) %&gt;% \n  filter(commuters &gt; minimum_commuters)\n\ng_filtered\n\n# A tbl_graph: 402 nodes and 360 edges\n#\n# A directed simple graph with 136 components\n#\n# Edge Data: 360 × 3 (active)\n    from    to commuters\n   &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;\n 1     4     2       143\n 2    10     2       103\n 3    11     2       142\n 4    15     2       178\n 5    16     2       101\n 6    17     2       115\n 7    18     2       284\n 8    18   402       158\n 9    19     2       136\n10    19   402       117\n# ℹ 350 more rows\n#\n# Node Data: 402 × 1\n  name       \n  &lt;chr&gt;      \n1 42003010300\n2 42003020100\n3 42003020300\n# ℹ 399 more rows\n\n\nThis code goes into the network data and excludes edges that connect to the downtown tract.\n\nfilter_tract &lt;- \"42003020100\"\n\nselected_node &lt;- manual_layout %&gt;% \n  filter(name != filter_tract) %&gt;% \n  pull(.ggraph.orig_index)\n\ng_filtered &lt;- g_filtered %&gt;% \n  activate(edges) %&gt;% \n  filter(to %in% selected_node)\n\ng_filtered\n\n# A tbl_graph: 402 nodes and 103 edges\n#\n# A directed simple graph with 333 components\n#\n# Edge Data: 103 × 3 (active)\n    from    to commuters\n   &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;\n 1    18   402       158\n 2    19   402       117\n 3    20   402       101\n 4    21   402       287\n 5    21     5       132\n 6    24   402       107\n 7    32   402       131\n 8    36   402       222\n 9    37   402       159\n10    50   402       158\n# ℹ 93 more rows\n#\n# Node Data: 402 × 1\n  name       \n  &lt;chr&gt;      \n1 42003010300\n2 42003020100\n3 42003020300\n# ℹ 399 more rows\n\n\nThis creates another layout for this plot:\n\nmanual_layout_filtered &lt;- create_layout(graph = g_filtered,\n                                        layout = node_pos)\n\n\nlegend_title &lt;- str_c(\"Minimum: \", minimum_commuters, \" commuters\")\n\nggraph(manual_layout_filtered) +\n  geom_sf(data = allegheny_tracts, size = .1, fill = NA, color = \"dark grey\") +\n  geom_sf(data = rivers, fill = \"white\", color = NA, alpha = .5) +\n  geom_edge_fan(aes(edge_width = commuters, edge_alpha = commuters\n  ),\n  arrow = arrow(length = unit(.5, 'lines')),\n  start_cap = circle(.1, 'lines'),\n  end_cap = circle(.2, 'lines'),\n  color = \"white\",\n  strength = .5) +\n  scale_edge_width_continuous(legend_title, range = c(.1, 1.5)) +\n  scale_edge_alpha_continuous(legend_title, range = c(.1, .8)) +\n  scale_fill_viridis_c() +\n  labs(x = NULL,\n       y = NULL,\n       title = \"Where do people commute from/to for work?\",\n       subtitle = \"Excludes within-tract commuters and commuters to downtown tract\",\n       caption = \"Based on 2015 US Census LODES dataset | @conor_tompkins\") +\n  theme_graph() +\n  theme(legend.background = element_rect(fill = \"light grey\"),\n        legend.text = element_text(color = \"black\"),\n        legend.title = element_text(color = \"black\"),\n        panel.background = element_rect(fill = \"black\"))"
  },
  {
    "objectID": "posts/analyzing-commuter-patterns-in-allegheny-county/index.html#set-up-facted-graphs",
    "href": "posts/analyzing-commuter-patterns-in-allegheny-county/index.html#set-up-facted-graphs",
    "title": "Analyzing Commuter Patterns in Allegheny County",
    "section": "Set up facted graphs",
    "text": "Set up facted graphs\nThis code creates a faceted plot that focuses on the top 5 tracts in terms of commuter outflow:\n\nminimum_commuters &lt;- 5\n\ntop_work_tracts &lt;- df_home %&gt;% \n  arrange(desc(commuters_out)) %&gt;% \n  top_n(5, commuters_out) %&gt;% \n  select(tract)\n\ng_facet &lt;- g %&gt;% \n  activate(edges) %&gt;% \n  left_join(manual_layout %&gt;% select(.ggraph.index, name), by = c(\"from\" = \".ggraph.index\")) %&gt;% \n  semi_join(top_work_tracts, by = c(\"name\" = \"tract\")) %&gt;% \n  filter(commuters &gt; minimum_commuters)\n\ng_facet %&gt;% \n  arrange(desc(commuters)) %&gt;% \n  activate(edges)\n\n# A tbl_graph: 402 nodes and 477 edges\n#\n# A directed simple graph with 237 components\n#\n# Edge Data: 477 × 4 (active)\n    from    to commuters name       \n   &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;chr&gt;      \n 1   122     2       552 42003409000\n 2   125     2       440 42003412001\n 3   127     2       389 42003413100\n 4   192   202       311 42003453004\n 5   150     2       300 42003426300\n 6   192     2       294 42003453004\n 7   125   402       241 42003412001\n 8   192   190       209 42003453004\n 9   192   189       180 42003453004\n10   122   124       170 42003409000\n# ℹ 467 more rows\n#\n# Node Data: 402 × 1\n  name       \n  &lt;chr&gt;      \n1 42003010300\n2 42003020100\n3 42003020300\n# ℹ 399 more rows\n\n\n\nmanual_layout_faceted &lt;- create_layout(graph = g_facet,\n                                       layout = node_pos)\nmanual_layout_faceted %&gt;% \n  as_tibble()\n\n# A tibble: 402 × 7\n   GEOID           x     y name        .ggraph.orig_index .ggraph.index circular\n   &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;                    &lt;int&gt;         &lt;int&gt; &lt;lgl&gt;   \n 1 42003010300 -80.0  40.4 42003010300                  1             1 FALSE   \n 2 42003020100 -80.0  40.4 42003020100                  2             2 FALSE   \n 3 42003020300 -80.0  40.5 42003020300                  3             3 FALSE   \n 4 42003030500 -80.0  40.4 42003030500                  4             4 FALSE   \n 5 42003040200 -80.0  40.4 42003040200                  5             5 FALSE   \n 6 42003040400 -79.9  40.4 42003040400                  6             6 FALSE   \n 7 42003040500 -80.0  40.4 42003040500                  7             7 FALSE   \n 8 42003040600 -80.0  40.4 42003040600                  8             8 FALSE   \n 9 42003040900 -80.0  40.4 42003040900                  9             9 FALSE   \n10 42003050100 -80.0  40.4 42003050100                 10            10 FALSE   \n# ℹ 392 more rows\n\n\n\nlegend_title &lt;- str_c(\"Minimum: \", minimum_commuters, \" commuters\")\n\nggraph(manual_layout_faceted) +\n  geom_sf(data = allegheny_tracts, size = .1, fill = NA, color = \"dark grey\") +\n  geom_sf(data = rivers, fill = \"white\", color = NA, alpha = .5) +\n  geom_edge_fan(aes(edge_width = commuters, edge_alpha = commuters),\n                arrow = arrow(length = unit(.5, 'lines')),\n                start_cap = circle(.1, 'lines'),\n                end_cap = circle(.2, 'lines'),\n                color = \"red\",\n                strength = .5) +\n  facet_edges(~name) +\n  scale_edge_width_continuous(legend_title, range = c(.1, 1.5)) +\n  scale_edge_alpha_continuous(legend_title, range = c(.1, 1)) +\n  scale_fill_viridis_c() +\n  labs(x = NULL,\n       y = NULL,\n       title = \"Where do people commute from/to for work?\",\n       subtitle = \"Not including within-tract commuters\",\n       caption = \"Based on 2015 US Census LODES dataset | @conor_tompkins\") +\n  theme_graph() +\n  theme(legend.background = element_rect(fill = \"light grey\"),\n        legend.text = element_text(color = \"black\"),\n        legend.title = element_text(color = \"black\"),\n        panel.background = element_rect(fill = \"black\"))"
  },
  {
    "objectID": "posts/analyzing-commuter-patterns-in-allegheny-county/index.html#zoom-in-on-one-tract",
    "href": "posts/analyzing-commuter-patterns-in-allegheny-county/index.html#zoom-in-on-one-tract",
    "title": "Analyzing Commuter Patterns in Allegheny County",
    "section": "Zoom in on one tract",
    "text": "Zoom in on one tract\nThere also may be tracts of interest outside of the main commuter inflow/outflow tracts. This code creates a framework for filtering on commuters from a selected tract and zooming in on that tract.\nSet the threshhold and filter:\n\nminimum_commuters &lt;- 15\n\ng_filtered &lt;- g %&gt;% \n  activate(edges) %&gt;% \n  filter(commuters &gt; minimum_commuters)\n\ng_filtered\n\n# A tbl_graph: 402 nodes and 5799 edges\n#\n# A directed simple graph with 11 components\n#\n# Edge Data: 5,799 × 3 (active)\n    from    to commuters\n   &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;\n 1     1     2        73\n 2     1     5        22\n 3     2     5        87\n 4     2   402        75\n 5     2     1        37\n 6     2   202        35\n 7     2    50        34\n 8     2   163        33\n 9     2     9        32\n10     2   400        31\n# ℹ 5,789 more rows\n#\n# Node Data: 402 × 1\n  name       \n  &lt;chr&gt;      \n1 42003010300\n2 42003020100\n3 42003020300\n# ℹ 399 more rows\n\n\nThis code gets the commuter outflow data for one tract and creates allegheny_tracts_highlight, which will be used to highlight the tract of interest.\n\nfilter_tract &lt;- \"42003473500\"\n\nallegheny_tracts_highlight &lt;- allegheny_tracts %&gt;% \n  semi_join(df_tracts_summarized %&gt;% \n              filter(h_tract == filter_tract), by = c(\"GEOID\" = \"w_tract\")) %&gt;% \n  filter(commuters_in &gt; minimum_commuters)\n\nallegheny_tracts_highlight\n\nSimple feature collection with 208 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -80.36087 ymin: 40.20845 xmax: -79.70383 ymax: 40.67494\nGeodetic CRS:  NAD83\n# A tibble: 208 × 6\n   GEOID    NAME  total_pop                  geometry commuters_out commuters_in\n * &lt;chr&gt;    &lt;chr&gt;     &lt;dbl&gt;        &lt;MULTIPOLYGON [°]&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n 1 4200301… Cens…      6600 (((-79.98077 40.43836, -…           390         5470\n 2 4200302… Cens…      3629 (((-79.9908 40.44442, -7…          1029        58444\n 3 4200302… Cens…       616 (((-79.98062 40.45904, -…           415         7155\n 4 4200304… Cens…      2604 (((-79.96802 40.4429, -7…           428        12340\n 5 4200304… Cens…      2488 (((-79.94652 40.44508, -…           357          955\n 6 4200304… Cens…      3694 (((-79.95497 40.4371, -7…           290         1817\n 7 4200304… Cens…      2969 (((-79.94986 40.43443, -…           805         7822\n 8 4200305… Cens…      2057 (((-79.96206 40.44863, -…           800          235\n 9 4200307… Cens…      2197 (((-79.93137 40.45274, -…           769         1343\n10 4200307… Cens…      2997 (((-79.92533 40.45499, -…          1367         1277\n# ℹ 198 more rows\n\nselected_node &lt;- manual_layout %&gt;% \n  filter(name == filter_tract) %&gt;% \n  pull(.ggraph.orig_index)\n\ng_filtered &lt;- g_filtered %&gt;% \n  activate(edges) %&gt;% \n  filter(from == selected_node)\n\ng_filtered\n\n# A tbl_graph: 402 nodes and 17 edges\n#\n# A rooted forest with 385 trees\n#\n# Edge Data: 17 × 3 (active)\n    from    to commuters\n   &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;\n 1   233     2       286\n 2   233   402        67\n 3   233     5        46\n 4   233   202        41\n 5   233   229        40\n 6   233   217        33\n 7   233   189        28\n 8   233   190        26\n 9   233   218        24\n10   233     3        23\n11   233     1        22\n12   233   234        21\n13   233   249        21\n14   233   241        17\n15   233    50        16\n16   233   163        16\n17   233   375        16\n#\n# Node Data: 402 × 1\n  name       \n  &lt;chr&gt;      \n1 42003010300\n2 42003020100\n3 42003020300\n# ℹ 399 more rows\n\n\nSet the layout for the graph:\n\nmanual_layout_filtered &lt;- create_layout(graph = g_filtered,\n                                        layout = node_pos)\n\n\n\n# A tibble: 402 × 7\n   GEOID           x     y name        .ggraph.orig_index .ggraph.index circular\n   &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;                    &lt;int&gt;         &lt;int&gt; &lt;lgl&gt;   \n 1 42003010300 -80.0  40.4 42003010300                  1             1 FALSE   \n 2 42003020100 -80.0  40.4 42003020100                  2             2 FALSE   \n 3 42003020300 -80.0  40.5 42003020300                  3             3 FALSE   \n 4 42003030500 -80.0  40.4 42003030500                  4             4 FALSE   \n 5 42003040200 -80.0  40.4 42003040200                  5             5 FALSE   \n 6 42003040400 -79.9  40.4 42003040400                  6             6 FALSE   \n 7 42003040500 -80.0  40.4 42003040500                  7             7 FALSE   \n 8 42003040600 -80.0  40.4 42003040600                  8             8 FALSE   \n 9 42003040900 -80.0  40.4 42003040900                  9             9 FALSE   \n10 42003050100 -80.0  40.4 42003050100                 10            10 FALSE   \n# ℹ 392 more rows\n\n\nThis will be used to zoom in on the selected tract:\n\nzoom_x &lt;- manual_layout_filtered %&gt;% \n  filter(.ggraph.orig_index == selected_node) %&gt;% \n  pull(x)\n\nzoom_y &lt;- manual_layout_filtered %&gt;% \n  filter(.ggraph.orig_index == selected_node) %&gt;% \n  pull(y)\n\nzoom_magnitude &lt;- .25\n\n\nlegend_title &lt;- str_c(\"Minimum: \", minimum_commuters, \" commuters\")\n\nggraph(manual_layout_filtered) +\n  geom_sf(data = allegheny_tracts, size = .1, fill = NA) +\n  geom_sf(data = rivers, fill = \"white\", color = NA, alpha = .5) +\n  geom_sf(data = allegheny_tracts %&gt;%  filter(GEOID == filter_tract), \n          fill = \"grey\") +\n  geom_edge_fan(aes(edge_width = commuters),\n                arrow = arrow(length = unit(.5, 'lines')),\n                start_cap = circle(.1, 'lines'),\n                end_cap = circle(.2, 'lines'),\n                color = \"red\",\n                strength = .5) +\n  coord_sf(xlim = c(zoom_x - zoom_magnitude, zoom_x + zoom_magnitude), \n           ylim = c(zoom_y - zoom_magnitude, zoom_y + zoom_magnitude)) +\n  scale_edge_width_continuous(legend_title, range = c(.1, 1.5)) +\n  scale_edge_alpha_continuous(legend_title, range = c(.1, .8)) +\n  scale_fill_viridis_c() +\n  labs(x = NULL,\n       y = NULL,\n       title = \"Where do people commute from/to for work?\",\n       subtitle = str_c(\"From tract\", filter_tract, sep = \" \"),\n       caption = \"Based on 2015 US Census LODES dataset | @conor_tompkins\") +\n  theme_graph() +\n  theme(legend.background = element_rect(fill = \"light grey\"),\n        legend.text = element_text(color = \"black\"),\n        legend.title = element_text(color = \"black\"),\n        panel.background = element_rect(fill = \"black\"))"
  },
  {
    "objectID": "posts/usl-in-the-538-global-club-soccer-rankings/index.html",
    "href": "posts/usl-in-the-538-global-club-soccer-rankings/index.html",
    "title": "USL in the 538 Global Club Soccer Rankings",
    "section": "",
    "text": "This post was originally run with data from August 2018. 538 does not provide historical rankings, so I had to rerun the code with June 2023 data when I migrated my blog.\n538 recently added the United Soccer League to their Soccer Power Index ratings. I’m a Riverhounds fan, so I wanted to see how the team compared to teams from leagues around the world.\n\nlibrary(tidyverse)\nlibrary(ggrepel)\n\ntheme_set(theme_bw())\n\n\ndf &lt;- read_csv(\"https://projects.fivethirtyeight.com/soccer-api/club/spi_global_rankings.csv\", progress = FALSE) %&gt;% \n  group_by(league) %&gt;% \n  mutate(league_spi = median(spi)) %&gt;% \n  ungroup() %&gt;% \n  mutate(league = fct_reorder(league, league_spi))\n\ndf\n\n# A tibble: 641 × 8\n    rank prev_rank name                     league    off   def   spi league_spi\n   &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;                    &lt;fct&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;\n 1     1         1 Manchester City          Barcla…  2.79  0.28  92         72.8\n 2     2         2 Bayern Munich            German…  3.04  0.68  87.7       67.6\n 3     3         3 Barcelona                Spanis…  2.45  0.43  86.4       67.0\n 4     4         4 Real Madrid              Spanis…  2.56  0.6   84.4       67.0\n 5     5         5 Liverpool                Barcla…  2.63  0.67  83.9       72.8\n 6     6         6 Arsenal                  Barcla…  2.53  0.61  83.9       72.8\n 7     7         7 Newcastle                Barcla…  2.38  0.53  83.7       72.8\n 8     8         8 Napoli                   Italy …  2.3   0.51  83.2       63.4\n 9     9         9 Borussia Dortmund        German…  2.83  0.84  82.9       67.6\n10    10        10 Brighton and Hove Albion Barcla…  2.47  0.73  80.9       72.8\n# ℹ 631 more rows\n\n\n\ndf %&gt;% \n  ggplot(aes(spi, league)) +\n  geom_jitter(aes(color = league), show.legend = FALSE,\n              height = .2,\n              alpha = .7) +\n  geom_jitter(data = df %&gt;% filter(name == \"Pittsburgh Riverhounds\"),\n              show.legend = FALSE,\n              height = .2,\n              alpha = .7) +\n  geom_label_repel(data = df %&gt;% filter(name == \"Pittsburgh Riverhounds\"), \n                   aes(label = name), \n                   size = 3,\n                   show.legend = FALSE,\n                   force = 6) +\n  labs(title = \"538 Soccer Power Index\",\n       subtitle = \"One dot = one team\",\n       y = NULL,\n       x = \"Soccer Power Index\",\n       caption = \"538 data, @conor_tompkins\")\n\n\n\n\n\n\n\n\n\ndf %&gt;% \n  ggplot(aes(spi, league)) +\n  geom_jitter(aes(color = league), show.legend = FALSE,\n              height = .2,\n              alpha = .7) +\n  labs(title = \"538 Soccer Power Index\",\n       subtitle = \"One dot = one team\",\n       y = NULL,\n       x = \"Soccer Power Index\",\n       caption = \"538 data, @conor_tompkins\")\n\n\n\n\n\n\n\n\nThis shows the offensive and defensive ratings of each USL team. The Riverhounds are squarely in the #LilleyBall quadrant.\n\ndf %&gt;% \n  filter(league == \"United Soccer League\") %&gt;% \n  ggplot(aes(off, def, label = name)) +\n  geom_point() +\n  geom_label_repel(size = 4,\n                   force = 4) +\n  scale_y_reverse() +\n  labs(title = \"538 Soccer Power Index\",\n       y = \"Defensive rating (scale reversed)\",\n       x = \"Offensive rating\",\n       caption = \"538 data, @conor_tompkins\")"
  },
  {
    "objectID": "posts/premier-league-538-spi-ratings/index.html",
    "href": "posts/premier-league-538-spi-ratings/index.html",
    "title": "Premier League 538 SPI Ratings",
    "section": "",
    "text": "538’s Soccer Power Index (SPI) rates the quality of soccer teams from a variety of leagues around the world. In this post I’ll use gganimate to animate team SPI over the past 3 seasons.\nThe SPI data is available on 538’s GitHub repo.\nSet up the environment:\n\nlibrary(tidyverse)\nlibrary(purrr)\nlibrary(gganimate)\nlibrary(ggrepel)\nlibrary(broom)\nlibrary(lubridate)\n\ntheme_set(theme_minimal(base_size = 18))\n\nLoad the data and make the data long, instead of having different columns for home and away results:\n\ndata &lt;- read_csv(\"https://projects.fivethirtyeight.com/soccer-api/club/spi_matches.csv\")\n\n\ndf_home &lt;- data %&gt;% \n  select(team1, date, league, spi1) %&gt;% \n  rename(team = team1,\n         spi = spi1) %&gt;% \n  mutate(venue = \"home\")\n\ndf_away &lt;- data %&gt;% \n  select(team2, date, league, spi2) %&gt;% \n  rename(team = team2,\n         spi = spi2) %&gt;% \n  mutate(venue = \"away\")\n\ndf_all &lt;- bind_rows(df_home, df_away) %&gt;% \n  filter(date &lt; \"2019-04-07\") %&gt;% \n  arrange(league, team, date) %&gt;% \n  group_by(league, team) %&gt;% \n  mutate(team_game_number = dense_rank(date)) %&gt;% \n  ungroup()\n\nFilter to EPL teams and add a season column:\n\ndf_epl &lt;- df_all %&gt;% \n  filter(date &lt; Sys.Date(),\n         league == \"Barclays Premier League\")\n\nseason1 &lt;- tibble(date = seq(ymd('2016-08-13'), ymd('2017-05-21'), by='days'),\n                  season = 1)\n\nseason2 &lt;- tibble(date = seq(ymd('2017-08-11'), ymd('2018-05-13'), by='days'),\n                  season = 2)\n\nseason3 &lt;- tibble(date = seq(ymd('2018-08-10'), ymd('2019-04-06'), by='days'),\n                  season = 3)\n\nseasons &lt;- bind_rows(season1, season2, season3)\n\ndf_epl_smooth &lt;- df_epl %&gt;%\n  left_join(seasons)\n\nCalculate the smoothed SPI per team per season using loess:\n\ndf_epl_smooth &lt;- df_epl_smooth %&gt;% \n  nest(-c(team, season)) %&gt;% \n  mutate(m = map(data, loess,\n                          formula = spi ~ team_game_number, span = .5),\n         spi_smooth = purrr::map(m, `[[`, \"fitted\"))\n\ndf_epl_smooth &lt;- df_epl_smooth %&gt;% \n  select(-m) %&gt;% \n  unnest()\n\ndf_epl_last &lt;- df_epl %&gt;% \n  group_by(team) %&gt;% \n  summarize(date = last(date),\n            spi = last(spi))\n\nCreate the animation:\n\nspi_smooth_gif &lt;- df_epl_smooth %&gt;% \n  ggplot(aes(date, spi_smooth, color = team, group = team)) +\n  geom_line() +\n  geom_point(size = 2) +\n  geom_segment(aes(xend = ymd(\"2019-04-05\"), yend = spi_smooth), linetype = 2, colour = 'grey') +\n  geom_label(aes(x = ymd(\"2019-04-05\"), label = team),\n             hjust = -.1,\n             vjust = 0) +\n  geom_rect(xmin = ymd(\"2017-05-25\"), xmax = ymd(\"2017-08-12\"),\n            ymin = -Inf, ymax = Inf,\n            fill = \"white\", color = \"white\") +\n  geom_rect(xmin = ymd(\"2018-05-18\"), xmax = ymd(\"2018-08-10\"),\n                ymin = -Inf, ymax = Inf, fill = \"white\", color = \"white\") +\n  guides(color = FALSE) +\n  labs(title = \"Premier League\",\n       subtitle = \"538 Soccer Power Index\",\n       x = NULL,\n       y = \"538 Soccer Power Index\",\n       caption = \"@conor_tompkins\") +\n  transition_reveal(date) +\n  coord_cartesian(clip = 'off') +\n  theme(plot.margin = margin(5.5, 110, 5.5, 5.5))\n\nanimate(spi_smooth_gif, height = 9, width = 9, duration = 15, nframes = 300)\n\n\n\n\n\n\n\n\nObservers of the EPL will know that in any given season there are 2-3 tiers of teams, given the economics and relegation structure of the league. In 2018 the difference between the top 6 and the rest of the league was particularly stark. This is partly due to the difficulties that Everton experienced after they sold Lukaku and signed older and less skilled players. This graph highlights Everton’s SPI:\n\neverton_gif &lt;- df_epl_smooth %&gt;% \n  mutate(everton_flag = case_when(team == \"Everton\" ~ \"Everton\",\n                                  team != \"Everton\" ~ \"\")) %&gt;% \n  ggplot(aes(date, spi_smooth, color = everton_flag, group = team)) +\n  geom_line() +\n  geom_point(size = 2) +\n  geom_segment(aes(xend = ymd(\"2019-04-05\"), yend = spi_smooth), linetype = 2, colour = 'grey') +\n  geom_label(aes(x = ymd(\"2019-04-05\"), label = team),\n             hjust = -.1,\n             vjust = 0) +\n  geom_rect(xmin = ymd(\"2017-05-25\"), xmax = ymd(\"2017-08-12\"),\n            ymin = -Inf, ymax = Inf,\n            fill = \"white\", color = \"white\") +\n  geom_rect(xmin = ymd(\"2018-05-18\"), xmax = ymd(\"2018-08-10\"),\n                ymin = -Inf, ymax = Inf, fill = \"white\", color = \"white\") +\n  scale_color_manual(values = c(\"light grey\", \"blue\")) +\n  guides(color = FALSE) +\n  labs(title = \"Premier League\",\n       subtitle = \"538 Soccer Power Index\",\n       x = NULL,\n       y = \"538 Soccer Power Index\",\n       caption = \"@conor_tompkins\") +\n  transition_reveal(date) +\n  coord_cartesian(clip = 'off') +\n  theme(plot.margin = margin(5.5, 110, 5.5, 5.5))\n\nanimate(everton_gif, height = 9, width = 9, duration = 15, nframes = 300)"
  },
  {
    "objectID": "posts/residential-zoning-in-pittsburgh/index.html",
    "href": "posts/residential-zoning-in-pittsburgh/index.html",
    "title": "Residential Zoning in Pittsburgh",
    "section": "",
    "text": "The New York Times recently published an article about zoning in U.S. cities, particularly single-unit detached residential housing. The article did not include Pittsburgh, so I downloaded the zone shapefile from the WPRDC and made my own map.\nThis blog quickly goes through the steps to make the map and other graphs about the data.\nFirst, load the required libraries and set up the environment:\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(ggmap)\nlibrary(janitor)\nlibrary(hrbrthemes)\n\noptions(scipen = 999)\n\nRead in the shapefile with st_read and inspect the data with glimpse:\n\nshapefile &lt;- st_read(\"post_data/Zoning-shp/Zoning.shp\")\n\nReading layer `Zoning' from data source \n  `/Users/conorotompkins/Documents/github_repos/ctompkins_quarto_blog/posts/residential-zoning-in-pittsburgh/post_data/Zoning-shp/Zoning.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1061 features and 19 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 1315934 ymin: 381925.6 xmax: 1379759 ymax: 433399.4\nProjected CRS: NAD83 / Pennsylvania South (ftUS)\n\nglimpse(shapefile)\n\nRows: 1,061\nColumns: 20\n$ OBJECTID   &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ perimeter  &lt;dbl&gt; 4020.2318, 3522.5948, 2748.8339, 715.3339, 4499.1899, 2498.…\n$ zoning_    &lt;int&gt; 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 18, 19,…\n$ zoning_id  &lt;int&gt; 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 18, 19,…\n$ zon_new    &lt;chr&gt; \"P\", \"P\", \"LNC\", \"LNC\", \"P\", \"HC\", \"LNC\", \"R1D-M\", \"R1D-M\",…\n$ shape_leng &lt;dbl&gt; 4020.2318, 3522.5948, 2748.8339, 715.3339, 4499.1899, 2498.…\n$ correction &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ full_zonin &lt;chr&gt; \"PARKS AND OPEN SPACE\", \"PARKS AND OPEN SPACE\", \"LOCAL NEIG…\n$ legendtype &lt;chr&gt; \"Parks\", \"Parks\", \"Local Neighborhood Commercial\", \"Local N…\n$ municode   &lt;chr&gt; \"http://library.municode.com/HTML/13525/level4/PIZOCO_TITNI…\n$ status     &lt;chr&gt; \"Approved\", \"Approved\", \"Approved\", \"Approved\", \"Approved\",…\n$ created_us &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ created_da &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ last_edite &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ last_edi_1 &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ pghdb_sde_ &lt;dbl&gt; 404170.093, 332097.787, 192093.378, 22816.707, 962349.849, …\n$ GlobalID   &lt;chr&gt; \"b54df7d1-57d2-4175-8a34-5334046e889b\", \"1ea53324-e92d-4394…\n$ Shape__Are &lt;dbl&gt; 404170.093, 332097.787, 192093.378, 22816.707, 962349.849, …\n$ Shape__Len &lt;dbl&gt; 4020.2318, 3522.5948, 2748.8339, 715.3339, 4499.1899, 2498.…\n$ geometry   &lt;MULTIPOLYGON [US_survey_foot]&gt; MULTIPOLYGON (((1367528 381..., M…\n\n\nWe need to munge the data to get it in shape for analysis. This makes some simple TRUE|FALSE flags for basic zone information and uses case_when to create type, which represents aggregated zone types.\n\ndf &lt;- shapefile %&gt;% \n  mutate(residential = str_detect(full_zonin, \"RESIDENT\"),\n         single_unit = str_detect(full_zonin, \"SINGLE-UNIT\"),\n         attached = str_detect(full_zonin, \"ATTACHED\"),\n         type = case_when(residential == TRUE & single_unit == TRUE & attached == FALSE ~ \"Single-unit detached residential\",\n                          residential == TRUE & single_unit == FALSE | attached == TRUE ~ \"Other residential\",\n                          full_zonin == \"EDUCATIONAL/MEDICAL INSTITUTION\" ~ \"Educational/Medical\",\n                          residential == FALSE ~ \"Other non-residential\"),\n         type = factor(type, levels = c(\"Single-unit detached residential\", \n                                        \"Other residential\",\n                                        \"Educational/Medical\",\n                                        \"Other non-residential\")),\n         alpha_flag = type == \"Single-unit detached residential\") |&gt; \n  rename(area = Shape__Are)\n\n\n\nRows: 1,061\nColumns: 24\n$ OBJECTID    &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ perimeter   &lt;dbl&gt; 4020.2318, 3522.5948, 2748.8339, 715.3339, 4499.1899, 2498…\n$ zoning_     &lt;int&gt; 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 18, 19…\n$ zoning_id   &lt;int&gt; 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 18, 19…\n$ zon_new     &lt;chr&gt; \"P\", \"P\", \"LNC\", \"LNC\", \"P\", \"HC\", \"LNC\", \"R1D-M\", \"R1D-M\"…\n$ shape_leng  &lt;dbl&gt; 4020.2318, 3522.5948, 2748.8339, 715.3339, 4499.1899, 2498…\n$ correction  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ full_zonin  &lt;chr&gt; \"PARKS AND OPEN SPACE\", \"PARKS AND OPEN SPACE\", \"LOCAL NEI…\n$ legendtype  &lt;chr&gt; \"Parks\", \"Parks\", \"Local Neighborhood Commercial\", \"Local …\n$ municode    &lt;chr&gt; \"http://library.municode.com/HTML/13525/level4/PIZOCO_TITN…\n$ status      &lt;chr&gt; \"Approved\", \"Approved\", \"Approved\", \"Approved\", \"Approved\"…\n$ created_us  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ created_da  &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ last_edite  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ last_edi_1  &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ pghdb_sde_  &lt;dbl&gt; 404170.093, 332097.787, 192093.378, 22816.707, 962349.849,…\n$ GlobalID    &lt;chr&gt; \"b54df7d1-57d2-4175-8a34-5334046e889b\", \"1ea53324-e92d-439…\n$ area        &lt;dbl&gt; 404170.093, 332097.787, 192093.378, 22816.707, 962349.849,…\n$ Shape__Len  &lt;dbl&gt; 4020.2318, 3522.5948, 2748.8339, 715.3339, 4499.1899, 2498…\n$ residential &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, TRU…\n$ single_unit &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, TRU…\n$ attached    &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA…\n$ type        &lt;fct&gt; Other non-residential, Other non-residential, Other non-re…\n$ alpha_flag  &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, TRU…\n\n\nThis counts the number of rows per full zone description (full_zonin) and type:\n\ndf_zones &lt;- df %&gt;% \n  count(full_zonin, type, sort = TRUE) %&gt;% \n  st_drop_geometry()\n\n\n\nRows: 59\nColumns: 3\n$ full_zonin &lt;chr&gt; \"PARKS AND OPEN SPACE\", \"LOCAL NEIGHBORHOOD COMMERCIAL\", \"S…\n$ type       &lt;fct&gt; Other non-residential, Other non-residential, Single-unit d…\n$ n          &lt;int&gt; 153, 125, 75, 70, 65, 54, 52, 44, 42, 42, 41, 38, 36, 19, 1…\n\n\nCreate a basic bar chart to show the distribution of type:\n\ndf %&gt;% \n  st_drop_geometry() %&gt;% \n  group_by(type, residential) %&gt;% \n  summarize(area = sum(area)) %&gt;% \n  ungroup() %&gt;% \n  mutate(type = fct_reorder(type, area)) %&gt;% \n  ggplot(aes(type, area / 1000000, fill = residential)) +\n    geom_col() +\n    scale_y_comma() +\n    scale_fill_discrete(\"Is the zone residential?\") +\n    labs(x = \"Zone type\",\n         y = \"Land area in millions of feet\") +\n    coord_flip() +\n    theme_ipsum()\n\n\n\n\n\n\n\n\nUse a bar chart to show the distribution of full_zonin:\n\ndf %&gt;% \n  st_drop_geometry() %&gt;% \n  group_by(full_zonin, residential) %&gt;% \n  summarize(area = sum(area)) %&gt;% \n  ungroup() %&gt;% \n  mutate(full_zonin = fct_reorder(full_zonin, area)) %&gt;% \n  ggplot(aes(full_zonin, area / 1000000, fill = residential)) +\n    geom_col() +\n    scale_y_comma() +\n    scale_fill_discrete(\"Is the zone residential?\") +\n    labs(x = \"Full zone description\",\n         y = \"Land area in millions of feet\") +\n    coord_flip() +\n    theme_ipsum()\n\n\n\n\n\n\n\n\nThis calculates the total land area zoned for any type of residential housing:\n\ndf %&gt;% \n  st_drop_geometry() %&gt;% \n  mutate(single_unit_flag = type == \"Single-unit detached residential\") %&gt;% \n  filter(residential == TRUE) %&gt;% \n  summarize(total_area = sum(area))\n\n  total_area\n1  751652465\n\n\nThis calculates the % of residential zoning that is zoned for single-unit detached residential housing units:\n\ndf %&gt;% \n  st_drop_geometry() %&gt;% \n  filter(residential == TRUE) %&gt;% \n  mutate(single_unit_flag = (type == \"Single-unit detached residential\")) %&gt;% \n  group_by(single_unit_flag) %&gt;% \n  summarize(zone_area = sum(area)) %&gt;% \n  mutate(pct_area = zone_area / sum(zone_area))\n\n# A tibble: 2 × 3\n  single_unit_flag  zone_area pct_area\n  &lt;lgl&gt;                 &lt;dbl&gt;    &lt;dbl&gt;\n1 FALSE            332150155.    0.442\n2 TRUE             419502310.    0.558\n\n\nThis creates a map of the zones, fills them by type, and overlays it on a GoogleMaps basemap. I also insert the boundaries of the City of Pittsburgh.\n\ncity_boundary &lt;- st_read(\"post_data/Pittsburgh_City_Boundary-shp/City_Boundary.shp\")\n\nReading layer `City_Boundary' from data source \n  `/Users/conorotompkins/Documents/github_repos/ctompkins_quarto_blog/posts/residential-zoning-in-pittsburgh/post_data/Pittsburgh_City_Boundary-shp/City_Boundary.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 8 features and 8 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 1315934 ymin: 381925.6 xmax: 1379780 ymax: 433399.4\nProjected CRS: NAD83 / Pennsylvania South (ftUS)\n\nggplot() +\n  geom_sf(data = df %&gt;% filter(type != \"Other non-residential\"), aes(fill = type), inherit.aes = FALSE, size = .5, alpha = 1, color = NA) +\n  geom_sf(data = city_boundary, inherit.aes = FALSE, alpha = 0, size = 2) +\n  coord_sf(crs = st_crs(4326)) +\n  scale_fill_manual(\"Zone type\",\n                      values = c(\"#ea60b9\", \"#4cafc5\", \"yellow\", \"light grey\")) +\n  labs(title = \"56% of residential zoned land area is single-family detached residential\",\n         subtitle = \"City of Pittsburgh zoning\",\n         caption = \"@conor_tompkins, data from WPRDC\") +\n  theme_void()\n\n\n\n\n\n\n\n\nI used scale_fill_manual to manually set the color palette to match the NYTimes article."
  },
  {
    "objectID": "posts/shifting_political_winds/index.html",
    "href": "posts/shifting_political_winds/index.html",
    "title": "Shifting political winds",
    "section": "",
    "text": "The purpose of this post is to recreate the “Shift from 2016” arrow map that the New York Times used to show which counties became more Democratic or Republican-leaning from 2016 to 2020. This is a screenshot of the NYTimes figure:\nI will use county-level Presidential election data from the MIT Election Data + Science Lab to recreate the chart. Since 2020 results are not final yet, I will focus on data from 2000-2016. I ran into multiple issues with the dataset, which I explain in the Code section below. The most signifcant issue was with the data from Alaska, which I excluded from the charts below because of problems with the data."
  },
  {
    "objectID": "posts/shifting_political_winds/index.html#recreating-the-nytimes-figure",
    "href": "posts/shifting_political_winds/index.html#recreating-the-nytimes-figure",
    "title": "Shifting political winds",
    "section": "Recreating the NYTimes Figure",
    "text": "Recreating the NYTimes Figure\nMy approach is to use {ggplot2} and {sf} to map the data and draw arrows at angles to display shifts in the Democratic margin.\nThis is the dataframe I use to make the final map. It contains the year, state, county, FIPS code, county and state geometries, and election results per county.\n\nglimpse(shift_map)\n\nRows: 12,610\nColumns: 22\n$ year                          &lt;dbl&gt; 2004, 2008, 2012, 2016, 2004, 2008, 2012…\n$ state                         &lt;chr&gt; \"ALABAMA\", \"ALABAMA\", \"ALABAMA\", \"ALABAM…\n$ county_name                   &lt;chr&gt; \"AUTAUGA\", \"AUTAUGA\", \"AUTAUGA\", \"AUTAUG…\n$ county_fips                   &lt;chr&gt; \"01001\", \"01001\", \"01001\", \"01001\", \"010…\n$ candidatevotes_sum_democrat   &lt;dbl&gt; 4758, 6093, 6363, 5936, 15599, 19386, 18…\n$ candidatevotes_sum_republican &lt;dbl&gt; 15196, 17403, 17379, 18172, 52971, 61271…\n$ pct_vote_democrat             &lt;dbl&gt; 0.23845, 0.25932, 0.26801, 0.24623, 0.22…\n$ pct_vote_republican           &lt;dbl&gt; 0.7616, 0.7407, 0.7320, 0.7538, 0.7725, …\n$ dem_margin_pct                &lt;dbl&gt; -0.52310, -0.48136, -0.46399, -0.50755, …\n$ dem_margin_votes              &lt;dbl&gt; -10438, -11310, -11016, -12236, -37372, …\n$ shift_pct                     &lt;dbl&gt; -0.106746, 0.041745, 0.017371, -0.043561…\n$ shift_votes                   &lt;dbl&gt; -3387, -872, 294, -1220, -10497, -4513, …\n$ shift_pct_scaled              &lt;dbl&gt; 71.83, 91.08, 87.92, 80.02, 78.51, 89.01…\n$ shift_votes_scaled            &lt;dbl&gt; 16602, 11700, 10573, 12378, 30460, 18796…\n$ shift_pct_binary              &lt;chr&gt; \"Republican\", \"Democratic\", \"Democratic\"…\n$ shift_votes_binned            &lt;lgl&gt; FALSE, TRUE, TRUE, TRUE, FALSE, FALSE, F…\n$ geometry                      &lt;POLYGON [m]&gt; POLYGON ((1269841 -1303980,..., …\n$ center                        &lt;list&gt; &lt;POINT (1253837 -1285138)&gt;, &lt;POINT (125…\n$ lng0                          &lt;dbl&gt; 1253837, 1253837, 1253837, 1253837, 1177…\n$ lat0                          &lt;dbl&gt; -1285138, -1285138, -1285138, -1285138, …\n$ lng1                          &lt;dbl&gt; 1259015, 1253616, 1254221, 1255982, 1183…\n$ lat1                          &lt;dbl&gt; -1269365, -1273441, -1274572, -1272948, …\n\n\n\nshift_map_filtered &lt;- shift_map %&gt;% \n  filter(state != \"ALASKA\") %&gt;%\n  filter(year == 2016) %&gt;% \n  mutate(shift_pct_binary = case_when(sign(shift_pct) == 1 ~ \"Democratic\",\n                                      sign(shift_pct) == -1 ~ \"Republican\"),\n         shift_pct_binary = as.factor(shift_pct_binary)) %&gt;% \n  mutate(shift_votes_binned = abs(shift_votes) &lt;= 3000)\n\nggplot() +\n  geom_sf(data = filter(state_geo, !str_detect(NAME, \"ALASKA\")),\n          linewidth = .2,\n          fill = NA) +\n  geom_point(data = filter(shift_map_filtered, abs(shift_votes) &lt;= 1500),\n             aes(x = lng0, y = lat0,\n                 color = shift_pct_binary),\n             size = .75,\n             alpha = .3) +\n  geom_segment(data = filter(shift_map_filtered, abs(shift_votes) &gt; 1500),\n               aes(x = lng0, xend = lng1,\n                   y = lat0, yend = lat1,\n                   color = shift_pct_binary,\n                   linewidth = shift_votes,\n                   alpha = shift_votes_binned),\n               linejoin = \"mitre\",\n               arrow = arrow(length = unit(0.08, \"inches\"))) +\n  scale_color_manual(values = c(\"#1375B7\", \"#C93135\"), guide = guide_legend(title.position = \"top\")) +\n  scale_linewidth_continuous(range = c(.001, 2), guide = \"none\") +\n  scale_alpha_manual(values = c(1, .3), guide = \"none\") +\n  labs(color = \"Shift in election margin\") +\n  facet_wrap(~year) +\n  theme_void(base_size = 25) +\n  theme(legend.direction = \"horizontal\",\n        legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nThe starting point of the line is the centroid of the county. The length and width of the lines are scaled to the shift in terms of number of votes. The NYTimes figure treats the shift as a binary variable when it rescales to degrees of the angle. In their graph, a Democratic shift is about 45 degrees (diagonal left) and a Republican shift is about 135 degrees (diagonal right). My figure maintains the continuous nature of the shift in %. I use the range 0-180 in degrees to indicate the shift. 0 degrees (all the way left) indicates a 100% shift towards Democrats, 90 degrees (pointing upwards) indicates no change, and 180 degrees (all the way to the right) indicates a 100% shift towards Republicans.\nThe end point of the line is calculated using the sine and cosine of the margin shift in % (re-scaled to be interpreted as degrees of an angle) multiplied by the margin shift in votes (re-scaled to be interpreted as meters), which is added to the origin point.\nI lower the opacity of the lines in counties where the vote totals did not shift much. I use points instead of lines for counties where there was a very small shift in votes. This prevents overplotting in geographically dense areas with small populations.\nThis animation shows the shift in Presidential election margin from 2004-2016.\n\npolitical_winds_anim &lt;- shift_map %&gt;% \n  filter(state != \"ALASKA\") %&gt;% \n  mutate(id = str_c(state, county_name, county_fips)) %&gt;% \n  mutate(year = as.integer(year)) %&gt;% \n  mutate(shift_pct_binary = case_when(sign(shift_pct) == 1 ~ \"Democratic\",\n                                      sign(shift_pct) == -1 ~ \"Republican\"),\n         shift_pct_binary = as.factor(shift_pct_binary)) %&gt;% \n  mutate(shift_votes_binned = abs(shift_votes) &lt;= 3000) %&gt;% \n  ggplot() +\n  geom_sf(data = filter(state_geo, NAME != \"ALASKA\"),\n          linewidth = .2,\n          fill = NA) +\n  geom_segment(aes(x = lng0, xend = lng1,\n                   y = lat0, yend = lat1,\n                   color = shift_pct_binary,\n                   linewidth = shift_votes,\n                   alpha = shift_votes_binned,\n                   group = id),\n               linejoin = \"mitre\",\n               arrow = arrow(length = unit(0.09, \"inches\"))) +\n  scale_color_manual(values = c(\"#1375B7\", \"#C93135\"), guide = guide_legend(title.position = \"top\")) +\n  scale_linewidth_continuous(range = c(.001, 1.3), guide = \"none\") +\n  scale_alpha_manual(values = c(1, .3), guide = \"none\") +\n  theme_void(base_size = 25) +\n  theme(legend.direction = \"horizontal\",\n        legend.position = \"bottom\") +\n  transition_states(year) +\n  labs(title = \"Shift in Presidential election Democratic margin\",\n       subtitle = \"Year: {closest_state}\",\n       color = \"Shift in Democratic margin\")\n\npolitical_winds_anim\n\n\n\n\n\n\n\n\nIn the animation there is less overplotting, so I do not replace lines with dots for counties where there was a very small shift in votes."
  },
  {
    "objectID": "posts/shifting_political_winds/index.html#code",
    "href": "posts/shifting_political_winds/index.html#code",
    "title": "Shifting political winds",
    "section": "Code",
    "text": "Code\n\nIngest\n\n#election shift\n#script to clean data\n\n#data from https://electionlab.mit.edu/data\n\n#fips info\n#https://en.wikipedia.org/wiki/Federal_Information_Processing_Standard_state_code#FIPS_state_codes\n#https://en.wikipedia.org/wiki/List_of_United_States_FIPS_codes_by_county\n#changes https://www.census.gov/programs-surveys/geography/technical-documentation/county-changes.2010.html\n\n#read in data\ndata &lt;- read_csv(\"post_data/countypres_2000-2020.csv\",\n                 col_types = cols(\n                   year = col_double(),\n                   state = col_character(),\n                   state_po = col_character(),\n                   county_name = col_character(),\n                   county_fips = col_character(),\n                   office = col_character(),\n                   candidate = col_character(),\n                   party = col_character(),\n                   candidatevotes = col_double(),\n                   totalvotes = col_double(),\n                   version = col_double()\n                 )) %&gt;% \n  clean_names() |&gt;\n  filter(year &lt;= 2016,\n         mode == \"TOTAL\") |&gt; \n  select(-mode)\n\nglimpse(data)\n\nRows: 50,524\nColumns: 11\n$ year           &lt;dbl&gt; 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2…\n$ state          &lt;chr&gt; \"ALABAMA\", \"ALABAMA\", \"ALABAMA\", \"ALABAMA\", \"ALABAMA\", …\n$ state_po       &lt;chr&gt; \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"…\n$ county_name    &lt;chr&gt; \"AUTAUGA\", \"AUTAUGA\", \"AUTAUGA\", \"AUTAUGA\", \"BALDWIN\", …\n$ county_fips    &lt;chr&gt; \"01001\", \"01001\", \"01001\", \"01001\", \"01003\", \"01003\", \"…\n$ office         &lt;chr&gt; \"US PRESIDENT\", \"US PRESIDENT\", \"US PRESIDENT\", \"US PRE…\n$ candidate      &lt;chr&gt; \"AL GORE\", \"GEORGE W. BUSH\", \"RALPH NADER\", \"OTHER\", \"A…\n$ party          &lt;chr&gt; \"DEMOCRAT\", \"REPUBLICAN\", \"GREEN\", \"OTHER\", \"DEMOCRAT\",…\n$ candidatevotes &lt;dbl&gt; 4942, 11993, 160, 113, 13997, 40872, 1033, 578, 5188, 5…\n$ totalvotes     &lt;dbl&gt; 17208, 17208, 17208, 17208, 56480, 56480, 56480, 56480,…\n$ version        &lt;dbl&gt; 20220315, 20220315, 20220315, 20220315, 20220315, 20220…\n\n\n\n\nClean\nThis code filters out state-wide vote tabulations and then filters only on the two-party Presidential vote.\n\ndata &lt;- data %&gt;% \n  rename(fips_raw = county_fips) %&gt;% \n  #filter out state-wide ballot collection\n  filter(!(state == \"CONNECTICUT\" & county_name == \"STATEWIDE WRITEIN\")) %&gt;% \n  filter(!(state == \"MAINE\" & county_name == \"MAINE UOCAVA\")) %&gt;% \n  filter(!(state == \"RHODE ISLAND\" & county_name == \"FEDERAL PRECINCT\"))\n\n#filter for only 2-party vote in presidential elections\ndata &lt;- data %&gt;% \n  filter(office == \"US PRESIDENT\",\n         party == \"DEMOCRAT\" | party == \"REPUBLICAN\") %&gt;% \n  arrange(state, county_name, fips_raw, year) %&gt;% \n  replace_na(list(candidatevotes = 0))\n\nMany of the FIPS codes from the source data dropped leading zeroes, which makes them unuseable for joining with Census data. This code adds the leading zeroes back.\nThese problems were fixed in a later update by MIT, so this code is not strictly necessary anymore\n\n#clean fips data\nstates_with_bad_fips &lt;- str_to_title(c(\"ALABAMA\", \"ALASKA\", \"ARIZONA\", \n                                      \"ARKANSAS\", \"CALIFORNIA\",\n                                      \"COLORADO\", \"CONNECTICUT\"))\ndata %&gt;% \n  filter(state %in% states_with_bad_fips) %&gt;% \n  mutate(county_fips = paste0(\"0\", fips_raw)) %&gt;% \n  distinct(fips_raw, county_fips)\n\n# A tibble: 0 × 2\n# ℹ 2 variables: fips_raw &lt;chr&gt;, county_fips &lt;chr&gt;\n\ndata &lt;- data %&gt;% \n  #add \"0\" to front of states where leading \"0\" was dropped\n  mutate(county_fips = case_when(state %in% states_with_bad_fips ~ paste0(\"0\", fips_raw),\n                          !(state %in% states_with_bad_fips) ~ fips_raw))\n\nI had to make a variety of decisions about how to clean up the data with regards to county geometries. The MIT data does not reflect cases where counties changed names or FIPS codes, or where counties merged. This code manually makes the changes necessary to join the data with Census geometry data. Note that I do not attempt to fix the data for Alaska, which was extremely different than the Census data. I was not confident that I could make accurate adjustments in this case, so I excluded Alaska entirely. These changes are not optimal, but I think it is close enough.\nThese problems were fixed in a later update by MIT, so this code is not strictly necessary anymore\n\n#decisions to make with wonky geometry\n#merge records for Shannnon and Oglala Lakota counties in SD\n#merge Kansas City Missouri and Jackson County Missouri\n#merge Bedford (city) fips 51515 with Bedford county 51019\n\ndata &lt;- data %&gt;% \n  #update Oglala Lakota SD fips\n  #changed in 2015 https://www.census.gov/programs-surveys/geography/technical-documentation/county-changes.2010.html\n  mutate(county_fips = case_when(state == \"SOUTH DAKOTA\" & county_name == \"OGLALA LAKOTA\" ~ \"46102\",\n                          TRUE ~ county_fips)) %&gt;% \n  #merge Kansas City Missouri with Jackson County Missouri\n  mutate(county_name = case_when(state == \"MISSOURI\" & county_name == \"KANSAS CITY\" ~ \"JACKSON\",\n                            TRUE ~ county_name),\n         county_fips = case_when(state == \"MISSOURI\" & county_name == \"JACKSON\" ~ \"29095\",\n                          TRUE ~ county_fips)) %&gt;% \n  #merge Bedford (city) fips 51515 with Bedford county 51019\n  mutate(county_fips = case_when(state == \"VIRGINIA\" & county_name == \"BEDFORD\" & county_fips == \"51515\" ~ \"51019\",\n                          TRUE ~ county_fips))\n\nThis compares the counties in the MIT data vs. what is in the Census API. Besides Alaska, this shows that my manual changes accounted for the issues I identified.\n\ncounties &lt;- get_acs(variables = \"B19013_001\",\n                      geography = \"county\",\n                      geometry = FALSE) %&gt;% \n  #mutate(census_geo_year = 2010) %&gt;% \n  select(NAME, GEOID)\n\nGetting data from the 2017-2021 5-year ACS\n\n\n\n#alaska falls out: this is expected\n#Broomfield County CO falls out for year 2000: was part of Boulder County in 2000\n#Oglala Lakota County SD falls out for year 2000: was Shannon County in 2000\n#\ndata %&gt;% \n  select(year, state, county_name, county_fips) %&gt;% \n  filter(state != \"ALASKA\") %&gt;% \n  anti_join(counties, by = c(\"county_fips\" = \"GEOID\")) %&gt;% \n  count(state, county_name)\n\n# A tibble: 1 × 3\n  state        county_name     n\n  &lt;chr&gt;        &lt;chr&gt;       &lt;int&gt;\n1 SOUTH DAKOTA SHANNON         8\n\n\nThe process of merging some counties meant that I had to summarize the election results to the level of my new “adjusted” counties. This code performs that process.\n\n#some counties have 4 records because of merging process\ndata %&gt;%\n  select(state, county_name, county_fips, year) %&gt;% \n  add_count(state, county_name, county_fips, year) %&gt;% \n  distinct(n)\n\n# A tibble: 2 × 1\n      n\n  &lt;int&gt;\n1     2\n2     4\n\n\n\n#summarize candidatevotes to account for merged counties\ndata %&gt;% \n  select(state, county_name, county_fips, year, office, party, candidate, candidatevotes) %&gt;% \n  group_by(state, county_name, county_fips, year, office, party, candidate) %&gt;% \n  summarize(candidatevotes_sum = sum(candidatevotes)) %&gt;% \n  ungroup() %&gt;% \n  add_count(state, county_name, county_fips, year) %&gt;% \n  #confirm that each county only has 2 records\n  distinct(n)\n\n`summarise()` has grouped output by 'state', 'county_name', 'county_fips',\n'year', 'office', 'party'. You can override using the `.groups` argument.\n\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1     2\n\n\n\ndata &lt;- data %&gt;% \n  select(state, county_name, county_fips, year, office, party, candidate, candidatevotes) %&gt;% \n  group_by(state, county_name, county_fips, year, office, party, candidate) %&gt;% \n  summarize(candidatevotes_sum = sum(candidatevotes)) %&gt;% \n  ungroup()\n\n`summarise()` has grouped output by 'state', 'county_name', 'county_fips',\n'year', 'office', 'party'. You can override using the `.groups` argument.\n\n\n\n\nMunge\nThis part performs the more straightfoward tasks of calculating a candidate’s % of the vote and the election-to-election shift in %.\n\npresidential_votes &lt;- data %&gt;% \n  group_by(year, state, county_name, county_fips) %&gt;% \n  mutate(pct_vote = candidatevotes_sum / sum(candidatevotes_sum)) %&gt;% \n  ungroup() %&gt;% \n  select(year, state, county_name, county_fips, party, candidatevotes_sum, pct_vote)\n\n\npresidential_votes_shift &lt;- presidential_votes %&gt;% \n  mutate(party = str_to_lower(party)) %&gt;%\n  pivot_wider(names_from = party, values_from = c(candidatevotes_sum, pct_vote)) %&gt;%\n  mutate(dem_margin_pct = pct_vote_democrat - pct_vote_republican,\n         dem_margin_votes = candidatevotes_sum_democrat - candidatevotes_sum_republican) %&gt;% \n  arrange(state, county_name, county_fips, year) %&gt;% \n  group_by(state, county_name, county_fips) %&gt;% \n  mutate(shift_pct = dem_margin_pct - lag(dem_margin_pct),\n         shift_votes = dem_margin_votes - lag(dem_margin_votes)) %&gt;% \n  filter(row_number() &gt; 1) %&gt;% \n  ungroup()\n\nFinally, this creates new variables that rescale the shift in % and votes to degrees and meters, respectively. I also create variations of shift_pct and shift_votes to use in the graph.\n\npresidential_votes_shift &lt;- presidential_votes_shift %&gt;% \n  mutate(shift_pct_scaled = rescale(shift_pct, to = c(0, 180)), #republican 0, democrat 180\n         shift_votes_scaled = rescale(abs(shift_votes), to = c(10^4, 10^6))) %&gt;% \n  mutate(shift_pct_binary = case_when(sign(shift_pct) == 1 ~ \"Democratic\",\n                                      sign(shift_pct) == -1 ~ \"Republican\"),\n         shift_pct_binary = as.factor(shift_pct_binary)) %&gt;% \n  mutate(shift_votes_binned = abs(shift_votes) &lt;= 3000)\n\n\n#create shift map object\nshift_map &lt;- presidential_votes_shift %&gt;% \n  left_join(county_geo, by = c(\"county_fips\" = \"GEOID\")) %&gt;% \n  st_sf() %&gt;% \n  rename(lng0 = center_lon_x,\n         lat0 = center_lat_y) %&gt;% \n  mutate(lng1 = lng0 + (shift_votes_scaled * cos(NISTdegTOradian(shift_pct_scaled))),\n         lat1 = lat0 + (shift_votes_scaled * sin(NISTdegTOradian(shift_pct_scaled))))\n\n\nshift_map_filtered &lt;- shift_map %&gt;% \n  filter(state != \"ALASKA\") %&gt;%\n  filter(year == 2016) %&gt;% \n  mutate(shift_pct_binary = case_when(sign(shift_pct) == 1 ~ \"Democratic\",\n                                      sign(shift_pct) == -1 ~ \"Republican\"),\n         shift_pct_binary = as.factor(shift_pct_binary))\n\nggplot() +\n  geom_sf(data = filter(state_geo, !str_detect(NAME, \"ALASKA\")),\n          linewidth = .2,\n          fill = NA) +\n  geom_point(data = filter(shift_map_filtered, abs(shift_votes) &lt;= 1500),\n             aes(x = lng0, y = lat0,\n                 color = shift_pct_binary),\n             size = .75,\n             alpha = .3) +\n  geom_segment(data = filter(shift_map_filtered, abs(shift_votes) &gt; 1500),\n               aes(x = lng0, xend = lng1,\n                   y = lat0, yend = lat1,\n                   color = shift_pct_binary,\n                   linewidth = shift_votes,\n                   alpha = shift_votes_binned),\n               linejoin = \"mitre\",\n               arrow = arrow(length = unit(0.08, \"inches\"))) +\n  scale_color_manual(values = c(\"#1375B7\", \"#C93135\"), guide = guide_legend(title.position = \"top\")) +\n  scale_linewidth_continuous(range = c(.001, 2), guide = \"none\") +\n  scale_alpha_manual(values = c(1, .3), guide = \"none\") +\n  labs(color = \"Shift in election margin\") +\n  facet_wrap(~year) +\n  theme_void(base_size = 25) +\n  theme(legend.direction = \"horizontal\",\n        legend.position = \"bottom\")\n\n\npolitical_winds_anim &lt;- shift_map %&gt;% \n  filter(state != \"Alaska\") %&gt;% \n  mutate(id = str_c(state, county_name, county_fips)) %&gt;% \n  mutate(year = as.integer(year)) %&gt;% \n  mutate(shift_votes_binned = abs(shift_votes) &lt;= 3000) %&gt;% \n  ggplot() +\n  geom_sf(data = filter(state_geo, NAME != \"Alaska\"),\n          linewidth = .2,\n          fill = NA) +\n  geom_segment(aes(x = lng0, xend = lng1,\n                   y = lat0, yend = lat1,\n                   color = shift_pct_binary,\n                   linewidth = shift_votes,\n                   alpha = shift_votes_binned,\n                   group = id),\n               linejoin = \"mitre\",\n               arrow = arrow(length = unit(0.09, \"inches\"))) +\n  scale_color_manual(values = c(\"#1375B7\", \"#C93135\"), guide = guide_legend(title.position = \"top\")) +\n  scale_size_continuous(range = c(.001, 1.3), guide = \"none\") +\n  scale_alpha_manual(values = c(1, .3), guide = \"none\") +\n  theme_void(base_size = 25) +\n  theme(legend.direction = \"horizontal\",\n        legend.position = \"bottom\") +\n  transition_states(year) +\n  labs(title = \"Shift in Presidential election Democratic margin\",\n       subtitle = \"Year: {closest_state}\",\n       color = \"Shift in Democratic margin\")\n\npolitical_winds_anim"
  },
  {
    "objectID": "posts/how-many-pittsburghers-cross-the-river-to-get-to-work/index.html",
    "href": "posts/how-many-pittsburghers-cross-the-river-to-get-to-work/index.html",
    "title": "How Many Pittsburghers Cross the River to Get to Work",
    "section": "",
    "text": "This post focuses on how many rivers Pittsburghers cross to get to work. I use the U.S. Census Bureau LEHD Origin-Destination Employment Statistics (LODES) dataset to draw lines between “home” census tracts and “work” census tracts, and then count how many “commuter lines” intersect with the 3 main rivers in Pittsburgh. This calculation is done in straight lines “as the crow flies”, not accounting for actual road routes."
  },
  {
    "objectID": "posts/how-many-pittsburghers-cross-the-river-to-get-to-work/index.html#tldr",
    "href": "posts/how-many-pittsburghers-cross-the-river-to-get-to-work/index.html#tldr",
    "title": "How Many Pittsburghers Cross the River to Get to Work",
    "section": "TLDR",
    "text": "TLDR\nA plurality of commuters don’t cross any rivers, and none cross three. \n\nMany commuters in the Golden Triangle and neighborhoods to the east don’t cross rivers to get to work. Commuters from the North and South Hills areas usually cross one river. Commuters from Sewickley, Coraopolis, and those that live close to the airport are most likely to cross two rivers."
  },
  {
    "objectID": "posts/how-many-pittsburghers-cross-the-river-to-get-to-work/index.html#data-munging-and-analysis",
    "href": "posts/how-many-pittsburghers-cross-the-river-to-get-to-work/index.html#data-munging-and-analysis",
    "title": "How Many Pittsburghers Cross the River to Get to Work",
    "section": "Data munging and analysis",
    "text": "Data munging and analysis\nI use the “pa_od_aux_JT00_2017.csv” file as shown here: \nIn my analysis I use many of the standard {tidyverse} packages, {sf}, {tidycensus}, {tidygraph}, and {ggraph}:\n\nlibrary(tidyverse)\nlibrary(vroom)\nlibrary(sf)\nlibrary(tigris)\nlibrary(tidycensus)\nlibrary(tidygraph)\nlibrary(ggraph)\n\nThe first step is to read in the geographies crosswalk:\n\ngeo_crosswalk &lt;- vroom(\"post_data/pa_xwalk.csv.gz\", col_types = cols(.default = \"c\"))\n\ngeo_crosswalk\n\n# A tibble: 421,545 × 43\n   tabblk2010    st    stusps stname cty   ctyname trct  trctname bgrp  bgrpname\n   &lt;chr&gt;         &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;   \n 1 420912030002… 42    PA     Penns… 42091 Montgo… 4209… 2030 (M… 4209… 2 (Trac…\n 2 420912070013… 42    PA     Penns… 42091 Montgo… 4209… 2070.01… 4209… 3 (Trac…\n 3 420912070013… 42    PA     Penns… 42091 Montgo… 4209… 2070.01… 4209… 3 (Trac…\n 4 420710144022… 42    PA     Penns… 42071 Lancas… 4207… 144.02 … 4207… 2 (Trac…\n 5 420710144022… 42    PA     Penns… 42071 Lancas… 4207… 144.02 … 4207… 2 (Trac…\n 6 420710134002… 42    PA     Penns… 42071 Lancas… 4207… 134 (La… 4207… 2 (Trac…\n 7 420710144022… 42    PA     Penns… 42071 Lancas… 4207… 144.02 … 4207… 2 (Trac…\n 8 420710144022… 42    PA     Penns… 42071 Lancas… 4207… 144.02 … 4207… 2 (Trac…\n 9 420710144022… 42    PA     Penns… 42071 Lancas… 4207… 144.02 … 4207… 2 (Trac…\n10 420710144022… 42    PA     Penns… 42071 Lancas… 4207… 144.02 … 4207… 2 (Trac…\n# ℹ 421,535 more rows\n# ℹ 33 more variables: cbsa &lt;chr&gt;, cbsaname &lt;chr&gt;, zcta &lt;chr&gt;, zctaname &lt;chr&gt;,\n#   stplc &lt;chr&gt;, stplcname &lt;chr&gt;, ctycsub &lt;chr&gt;, ctycsubname &lt;chr&gt;,\n#   stcd116 &lt;chr&gt;, stcd116name &lt;chr&gt;, stsldl &lt;chr&gt;, stsldlname &lt;chr&gt;,\n#   stsldu &lt;chr&gt;, stslduname &lt;chr&gt;, stschool &lt;chr&gt;, stschoolname &lt;chr&gt;,\n#   stsecon &lt;chr&gt;, stseconname &lt;chr&gt;, trib &lt;chr&gt;, tribname &lt;chr&gt;, tsub &lt;chr&gt;,\n#   tsubname &lt;chr&gt;, stanrc &lt;chr&gt;, stanrcname &lt;chr&gt;, necta &lt;chr&gt;, …\n\n\nThis downloads the census tract shapefiles:\n\nallegheny_tracts &lt;- get_decennial(geography = \"tract\",\n                           variables = c(total_pop = \"P001001\"),\n                           state = \"PA\",\n                           county = \"Allegheny County\",\n                           geometry = TRUE,\n                           output = \"wide\",\n                           year = 2010)\n\nallegheny_tracts\n\nSimple feature collection with 402 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -80.36087 ymin: 40.19435 xmax: -79.68885 ymax: 40.67494\nGeodetic CRS:  NAD83\n# A tibble: 402 × 4\n   GEOID       NAME                          total_pop                  geometry\n   &lt;chr&gt;       &lt;chr&gt;                             &lt;dbl&gt;        &lt;MULTIPOLYGON [°]&gt;\n 1 42003412002 Census Tract 4120.02, Allegh…      4865 (((-80.07936 40.58043, -…\n 2 42003413100 Census Tract 4131, Allegheny…      6609 (((-80.06788 40.60846, -…\n 3 42003413300 Census Tract 4133, Allegheny…      4742 (((-80.03822 40.55349, -…\n 4 42003416000 Census Tract 4160, Allegheny…      1636 (((-79.77054 40.56028, -…\n 5 42003417200 Census Tract 4172, Allegheny…      1260 (((-79.78122 40.54113, -…\n 6 42003423000 Census Tract 4230, Allegheny…      2801 (((-79.90692 40.4871, -7…\n 7 42003426800 Census Tract 4268, Allegheny…      5369 (((-79.94408 40.53137, -…\n 8 42003428100 Census Tract 4281, Allegheny…      1242 (((-79.97941 40.47738, -…\n 9 42003429500 Census Tract 4295, Allegheny…      4212 (((-80.01937 40.55063, -…\n10 42003431100 Census Tract 4311, Allegheny…      3380 (((-80.05242 40.49402, -…\n# ℹ 392 more rows\n\n\nThis is the shapefile of the rivers:\n\nrivers &lt;- st_read(\"post_data/Allegheny_County_Major_Rivers/Allegheny_County_Major_Rivers.shp\") %&gt;% \n  group_by(NAME) %&gt;% \n  summarize() %&gt;% \n  filter(!is.na(NAME))\n\nReading layer `Allegheny_County_Major_Rivers' from data source \n  `/Users/conorotompkins/Documents/github_repos/ctompkins_quarto_blog/posts/how-many-pittsburghers-cross-the-river-to-get-to-work/post_data/Allegheny_County_Major_Rivers/Allegheny_County_Major_Rivers.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 4 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -80.23017 ymin: 40.19435 xmax: -79.68877 ymax: 40.66965\nGeodetic CRS:  WGS 84\n\nrivers\n\nSimple feature collection with 3 features and 1 field\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -80.23017 ymin: 40.19435 xmax: -79.68877 ymax: 40.66965\nGeodetic CRS:  WGS 84\n# A tibble: 3 × 2\n  NAME                                                                  geometry\n* &lt;chr&gt;                                                            &lt;POLYGON [°]&gt;\n1 Allegheny River   ((-80.01324 40.44201, -80.01323 40.44203, -80.01316 40.4421…\n2 Monongahela River ((-80.01541 40.43983, -80.01531 40.43984, -80.01519 40.4396…\n3 Ohio River        ((-80.01329 40.44192, -80.01423 40.4447, -80.01433 40.4447,…\n\n\nThese are the rivers:\n\nrivers %&gt;% \n  ggplot() +\n    geom_sf(aes(color = NAME), show.legend = FALSE) +\n    geom_sf_label(aes(label = NAME, fill = NAME), show.legend = FALSE) +\n    theme_graph()\n\n\n\n\n\n\n\n\nThis shows the outlines of the tracts used in the analysis.\n\nallegheny_tracts %&gt;% \n  ggplot() +\n    geom_sf() +\n    theme_minimal()\n\n\n\n\n\n\n\n\nNext I read in the main LODES data. This is a big file, so it takes a moment.\n\ndf &lt;- vroom(\"post_data/pa_od_main_JT00_2017.csv.gz\", col_types = cols(.default = \"c\")) %&gt;% \n  mutate(S000 = as.numeric(S000)) %&gt;% \n  select(h_geocode, w_geocode, S000)\n\ndf\n\n# A tibble: 5,034,208 × 3\n   h_geocode       w_geocode        S000\n   &lt;chr&gt;           &lt;chr&gt;           &lt;dbl&gt;\n 1 420010301022005 420010301011003     1\n 2 420010303002001 420010301011003     1\n 3 420010301023038 420010301011012     1\n 4 420010314011078 420010301011012     1\n 5 420010301011027 420010301011016     1\n 6 420010301011033 420010301011016     1\n 7 420010301011038 420010301011016     1\n 8 420010301011116 420010301011016     1\n 9 420010301011123 420010301011016     1\n10 420010302001018 420010301011016     1\n# ℹ 5,034,198 more rows\n\n\nNext I summarize the number of commuters per home-work tract combination. The original file uses census block codes, which are too granular for this analysis. I link the blocks to census tracts and aggregate to that level.\n\ndf_tracts_summarized &lt;- df %&gt;% \n  group_by(h_geocode, w_geocode) %&gt;% \n  summarize(commuters = sum(S000)) %&gt;% \n  ungroup() %&gt;% \n  arrange(desc(commuters))\n\ndf_tracts_summarized &lt;- df_tracts_summarized %&gt;% \n  left_join(geo_crosswalk %&gt;% select(tabblk2010, trct), by = c(\"h_geocode\" = \"tabblk2010\")) %&gt;% \n  rename(h_tract = trct) %&gt;% \n  left_join(geo_crosswalk %&gt;% select(tabblk2010, trct), by = c(\"w_geocode\" = \"tabblk2010\")) %&gt;% \n  rename(w_tract = trct)\n\ndf_tracts_summarized &lt;- df_tracts_summarized %&gt;% \n  group_by(h_tract, w_tract) %&gt;% \n  summarize(commuters = sum(commuters)) %&gt;% \n  ungroup() %&gt;% \n  arrange(desc(commuters))\n\ndf_tracts_summarized &lt;- df_tracts_summarized %&gt;% \n  semi_join(allegheny_tracts, by = c(\"h_tract\" = \"GEOID\")) %&gt;% \n  semi_join(allegheny_tracts, by = c(\"w_tract\" = \"GEOID\"))\n\n# df_tracts_summarized %&gt;% \n#    summarize(jobs = sum(commuters))\n# 479006 total commuters\n\nThis code finds the center of each tract, which I use as the nodes in the network plots:\n\nallegheny_tracts &lt;- allegheny_tracts %&gt;% \n  arrange(GEOID)\n\nallegheny_tracts_centroids &lt;- cbind(allegheny_tracts,\n                                    st_coordinates(st_centroid(allegheny_tracts))) %&gt;% \n  st_set_geometry(NULL) %&gt;% \n  as_tibble() %&gt;% \n  rename(x = X,\n         y = Y) %&gt;% \n  select(GEOID, x, y)\n\nThis shows that the centroids correctly appear in the center of each tract:\n\nallegheny_tracts %&gt;% \n  ggplot() +\n    geom_sf() +\n    geom_point(data = allegheny_tracts_centroids, aes(x, y), size = .2) +\n    geom_sf(data = rivers, aes(color = NAME), show.legend = FALSE) +\n    geom_sf_label(data = rivers, aes(color = NAME, label = NAME),\n                  show.legend = FALSE) +\n    theme_void()\n\n\n\n\n\n\n\n\nHere I filter on commuter lines that have at least 25 commuters.\n\ng &lt;- df_tracts_summarized %&gt;% \n  as_tbl_graph(directed = TRUE) %&gt;% \n  activate(edges) %&gt;% \n  filter(commuters &gt; 25)\n\ng\n\n# A tbl_graph: 402 nodes and 2969 edges\n#\n# A directed multigraph with 17 components\n#\n# Edge Data: 2,969 × 3 (active)\n    from    to commuters\n   &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;\n 1     1     1       723\n 2     2     1       620\n 3     2     2       488\n 4     3     1       487\n 5     4     1       442\n 6     5     1       399\n 7     6     1       371\n 8     7     1       364\n 9     8     1       358\n10     9     1       355\n# ℹ 2,959 more rows\n#\n# Node Data: 402 × 1\n  name       \n  &lt;chr&gt;      \n1 42003020100\n2 42003409000\n3 42003191800\n# ℹ 399 more rows\n\n# df_tracts_summarized %&gt;%\n#   as_tbl_graph(directed = TRUE) %&gt;%\n#   activate(edges) %&gt;%\n#   filter(commuters &gt; 25) %&gt;%\n#   as_tibble() %&gt;%\n#   summarize(jobs = sum(commuters))\n# 184404 total commuters\n\nHere I set a manual layout for the ggraph object. I use the centroids of the census tracts as the nodes in the network graph.\n\nnode_pos &lt;- allegheny_tracts_centroids\n\nmanual_layout &lt;- create_layout(graph = g,\n                               layout = node_pos)\n\nmanual_layout %&gt;% \n  as_tibble()\n\n# A tibble: 402 × 7\n   GEOID           x     y name        .ggraph.orig_index .ggraph.index circular\n   &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;                    &lt;int&gt;         &lt;int&gt; &lt;lgl&gt;   \n 1 42003010300 -80.0  40.4 42003020100                  1             1 FALSE   \n 2 42003020100 -80.0  40.4 42003409000                  2             2 FALSE   \n 3 42003020300 -80.0  40.5 42003191800                  3             3 FALSE   \n 4 42003030500 -80.0  40.4 42003412001                  4             4 FALSE   \n 5 42003040200 -80.0  40.4 42003411000                  5             5 FALSE   \n 6 42003040400 -79.9  40.4 42003456003                  6             6 FALSE   \n 7 42003040500 -80.0  40.4 42003191700                  7             7 FALSE   \n 8 42003040600 -80.0  40.4 42003413100                  8             8 FALSE   \n 9 42003040900 -80.0  40.4 42003473100                  9             9 FALSE   \n10 42003050100 -80.0  40.4 42003426300                 10            10 FALSE   \n# ℹ 392 more rows\n\n\nThis graphs the commuter lines on top of the census tracts and rivers:\n\nggraph(manual_layout) +\n  geom_sf(data = allegheny_tracts, color = \"dark grey\", fill = NA) +\n  geom_sf(data = rivers, aes(color = NAME), show.legend = FALSE) +\n  geom_edge_fan(aes(edge_width = log10(commuters), \n                    edge_alpha = log10(commuters)),\n                arrow = arrow(length = unit(.5, 'lines')), \n                start_cap = circle(.1, 'lines'),\n                end_cap = circle(.2, 'lines'),\n                color = \"white\",\n                strength = .5) +\n  scale_edge_width_continuous(range = c(.1, 1)) +\n  scale_edge_alpha_continuous(range = c(.01, .4)) +\n  labs(x = NULL,\n       y = NULL,\n       title = \"Where do people commute from/to for work?\",\n       subtitle = \"Excludes within-tract commuters\",\n       caption = \"Based on 2017 US Census LODES dataset | @conor_tompkins\") +\n  theme_graph() +\n  theme(legend.background = element_rect(fill = \"black\"),\n        legend.text = element_text(color = \"white\"),\n        legend.title = element_text(color = \"white\"),\n        panel.background = element_rect(fill = \"black\"))\n\n\n\n\n\n\n\n\nThis calculates the centroids I will use to draw lines later on:\n\nallegheny_lines &lt;- cbind(allegheny_tracts, st_coordinates(st_centroid(allegheny_tracts))) %&gt;% \n  select(-c(NAME, total_pop)) %&gt;% \n  st_drop_geometry()\n\nallegheny_lines %&gt;% \n  ggplot() +\n    geom_point(aes(X, Y)) +\n    theme_minimal()\n\n\n\n\n\n\n\n\nHere I calculate the edges and nodes for the network graph:\n\ndf_edges &lt;- g %&gt;% \n  activate(edges) %&gt;% \n  as_tibble()\n\n# df_edges %&gt;%\n#   summarize(commuters = sum(commuters))\n# 184404 total commuters\n\ndf_nodes &lt;- g %&gt;% \n  activate(nodes) %&gt;% \n  as_tibble() %&gt;% \n  mutate(id = row_number())\n\nThe df_lines is pivoted long so there is a “to” and “from” row for each commuter line:\n\ndf_lines &lt;- df_edges %&gt;% \n  mutate(line_id = row_number()) %&gt;% \n  pivot_longer(c(from, to), names_to = \"point_type\", values_to = \"edge_id\")\n\ndf_lines\n\n# A tibble: 5,938 × 4\n   commuters line_id point_type edge_id\n       &lt;dbl&gt;   &lt;int&gt; &lt;chr&gt;        &lt;int&gt;\n 1       723       1 from             1\n 2       723       1 to               1\n 3       620       2 from             2\n 4       620       2 to               1\n 5       488       3 from             2\n 6       488       3 to               2\n 7       487       4 from             3\n 8       487       4 to               1\n 9       442       5 from             4\n10       442       5 to               1\n# ℹ 5,928 more rows\n\n# df_lines %&gt;%\n#   distinct(line_id, commuters) %&gt;%\n#   summarize(jobs = sum(commuters))\n# 184404 total commuters\n\nSince some commuter “lines” are really just points that start and end at the same centroid, I separate the commuter “lines” from the “points” for purposes of manipulating the geometries.\n\ndf_line_types &lt;- df_lines %&gt;% \n  pivot_wider(names_from = point_type, values_from = edge_id) %&gt;% \n  mutate(line_type = case_when(from == to ~ \"point\",\n                               from != to ~ \"linestring\")) %&gt;% \n  pivot_longer(cols = c(from, to), names_to = \"edge_type\", values_to = \"edge_id\")\n\ndf_line_types\n\n# A tibble: 5,938 × 5\n   commuters line_id line_type  edge_type edge_id\n       &lt;dbl&gt;   &lt;int&gt; &lt;chr&gt;      &lt;chr&gt;       &lt;int&gt;\n 1       723       1 point      from            1\n 2       723       1 point      to              1\n 3       620       2 linestring from            2\n 4       620       2 linestring to              1\n 5       488       3 point      from            2\n 6       488       3 point      to              2\n 7       487       4 linestring from            3\n 8       487       4 linestring to              1\n 9       442       5 linestring from            4\n10       442       5 linestring to              1\n# ℹ 5,928 more rows\n\n# df_line_types %&gt;%\n#   distinct(line_id, commuters) %&gt;%\n#   summarize(commuters = sum(commuters))\n# # 184404 total commuters\n\n\ndf_linestrings &lt;- df_line_types %&gt;% \n  filter(line_type == \"linestring\")\n\ndf_points &lt;- df_line_types %&gt;% \n  filter(line_type == \"point\")\n\nThis creates the geometries for the lines, points, and rivers. Then I set them all to the same coordinate system with st_set_crs.\n\ndf_linestrings &lt;- df_linestrings %&gt;% \n  left_join(df_nodes, by = c(\"edge_id\" = \"id\")) %&gt;% \n  left_join(allegheny_lines, by = c(\"name\" = \"GEOID\")) %&gt;% \n  st_as_sf(coords = c(\"X\", \"Y\")) %&gt;% \n  group_by(line_id, commuters) %&gt;%\n  summarise() %&gt;% # union points into lines using our created lineid\n  st_cast(\"LINESTRING\") %&gt;% \n  st_set_crs(4326)\n\n# df_linestrings %&gt;% \n#   ungroup() %&gt;% \n#   st_drop_geometry() %&gt;% \n#   summarize(commuters = sum(commuters))\n\n# 167409 commuters that change tracts\n\ndf_points &lt;- df_points %&gt;% \n  left_join(df_nodes, by = c(\"edge_id\" = \"id\")) %&gt;% \n  left_join(allegheny_lines, by = c(\"name\" = \"GEOID\")) %&gt;% \n  st_as_sf(coords = c(\"X\", \"Y\")) %&gt;% \n  group_by(line_id, commuters) %&gt;%\n  summarise() %&gt;%\n  st_cast(\"POINT\") %&gt;% \n  st_set_crs(4326)\n\n# df_points %&gt;% \n#   ungroup() %&gt;% \n#   st_drop_geometry() %&gt;% \n#   summarize(commuters = sum(commuters))\n\n# 16995 commuters that stay within a tract\n\n\nrivers &lt;- rivers %&gt;% \n  st_set_crs(4326)\n\nHere I calculate which commuter lines intersect with which rivers using st_intersects:\n\ndf_linestrings_intersect &lt;- df_linestrings %&gt;% \n  ungroup() %&gt;% \n  mutate(intersects_ohio = st_intersects(., rivers %&gt;% \n                                            filter(NAME == \"Ohio River\")) %&gt;% as.logical(),\n         intersects_allegheny = st_intersects(., rivers %&gt;% \n                                                filter(NAME == \"Allegheny River\")) %&gt;% as.logical(),\n         intersects_monongahela = st_intersects(., rivers %&gt;% \n                                                  filter(NAME == \"Monongahela River\")) %&gt;% as.logical())\n\ndf_commuter_rivers &lt;- df_linestrings_intersect %&gt;% \n  pivot_longer(c(contains(\"intersects\")), names_to = \"river_intersected\", values_to = \"value\") %&gt;% \n  mutate(value = case_when(is.na(value) ~ FALSE,\n                           !is.na(value) ~ value))\n \n# df_commuter_rivers %&gt;%\n#   distinct(line_id, commuters) %&gt;%\n#   summarize(jobs = sum(commuters))\n\n# 167409 commuters that change tracts\n\nThis shows that the intersection calculation was successful:\n\ndf_commuter_rivers %&gt;% \n  filter(value == TRUE) %&gt;% \n  ggplot() +\n    geom_sf(data = allegheny_tracts, color = NA, show.legend = FALSE) +\n    geom_sf(data = rivers, \n            aes(color = NAME),\n            show.legend = FALSE) +\n    geom_sf(aes(geometry = geometry,\n                size = commuters),\n            show.legend = TRUE) +\n    facet_wrap(~river_intersected,\n               ncol = 1) +\n    guides(color = FALSE,\n           size = FALSE) +\n    theme_graph() +\n    scale_size_continuous(range = c(.1, .5))\n\n\n\n\n\n\n\n\nThis combines the dataframes with the lines and points, and then summarizes to count how many of the geometries intersected with a river:\n\n#this was double counting\ndf_commuter_rivers_combined &lt;- df_commuter_rivers %&gt;% \n  bind_rows(df_points %&gt;% \n              mutate(value = FALSE))\n\n#use for lookup\ndf_lines_that_cross_rivers &lt;- df_commuter_rivers_combined %&gt;% \n  group_by(line_id) %&gt;% \n  summarize(rivers_crossed = sum(value)) %&gt;% \n  ungroup()\n\n#find the distinct line_ids and then summarize \ndf_commuter_rivers_summary &lt;- df_commuter_rivers_combined %&gt;% \n  distinct(line_id, commuters) %&gt;% \n  left_join(df_lines_that_cross_rivers) %&gt;% \n  group_by(rivers_crossed) %&gt;% \n  summarize(commuters = sum(commuters)) %&gt;% \n  ungroup()\n  \n\n# df_commuter_rivers_summary %&gt;% \n#   summarize(commuters = sum(commuters))\n# 184404 total commuters\n\ndf_commuter_rivers_summary %&gt;% \n  ggplot(aes(rivers_crossed, commuters)) +\n    geom_col(color = \"black\") +\n    geom_text(aes(y = commuters + 5000, label = scales::comma(commuters))) +\n    scale_y_continuous(labels = scales::comma) +\n    labs(title = \"Commuter travel patterns\",\n         subtitle = \"2017 U.S. Census LODES dataset\",\n         x = \"Rivers crossed\",\n         y = \"Number of commuters\",\n         caption = \"@conor_tompkins\") +\n    theme_bw()\n\n\n\n\n\n\n\n\nIt is always reassuring when your analysis doesn’t stray to far from conventional wisdom. Very few Pittsburghers in the dataset cross two rivers to get to work, and none cross three."
  },
  {
    "objectID": "posts/how-many-pittsburghers-cross-the-river-to-get-to-work/index.html#mapping-commuter-patterns",
    "href": "posts/how-many-pittsburghers-cross-the-river-to-get-to-work/index.html#mapping-commuter-patterns",
    "title": "How Many Pittsburghers Cross the River to Get to Work",
    "section": "Mapping commuter patterns",
    "text": "Mapping commuter patterns\nThe next step is to put this data on a map, since it is obviously spatial. The goal is to calculate the percentage of each census tract’s “from” commuters that crossed zero, one, two, and 3 rivers.\nThis prepares the edge data to be used to make a chloropleth map:\n\ndf_lines_chloro &lt;- df_edges %&gt;% \n  mutate(line_id = row_number()) %&gt;% \n  pivot_longer(c(from, to), names_to = \"point_type\", values_to = \"edge_id\")\n\ndf_lines_chloro\n\n# A tibble: 5,938 × 4\n   commuters line_id point_type edge_id\n       &lt;dbl&gt;   &lt;int&gt; &lt;chr&gt;        &lt;int&gt;\n 1       723       1 from             1\n 2       723       1 to               1\n 3       620       2 from             2\n 4       620       2 to               1\n 5       488       3 from             2\n 6       488       3 to               2\n 7       487       4 from             3\n 8       487       4 to               1\n 9       442       5 from             4\n10       442       5 to               1\n# ℹ 5,928 more rows\n\n# df_lines_chloro %&gt;%\n#   distinct(line_id, commuters) %&gt;%\n#   summarize(commuters = sum(commuters))\n# 184404 total commuters\n\nThe next few steps are largely just coercing the geometry to do what I want it to do. Interested parties can read the code.\n\ndf_line_types_chloro &lt;- df_lines_chloro %&gt;% \n  pivot_wider(names_from = point_type, values_from = edge_id) %&gt;% \n  mutate(line_type = case_when(from == to ~ \"point\",\n                               from != to ~ \"linestring\")) %&gt;% \n  pivot_longer(cols = c(from, to), names_to = \"edge_type\", values_to = \"edge_id\")\n\n# df_line_types_chloro %&gt;%\n#    distinct(line_id, commuters) %&gt;%\n#    summarize(commuters = sum(commuters))\n#184404 total commuters\n\n\ndf_linestrings_chloro &lt;- df_line_types_chloro %&gt;% \n  filter(line_type == \"linestring\")\n\n# df_linestrings_chloro %&gt;%\n#   distinct(line_id, commuters) %&gt;%\n#   summarize(commuters = sum(commuters))\n#167409 commuters that change tracts\n\ndf_linestrings_chloro_lookup &lt;- df_linestrings_chloro %&gt;% \n  select(line_id, edge_type, edge_id) %&gt;% \n  pivot_wider(names_from = edge_type, values_from = edge_id)\n\ndf_points_chloro &lt;- df_line_types %&gt;% \n  filter(line_type == \"point\")\n\n# df_points_chloro %&gt;%\n#   distinct(line_id, commuters) %&gt;%\n#   summarize(commuters = sum(commuters))\n#16995 commuters that stay within a tract\n\n\ndf_linestrings_chloro &lt;- df_linestrings_chloro %&gt;% \n  left_join(df_nodes, by = c(\"edge_id\" = \"id\")) %&gt;% \n  left_join(allegheny_lines, by = c(\"name\" = \"GEOID\")) %&gt;% \n  st_as_sf(coords = c(\"X\", \"Y\")) %&gt;% \n  group_by(line_id, commuters) %&gt;%\n  summarise() %&gt;% # union points into lines using our created lineid\n  st_cast(\"LINESTRING\") %&gt;% \n  st_set_crs(4326) %&gt;% \n  left_join(df_linestrings_chloro_lookup, by = c(\"line_id\" = \"line_id\")) %&gt;% \n  left_join(df_nodes, by = c(\"from\" = \"id\")) %&gt;% \n  left_join(allegheny_lines, by = c(\"name\" = \"GEOID\"))\n\n# df_linestrings_chloro %&gt;%\n#   ungroup() %&gt;%\n#   st_drop_geometry() %&gt;%\n#   summarize(commuters = sum(commuters))\n#167409 commuters that change tracts\n\ndf_points_chloro &lt;- df_points_chloro %&gt;% \n  left_join(df_nodes, by = c(\"edge_id\" = \"id\")) %&gt;% \n  left_join(allegheny_lines, by = c(\"name\" = \"GEOID\")) %&gt;% \n  st_as_sf(coords = c(\"X\", \"Y\")) %&gt;% \n  group_by(line_id, name, commuters) %&gt;%\n  st_cast(\"POINT\") %&gt;% \n  st_set_crs(4326)\n\n# df_points_chloro %&gt;%\n#   ungroup() %&gt;%\n#   st_drop_geometry() %&gt;%\n#   distinct(line_id, commuters) %&gt;%\n#   summarize(commuters = sum(commuters))\n#16995 commuters that stay within a tract\n\n\ndf_linestrings_chloro_intersect &lt;- df_linestrings_chloro %&gt;% \n  ungroup() %&gt;% \n  mutate(intersects_ohio = st_intersects(., rivers %&gt;% \n                                            filter(NAME == \"Ohio River\")) %&gt;% as.logical(),\n         intersects_allegheny = st_intersects(., rivers %&gt;% \n                                                filter(NAME == \"Allegheny River\")) %&gt;% as.logical(),\n         intersects_monongahela = st_intersects(., rivers %&gt;% \n                                                  filter(NAME == \"Monongahela River\")) %&gt;% as.logical()) %&gt;% \n  st_set_geometry(NULL) %&gt;% \n  select(-c(from, to)) %&gt;% \n  rename(from = name)\n\n# df_linestrings_chloro_intersect %&gt;%\n#   summarize(commuters = sum(commuters))\n#167409 commuters that change tracts\n\n\ndf_linestrings_chloro_intersect &lt;- df_linestrings_chloro_intersect %&gt;% \n  mutate_at(vars(contains(\"intersect\")), ~case_when(is.na(.) ~ FALSE,\n                           !is.na(.) ~ .)) %&gt;%\n  mutate(no_intersect = case_when(intersects_allegheny == FALSE & intersects_monongahela == FALSE & intersects_ohio == FALSE ~ TRUE,\n                                  TRUE ~ FALSE)) %&gt;% \n  select(line_id, from, contains(\"intersect\"), commuters) %&gt;% \n  pivot_longer(contains(\"intersect\"), names_to = \"river_intersected\", values_to = \"value\")\n\n# df_linestrings_chloro_intersect %&gt;%\n#   distinct(line_id, commuters) %&gt;%\n#   summarize(commuters = sum(commuters))\n#167409 commuters that change tracts\n\n\ndf_linestrings_chloro_intersect &lt;- df_linestrings_chloro_intersect %&gt;% \n  mutate(commuters = case_when(value == FALSE ~ 0,\n                               TRUE ~ commuters))\n\n# df_linestrings_chloro_intersect %&gt;%\n#   distinct(line_id, commuters) %&gt;%\n#   summarize(commuters = sum(commuters))\n#167409 commuters that change tracts\n\n\ndf_linestrings_chloro_intersect &lt;- df_linestrings_chloro_intersect %&gt;% \n  filter(value == TRUE) %&gt;% \n  group_by(line_id, from, commuters) %&gt;% \n  summarize(count_rivers_intersected = sum(river_intersected != \"no_intersect\")) %&gt;% \n  ungroup()\n\n# df_linestrings_chloro_intersect %&gt;%\n#   summarize(commuters = sum(commuters))\n#167409 commuters that change tracts  \n\n\ndf_points_chloro &lt;- cbind(df_points_chloro, st_coordinates(st_centroid(df_points_chloro))) %&gt;% \n  st_drop_geometry() %&gt;% \n  rename(from = name) %&gt;% \n  distinct(from, line_id, commuters) %&gt;% \n  mutate(count_rivers_intersected = 0)\n  \n\n# df_points_chloro %&gt;%\n#    summarize(commuters = sum(commuters))\n#16995 commuters that stay within a tract\n\n\ndf_combined &lt;- bind_rows(df_linestrings_chloro_intersect, df_points_chloro)\n\n# df_combined %&gt;%\n#     filter(is.na(from))\n# \n# df_combined %&gt;%\n#    summarize(commuters = sum(commuters))\n#184404 total commuters\n\n\ndf_combined &lt;- df_combined %&gt;% \n  arrange(from, desc(count_rivers_intersected), desc(commuters))\n\nThese are the final steps to create the chloropleth:\n\ndf_combined\n\n# A tibble: 2,969 × 4\n   line_id from        commuters count_rivers_intersected\n     &lt;int&gt; &lt;chr&gt;           &lt;dbl&gt;                    &lt;dbl&gt;\n 1     748 42003010300        63                        0\n 2    2192 42003020100        31                        2\n 3    1372 42003020100        42                        1\n 4    2282 42003020100        30                        1\n 5       1 42003020100       723                        0\n 6     632 42003020100        70                        0\n 7     730 42003020100        64                        0\n 8    1102 42003020100        49                        0\n 9    1761 42003020100        36                        0\n10     350 42003020300       112                        0\n# ℹ 2,959 more rows\n\n\nThis counts the number of commuters per “from” tract and “count of rivers intersected”. Note that there are multiple rows per “from” tract.\n\ndf_chloro_map &lt;- df_combined %&gt;%\n  ungroup() %&gt;% \n  group_by(from, count_rivers_intersected) %&gt;% \n  summarize(total_commuters = sum(commuters)) %&gt;% \n  ungroup()\n\ndf_chloro_map\n\n# A tibble: 723 × 3\n   from        count_rivers_intersected total_commuters\n   &lt;chr&gt;                          &lt;dbl&gt;           &lt;dbl&gt;\n 1 42003010300                        0              63\n 2 42003020100                        0             942\n 3 42003020100                        1              72\n 4 42003020100                        2              31\n 5 42003020300                        0             258\n 6 42003030500                        0             231\n 7 42003040200                        0             143\n 8 42003040400                        0             108\n 9 42003040500                        0              87\n10 42003040600                        0              59\n# ℹ 713 more rows\n\n# df_chloro_map %&gt;%\n#   filter(is.na(count_rivers_intersected))\n# \n# df_chloro_map %&gt;% \n#     summarize(commuters = sum(total_commuters))\n#184404 total commuters\n\nThe next step is to calculate the percent of a “from” tract’s commuters that crossed a given number of rivers:\n\ndf_chloro_map &lt;- df_chloro_map %&gt;% \n  group_by(from) %&gt;% \n  mutate(pct_of_commuters = total_commuters / sum(total_commuters)) %&gt;% \n  ungroup()\n\ndf_chloro_map\n\n# A tibble: 723 × 4\n   from        count_rivers_intersected total_commuters pct_of_commuters\n   &lt;chr&gt;                          &lt;dbl&gt;           &lt;dbl&gt;            &lt;dbl&gt;\n 1 42003010300                        0              63           1     \n 2 42003020100                        0             942           0.901 \n 3 42003020100                        1              72           0.0689\n 4 42003020100                        2              31           0.0297\n 5 42003020300                        0             258           1     \n 6 42003030500                        0             231           1     \n 7 42003040200                        0             143           1     \n 8 42003040400                        0             108           1     \n 9 42003040500                        0              87           1     \n10 42003040600                        0              59           1     \n# ℹ 713 more rows\n\n# df_chloro_map %&gt;% \n#   summarize(commuters = sum(total_commuters))\n#184404 total commuters\n\nThen I join df_chloro_map against the census tract geometry to get a complete list of all the tracts. I use complete to add rows for combinations of “from” tracts and count_rivers_intersected that did not appear in the data. Those added rows are given 0 for pct_of_commuters and total_commuters.\n\ndf_chloro_map &lt;- df_chloro_map %&gt;% \n  right_join(allegheny_tracts %&gt;% select(GEOID) %&gt;% st_set_geometry(NULL), by = c(\"from\" = \"GEOID\")) %&gt;% \n  complete(from, count_rivers_intersected = c(0, 1, 2)) %&gt;%\n  filter(!is.na(count_rivers_intersected)) %&gt;% #exclude tracts brought in from the right_join\n  replace_na(list(pct_of_commuters = 0, total_commuters = 0))\n\n# df_chloro_map %&gt;% \n#   filter(is.na(count_rivers_intersected))\n# \n# df_chloro_map %&gt;% \n#   summarize(commuters = sum(total_commuters, na.rm = TRUE))\n#184404 total commuters\n\nThe final step is to right join againt the census tract data to bring over the geometry.\n\ndf_chloro_map &lt;- df_chloro_map %&gt;% \n  right_join(allegheny_tracts, by = c(\"from\" = \"GEOID\"))\n\nglimpse(df_chloro_map)\n\nRows: 1,206\nColumns: 7\n$ from                     &lt;chr&gt; \"42003010300\", \"42003010300\", \"42003010300\", …\n$ count_rivers_intersected &lt;dbl&gt; 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, …\n$ total_commuters          &lt;dbl&gt; 63, 0, 0, 942, 72, 31, 258, 0, 0, 231, 0, 0, …\n$ pct_of_commuters         &lt;dbl&gt; 1.00000000, 0.00000000, 0.00000000, 0.9014354…\n$ NAME                     &lt;chr&gt; \"Census Tract 103, Allegheny County, Pennsylv…\n$ total_pop                &lt;dbl&gt; 6600, 6600, 6600, 3629, 3629, 3629, 616, 616,…\n$ geometry                 &lt;MULTIPOLYGON [°]&gt; MULTIPOLYGON (((-79.98077 4..., …\n\n\n\ndf_chloro_map %&gt;% \n  mutate(count_rivers_intersected = str_c(\"Rivers intersected:\", count_rivers_intersected, sep = \" \")) %&gt;% \n  ggplot() +\n    geom_sf(aes(geometry = geometry,\n                fill = pct_of_commuters),\n            color = NA) +\n    geom_sf(data = rivers,\n            aes(color = NAME),\n            size = 1,\n            show.legend = FALSE) +\n    facet_wrap(~count_rivers_intersected,\n               nrow = 1) +\n    scale_fill_viridis_c(\"% of commuters\",\n                         labels = scales::percent) +\n    labs(title = \"Commuter travel patterns\",\n         subtitle = \"2017 U.S. Census LODES dataset\",\n         caption = \"@conor_tompkins\") +\n    theme_void()\n\n\n\n\n\n\n\n\n\nErrata\nThe previous version of the bar chart double counted the commuters. This is the old version:\n\ndf_commuter_rivers_summary &lt;- df_commuter_rivers %&gt;% \n  bind_rows(df_points %&gt;% \n              mutate(value = FALSE)) %&gt;% \n  group_by(line_id) %&gt;% \n  summarize(rivers_crossed = sum(value),\n            commuters = sum(commuters))\n\ndf_commuter_rivers_summary %&gt;% \n  st_drop_geometry() %&gt;%\n  group_by(rivers_crossed) %&gt;% \n  summarize(commuters = sum(commuters)) %&gt;% \n  ggplot(aes(rivers_crossed, commuters)) +\n    geom_col() +\n    scale_y_continuous(labels = scales::comma) +\n    labs(title = \"Commuter travel patterns\",\n         subtitle = \"2017 U.S. Census LODES dataset\",\n         x = \"Rivers crossed\",\n         y = \"Number of commuters\",\n         caption = \"@conor_tompkins\") +\n    theme_bw()\n\n\n\n\n\n\n\n\nA similar but less impactful bug affected the chloropleth chart. For completeness, this is the old verison of that graph: \n\n\nReferences\n\nhttps://lehd.ces.census.gov/data/\nhttps://lehd.ces.census.gov/data/lodes/LODES7/LODESTechDoc7.4.pdf\nhttps://lehd.ces.census.gov/doc/workshop/2017/Presentations/TheaEvans.pdf\nhttps://medium.com/@urban_institute/open-accessible-data-on-jobs-and-workers-tract-level-lodes-data-945fcac9e280"
  },
  {
    "objectID": "posts/r-311-pothole-workshop-code-for-pittsburgh/index.html",
    "href": "posts/r-311-pothole-workshop-code-for-pittsburgh/index.html",
    "title": "R 311 Pothole Workshop Code for Pittsburgh",
    "section": "",
    "text": "This material was presented at Code & Supply in 2018."
  },
  {
    "objectID": "posts/r-311-pothole-workshop-code-for-pittsburgh/index.html#basic-functions",
    "href": "posts/r-311-pothole-workshop-code-for-pittsburgh/index.html#basic-functions",
    "title": "R 311 Pothole Workshop Code for Pittsburgh",
    "section": "Basic Functions",
    "text": "Basic Functions\n\nadd\nsubtract\nstrings\n\n\n1\n\n[1] 1\n\n\n\n1 + 2\n\n[1] 3\n\n\n\n10 / 2\n\n[1] 5\n\n\n\n5 * 2\n\n[1] 10\n\n\n\n\"this is a string. strings in R are surrounded by quotation marks.\"\n\n[1] \"this is a string. strings in R are surrounded by quotation marks.\"\n\n\nType matters\n\n\"1\" + 1\n\nError in \"1\" + 1: non-numeric argument to binary operator\n\n\nstr() checks the type of the object\n\nstr(1)\n\n num 1\n\nstr(\"1\")\n\n chr \"1\""
  },
  {
    "objectID": "posts/r-311-pothole-workshop-code-for-pittsburgh/index.html#objects-functions-and-assignment",
    "href": "posts/r-311-pothole-workshop-code-for-pittsburgh/index.html#objects-functions-and-assignment",
    "title": "R 311 Pothole Workshop Code for Pittsburgh",
    "section": "Objects, Functions, and Assignment",
    "text": "Objects, Functions, and Assignment\nReminder that objects are shown in the Environment panel (top right panel)\n\nx\n\nError in eval(expr, envir, enclos): object 'x' not found\n\n\nYou assign values to objects using “&lt;-”\n\nx &lt;- 1\nx \n\n[1] 1\n\n\nType out the object’s name and execute it to print it in the console\nYou can overwrite (or update) an object’s value\n\nx &lt;- 2\nx\n\n[1] 2\n\n\nYou can manipulate objects with operators\n\nx &lt;- 1\ny &lt;- 5\n\nx + y\n\n[1] 6\n\n\nc() means “concatenate”. It creates vectors\n\na &lt;- c(x, y)\na\n\n[1] 1 5\n\n\n: creates a sequence of numbers\n\n1:10\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nYou can perform functions on objects\n\nz &lt;- sum(a)\nz\n\n[1] 6"
  },
  {
    "objectID": "posts/r-311-pothole-workshop-code-for-pittsburgh/index.html#dataframes",
    "href": "posts/r-311-pothole-workshop-code-for-pittsburgh/index.html#dataframes",
    "title": "R 311 Pothole Workshop Code for Pittsburgh",
    "section": "Dataframes",
    "text": "Dataframes\nDataframes are rectangular objects that consist of rows and columns, similar to what you see in an Excel spreadsheet\n\nmy_df &lt;- data.frame(a = 1:5,\n                b = 6:10,\n                c = c(\"a\", \"b\", \"c\", \"d\", \"e\"))\nmy_df\n\n  a  b c\n1 1  6 a\n2 2  7 b\n3 3  8 c\n4 4  9 d\n5 5 10 e\n\n\nSelect individual columns in a dataframe with the $ operator\n\nmy_df$a\n\n[1] 1 2 3 4 5\n\n\n“&lt;-” and “=” do the same thing. To minimize confusion, many people use “&lt;-” for objects and “=” for assigning variables within functions or dataframes\n\nx &lt;- 1\n\nz &lt;- data.frame(a = 1:5,\n                b = 6:10)\nz\n\n  a  b\n1 1  6\n2 2  7\n3 3  8\n4 4  9\n5 5 10"
  },
  {
    "objectID": "posts/r-311-pothole-workshop-code-for-pittsburgh/index.html#logic",
    "href": "posts/r-311-pothole-workshop-code-for-pittsburgh/index.html#logic",
    "title": "R 311 Pothole Workshop Code for Pittsburgh",
    "section": "Logic",
    "text": "Logic\n“x == y” means “is x equal to y?”\n\n1 == 2\n\n[1] FALSE\n\n\n“!” means “not”\n\n!FALSE\n\n[1] TRUE\n\n\nTRUE = 1, FALSE = 0\n\nTRUE + FALSE\n\n[1] 1\n\n\n\nTRUE + TRUE\n\n[1] 2\n\n\nR is case-sensitive\n\n\"a\" == \"A\"\n\n[1] FALSE"
  },
  {
    "objectID": "posts/r-311-pothole-workshop-code-for-pittsburgh/index.html#loading-packages",
    "href": "posts/r-311-pothole-workshop-code-for-pittsburgh/index.html#loading-packages",
    "title": "R 311 Pothole Workshop Code for Pittsburgh",
    "section": "Loading packages",
    "text": "Loading packages\n\nlibrary(package_name)\n\nYou have to load your packages each time you start R. Do not use quotation marks in the library() function"
  },
  {
    "objectID": "posts/r-311-pothole-workshop-code-for-pittsburgh/index.html#commenting",
    "href": "posts/r-311-pothole-workshop-code-for-pittsburgh/index.html#commenting",
    "title": "R 311 Pothole Workshop Code for Pittsburgh",
    "section": "Commenting",
    "text": "Commenting\nAny code that follows a “#” is treated as a comment, and is not executed\n\n1 + 1\n\n[1] 2\n\n#1 + 1\n#code that is \"commented out\" will not be executed\n\nComment your code to make sure you understand it. It is aso useful to other people who use your code, including Future You.\nBe kind to Future You. Comment your code."
  },
  {
    "objectID": "posts/r-311-pothole-workshop-code-for-pittsburgh/index.html#getting-help-with-r",
    "href": "posts/r-311-pothole-workshop-code-for-pittsburgh/index.html#getting-help-with-r",
    "title": "R 311 Pothole Workshop Code for Pittsburgh",
    "section": "Getting help with R",
    "text": "Getting help with R\nUse the built-in documentation. Put a “?” before the name of a function to access the documentation in the Help panel\n\n?mean\n\nStackOverflow"
  },
  {
    "objectID": "posts/r-311-pothole-workshop-code-for-pittsburgh/index.html#working-directory",
    "href": "posts/r-311-pothole-workshop-code-for-pittsburgh/index.html#working-directory",
    "title": "R 311 Pothole Workshop Code for Pittsburgh",
    "section": "Working Directory",
    "text": "Working Directory\nThe working directory is where your R scripts and your data are stored\n\nHow to set up the working directory\nThis command prints the current working directory\n\ngetwd()\n\nUse the menu to set up your working directory\nSession menu -&gt; Set working directory -&gt; choose your folder\nThis command does the same thing\n\nsetwd()"
  },
  {
    "objectID": "posts/r-311-pothole-workshop-code-for-pittsburgh/index.html#compare-to-excel",
    "href": "posts/r-311-pothole-workshop-code-for-pittsburgh/index.html#compare-to-excel",
    "title": "R 311 Pothole Workshop Code for Pittsburgh",
    "section": "Compare to Excel",
    "text": "Compare to Excel\nR separates the data from the analysis. The data is stored in files (CSV, JSON, etc). The analysis is stored in scripts. This makes it easier to share analysis performed in R. No need to take screenshots of your workflow in Excel. You have a record of everything that was done to the data. R also allows you to scale your analysis up to larger datasets and more complex workflows, where Excel would require lots of risky repetition of the same task."
  },
  {
    "objectID": "posts/r-311-pothole-workshop-code-for-pittsburgh/index.html#key-tidyverse-functions-and-operators",
    "href": "posts/r-311-pothole-workshop-code-for-pittsburgh/index.html#key-tidyverse-functions-and-operators",
    "title": "R 311 Pothole Workshop Code for Pittsburgh",
    "section": "Key Tidyverse functions and operators",
    "text": "Key Tidyverse functions and operators\n\nselect columns\nfilter rows\nmutate new columns\ngroup_by and summarize rows\nggplot2 your data\nThe pipe %&gt;%\n\n\nlibrary(tidyverse)\nlibrary(lubridate)\n\nread_csv() reads CSV files from your working directory\n\ndf &lt;- read_csv(\"your_file_name_here.csv\")\n\n\ncolnames(df) &lt;- tolower(colnames(df)) #make all the column names lowercase\n\n#initial data munging to get the dates in shape\ndf %&gt;%\n  mutate(date = ymd(str_sub(created_on, 1, 10)),\n         time = hms(str_sub(created_on, 11, 18)),\n         month = month(date, label = TRUE), \n         year = year(date),\n         yday = yday(date)) %&gt;% \n  select(-c(created_on, time)) -&gt; df\n\nExplore the data\n\ndf #type the name of the object to preview it\n\n# A tibble: 225,189 × 21\n    `_id` request_id request_type  request_origin status department neighborhood\n    &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;       \n 1 154245      54111 Rodent contr… Call Center         1 Animal Ca… Middle Hill \n 2 154246      53833 Rodent contr… Call Center         1 Animal Ca… Squirrel Hi…\n 3 154247      52574 Potholes      Call Center         1 DPW - Str… Larimer     \n 4 154248      54293 Building Wit… Control Panel       1 Permits, … &lt;NA&gt;        \n 5 154249      53560 Potholes      Call Center         1 DPW - Str… Homewood No…\n 6 154250      49519 Potholes      Call Center         1 DPW - Str… Homewood No…\n 7 154251      49484 Potholes      Call Center         1 DPW - Str… Homewood No…\n 8 154252      53787 Rodent contr… Call Center         1 Animal Ca… South Side …\n 9 154253      52887 Potholes      Call Center         1 DPW - Str… East Hills  \n10 154254      53599 Rodent contr… Call Center         1 Animal Ca… East Allegh…\n# ℹ 225,179 more rows\n# ℹ 14 more variables: council_district &lt;dbl&gt;, ward &lt;dbl&gt;, tract &lt;dbl&gt;,\n#   public_works_division &lt;dbl&gt;, pli_division &lt;dbl&gt;, police_zone &lt;dbl&gt;,\n#   fire_zone &lt;chr&gt;, x &lt;dbl&gt;, y &lt;dbl&gt;, geo_accuracy &lt;chr&gt;, date &lt;date&gt;,\n#   month &lt;ord&gt;, year &lt;dbl&gt;, yday &lt;dbl&gt;\n\n\n\nglimpse(df) #get a summary of the dataframe\n\nRows: 225,189\nColumns: 21\n$ `_id`                 &lt;dbl&gt; 154245, 154246, 154247, 154248, 154249, 154250, …\n$ request_id            &lt;dbl&gt; 54111, 53833, 52574, 54293, 53560, 49519, 49484,…\n$ request_type          &lt;chr&gt; \"Rodent control\", \"Rodent control\", \"Potholes\", …\n$ request_origin        &lt;chr&gt; \"Call Center\", \"Call Center\", \"Call Center\", \"Co…\n$ status                &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ department            &lt;chr&gt; \"Animal Care & Control\", \"Animal Care & Control\"…\n$ neighborhood          &lt;chr&gt; \"Middle Hill\", \"Squirrel Hill North\", \"Larimer\",…\n$ council_district      &lt;dbl&gt; 6, 8, 9, NA, 9, 9, 9, 3, 9, 1, 4, 4, 9, 9, 9, 7,…\n$ ward                  &lt;dbl&gt; 5, 14, 12, NA, 13, 13, 13, 16, 13, 23, 19, 32, 1…\n$ tract                 &lt;dbl&gt; 42003050100, 42003140300, 42003120800, NA, 42003…\n$ public_works_division &lt;dbl&gt; 3, 3, 2, NA, 2, 2, 2, 4, 2, 1, 4, 4, 2, 2, 2, 2,…\n$ pli_division          &lt;dbl&gt; 5, 14, 12, NA, 13, 13, 13, 16, 13, 23, 19, 32, 1…\n$ police_zone           &lt;dbl&gt; 2, 4, 5, NA, 5, 5, 5, 3, 5, 1, 6, 3, 5, 5, 4, 5,…\n$ fire_zone             &lt;chr&gt; \"2-1\", \"2-18\", \"3-12\", NA, \"3-17\", \"3-17\", \"3-17…\n$ x                     &lt;dbl&gt; -79.97765, -79.92450, -79.91455, NA, -79.89539, …\n$ y                     &lt;dbl&gt; 40.44579, 40.43986, 40.46527, NA, 40.45929, 40.4…\n$ geo_accuracy          &lt;chr&gt; \"APPROXIMATE\", \"APPROXIMATE\", \"EXACT\", \"OUT_OF_B…\n$ date                  &lt;date&gt; 2016-03-10, 2016-03-09, 2016-03-03, 2016-03-11,…\n$ month                 &lt;ord&gt; Mar, Mar, Mar, Mar, Mar, Feb, Feb, Mar, Mar, Mar…\n$ year                  &lt;dbl&gt; 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, …\n$ yday                  &lt;dbl&gt; 70, 69, 63, 71, 68, 53, 53, 69, 64, 68, 69, 71, …\n\n\n\nThe pipe\n%&gt;% means “and then”\n%&gt;% passes the dataframe to the next function\n\n\nselect\nselect() selects the columns you want to work with. You can also exclude columns using “-”\n\ndf %&gt;% #select the dataframe\n  select(date, request_type) #select the date and request_type columns\n\n# A tibble: 225,189 × 2\n   date       request_type             \n   &lt;date&gt;     &lt;chr&gt;                    \n 1 2016-03-10 Rodent control           \n 2 2016-03-09 Rodent control           \n 3 2016-03-03 Potholes                 \n 4 2016-03-11 Building Without a Permit\n 5 2016-03-08 Potholes                 \n 6 2016-02-22 Potholes                 \n 7 2016-02-22 Potholes                 \n 8 2016-03-09 Rodent control           \n 9 2016-03-04 Potholes                 \n10 2016-03-08 Rodent control           \n# ℹ 225,179 more rows\n\n\n\n\nfilter\nfilter() uses logic to include or exclude rows based on the criteria you set\nYou can translate the following code into this English sentence: Take our dataframe “df”, and then select the date and request_type columns, and then filter only the rows where the request_type is “Potholes”.\n\ndf %&gt;% \n  select(date, request_type) %&gt;% \n  filter(request_type == \"Potholes\") #use the string \"Potholes\" to filter the dataframe\n\n# A tibble: 31,735 × 2\n   date       request_type\n   &lt;date&gt;     &lt;chr&gt;       \n 1 2016-03-03 Potholes    \n 2 2016-03-08 Potholes    \n 3 2016-02-22 Potholes    \n 4 2016-02-22 Potholes    \n 5 2016-03-04 Potholes    \n 6 2016-03-11 Potholes    \n 7 2016-03-08 Potholes    \n 8 2016-03-08 Potholes    \n 9 2016-03-08 Potholes    \n10 2016-03-08 Potholes    \n# ℹ 31,725 more rows\n\n\n\n\nmutate\nmutate() adds new columns, or modifies existing columns\n\ndf %&gt;% \n  select(date, request_type) %&gt;% \n  filter(request_type == \"Potholes\") %&gt;% \n  mutate(weekday = wday(date, label = TRUE)) #add the wday column for day of the week\n\n# A tibble: 31,735 × 3\n   date       request_type weekday\n   &lt;date&gt;     &lt;chr&gt;        &lt;ord&gt;  \n 1 2016-03-03 Potholes     Thu    \n 2 2016-03-08 Potholes     Tue    \n 3 2016-02-22 Potholes     Mon    \n 4 2016-02-22 Potholes     Mon    \n 5 2016-03-04 Potholes     Fri    \n 6 2016-03-11 Potholes     Fri    \n 7 2016-03-08 Potholes     Tue    \n 8 2016-03-08 Potholes     Tue    \n 9 2016-03-08 Potholes     Tue    \n10 2016-03-08 Potholes     Tue    \n# ℹ 31,725 more rows\n\n\n\n\ngroup_by and summarize\ngroup_by() and summarize() allow you to gather groups of rows and perform functions on them\nTypical functions\n\nsum()\nmean()\nsd() standard deviation\nn() the number of rows\n\n\n(df %&gt;% \n  select(date, request_type) %&gt;% #select columns\n  filter(request_type == \"Potholes\") %&gt;% #filter by \"Potholes\"\n  mutate(month = month(date, label = TRUE)) %&gt;% #add month column\n  group_by(request_type, month) %&gt;% #group by the unqiue request_type values and month values\n  summarize(count = n()) %&gt;% #summarize to count the number of rows in each combination of request_type and month\n  arrange(desc(count)) -&gt; df_potholes_month) #arrange the rows by the number of requests\n\n# A tibble: 12 × 3\n# Groups:   request_type [1]\n   request_type month count\n   &lt;chr&gt;        &lt;ord&gt; &lt;int&gt;\n 1 Potholes     Feb    5569\n 2 Potholes     Mar    3961\n 3 Potholes     Apr    3873\n 4 Potholes     May    3388\n 5 Potholes     Jan    3089\n 6 Potholes     Jun    2896\n 7 Potholes     Jul    2688\n 8 Potholes     Aug    1913\n 9 Potholes     Nov    1344\n10 Potholes     Sep    1260\n11 Potholes     Oct    1113\n12 Potholes     Dec     641\n\n\nPut your code in parentheses to execute it AND print the output in the console"
  },
  {
    "objectID": "posts/r-311-pothole-workshop-code-for-pittsburgh/index.html#ggplot2",
    "href": "posts/r-311-pothole-workshop-code-for-pittsburgh/index.html#ggplot2",
    "title": "R 311 Pothole Workshop Code for Pittsburgh",
    "section": "ggplot2",
    "text": "ggplot2\n\naesthetics (the columns you want to graph with)\ngeoms (the shapes you want to use to graph the data)\n\n\nggplot(data = _ , aes(x = _, y = _)) +\n  geom_()\n\nGraph the number of pothole requests per month\n\nggplot(data = df_potholes_month, aes(x = month, y = count)) +\n  geom_col()\n\n\n\n\n\n\n\n\nPipe your data directly into ggplot2\n\ndf_potholes_month %&gt;% \n  ggplot(aes(x = month, y = count)) + #put the month column on the x axis, count on the y axis\n  geom_col() #graph the data with columns\n\n\n\n\n\n\n\n\nMake it pretty. Add a title, subtitle, axes labels, captions, and themes\n\ndf_potholes_month %&gt;% \n  ggplot(aes(month, count)) +\n  geom_col() + \n  labs(title = \"Pothole requests to Pittsburgh 311\",\n       x = \"\",\n       y = \"Number of requests\",\n       caption = \"Source: Western Pennsylvania Regional Datacenter\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nOne problems with this graph is that the data is not complete for the years 2015 and 2018\n\ndf %&gt;%\n  distinct(year, date) %&gt;% #get the unique combinations of year and date\n  count(year) #shortcut for group_by + summarize for counting. returns column \"n\". calculate how many days of data each year has\n\n# A tibble: 4 × 2\n   year     n\n  &lt;dbl&gt; &lt;int&gt;\n1  2015   231\n2  2016   366\n3  2017   365\n4  2018   100\n\n\nInstead of plotting the raw sum, we can calculate and plot the mean number of requests per month\n\n(df %&gt;% \n  filter(date &gt;= \"2016-01-01\", #only select the rows where the date is after 2016-01-01 and before 2018-01-1\n         date &lt;= \"2018-01-01\",\n         request_type == \"Potholes\") %&gt;% \n  count(request_type, year, month) -&gt; df_filtered)\n\n# A tibble: 24 × 4\n   request_type  year month     n\n   &lt;chr&gt;        &lt;dbl&gt; &lt;ord&gt; &lt;int&gt;\n 1 Potholes      2016 Jan     222\n 2 Potholes      2016 Feb     594\n 3 Potholes      2016 Mar     973\n 4 Potholes      2016 Apr     759\n 5 Potholes      2016 May     822\n 6 Potholes      2016 Jun     784\n 7 Potholes      2016 Jul     604\n 8 Potholes      2016 Aug     556\n 9 Potholes      2016 Sep     364\n10 Potholes      2016 Oct     318\n# ℹ 14 more rows\n\n\n\ndf_filtered %&gt;% \n  group_by(month) %&gt;% \n  summarize(mean_requests = mean(n)) -&gt; df_filtered_months\n\n\ndf_filtered_months %&gt;% \n  ggplot(aes(month, mean_requests)) +\n  geom_col() +\n    labs(title = \"Pothole requests to Pittsburgh 311\",\n       x = \"\",\n       y = \"Mean number of requests\",\n       caption = \"Source: Western Pennsylvania Regional Datacenter\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nMake a line graph of the number of pothole requests in the dataset by date\n\ndf %&gt;% \n  filter(request_type == \"Potholes\") %&gt;% \n  count(date) #group_by and summarize the number of rows per date\n\n# A tibble: 983 × 2\n   date           n\n   &lt;date&gt;     &lt;int&gt;\n 1 2015-04-20   119\n 2 2015-04-21   101\n 3 2015-04-22   109\n 4 2015-04-23   102\n 5 2015-04-24    84\n 6 2015-04-27    85\n 7 2015-04-28   101\n 8 2015-04-29   107\n 9 2015-04-30    83\n10 2015-05-01    66\n# ℹ 973 more rows\n\n\n\n#assign labels to objects to save some typing\nmy_title &lt;- \"Pothole requests to Pittsburgh 311\"\nmy_caption &lt;- \"Source: Western Pennsylvania Regional Datacenter\"\n\ndf %&gt;% \n  filter(request_type == \"Potholes\") %&gt;% \n  count(date) %&gt;% \n  ggplot(aes(date, n)) +\n  geom_line() + #use a line to graph the data\n  labs(title = my_title, #use the object you created earlier\n       x = \"\",\n       y = \"Number of requests\",\n       caption = my_caption) + #use the object you created earlier\n  theme_bw(base_size = 18) #base_family modifies the size of the font\n\n\n\n\n\n\n\n\nNote that ggplot2 automatically formats the axis labels for dates\nGraph the data by number of requests per day of the year\n\n(df %&gt;% \n  select(request_type, date) %&gt;% \n  filter(request_type == \"Potholes\") %&gt;% \n  mutate(year = year(date), #create a year column\n         yday = yday(date)) %&gt;% #create a day of the year column\n  count(year, yday) -&gt; df_day_of_year)  \n\n# A tibble: 983 × 3\n    year  yday     n\n   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n 1  2015   110   119\n 2  2015   111   101\n 3  2015   112   109\n 4  2015   113   102\n 5  2015   114    84\n 6  2015   117    85\n 7  2015   118   101\n 8  2015   119   107\n 9  2015   120    83\n10  2015   121    66\n# ℹ 973 more rows\n\n\n\ndf_day_of_year %&gt;% \n  ggplot(aes(yday, n, group = year)) + #color the lines by year. as.factor() turns the year column from integer to factor, which has an inherent order\n  geom_line() + \n  labs(title = my_title,\n       x = \"Day of the year\",\n       y = \"Number of requests\",\n       caption = my_caption) +\n  theme_bw(base_size = 18)\n\n\n\n\n\n\n\n\nThat plotted a line for each year, but there is no way to tell which line corresponds with which year\nColor the lines by the year\n\ndf_day_of_year %&gt;% \n  ggplot(aes(yday, n, color = as.factor(year))) + #color the lines by year. #as.factor() turns the year column from integer to factor (ordinal string)\n  geom_line() + \n  labs(title = my_title,\n       x = \"Day of the year\",\n       y = \"Number of requests\",\n       caption = my_caption) +\n  theme_bw(base_size = 18)\n\n\n\n\n\n\n\n\nGraph the cumulative sum of pothole requests per year\n\n(df %&gt;% \n  select(request_type, date) %&gt;% \n  filter(request_type == \"Potholes\") %&gt;% \n  mutate(year = year(date),\n         yday = yday(date)) %&gt;% \n  arrange(date) %&gt;% #always arrange your data for cumulative sums\n  group_by(year, yday) %&gt;%\n  summarize(n = n()) %&gt;% \n  ungroup() %&gt;% #ungroup () resets whatever grouping you had before\n  group_by(year) %&gt;% \n  mutate(cumsum = cumsum(n)) -&gt; df_cumulative_sum) #calculate the cumulative sum per year\n\n# A tibble: 983 × 4\n# Groups:   year [4]\n    year  yday     n cumsum\n   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;  &lt;int&gt;\n 1  2015   110   119    119\n 2  2015   111   101    220\n 3  2015   112   109    329\n 4  2015   113   102    431\n 5  2015   114    84    515\n 6  2015   117    85    600\n 7  2015   118   101    701\n 8  2015   119   107    808\n 9  2015   120    83    891\n10  2015   121    66    957\n# ℹ 973 more rows\n\n\n\ndf_cumulative_sum %&gt;% \n  ggplot(aes(yday, cumsum, color = as.factor(year))) +\n  geom_line(size = 2) +\n  labs(title = my_title,\n       x = \"Day of the year\",\n       y = \"Cumulative sum of requests\",\n       caption = my_caption) +\n  scale_color_discrete(\"Year\") +\n  theme_bw(base_size = 18)"
  },
  {
    "objectID": "posts/r-311-pothole-workshop-code-for-pittsburgh/index.html#making-an-area-chart",
    "href": "posts/r-311-pothole-workshop-code-for-pittsburgh/index.html#making-an-area-chart",
    "title": "R 311 Pothole Workshop Code for Pittsburgh",
    "section": "Making an area chart",
    "text": "Making an area chart\nSince 2015 and 2018 have incomplete data, filter them out\n\ndf %&gt;% \n  count(request_type, sort = TRUE) %&gt;% \n  top_n(5) %&gt;% #select the top 5 request types\n  ungroup() -&gt; df_top_requests\n\n\ndf %&gt;% \n  filter(date &gt;= \"2016-01-01\", #only select the rows where the date is after 2016-01-01 and before 2018-01-1\n         date &lt;= \"2018-01-01\") %&gt;% \n  semi_join(df_top_requests) %&gt;% #joins are ways to combine two dataframes\n  count(request_type, month) %&gt;% \n  ggplot(aes(month, n, group = request_type, fill = request_type)) +\n  geom_area() +\n  scale_fill_discrete(\"Request type\") + #change the name of the color legend\n  scale_y_continuous(expand = c(0, 0)) + #remove the padding around the edges\n  scale_x_discrete(expand = c(0, 0)) +\n  labs(title = \"Top 5 types of 311 requests in Pittsburgh\",\n       subtitle = \"2016 to 2017\",\n       x = \"\",\n       y = \"Number of requests\",\n       caption = my_caption) +\n  theme_bw(base_size = 18) +\n  theme(panel.grid = element_blank()) #remove the gridlines fom the plot"
  },
  {
    "objectID": "posts/r-311-pothole-workshop-code-for-pittsburgh/index.html#faceting",
    "href": "posts/r-311-pothole-workshop-code-for-pittsburgh/index.html#faceting",
    "title": "R 311 Pothole Workshop Code for Pittsburgh",
    "section": "Faceting",
    "text": "Faceting\nFacets allow you to split a chart by a variable\nWhere do pothole requests come from?\n\ndf %&gt;% \n  count(request_origin, sort = TRUE)\n\n# A tibble: 10 × 2\n   request_origin          n\n   &lt;chr&gt;               &lt;int&gt;\n 1 Call Center        143716\n 2 Website             41106\n 3 Control Panel       26144\n 4 Report2Gov iOS       6272\n 5 Twitter              4425\n 6 Report2Gov Android   2371\n 7 Text Message         1086\n 8 Report2Gov Website     42\n 9 Email                  22\n10 QAlert Mobile iOS       5\n\n\nMake a line chart for the number of requests per day\nUse facets to distinguish where the request came from\n\ndf %&gt;% \n  select(date, request_type, request_origin) %&gt;% \n  filter(request_type == \"Potholes\") %&gt;% \n  count(date, request_type, request_origin) %&gt;% \n  ggplot(aes(x = date, y = n)) +\n    geom_line() +\n    facet_wrap(~request_origin) + #facet by request_origin\n    labs(title = my_title,\n         subtitle = \"By Request Origin\",\n         x = \"\",\n         y = \"Number of requests\",\n         caption = my_caption) +\n    theme_bw(base_size = 18)"
  },
  {
    "objectID": "posts/r-311-pothole-workshop-code-for-pittsburgh/index.html#mapping",
    "href": "posts/r-311-pothole-workshop-code-for-pittsburgh/index.html#mapping",
    "title": "R 311 Pothole Workshop Code for Pittsburgh",
    "section": "Mapping",
    "text": "Mapping\nLoad the ggmap package, which works with ggplot2\n\nlibrary(ggmap)\n\nSelect the request_type, x, and y columns. x and y are longitude and latitude\n\n(df %&gt;% \n  select(request_type, x, y) %&gt;% \n  filter(!is.na(x), !is.na(y),\n         request_type == \"Potholes\") -&gt; df_map) #remove missing x and y values\n\n# A tibble: 31,735 × 3\n   request_type     x     y\n   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;\n 1 Potholes     -79.9  40.5\n 2 Potholes     -79.9  40.5\n 3 Potholes     -79.9  40.5\n 4 Potholes     -79.9  40.5\n 5 Potholes     -79.9  40.5\n 6 Potholes     -80.0  40.4\n 7 Potholes     -79.9  40.5\n 8 Potholes     -79.9  40.5\n 9 Potholes     -79.9  40.5\n10 Potholes     -79.9  40.5\n# ℹ 31,725 more rows\n\n\n\npgh_coords &lt;- c(lon = -79.9, lat = 40.4)\n\ncity_map &lt;- get_googlemap(pgh_coords, zoom = 11)\n\n(city_map &lt;- ggmap(city_map))\n\n\n\n\n\n\n\n\nPut the data on the map\n\ncity_map +\n  geom_point(data = df_map, aes(x, y, color = request_type)) #graph the data with dots\n\n\n\n\n\n\n\n\nThere is too much data on the graph. Make the dots more transparent to show density\n\ncity_map +\n  geom_point(data = df_map, aes(x, y, color = request_type), alpha = .1) #graph the data with dots\n\n\n\n\n\n\n\n\nStill not great\nDensity plots are better for showing overplotted data\n\n#Put the data on the map\ncity_map +\n  stat_density_2d(data = df_map, #Using a 2d density contour\n                  aes(x, #longitude\n                      y, #latitude,\n                      fill = request_type,\n                      alpha = ..level..), #Use alpha so you can see the map under the data\n                  geom = \"polygon\") + #We want the contour in a polygon\n  scale_alpha_continuous(range = c(.1, 1)) + #manually set the range for the alpha\n  guides(alpha = guide_legend(\"Number of requests\"),\n         fill = FALSE) +\n  labs(title = \"Pothole requests in Pittsburgh\",\n       subtitle = \"311 data\",\n       x = \"\",\n       y = \"\",\n       caption = my_caption) +\n  theme_bw(base_size = 18) +\n  theme(axis.text = element_blank())"
  },
  {
    "objectID": "posts/bivariate_transit_map/index.html",
    "href": "posts/bivariate_transit_map/index.html",
    "title": "Driving Alone vs. Public Transportation in Pittsburgh",
    "section": "",
    "text": "Intro\nThe clash between public transportation and single passenger vehicles is a heated topic of discussion nationally and in the Pittsburgh area. Public transit ridership has been heavily reduced by COVID-19 in many countries. These two commuting modes compete for the same riders, and investment dollars, and space. Car drivers are frustrated when a bus stops during rush hour to pick up passengers, while bus passengers are frustrated sitting in traffic caused by single passenger vehicles because transit doesn’t have right-of-way.\nFrom my point of view, Pittsburgh’s geography lends itself to a focus on public transit, at the expense of the single passenger vehicle. Most of the jobs in the county are in a single census tract Downtown, which is reflected in the spoke (and no wheel) design of the transit system. Downtown is surrounded by rivers and mountains, which drastically narrows the geography suited to infrastructure. You pretty much have to use a tunnel or bridge to commute Downtown, unless you are coming from directly east. It would make sense to give public transit priority access to those tunnels and bridges, since their throughput is many times higher than roads designated for single passenger vehicles.\n\n\n\n\n\n\n\n\n\nThe historical priority towards single passenger vehicles is reflected in the Census statistics about commuting modes in the area. Most people in the area commute to work by themselves in cars. In the Wexford-area census tract, 78% (5,141) of commuters drive to work alone (and sit in traffic on the parkway together). Public transit use is limited to areas where the government invested in transit, but even there transit is not typically the majority mode.\nIn this post I will use {tidycensus} to pull data about how many people commute by driving alone or taking public transit Allegheny County. I chose these two modes because they are the two most popular modes in the county, and are the most different in terms of style. I then graph the data with {ggplot2} and {biscale}. I hack the {biscale} legend a bit to get it to show the % of commuters, which may be of interest to other R users.\n\n\nCode and graphs\nLoad libraries and set up the environment:\n\nlibrary(tidyverse)\nlibrary(tidycensus)\nlibrary(sf)\nlibrary(tigris)\nlibrary(janitor)\nlibrary(biscale)\nlibrary(patchwork)\nlibrary(hrbrthemes)\nlibrary(kableExtra)\n\noptions(scipen = 999, digits = 4)\n\ntheme_set(theme_ipsum(base_size = 25))\n\nThese are the variables about commuter mode that the Census has for the 2019 American Community Survey (ACS):\n\nacs1_vars &lt;- load_variables(2019, 'acs1') %&gt;% \n  mutate(across(c(label, concept), str_to_lower))\n\nacs1_vars %&gt;%\n  filter(str_detect(name, \"^B08301_\")) %&gt;% \n  kbl() %&gt;% \n  scroll_box(height = \"400px\") %&gt;% \n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"),\n                position = \"left\")\n\n\n\n\n\nname\nlabel\nconcept\n\n\n\n\nB08301_001\nestimate!!total:\nmeans of transportation to work\n\n\nB08301_002\nestimate!!total:!!car, truck, or van:\nmeans of transportation to work\n\n\nB08301_003\nestimate!!total:!!car, truck, or van:!!drove alone\nmeans of transportation to work\n\n\nB08301_004\nestimate!!total:!!car, truck, or van:!!carpooled:\nmeans of transportation to work\n\n\nB08301_005\nestimate!!total:!!car, truck, or van:!!carpooled:!!in 2-person carpool\nmeans of transportation to work\n\n\nB08301_006\nestimate!!total:!!car, truck, or van:!!carpooled:!!in 3-person carpool\nmeans of transportation to work\n\n\nB08301_007\nestimate!!total:!!car, truck, or van:!!carpooled:!!in 4-person carpool\nmeans of transportation to work\n\n\nB08301_008\nestimate!!total:!!car, truck, or van:!!carpooled:!!in 5- or 6-person carpool\nmeans of transportation to work\n\n\nB08301_009\nestimate!!total:!!car, truck, or van:!!carpooled:!!in 7-or-more-person carpool\nmeans of transportation to work\n\n\nB08301_010\nestimate!!total:!!public transportation (excluding taxicab):\nmeans of transportation to work\n\n\nB08301_011\nestimate!!total:!!public transportation (excluding taxicab):!!bus\nmeans of transportation to work\n\n\nB08301_012\nestimate!!total:!!public transportation (excluding taxicab):!!subway or elevated rail\nmeans of transportation to work\n\n\nB08301_013\nestimate!!total:!!public transportation (excluding taxicab):!!long-distance train or commuter rail\nmeans of transportation to work\n\n\nB08301_014\nestimate!!total:!!public transportation (excluding taxicab):!!light rail, streetcar or trolley (carro público in puerto rico)\nmeans of transportation to work\n\n\nB08301_015\nestimate!!total:!!public transportation (excluding taxicab):!!ferryboat\nmeans of transportation to work\n\n\nB08301_016\nestimate!!total:!!taxicab\nmeans of transportation to work\n\n\nB08301_017\nestimate!!total:!!motorcycle\nmeans of transportation to work\n\n\nB08301_018\nestimate!!total:!!bicycle\nmeans of transportation to work\n\n\nB08301_019\nestimate!!total:!!walked\nmeans of transportation to work\n\n\nB08301_020\nestimate!!total:!!other means\nmeans of transportation to work\n\n\nB08301_021\nestimate!!total:!!worked from home\nmeans of transportation to work\n\n\n\n\n\n\n\nDriving alone in a single-passenger vehicle is by far the dominant commuting mode in the county.\n\nall_transit_vars &lt;- c(\"B08301_003\", \n                      \"B08301_004\", \n                      \"B08301_010\", \n                      \"B08301_016\", \n                      \"B08301_017\", \n                      \"B08301_018\", \n                      \"B08301_019\", \n                      \"B08301_020\",\n                      \"B08301_021\")\n\nall_transit_modes &lt;- get_acs(geography = \"county\", \n                             variables = acs1_vars %&gt;%\n                               filter(name %in% all_transit_vars) %&gt;% \n                               pull(name, label),\n                             summary_var = \"B08301_001\",\n                             year = 2019, state = \"PA\", county = \"Allegheny\",\n                             geometry = F)\n\nall_transit_modes %&gt;% \n  mutate(variable = str_remove(variable, \"^estimate!!total:\"),\n         variable = str_remove(variable, \"\\\\(excluding taxicab\\\\)\"),\n         variable = str_remove_all(variable, \"\\\\!\"),\n         variable = str_remove(variable, \":$\"),\n         variable = str_replace(variable, \":\", \" : \"),\n         variable = str_trim(variable),\n         variable = str_to_title(variable)) %&gt;% \n  group_by(variable) %&gt;% \n  summarize(estimate = sum(estimate),) %&gt;% \n  mutate(variable = fct_reorder(variable, estimate),\n         pct = estimate / sum(estimate)) %&gt;% \n  ggplot(aes(estimate, variable)) +\n  geom_col() +\n  geom_text(aes(x = estimate + 26000, label = scales::percent(pct, 1)),\n            size = 4) +\n  labs(title = \"Allegheny County Commuter Modes\",\n       subtitle = \"2019 American Community Survey\",\n       x = \"Commuters\",\n       y = NULL) +\n  scale_x_comma(limits = c(0, 500000),\n                labels = c(\"0\", \"1k\", \"2k\", \"3k\", \"4k\", \"5k\")) +\n  theme_ipsum(axis_text_size = 15)\n\n\n\n\n\n\n\n\nI will use these two variables to directly compare the use of single-passenger vehicles and public transit in the county.\n\nvars &lt;- c(\"Drove alone\" = \"B08301_003\",\n          \"Public transportation\" = \"B08301_010\")\n\nacs1_vars %&gt;%\n  filter(name %in% vars) %&gt;% \n  pull(label)\n\n[1] \"estimate!!total:!!car, truck, or van:!!drove alone\"          \n[2] \"estimate!!total:!!public transportation (excluding taxicab):\"\n\n\nThis downloads the commuter mode data and subtracts the rivers from the census tract polygons so it looks nice on a map:\n\ntract_transit %&gt;% \n  glimpse()\n\nRows: 804\nColumns: 5\n$ GEOID       &lt;chr&gt; \"42003408002\", \"42003408002\", \"42003210700\", \"42003210700\"…\n$ variable    &lt;chr&gt; \"Drove alone\", \"Public transportation\", \"Drove alone\", \"Pu…\n$ estimate    &lt;dbl&gt; 2815, 8, 589, 189, 566, 146, 1224, 260, 466, 100, 1063, 21…\n$ summary_est &lt;dbl&gt; 3165, 3165, 1231, 1231, 1110, 1110, 1992, 1992, 702, 702, …\n$ geometry    &lt;POLYGON [°]&gt; POLYGON ((-79.99 40.61, -79..., POLYGON ((-79.99 4…\n\n\nAs discussed earlier, public transit is not the majority commuting mode in most areas:\n\ntract_transit %&gt;% \n  st_drop_geometry() %&gt;% \n  group_by(GEOID) %&gt;%\n  mutate(pct_tract_commuters = estimate / sum(estimate),\n         combined_commuters = sum(estimate)) %&gt;%\n  ungroup() %&gt;%\n  mutate(GEOID = fct_reorder(GEOID, summary_est)) %&gt;%\n  arrange(desc(GEOID), desc(summary_est)) %&gt;% \n  mutate(is_downtown_label = case_when(GEOID == \"42003020100\" & variable == \"Drove alone\" ~ \"Downtown*\",\n                                       TRUE ~ NA_character_)) %&gt;% \n  slice(1:60) %&gt;% \n  ggplot(aes(estimate, GEOID, fill = variable)) +\n  geom_col(color = \"black\") +\n  geom_text(aes(x = estimate + 3000, label = is_downtown_label)) +\n  labs(title = \"Top 30 census tracts\",\n       subtitle = \"Total commuter population from all modes\",\n       x = \"Commuters\",\n       y = \"Census tracts\",\n       fill = \"Commute mode\") +\n  scale_x_comma() +\n  theme_ipsum(base_size = 15) +\n  theme(axis.text.y = element_blank(),\n        panel.grid.major = element_blank())\n\n\n\n\n\n\n\n\n*Most commuters that live in Downtown walk to work.\nThis shows that in absolute numbers, driving alone swamps public transit across the county.\n\nscatter_graph &lt;- tract_transit %&gt;% \n  st_drop_geometry() %&gt;% \n  select(GEOID, variable, estimate) %&gt;% \n  pivot_wider(names_from = variable, values_from = estimate) %&gt;% \n  clean_names() %&gt;% \n  ggplot(aes(drove_alone, public_transportation)) +\n  geom_point(alpha = .7, size = 1) +\n  labs(title = \"Commuter modes in Allegheny County\",\n       x = \"Driving Alone\",\n       y = \"Using Public Transportation\") +\n  scale_x_comma() +\n  scale_y_comma() +\n  tune::coord_obs_pred() +\n  theme_ipsum(base_size = 15)\n\nscatter_graph\n\n\n\n\n\n\n\n\nI made the X and Y axes symmetric to emphasize the difference in scale between the two variables.\nThis uses the bi_class function to divide the data into discrete bins based on how many people drive alone vs. use public transit. This turns two continuous variables into one categorical variable. I had to play around with the style argument to find an option that worked for the unbalanced data.\n\ntransit_bivariate_geo &lt;- tract_transit %&gt;% \n  st_drop_geometry() %&gt;% \n  drop_na(estimate) %&gt;% \n  select(GEOID, variable, estimate) %&gt;% \n  pivot_wider(names_from = variable, values_from = estimate) %&gt;% \n  clean_names() %&gt;% \n  replace_na(list(drove_alone = 0, public_transportation = 0)) %&gt;% \n  bi_class(x = drove_alone, \n           y = public_transportation, \n           style = \"fisher\", \n           dim = 3) %&gt;% \n  left_join(tracts, \n            by = c(\"geoid\" = \"GEOID\")) %&gt;% \n  st_sf()\n\nglimpse(transit_bivariate_geo)\n\nRows: 402\nColumns: 9\n$ geoid                 &lt;chr&gt; \"42003408002\", \"42003210700\", \"42003220600\", \"42…\n$ drove_alone           &lt;dbl&gt; 2815, 589, 566, 1224, 466, 1063, 887, 826, 551, …\n$ public_transportation &lt;dbl&gt; 8, 189, 146, 260, 100, 215, 262, 342, 670, 61, 3…\n$ bi_class              &lt;chr&gt; \"3-1\", \"1-2\", \"1-1\", \"2-2\", \"1-1\", \"2-2\", \"1-2\",…\n$ NAME                  &lt;chr&gt; \"Census Tract 4080.02, Allegheny County, Pennsyl…\n$ variable              &lt;chr&gt; \"B08301_001\", \"B08301_001\", \"B08301_001\", \"B0830…\n$ estimate              &lt;dbl&gt; 3165, 1231, 1110, 1992, 702, 1487, 1317, 1712, 1…\n$ moe                   &lt;dbl&gt; 231, 199, 116, 189, 80, 233, 163, 185, 257, 104,…\n$ geometry              &lt;POLYGON [°]&gt; POLYGON ((-79.99 40.61, -79..., POLYGON …\n\n\n\ntable(transit_bivariate_geo$bi_class) %&gt;% \n  enframe(name = \"bi_class\", value = \"count_tracts\") %&gt;% \n  kbl()\n\n\n\n\nbi_class\ncount_tracts\n\n\n\n\n1-1\n124\n\n\n1-2\n72\n\n\n1-3\n9\n\n\n2-1\n84\n\n\n2-2\n66\n\n\n2-3\n9\n\n\n3-1\n34\n\n\n3-2\n4\n\n\n\n\n\n\n\nThis graph overlays the discrete biscale bins on the previous data to show how the function discretized the data.\n\ntransit_bivariate_geo %&gt;% \n  ggplot(aes(drove_alone, public_transportation, color = bi_class)) +\n  geom_point(alpha = .75, size = 1) +\n  scale_x_comma() +\n  labs(x = \"Drove Alone\",\n       y = \"Used Public Transit\") +\n  guides(color = FALSE) +\n  theme_ipsum(base_size = 15)\n\n\n\n\n\n\n\n\nNote that the X and Y axes are independent in this graph.\nThis creates the biscale legend I will put next to the map.\n\nbi_var_legend &lt;- bi_legend(pal = \"DkBlue\",\n                           dim = 3,\n                           xlab = \" More drove alone\",\n                           ylab = \"More used public transit\",\n                           size = 26) +\n  theme(plot.background = element_rect(fill = alpha(\"white\", 0)),\n        panel.background = element_rect(fill = alpha(\"white\", 0)))\n\nbi_var_legend\n\n\n\n\n\n\n\n\nI would like to show the % of commuters that each bin represents, so I extract the color palette from the ggplot2 object and make my own legend with geom_tile.\n\nbuilt_legend &lt;- ggplot_build(bi_var_legend)\n\nlegend_palette &lt;- built_legend$data[[1]] %&gt;%\n  mutate(bi_class = str_c(x, y, sep = \"-\")) %&gt;% \n  select(fill, bi_class)\n\nlegend_palette %&gt;% \n  kbl()\n\n\n\n\nfill\nbi_class\n\n\n\n\n#e8e8e8\n1-1\n\n\n#ace4e4\n2-1\n\n\n#5ac8c8\n3-1\n\n\n#dfb0d6\n1-2\n\n\n#a5add3\n2-2\n\n\n#5698b9\n3-2\n\n\n#be64ac\n1-3\n\n\n#8c62aa\n2-3\n\n\n#3b4994\n3-3\n\n\n\n\n\n\n\n\ntransit_bivariate &lt;- transit_bivariate_geo %&gt;% \n  st_drop_geometry() %&gt;% \n  select(geoid, bi_class, drove_alone, public_transportation) %&gt;% \n  separate(bi_class, \n           into = c(\"drove_alone_bi\", \"public_transportation_bi\"), \n           sep = \"-\",\n           remove = FALSE) %&gt;% \n  complete(drove_alone_bi, public_transportation_bi, fill = list(drove_alone = 0, public_transportation = 0)) %&gt;% \n  mutate(bi_class = str_c(drove_alone_bi, public_transportation_bi, sep = \"-\"),\n         total = drove_alone + public_transportation,\n         pct_commuters = total / sum(total)) %&gt;%\n  group_by(bi_class, drove_alone_bi, public_transportation_bi) %&gt;% \n  summarize(count_tract = n(),\n            pct_commuters = sum(pct_commuters)) %&gt;% \n  ungroup()\n\nglimpse(transit_bivariate)\n\nRows: 9\nColumns: 5\n$ bi_class                 &lt;chr&gt; \"1-1\", \"1-2\", \"1-3\", \"2-1\", \"2-2\", \"2-3\", \"3-…\n$ drove_alone_bi           &lt;chr&gt; \"1\", \"1\", \"1\", \"2\", \"2\", \"2\", \"3\", \"3\", \"3\"\n$ public_transportation_bi &lt;chr&gt; \"1\", \"2\", \"3\", \"1\", \"2\", \"3\", \"1\", \"2\", \"3\"\n$ count_tract              &lt;int&gt; 124, 72, 9, 84, 66, 9, 34, 4, 1\n$ pct_commuters            &lt;dbl&gt; 0.13661, 0.11754, 0.02073, 0.25273, 0.22252, …\n\n\n\nlegend_palette &lt;- transit_bivariate %&gt;% \n  distinct(bi_class) %&gt;% \n  left_join(legend_palette, by = \"bi_class\")\n\nlegend_palette %&gt;% \n  kbl()\n\n\n\n\nbi_class\nfill\n\n\n\n\n1-1\n#e8e8e8\n\n\n1-2\n#dfb0d6\n\n\n1-3\n#be64ac\n\n\n2-1\n#ace4e4\n\n\n2-2\n#a5add3\n\n\n2-3\n#8c62aa\n\n\n3-1\n#5ac8c8\n\n\n3-2\n#5698b9\n\n\n3-3\n#3b4994\n\n\n\n\n\n\n\nNote that scale_fill_manual uses the palette I extracted from the ggplot2 object.\n\nbi_var_legend_new &lt;- transit_bivariate %&gt;% \n  mutate(pct_commuters = scales::percent(pct_commuters, accuracy = 1)) %&gt;% \n  ggplot(aes(x = drove_alone_bi, y = public_transportation_bi, fill = bi_class)) +\n  geom_tile() +\n  geom_label(fill = \"white\", alpha = .75, size = 12, label = \"    \") +\n  geom_text(aes(label = pct_commuters), alpha = 1, size = 7) +\n  coord_fixed(ratio = 1) +\n  labs(x = substitute(paste(\"More drove alone\", \"\" %-&gt;% \"\")),\n       y = substitute(paste(\"More used public transit\", \"\" %-&gt;% \"\"))) +\n  guides(fill = FALSE) +\n  scale_fill_manual(values = pull(legend_palette, fill)) +\n  theme_ipsum(plot_title_size = 30,\n              axis_title_size = 30) +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        panel.background = element_blank(),\n        axis.text.x = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks = element_blank())\n\nbi_var_legend_new +\n  labs(title = 'Percent of \"drive alone\" + \"public transit\" commuters')\n\n\n\n\n\n\n\n\nThis creates the map of commuter mode by census tract, filled by the discretized biscale bin.\n\ntransit_bi_var_plot &lt;- transit_bivariate_geo %&gt;% \n  ggplot(aes(fill = bi_class)) +\n  geom_sf(show.legend = FALSE, lwd = 0) +\n  geom_sf(data = rivers, fill = \"black\", color = \"black\") +\n  bi_scale_fill(pal = \"DkBlue\", dim = 3) +\n  bi_theme() +\n  theme_ipsum(base_size = 15) +\n  theme(axis.text.x = element_blank(),\n        axis.text.y = element_blank(),\n        panel.background = element_blank(),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank())\n\ntransit_bi_var_plot\n\n\n\n\n\n\n\n\nNow that I have my legend and map, I use patchwork to stitch them together.\n\ndesign = c(area(t = 2, l = 4, b = 20, r = 20),\n           area(t = 1, l = 1, b = 6, r = 6))\n\nplot(design)\n\n\n\n\n\n\n\n\n\ncombined_bi_var_plot &lt;- transit_bi_var_plot + bi_var_legend_new +\n  plot_layout(design = design) +\n  plot_annotation(title = \"Allegheny County Commuter Patterns\",\n                  subtitle = \"Legend: % of commuters that drove alone or use public transit\",\n                  caption = \"2019 American Community Survey\",\n                  theme = theme(panel.background = element_rect(fill = \"black\"),\n                                plot.title = element_text(size = 30),\n                                plot.subtitle = element_text(size = 25),\n                                plot.caption = element_text(size = 25)))\n\n\n\n\n\n\n\n\n\n\n\nLinks:\n\nhttps://www.pghcitypaper.com/pittsburgh/low-income-pittsburghers-are-becoming-increasingly-reliant-on-public-transit-bikes-walking-and-alternative-transportation/Content?oid=19059768\nhttps://www.pghcitypaper.com/pittsburgh/new-commutes-analyzing-the-changing-ways-pittsburghers-get-to-work/Content?oid=6405396\nhttps://www.pghcitypaper.com/pittsburgh/pittsburgh-is-the-7th-least-car-dependent-metro-in-america-study-says/Content?oid=16755873\nhttps://www.nytimes.com/2020/07/09/opinion/sunday/ban-cars-manhattan-cities.html\nhttps://www.nytimes.com/2021/03/25/climate/buses-trains-ridership-climate-change.html\nhttps://nacto.org/publication/transit-street-design-guide/introduction/why/designing-move-people/\nhttps://rweekly.org/"
  },
  {
    "objectID": "posts/roughly-calculating-allegheny-county-transit-efficiency/index.html",
    "href": "posts/roughly-calculating-allegheny-county-transit-efficiency/index.html",
    "title": "Roughly Calculating Allegheny County Transit Efficiency",
    "section": "",
    "text": "As part my work on transit lines in Allegheny County, I am interested in which transit lines are most efficient, in terms of residents and jobs served. This is possible with the Port Authority transit line datasets hosted on the WPRDC and data from the Census.\nLoad libraries and set up the environment:\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(janitor)\nlibrary(ggrepel)\nlibrary(hrbrthemes)\n\noptions(scipen = 999, digits = 2,\n        fig.width = 9,\n        fig.height = 9)\n\ntheme_set(theme_bw())\n\nThis loads the summarized LODES census tract data (munging script here)\n\ndf_tract_centroid_summary &lt;- st_read(\"post_data/shapefiles/tract_centroid_summary/tract_centroid_summary.shp\")\n\nTo recap, the distribution of jobs and residents across census tracts is vaguely normal. The main outlier in the “jobs” measure is the census tract for the Golden Triangle (Downtown).\n\ndf_tract_centroid_summary %&gt;% \n  select(GEOID, residents, jobs) %&gt;% \n  st_drop_geometry() %&gt;% \n  pivot_longer(cols = c(residents, jobs), names_to = \"measure\", values_to = \"value\") %&gt;% \n  ggplot(aes(value, fill = measure)) +\n    geom_density() +\n    facet_wrap(~str_to_title(measure), ncol = 1, scales = \"free\") +\n    scale_x_log10() +\n    guides(fill = \"none\") +\n    labs(x = \"Log 10 scale\",\n         y = \"Density\")\n\n\n\n\n\n\n\n\n\ndf_tract_centroid_summary %&gt;% \n  ggplot(aes(residents, jobs)) +\n    geom_point() +\n    geom_label_repel(data = df_tract_centroid_summary %&gt;% filter(jobs == max(jobs)),\n                     label = \"Downtown\") +\n    scale_y_comma() +\n    scale_x_comma() +\n    labs(x = \"Residents\",\n         y = \"Jobs\")\n\n\n\n\n\n\n\n\nThis code grabs the shapefile with the transit route stats and stop geometry. This code:\n\ncalculates how many residents and jobs are in each census tract\ncalculates which transit lines stops serve which census tracts\nsummarizes how many residents and jobs a transit line servers\n\n\ndf_route_stats &lt;- st_read(\"post_data/shapefiles/route_stats/route_stats.shp\") %&gt;% \n  rename(route_id = route_d,\n         service_type = srvc_ty,\n         residents = resdnts,\n         stop_count = stp_cnt,\n         route_name = rout_nm,\n         route_length_miles = rt_lng_,\n         stops_per_mile = stps_p_)\n\n\ndf_route_stats\n\nSimple feature collection with 102 features and 8 fields\nGeometry type: MULTIPOINT\nDimension:     XY\nBounding box:  xmin: -80 ymin: 40 xmax: -80 ymax: 41\nGeodetic CRS:  WGS 84\nFirst 10 features:\n   route_id service_type   jobs residents stop_count       route_name\n1         1        Local  21282     10119        224    Freeport Road\n2        11        Local    998      4102         62         Fineview\n3        12        Local   4640      2406        112         McKnight\n4        13        Local  26667     15659        140         Bellevue\n5        14        Local  31869     10017        140      Ohio Valley\n6        15        Local  21376      3934         98          Charles\n7        16 Key Corridor  22588      7712        124         Brighton\n8        17        Local  30226     10285        108        Shadeland\n9        18        Local   8380      4746         47       Manchester\n10      19L      Express 294211     18845         94 Emsworth Limited\n   route_length_miles stops_per_mile                       geometry\n1                44.4            5.0 MULTIPOINT ((-80 40), (-80 ...\n2                 5.6           11.1 MULTIPOINT ((-80 40), (-80 ...\n3                30.6            3.7 MULTIPOINT ((-80 41), (-80 ...\n4                15.5            9.0 MULTIPOINT ((-80 40), (-80 ...\n5                33.4            4.2 MULTIPOINT ((-80 41), (-80 ...\n6                 8.9           11.0 MULTIPOINT ((-80 40), (-80 ...\n7                 8.3           15.0 MULTIPOINT ((-80 41), (-80 ...\n8                13.0            8.3 MULTIPOINT ((-80 40), (-80 ...\n9                 5.0            9.5 MULTIPOINT ((-80 40), (-80 ...\n10               15.6            6.0 MULTIPOINT ((-80 41), (-80 ...\n\n\nThis is a basic plot of all the transit stops in the county:\n\ndf_route_stats %&gt;% \n  ggplot() +\n    geom_sf(size = .1, alpha = .5) +\n    theme_void()\n\n\n\n\n\n\n\n\nThe immediate question that comes to mind is “how many residents and jobs does a transit line serve?”. Keep in mind that more than one transit line can serve a given resident or job. This shows that the “Key Corridor” lines serve the most jobs.\n\ndf_route_stats %&gt;% \n  ggplot(aes(jobs, residents, fill = service_type)) +\n    geom_label(aes(label = route_id), alpha = .5) +\n    scale_x_comma() +\n    scale_y_comma() +\n    scale_fill_discrete(\"Service type\") +\n    labs(x = \"Jobs served\",\n         y = \"Residents served\")\n\n\n\n\n\n\n\n\nThis graph compares how many residents/jobs a transit line serves to how long the line is. The 28X and P10 are the least efficient in both cases. However, the 28X serves the Pittsburgh International Airport, and that utility is not captured in this analysis.\n\ndf_route_stats %&gt;% \n  filter(!is.na(route_id)) %&gt;% \n  select(route_id, service_type, route_length_miles, residents, jobs) %&gt;% \n  pivot_longer(cols = c(residents, jobs), names_to = \"variable\", values_to = \"value\") %&gt;% \n  ggplot(aes(route_length_miles, value, fill = service_type)) +\n    geom_label(aes(label = route_id), alpha = .5) +\n    facet_wrap(~str_to_title(str_c(variable, \"served\", sep = \" \")),\n               scales = \"free_y\",\n               ncol = 1,\n               strip.position = \"left\") +\n    scale_y_comma() +\n    scale_fill_discrete(\"Service Type\") +\n    labs(x = \"Route length (miles)\",\n         y = NULL) +\n    theme(strip.text = element_text(hjust = .5),\n          strip.background = element_rect(color = \"grey\"),\n          strip.placement = \"outside\")\n\n\n\n\n\n\n\n\nThis performs a similar comparison, but uses the number of stops per line instead of line distance. The 71/61 lines are very efficient in terms of jobs/stops, and the 59 appears to be the most inefficient.\n\ndf_route_stats %&gt;% \n  filter(!is.na(route_id)) %&gt;% \n  select(route_id, service_type, stop_count, residents, jobs) %&gt;% \n  pivot_longer(cols = c(residents, jobs), names_to = \"variable\", values_to = \"value\") %&gt;% \n  ggplot(aes(stop_count, value, fill = service_type)) +\n    geom_label(aes(label = route_id), alpha = .5) +\n    facet_wrap(~str_to_title(str_c(variable, \"served\", sep = \" \")),\n               scales = \"free_y\",\n               ncol = 1,\n               strip.position = \"left\") +\n    scale_y_comma() +\n    scale_fill_discrete(\"Service type\") +\n    labs(x = \"Number of stops\",\n         y = NULL) +\n    theme(strip.text = element_text(hjust = .5),\n          strip.background = element_rect(color = \"grey\"),\n          strip.placement = \"outside\")\n\n\n\n\n\n\n\n\nIn terms of stops per mile, the Express lines are most efficient. The incline lines are least efficient, but that is an artifact of their unique geography.\n\ndf_route_stats %&gt;% \n  filter(!is.na(route_id)) %&gt;% \n  select(route_id, service_type, stops_per_mile, residents, jobs) %&gt;% \n  pivot_longer(cols = c(residents, jobs), names_to = \"variable\", values_to = \"value\") %&gt;% \n  ggplot(aes(stops_per_mile, value, fill = service_type)) +\n    geom_label(aes(label = route_id), alpha = .5) +\n    facet_wrap(~str_to_title(str_c(variable, \"served\", sep = \" \")), \n               scales = \"free_y\",\n               ncol = 1,\n               strip.position = \"left\") +\n    scale_y_comma() +\n    scale_fill_discrete(\"Service type\") +\n    labs(x = \"Stops per mile\",\n         y = NULL) +\n    theme(strip.text = element_text(hjust = .5),\n          strip.background = element_rect(color = \"grey\"),\n          strip.placement = \"outside\")\n\n\n\n\n\n\n\n\nThis graph attempts to summarize everything by adding residents + jobs and comparing that to stops per mile. The “Express” and “Key Corridor” lines do the best here.\n\nplot &lt;- df_route_stats %&gt;% \n  filter(!is.na(route_id)) %&gt;% \n  select(route_id, service_type, stops_per_mile, residents, jobs) %&gt;% \n  mutate(residents_plus_jobs = residents + jobs) %&gt;% \n  ggplot(aes(stops_per_mile, residents_plus_jobs, fill = service_type, label = route_id)) +\n    geom_label(alpha = .5) +\n    labs(x = \"Stops per mile\",\n         y = \"Residents plus jobs served\",\n         caption = \"'Served' means the line came within 200 meters of the center of a census tract\") +\n    scale_x_continuous(expand = c(.1, .1)) +\n    scale_y_comma(expand = c(.1, .1)) +\n    scale_fill_discrete(\"Service type\") +\n    facet_wrap(vars(service_type), ncol = 1) +\n  theme(legend.position = \"bottom\")\n\nplot"
  },
  {
    "objectID": "posts/effect-of-geographic-resolution-on-ebirdst-abundance/index.html",
    "href": "posts/effect-of-geographic-resolution-on-ebirdst-abundance/index.html",
    "title": "Effect of Geographic Resolution on ebirdst Abundance",
    "section": "",
    "text": "While exploring some of the citizen science bird observation data available through ebirdst, I was confused by how to understand the calculation of ebirdst’s abundance metric.\nThe ebirdst documentation (?ebirdst::load_raster) defines abundance as:\nI had seen some weird results when trying to manually calculate abundance as occurrence * count. My initial attempt had aggregated the results by month.\nThe underlying problem is that abundance and count are the results of models, and are subject to model error. I also believe that the data outputted from load_raster lacks the necessary significant digits to accurately recreate abundance. Lowering the resolution or aggregating the data will exacerbate this issue.\nThis code loads my convenience function to retrieve a metric for a species at a given geographic resolution. This gets occurrence, count, and abundance for the Northern Cardinal at high (3 km), medium (9 km), and low resolutions (27 km). The function also crops the underlying raster data to Pennsylvania.\nlibrary(here)\nlibrary(hrbrthemes)\nlibrary(patchwork)\n\nsource(\"https://raw.githubusercontent.com/conorotompkins/ebird_shiny_app/main/scripts/functions/get_species_metric.R\")\n\ntheme_set(theme_ipsum())\n\nspecies_table &lt;- crossing(location = \"Pennsylvania\",\n                          species = c(\"Northern Cardinal\"),\n                          metric = c(\"occurrence\", \"count\", \"abundance\"),\n                          resolution = c(\"hr\", \"mr\", \"lr\"))\nspecies_table\n\n# A tibble: 9 × 4\n  location     species           metric     resolution\n  &lt;chr&gt;        &lt;chr&gt;             &lt;chr&gt;      &lt;chr&gt;     \n1 Pennsylvania Northern Cardinal abundance  hr        \n2 Pennsylvania Northern Cardinal abundance  lr        \n3 Pennsylvania Northern Cardinal abundance  mr        \n4 Pennsylvania Northern Cardinal count      hr        \n5 Pennsylvania Northern Cardinal count      lr        \n6 Pennsylvania Northern Cardinal count      mr        \n7 Pennsylvania Northern Cardinal occurrence hr        \n8 Pennsylvania Northern Cardinal occurrence lr        \n9 Pennsylvania Northern Cardinal occurrence mr\nspecies_metrics &lt;- species_table %&gt;% \n  mutate(data = pmap(list(location, species, metric, resolution), ~get_species_metric(..1, ..2, ..2, ..3, ..4))) %&gt;% \n  mutate(resolution = fct_relevel(resolution, c(\"hr\", \"mr\", \"lr\"))) %&gt;% \n  arrange(species, metric, resolution) |&gt; \n  unnest(data) %&gt;% \n  unnest(data)\nspecies_metrics\n\n# A tibble: 1,243,320 × 13\n   location species metric resolution family_common_name common_name metric_desc\n   &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;              &lt;chr&gt;       &lt;chr&gt;      \n 1 Pennsyl… Northe… abund… hr         Northern Cardinal  Northern C… abundance  \n 2 Pennsyl… Northe… abund… hr         Northern Cardinal  Northern C… abundance  \n 3 Pennsyl… Northe… abund… hr         Northern Cardinal  Northern C… abundance  \n 4 Pennsyl… Northe… abund… hr         Northern Cardinal  Northern C… abundance  \n 5 Pennsyl… Northe… abund… hr         Northern Cardinal  Northern C… abundance  \n 6 Pennsyl… Northe… abund… hr         Northern Cardinal  Northern C… abundance  \n 7 Pennsyl… Northe… abund… hr         Northern Cardinal  Northern C… abundance  \n 8 Pennsyl… Northe… abund… hr         Northern Cardinal  Northern C… abundance  \n 9 Pennsyl… Northe… abund… hr         Northern Cardinal  Northern C… abundance  \n10 Pennsyl… Northe… abund… hr         Northern Cardinal  Northern C… abundance  \n# ℹ 1,243,310 more rows\n# ℹ 6 more variables: date &lt;date&gt;, value &lt;dbl&gt;, month &lt;chr&gt;, region &lt;chr&gt;,\n#   x &lt;dbl&gt;, y &lt;dbl&gt;\nThis unnests the data and recalculates abundance (abundance_test) and the difference between actual abundance and abundance_test.\nspecies_table_unnested &lt;- species_metrics %&gt;%\n  select(species, resolution, date, month, x, y, metric_desc, value) %&gt;% \n  pivot_wider(id_cols = c(species, resolution, date, month, x, y),\n              names_from = metric_desc,\n              values_from = value) %&gt;% \n  select(species, resolution, date, month, x, y, count, occurrence, abundance) %&gt;% \n  mutate(abundance_test = count * occurrence,\n         diff = abundance - abundance_test)\nGrouping by month to get to the county level changes the grain of the data so much that abundance_test undershoots abundance by 20%. This occurs at all resolutions.\nspecies_metrics %&gt;%\n  select(species, resolution, date, month, x, y, metric_desc, value) %&gt;% \n  pivot_wider(id_cols = c(species, resolution, date, month, x, y),\n              names_from = metric_desc,\n              values_from = value) %&gt;% \n  select(species, resolution, date, month, x, y, count, occurrence, abundance) %&gt;% \n  group_by(species, month, resolution) %&gt;% \n  summarize(occurrence = mean(occurrence, na.rm = T),\n            count = mean(count, na.rm = T),\n            abundance = mean(abundance, na.rm = T)) %&gt;% \n  ungroup() %&gt;% \n  mutate(abundance_test = count * occurrence,\n         diff = abundance - abundance_test) %&gt;% \n  ggplot(aes(abundance, abundance_test)) +\n  geom_abline() +\n  geom_point() +\n  facet_wrap(~resolution) +\n  tune::coord_obs_pred()\n\n`summarise()` has grouped output by 'species', 'month'. You can override using\nthe `.groups` argument.\nTotally un-aggregated, abundance_test closely resembles abundance, but degrades as resolution decreases.\nspecies_table_unnested %&gt;% \n  select(abundance, abundance_test, resolution) %&gt;% \n  drop_na() %&gt;% \n  ggplot(aes(abundance, abundance_test)) +\n  geom_density_2d_filled(contour_var = \"ndensity\") +\n  geom_abline(color = \"white\") +\n  facet_wrap(~resolution) +\n  tune::coord_obs_pred() +\n  coord_cartesian(xlim = c(0, 4),\n                  ylim = c(0, 4)) +\n  guides(fill = guide_colorsteps())\n\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\nAt lower resolutions, the difference is positively skewed, which means that abundance is higher than abundance_test.\nspecies_table_unnested %&gt;% \n  drop_na(diff) %&gt;% \n  ggplot(aes(diff)) +\n  geom_histogram() +\n  facet_wrap(~resolution, scale = \"free_y\", ncol = 1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\nAt the highest resolution, diff is heteroskedastic. At lower resolutions, there are patterns to the error.\nspecies_table_unnested %&gt;% \n  drop_na(occurrence, diff) %&gt;% \n  ggplot(aes(occurrence, diff)) +\n  geom_density_2d_filled(contour_var = \"ndensity\") +\n  facet_wrap(~resolution) + \n  scale_x_percent() +\n  guides(fill = guide_colorsteps())\nThis was a useful exercise for me to understand how the geographic resolution and other aggregation of the data can affect estimated metrics, specifically in the citizen science context."
  },
  {
    "objectID": "posts/effect-of-geographic-resolution-on-ebirdst-abundance/index.html#update",
    "href": "posts/effect-of-geographic-resolution-on-ebirdst-abundance/index.html#update",
    "title": "Effect of Geographic Resolution on ebirdst Abundance",
    "section": "Update",
    "text": "Update\nI made an issue on the ebirdst Github page and talked to one of the maintainers about their definitions of count and abundance. I now have a much stronger understanding of these variables.\nThe following code reproduces the graph I attached to the issue:\n\nlibrary(hrbrthemes)\ntheme_set(theme_ipsum())\n\nnorcar_table &lt;- crossing(location = \"Pennsylvania\",\n                         species = c(\"Northern Cardinal\"),\n                         metric = c(\"occurrence\", \"count\", \"abundance\"),\n                         resolution = c(\"hr\"))\n\n\nnorcar_metrics &lt;- norcar_table %&gt;% \n  mutate(data = pmap(list(location, species, metric, resolution), ~get_species_metric(..1, ..2, ..2, ..3, ..4))) %&gt;% \n  mutate(resolution = fct_relevel(resolution, c(\"hr\", \"mr\", \"lr\"))) %&gt;% \n  arrange(species, metric, resolution) %&gt;%\n  unnest(data) |&gt; \n  unnest(data)\n\n\nnorcar_metrics_wide &lt;- norcar_metrics %&gt;% \n  select(species, date, x, y, metric_desc, value) %&gt;% \n  pivot_wider(names_from = metric_desc,\n              values_from = value)\n\nplot_1 &lt;- norcar_metrics_wide %&gt;% \n  drop_na(occurrence, count) %&gt;% \n  ggplot(aes(occurrence, count)) +\n  geom_density_2d_filled(contour_var = \"ndensity\") +\n  scale_x_percent() +\n  guides(fill = \"none\") +\n  theme_bw()\n\nplot_2 &lt;- norcar_metrics_wide %&gt;% \n  drop_na() %&gt;% \n  ggplot(aes(occurrence, abundance)) +\n  geom_density_2d_filled(contour_var = \"ndensity\") +\n  scale_x_percent() +\n  guides(fill = \"none\") +\n  theme_bw()\n\nplot_3 &lt;- norcar_metrics_wide %&gt;%\n  drop_na() %&gt;% \n  ggplot(aes(count, abundance)) +\n  geom_density_2d_filled(contour_var = \"ndensity\") +\n  geom_abline() +\n  guides(fill = \"none\") +\n  theme_bw()\n\nlayout &lt;- \"\nAACC\nBBCC\n\"\n\nplot_1 + plot_2 + plot_3 + \n  plot_layout(guides = 'collect', design = layout) +\n  plot_annotation(title = \"Northern Cardinal in Pennsylvania\")"
  },
  {
    "objectID": "posts/effect-of-geographic-resolution-on-ebirdst-abundance/index.html#citations",
    "href": "posts/effect-of-geographic-resolution-on-ebirdst-abundance/index.html#citations",
    "title": "Effect of Geographic Resolution on ebirdst Abundance",
    "section": "Citations",
    "text": "Citations\nFink, D., T. Auer, A. Johnston, M. Strimas-Mackey, O. Robinson, S. Ligocki, W. Hochachka, C. Wood, I. Davies, M. Iliff, L. Seitz. 2020. eBird Status and Trends, Data Version: 2019; Released: 2020 Cornell Lab of Ornithology, Ithaca, New York. https://doi.org/10.2173/ebirdst.2019"
  },
  {
    "objectID": "posts/graphing-allegheny-county-covid-19-data/index.html",
    "href": "posts/graphing-allegheny-county-covid-19-data/index.html",
    "title": "Graphing Allegheny County COVID-19 data",
    "section": "",
    "text": "In this post, I review the process I use to make daily graphs from data published by the Allegheny County Health Department. I use the data posted by Franklin Chen, who scrapes the data from the County’s email updates.\nFirst, load the required packages and set up the environment.\n\n#load libraries\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(tidyquant)\nlibrary(hrbrthemes)\n\n#settings\ntheme_set(theme_ipsum(base_size = 15, strip_text_size = 15, axis_title_size = 15))\n\noptions(scipen = 999, digits = 4)\n\nThis reads in the raw data from the GitHub repository.\n\n#read in data\ndf &lt;- read_csv(\"https://raw.githubusercontent.com/FranklinChen/covid-19-allegheny-county/master/covid-19-allegheny-county.csv\") %&gt;% \n  mutate(state = \"Pennsylvania\",\n         county = \"Allegheny County\") %&gt;% \n  filter(date &lt; \"2020-07-08\")\n\nglimpse(df)\n\nOn July 7th, the County added deaths that occurred between April and June, but marked them as reported on July 7th. For the purposes of visualization, I remove those deaths.\n\n#remove deaths from July 7th\n#The deaths reported today are from the state’s use of the Electronic Data Reporting System (EDRS) and include #deaths from April 5 – June 13, all decedents were 65 or older.\n#https://twitter.com/HealthAllegheny/status/1280517051589722117?s=20\n\ndf &lt;- df %&gt;% \n  mutate(deaths = case_when(date == \"2020-07-07\" ~ NA_real_,\n                            date != \"2020-07-07\" ~ deaths))\n\nThis calculates new cases, hospitalizations, and deaths.\n\ndf &lt;- df %&gt;% \n  mutate(cases_new = cases - lag(cases),\n         hospitalizations_new = hospitalizations - lag(hospitalizations),\n         deaths_new = deaths - lag(deaths))\n\nThere are instances where the number of cumulative hospitalizations or deaths decreases.\n\ndf %&gt;% \n  mutate(hospitalizations_lag = lag(hospitalizations)) %&gt;% \n  select(date, date, hospitalizations, hospitalizations_lag, hospitalizations_new) %&gt;% \n  filter(hospitalizations_new &lt; 0)\n\n# A tibble: 3 × 4\n  date       hospitalizations hospitalizations_lag hospitalizations_new\n  &lt;date&gt;                &lt;dbl&gt;                &lt;dbl&gt;                &lt;dbl&gt;\n1 2020-04-27              213                  214                   -1\n2 2020-05-01              235                  236                   -1\n3 2020-05-14              283                  285                   -2\n\n\n\ndf %&gt;% \n  mutate(deaths_lag = lag(deaths)) %&gt;% \n  select(date, date, deaths, deaths_lag, deaths_new) %&gt;% \n  filter(deaths_new &lt; 0)\n\n# A tibble: 2 × 4\n  date       deaths deaths_lag deaths_new\n  &lt;date&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 2020-04-23     69         74         -5\n2 2020-04-29     86         87         -1\n\n\nIn these cases, I remove the values and do not replace them.\n\n#when new cases/hospitalizations/deaths is negative, replace with NA\ndf &lt;- df %&gt;% \n  mutate(cases_new = case_when(cases_new &lt; 0 ~ NA_real_,\n                               cases_new &gt;= 0 ~ cases_new),\n         hospitalizations_new = case_when(hospitalizations_new &lt; 0 ~ NA_real_,\n                                          hospitalizations_new &gt;= 0 ~ hospitalizations_new),\n         deaths_new = case_when(deaths_new &lt; 0 ~ NA_real_,\n                                deaths_new &gt;= 0 ~ deaths_new))\n\nThis calculates rolling 14-day averages for new cases/hospitalizations/deaths.\n\n#calculate rolling 14 day averages for cases/hospitalizations/deaths\ndf &lt;- df %&gt;% \n  tq_mutate(\n    # tq_mutate args\n    select     = cases_new,\n    mutate_fun = rollapply, \n    # rollapply args\n    width      = 14,\n    align      = \"right\",\n    FUN        = mean,\n    # mean args\n    na.rm      = TRUE,\n    # tq_mutate args\n    col_rename = \"cases_new_rolling_14\"\n  ) %&gt;% \n  tq_mutate(\n    # tq_mutate args\n    select     = hospitalizations_new,\n    mutate_fun = rollapply, \n    # rollapply args\n    width      = 14,\n    align      = \"right\",\n    FUN        = mean,\n    # mean args\n    na.rm      = TRUE,\n    # tq_mutate args\n    col_rename = \"hospitalizations_new_rolling_14\"\n  ) %&gt;% \n  tq_mutate(\n    # tq_mutate args\n    select     = deaths_new,\n    mutate_fun = rollapply, \n    # rollapply args\n    width      = 14,\n    align      = \"right\",\n    FUN        = mean,\n    # mean args\n    na.rm      = TRUE,\n    # tq_mutate args\n    col_rename = \"deaths_new_rolling_14\"\n  ) %&gt;% \n  select(state, county, date, contains(\"_new\"), contains(\"rolling\"))\n\nglimpse(df)\n\nRows: 126\nColumns: 9\n$ state                           &lt;chr&gt; \"Pennsylvania\", \"Pennsylvania\", \"Penns…\n$ county                          &lt;chr&gt; \"Allegheny County\", \"Allegheny County\"…\n$ date                            &lt;date&gt; 2020-03-04, 2020-03-05, 2020-03-06, 2…\n$ cases_new                       &lt;dbl&gt; NA, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2…\n$ hospitalizations_new            &lt;dbl&gt; NA, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ deaths_new                      &lt;dbl&gt; NA, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ cases_new_rolling_14            &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ hospitalizations_new_rolling_14 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ deaths_new_rolling_14           &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n\n\nTo prepare the data for visualization in ggplot2, I pivot the rolling averages longer and move them into a separate table.\n\n#pivot rolling average data longer\ndf_rolling &lt;- df %&gt;% \n  select(state, county, date, contains(\"rolling\")) %&gt;% \n  pivot_longer(cols = contains(\"rolling\"), names_to = \"metric\") %&gt;% \n  mutate(metric = case_when(str_detect(metric, \"cases\") ~ \"New cases\",\n                            str_detect(metric, \"deaths\") ~ \"New deaths\",\n                            str_detect(metric, \"hospitalizations\") ~ \"New hospitalizations\")) %&gt;% \n  mutate(metric = factor(metric, levels = c(\"New cases\", \"New hospitalizations\", \"New deaths\")))\n\nglimpse(df_rolling)\n\nRows: 378\nColumns: 5\n$ state  &lt;chr&gt; \"Pennsylvania\", \"Pennsylvania\", \"Pennsylvania\", \"Pennsylvania\",…\n$ county &lt;chr&gt; \"Allegheny County\", \"Allegheny County\", \"Allegheny County\", \"Al…\n$ date   &lt;date&gt; 2020-03-04, 2020-03-04, 2020-03-04, 2020-03-05, 2020-03-05, 20…\n$ metric &lt;fct&gt; New cases, New hospitalizations, New deaths, New cases, New hos…\n$ value  &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n\n\nI do the same for the daily data.\n\n#pivot daily data longer\ndf_new &lt;- df %&gt;% \n  select(state, county, date, !contains(\"rolling\")) %&gt;% \n  pivot_longer(cols = contains(\"_new\"), names_to = \"metric\") %&gt;% \n  mutate(metric = case_when(str_detect(metric, \"cases\") ~ \"New cases\",\n                            str_detect(metric, \"deaths\") ~ \"New deaths\",\n                            str_detect(metric, \"hospitalizations\") ~ \"New hospitalizations\")) %&gt;% \n  mutate(metric = factor(metric, levels = c(\"New cases\", \"New hospitalizations\", \"New deaths\")))\n\nglimpse(df_new)\n\nRows: 378\nColumns: 5\n$ state  &lt;chr&gt; \"Pennsylvania\", \"Pennsylvania\", \"Pennsylvania\", \"Pennsylvania\",…\n$ county &lt;chr&gt; \"Allegheny County\", \"Allegheny County\", \"Allegheny County\", \"Al…\n$ date   &lt;date&gt; 2020-03-04, 2020-03-04, 2020-03-04, 2020-03-05, 2020-03-05, 20…\n$ metric &lt;fct&gt; New cases, New hospitalizations, New deaths, New cases, New hos…\n$ value  &lt;dbl&gt; NA, NA, NA, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n\n\nIn the daily data, I remove rows where the date is before the first non-zero value of that metric.\n\n#identify first non-zero value in each metric.\n##filter out rows before first non-zero value\ndf_new &lt;- df_new %&gt;% \n  arrange(state, county, metric, date) %&gt;% \n  group_by(state, county, metric) %&gt;% \n  filter(row_number() != 1) %&gt;% \n  mutate(first_non_zero_value = cumsum(coalesce(value, 0) &gt; 0) &gt;= 1) %&gt;% \n  ungroup() %&gt;% \n  filter(first_non_zero_value == TRUE)\n\nThese graphs show the rolling and daily new data:\n\n#preview rolling data\ndf_rolling %&gt;% \n  ggplot(aes(date, value)) +\n  geom_line() +\n  facet_wrap(~metric, ncol = 1, scales = \"free_y\")\n\n\n\n\n\n\n\n\n\n#preview daily data\ndf_new %&gt;% \n  ggplot(aes(date, value)) +\n  geom_point() +\n  facet_wrap(~metric, ncol = 1, scales = \"free_y\")\n\n\n\n\n\n\n\n\nThis finds the most recent date in the data, which I insert into the final graph.\n\n#find most recent date\nlast_updated &lt;- last(df_rolling$date)\n\nThis creates the final graph. Since I pivoted the data longer, I can facet by metric, which lets me show cases/hospitalizations/deaths on separate y axes. I insert colored rectanges behind the data to show the stages of state intervention.\n\n#make graph\nallegheny_county_timeline &lt;- df_rolling %&gt;% \n  filter(!is.na(value)) %&gt;% \n  ggplot(aes(x = date, y = value)) +\n  #create colored rectangles showing various government intervention timelines\n  annotate(geom = \"rect\", xmin = ymd(\"2020-03-23\"), xmax = ymd(\"2020-05-15\"), ymin = as.Date(-Inf), ymax = as.Date(Inf), \n           fill = \"red\", alpha = .3) +\n  annotate(geom = \"rect\", xmin = ymd(\"2020-05-15\"), xmax = ymd(\"2020-06-05\"), ymin = as.Date(-Inf), ymax = as.Date(Inf), \n           fill = \"yellow\", alpha = .3) +\n  annotate(geom = \"rect\", xmin = ymd(\"2020-06-05\"), xmax = ymd(\"2020-06-28\"), ymin = as.Date(-Inf), ymax = as.Date(Inf), \n           fill = \"green\", alpha = .3) +\n  annotate(geom = \"rect\", xmin = ymd(\"2020-06-28\"), xmax = as.Date(Inf), ymin = as.Date(-Inf), ymax = as.Date(Inf),\n           fill = \"#aaff00\", alpha = .3) +\n  #plot daily data as points, rolling average as lines\n  geom_point(data = df_new, aes(y = value), alpha = .3)+\n  geom_line(size = 1.5) +\n  #facet by metric\n  facet_wrap(~metric, ncol = 1, scales = \"free_y\") +\n  labs(title = str_c(\"Allegheny County COVID-19 response timeline (last updated \", last_updated, \")\"),\n       x = NULL,\n       y = NULL,\n       subtitle = \"14-day rolling average\",\n       caption = \"@conor_tompkins, data from Allegheny County via Franklin Chen\")\n\nallegheny_county_timeline"
  },
  {
    "objectID": "posts/working-with-paycheck-protection-program-data-in-r/index.html",
    "href": "posts/working-with-paycheck-protection-program-data-in-r/index.html",
    "title": "Working with Paycheck Protection Program data in R",
    "section": "",
    "text": "In this post, I walk through the process of reading in the data from the Paycheck Protection Program, and show some basic analyses that can be done.\nThis post was migrated to Quarto in April 2024. The SBA significantly restructured how the data is structured between 2020-2024, so some of the older analyis is no longer applicable.\nLoad packages and set up environment:\nlibrary(tidyverse)\nlibrary(vroom)\nlibrary(sf)\nlibrary(tigris)\nlibrary(lubridate)\nlibrary(janitor)\nlibrary(hrbrthemes)\nlibrary(scales)\n\ntheme_set(theme_ipsum(base_size = 18))\n\noptions(scipen = 999, digits = 4, tigris_use_cache = TRUE)"
  },
  {
    "objectID": "posts/working-with-paycheck-protection-program-data-in-r/index.html#load-and-clean-data",
    "href": "posts/working-with-paycheck-protection-program-data-in-r/index.html#load-and-clean-data",
    "title": "Working with Paycheck Protection Program data in R",
    "section": "Load and clean data",
    "text": "Load and clean data\nThe data is available at this site: https://data.sba.gov/dataset/ppp-foia\nThe data exists in numerous CSV files, separated by state and sometimes loan amount.\nThis code chunk identifies all the CSV files in the folder, reads them in, and combines them.\n\npath &lt;- \"posts/working-with-paycheck-protection-program-data-in-r/post_data/ppp\"\n\n#find all files in the folder that end in .csv\nppp_files &lt;- list.files(path, full.names = TRUE, recursive = TRUE) %&gt;% \n  keep(str_detect(., \".csv$\")) %&gt;% \n  #read each file with read_csv and combine them\n  set_names()\n\nppp_files |&gt; \n  pluck(1) |&gt; \n  read_csv(n_max = 100) |&gt; \n  glimpse()\n\nread_and_filter &lt;- function(x){\n  \n  print(x)\n  \n  read_csv(x, col_types = cols(.default = \"c\")) |&gt; \n    filter(mdy(DateApproved) &lt; \"2020-07-01\") |&gt; \n    clean_names()\n  \n}\n\nppp_data &lt;- ppp_files |&gt; \n  map(read_and_filter)\n\nppp_data |&gt;\n  pluck(1)\n\nglimpse(ppp_data)\n\nlist_rbind(ppp_data, names_to = \"file_name\") |&gt; \n  write_csv(\"posts/working-with-paycheck-protection-program-data-in-r/post_data/combined_ppp.csv\")\n\n\nppp_combined &lt;- vroom(\"post_data/combined_ppp.csv\", delim = \",\") %&gt;%\n  mutate(date_approved = mdy(date_approved)) |&gt; \n  mutate(loan_range = cut(current_approval_amount,\n                          breaks = c(0, 150000, 350000, 1000000, 2000000, 5000000, 10000000),\n                          labels = c(\"Under $150k\", \"$150k-$350k\", \"$350k-$1 million\", \"$1 million-$2 million\", \"$2 million-$5 million\", \"$5 million-$10 million\"))) |&gt; \n  separate(cd, into = c(\"state_2\", \"district\"), remove = FALSE)\n\nThis changes how the columns are interpreted, and separates the congressional district column into state and district. If the district value is blank, I replace it with NA.\nThere are cases where the value in the state column doesn’t match the state embedded in the congressional district cd column.\n\n#preview data where state doesn't match congressional district\nppp_combined %&gt;% \n  count(file_name, project_state, cd, state_2, sort = TRUE) %&gt;% \n  mutate(flag_match = project_state == state_2) %&gt;% \n  filter(flag_match == FALSE) %&gt;% \n  slice(1:5)\n\n# A tibble: 5 × 6\n  file_name                         project_state cd    state_2     n flag_match\n  &lt;chr&gt;                             &lt;chr&gt;         &lt;chr&gt; &lt;chr&gt;   &lt;int&gt; &lt;lgl&gt;     \n1 posts/working-with-paycheck-prot… MI            MT-02 MT         18 FALSE     \n2 posts/working-with-paycheck-prot… MN            MT-02 MT         15 FALSE     \n3 posts/working-with-paycheck-prot… NE            SD-   SD          4 FALSE     \n4 posts/working-with-paycheck-prot… NY            NJ-05 NJ          4 FALSE     \n5 posts/working-with-paycheck-prot… TX            NM-03 NM          3 FALSE     \n\n\nThere are only 423 rows where this occurs, and it is less than 1% of the dataset.\n\n#summarize cases where mismatch occurs\nppp_combined %&gt;% \n  mutate(flag_match = project_state == state_2) %&gt;% \n  count(flag_match) %&gt;% \n  mutate(pct_mismatch = n / sum(n),\n         pct_mismatch = round(pct_mismatch, 4)) %&gt;% \n  filter(flag_match == FALSE) %&gt;% \n  arrange(-pct_mismatch)\n\n# A tibble: 1 × 3\n  flag_match     n pct_mismatch\n  &lt;lgl&gt;      &lt;int&gt;        &lt;dbl&gt;\n1 FALSE        423       0.0001\n\n\nI filter out the rows where the state doesn’t match the congressional district.\n\nppp_combined |&gt; \n  filter(project_state != state_2) |&gt; \n  distinct(project_state, state_2)\n\n# A tibble: 236 × 2\n   project_state state_2\n   &lt;chr&gt;         &lt;chr&gt;  \n 1 AL            WA     \n 2 AL            FL     \n 3 AZ            TX     \n 4 CA            ME     \n 5 CA            FL     \n 6 CA            AP     \n 7 CT            NJ     \n 8 FL            TX     \n 9 FL            WV     \n10 FL            CA     \n# ℹ 226 more rows\n\nppp_combined &lt;- ppp_combined %&gt;% \n  filter(project_state == state_2)\n\nThis shows that there are some negative values in loan_amount. I filter out those values.\n\nppp_combined %&gt;% \n  mutate(loan_type = current_approval_amount &gt; 0) %&gt;% \n  count(loan_type)\n\n# A tibble: 2 × 2\n  loan_type       n\n  &lt;lgl&gt;       &lt;int&gt;\n1 FALSE           1\n2 TRUE      4811540\n\nppp_combined &lt;- ppp_combined %&gt;% \n  filter(current_approval_amount &gt; 0, !is.na(current_approval_amount))"
  },
  {
    "objectID": "posts/working-with-paycheck-protection-program-data-in-r/index.html#analysis",
    "href": "posts/working-with-paycheck-protection-program-data-in-r/index.html#analysis",
    "title": "Working with Paycheck Protection Program data in R",
    "section": "Analysis",
    "text": "Analysis\n\nLoan amount\nThe first step is to split the data into 2 buckets. For loans above $150k, the SBA binned the loan amount instead of reporting the actual value.\n\nppp_under_150 &lt;- ppp_combined %&gt;% \n  filter(!str_detect(file_name, \"150k_plus\"))\n\nppp_over_150 &lt;- ppp_combined %&gt;% \n  filter(str_detect(file_name, \"150k_plus\"))\n\nAmong loans less than 150k, most are less than 21k, and the distribution is very heavily skewed to the right.\n\nquantiles &lt;- ppp_under_150 %&gt;% \n  reframe(quantiles = quantile(current_approval_amount, probs = c(.25, .50, .75)), \n            probability = c(\"25th\", \"50th\", \"75th\")) %&gt;% \n  mutate(probability = as.factor(probability))\n\nppp_under_150 %&gt;% \n  ggplot(aes(current_approval_amount)) +\n  geom_histogram() +\n  geom_vline(data = quantiles, aes(xintercept = quantiles, color = probability)) +\n  scale_y_comma() +\n  scale_x_comma(labels = scales::dollar) +\n  labs(title = \"SBA PPP Loans\",\n       subtitle = \"Among loans less than $150k\",\n       x = \"Loan amount\",\n       y = \"Number of loans\",\n       color = \"Quantile\")\n\n\n\n\n\n\n\n\nLoans under $150k make up the vast majority of all PPP loans.\n\nppp_combined |&gt; \n  count(loan_range) %&gt;%   \n  mutate(loan_range = fct_reorder(loan_range, n)) %&gt;% \n  ggplot(aes(n, loan_range)) +\n  geom_col() +\n  labs(title = \"SBA PPP Loans\",\n       subtitle = \"All loans\",\n       x = \"Number of loans\",\n       y = \"Loan range\") +\n  scale_x_comma()\n\n\n\n\n\n\n\n\nLoan approvals peaked in late April 2020 when the program began accepting applications for the second round, and picked up in July. There is extreme weekday-weekend seasonality in the data after April. The “U” shape from May to July generally coincides with the effort to “reopen” economies nationwide.\n\nppp_combined %&gt;% \n  count(date_approved) %&gt;% \n  ggplot(aes(date_approved, n)) +\n  geom_point() +\n  geom_vline(xintercept = ymd(\"2020-04-16\"), linetype = 2) +\n  labs(title = \"SBA PPP Loans\",\n       x = NULL,\n       y = \"Number of loans\") +\n  scale_y_comma()\n\n\n\n\n\n\n\n\nThis shows that bigger loans tended to be approved earlier in the program, which was a major criticism.\n\nppp_combined %&gt;% \n  count(date_approved, loan_range) %&gt;% \n  mutate(loan_range = fct_reorder(loan_range, n) %&gt;% fct_rev) %&gt;% \n  group_by(date_approved) %&gt;% \n  mutate(pct_of_loans = n / sum(n)) %&gt;% \n  ungroup() %&gt;% \n  ggplot(aes(date_approved, pct_of_loans, color = loan_range)) +\n  geom_line() +\n  scale_x_date(expand = c(0,0)) +\n  scale_y_percent(limits = c(0,1.1)) +\n  scale_color_viridis_d() +\n  labs(title = \"SBA PPP Loans\",\n       subtitle = \"% of loans approved on a date\",\n       x = NULL,\n       y = \"% of loans\",\n       fill = \"Loan range\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nMost states received similar proportions of small and large loans. Territories received more small loans and fewer large loans.\n\nppp_combined %&gt;% \n  mutate(loan_range = fct_inorder(loan_range)) %&gt;% \n  count(project_state, loan_range) %&gt;% \n  group_by(project_state) %&gt;% \n  mutate(pct_of_loans = n / sum(n)) %&gt;% \n  ungroup() %&gt;% \n  filter(!is.na(project_state)) %&gt;% \n  mutate(project_state = fct_reorder(project_state, n, .fun = sum)) %&gt;% \n  ggplot(aes(pct_of_loans, project_state, fill = loan_range)) +\n  geom_col() +\n  scale_fill_viridis_d(direction = -1) +\n  scale_x_percent() +\n  labs(title = \"SBA PPP Loans\",\n       subtitle = \"% of loan range by state\",\n       x = \"% of loans\",\n       y = NULL,\n       fill = \"Loan range\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\nJobs\nBigger loans tended to retain more jobs.\n\nppp_combined %&gt;% \n  filter(!is.na(jobs_reported)) %&gt;% \n  mutate(loan_range = fct_inorder(loan_range) %&gt;% fct_rev()) %&gt;% \n  ggplot(aes(jobs_reported, fill = loan_range)) +\n  #geom_histogram(bins = 200) +\n  geom_density() +\n  facet_wrap(~loan_range, ncol = 2, scales = \"free\") +\n  scale_x_comma() +\n  #scale_y_comma() +\n  scale_fill_viridis_d() +\n  labs(title = \"SBA PPP Loan Program\",\n       subtitle = \"Jobs retained\",\n       x = \"Number of jobs retained\",\n       y = \"Density\",\n       fill = \"Loan range\")\n\n\n\n\n\n\n\n\nThe timeline of jobs_reported closely matches the trend of when loans were approved.\n\nppp_combined %&gt;% \n  filter(!is.na(jobs_reported)) %&gt;% \n  group_by(date_approved) %&gt;% \n  summarize(jobs_reported = sum(jobs_reported)) %&gt;% \n  ungroup() %&gt;% \n  ggplot(aes(date_approved, jobs_reported)) +\n  geom_point() +\n  scale_y_comma() +\n  labs(title = \"SBA PPP Loan Program\",\n       subtitle = \"Jobs Retained\",\n       x = NULL,\n       y = \"Jobs retained\")\n\n\n\n\n\n\n\n\nAmong loans less than 150k and where the data was reported, the median loan retained 7 jobs per 50k spent. Systemic reporting bias probably makes this number less reliable.\n\nppp_combined %&gt;% \n  filter(loan_range == \"Under $150k\",\n         !is.na(jobs_reported),\n         !is.na(current_approval_amount)) %&gt;% \n  select(jobs_reported, current_approval_amount) %&gt;% \n  mutate(jobs_retained_per_50k = (jobs_reported / current_approval_amount) * 50000) %&gt;% \n  summarize(jobs_retained_per_50k = median(jobs_retained_per_50k))\n\n# A tibble: 1 × 1\n  jobs_retained_per_50k\n                  &lt;dbl&gt;\n1                  7.11\n\n\nIn the same loan range, bigger loans generally meant more jobs retained.\n\ntest &lt;- ppp_combined %&gt;% \n  filter(loan_range == \"Under $150k\") %&gt;% \n  count(current_approval_amount, jobs_reported, sort = TRUE) %&gt;%\n  slice(1:5000)\n\ntest %&gt;% \n  ggplot(aes(current_approval_amount, jobs_reported)) +\n  geom_density_2d_filled() +\n  scale_x_comma(labels = scales::dollar) +\n  labs(title = \"SBA PPP Loan Program\",\n       subtitle = NULL,\n       x = \"Loan amount\",\n       y = \"Jobs retained\",\n       fill = \"Density of observations\")\n\n\n\n\n\n\n\n\n\n\nMapping the data\nThe dataset identifies which federal congressional district the applicant is from. I use tigris to retrieve the shapefiles for the most recent districts.\n\ncongress_districts &lt;- suppressMessages(congressional_districts(cb = TRUE, resolution = \"500k\", year = 2020)) %&gt;% \n  st_as_sf() %&gt;% \n  clean_names() %&gt;% \n  filter(statefp == 42)\n\nThis counts how many of each loan_range a district received. Note that there are missing values and what appear to be defunct district IDs in the district column.\n\nppp_pa_districts_loan_range &lt;- ppp_combined %&gt;% \n  filter(project_state == \"PA\") %&gt;% \n  count(cd, loan_range, sort = TRUE)\n\nppp_pa_districts_loan_range %&gt;% \n  mutate(cd = fct_explicit_na(cd),\n         cd = fct_reorder(cd, n, .fun = sum)) %&gt;% \n  ggplot(aes(n, cd, fill = loan_range)) +\n  geom_col() +\n  scale_x_comma() +\n  scale_fill_viridis_d() +\n  labs(title = \"SBA PPP Loan Program\",\n       subtitle = \"Pennsylvania\",\n       x = \"Number of loans\",\n       y = \"Congressional District\",\n       fill =  \"Loan range\")\n\n\n\n\n\n\n\n\nDistricts in eastern Pennsylvania near Philadelphia received more loans from the program.\n\nppp_pa_districts &lt;- ppp_combined %&gt;% \n  filter(project_state == \"PA\") %&gt;% \n  count(district, sort = TRUE)\n\ncongress_districts %&gt;% \n  right_join(ppp_pa_districts, by = c(\"cd116fp\" = \"district\")) %&gt;% \n  ggplot() +\n  geom_sf(aes(fill = n)) +\n  scale_fill_viridis_c(option = \"A\", labels = scales::comma) +\n  labs(title = \"SBA PPP Loan Program\",\n       subtitle = \"Pennsylvania\",\n       fill = \"Number of loans\") +\n  theme_void()"
  },
  {
    "objectID": "posts/riverhounds_lilley/index.html",
    "href": "posts/riverhounds_lilley/index.html",
    "title": "Pittsburgh Riverhounds under Coach Lilley",
    "section": "",
    "text": "I have been a season-ticket holder with the Pittsburgh Riverhounds for a couple seasons now. The stadium has a great fan experience, and the team has gotten a lot better over the past few years. A major part of that is the head coach, Bob Lilley. I will use some data from American Soccer Analysis to show how the Riverhounds have improved. Their website has an explainer on expected goals and other metrics they calculate.\nLoad libraries and configure settings:\n\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(hrbrthemes)\nlibrary(ggrepel)\n\ntheme_set(theme_ipsum(base_size = 18))\n\n#source https://app.americansocceranalysis.com/#!/\n\nI pulled a CSV of team-level goal metrics for the last 4 USL seasons from the ASA website. This shows the available data:\n\nusl &lt;- read_csv(\"post_data/american_soccer_analysis_uslc_xgoals_teams_2023-10-15.csv\") %&gt;% \n  clean_names() %&gt;% \n  select(-x1) %&gt;% \n  mutate(coach = case_when(team == \"PIT\" & season &gt;= 2018 ~ \"Lilley\",\n                           team == \"PIT\" & season &lt; 2018 ~ \"Brandt\",\n                           TRUE ~ NA_character_)) |&gt; \n  filter(season &lt; 2021)\n\nglimpse(usl)\n\nRows: 134\nColumns: 15\n$ team    &lt;chr&gt; \"PHX\", \"CIN\", \"RNO\", \"LOU\", \"HFD\", \"PIT\", \"SLC\", \"SA\", \"TBR\", …\n$ season  &lt;dbl&gt; 2019, 2018, 2020, 2020, 2020, 2020, 2017, 2020, 2020, 2020, 20…\n$ games   &lt;dbl&gt; 34, 34, 16, 16, 16, 16, 32, 16, 16, 16, 34, 15, 16, 34, 34, 34…\n$ sht_f   &lt;dbl&gt; 16.79, 12.68, 16.06, 15.06, 10.31, 10.81, 11.88, 14.81, 12.63,…\n$ sht_a   &lt;dbl&gt; 13.32, 15.15, 14.94, 9.56, 12.19, 7.81, 11.41, 12.44, 9.38, 13…\n$ gf      &lt;dbl&gt; 2.53, 2.06, 2.69, 1.75, 1.88, 2.38, 1.84, 1.88, 1.56, 2.75, 1.…\n$ ga      &lt;dbl&gt; 1.00, 0.97, 1.31, 0.75, 1.44, 0.63, 0.91, 0.75, 0.63, 1.19, 0.…\n$ gd      &lt;dbl&gt; 1.53, 1.09, 1.38, 1.00, 0.44, 1.75, 0.94, 1.13, 0.94, 1.56, 0.…\n$ x_gf    &lt;dbl&gt; 2.08, 1.43, 2.25, 1.48, 1.32, 1.69, 1.46, 1.58, 1.63, 2.39, 1.…\n$ x_ga    &lt;dbl&gt; 1.37, 1.26, 1.53, 1.01, 1.35, 0.94, 1.34, 1.17, 0.84, 1.30, 0.…\n$ x_gd    &lt;dbl&gt; 0.71, 0.17, 0.72, 0.47, -0.03, 0.75, 0.12, 0.42, 0.79, 1.10, 0…\n$ gd_x_gd &lt;dbl&gt; 0.82, 0.92, 0.65, 0.53, 0.46, 1.00, 0.81, 0.71, 0.15, 0.47, 0.…\n$ pts     &lt;dbl&gt; 2.29, 2.26, 2.25, 2.19, 2.19, 2.13, 2.09, 2.06, 2.06, 2.00, 2.…\n$ x_pts   &lt;dbl&gt; 1.80, 1.49, 1.83, 1.70, 1.39, 1.86, 1.44, 1.64, 1.85, 1.98, 1.…\n$ coach   &lt;chr&gt; NA, NA, NA, NA, NA, \"Lilley\", NA, NA, NA, NA, \"Lilley\", NA, NA…\n\n\nThe Riverhound’s statistics show clear improvement in 2018 when Lilley took over from Brandt. The team immediately began scoring more than they allowed. The team’s expected goals for and against also improved, which shows that the improvement wasn’t a matter of luck.\n\ngoal_data &lt;- usl %&gt;% \n  filter(team == \"PIT\") %&gt;% \n  select(team, season, gf, x_gf, ga, x_ga) %&gt;% \n  pivot_longer(cols = c(gf, x_gf, ga, x_ga), names_to = \"g_type\", values_to = \"g_value\") %&gt;%\n  mutate(goal_type = case_when(str_detect(g_type, \"gf$\") ~ \"For\",\n                               TRUE ~ \"Against\")) %&gt;% \n  mutate(metric_type = case_when(str_detect(g_type, \"^x_\") ~ \"Expected\",\n                                 TRUE ~ \"Actual\"))\n\ngoal_data %&gt;% \n  ggplot(aes(season, g_value, color = goal_type, lty = metric_type)) +\n  geom_line(size = 1.5) +\n  geom_point(data = filter(goal_data, metric_type == \"Actual\"), size = 2) +\n  labs(title = \"Pittsburgh Riverhounds\",\n       subtitle = \"Expected and Actual Goals per game\",\n       x = \"Season\",\n       y = \"Goals\",\n       color = \"Goal Type\",\n       lty = \"Metric Type\")\n\n\n\n\n\n\n\n\nThis shows that in terms of expected goal difference, the Riverhounds became one of the top teams in the USL once Lilley took over.\n\nusl %&gt;% \n  ggplot(aes(season, x_gd, group = team)) +\n  geom_hline(yintercept = 0, size = 1, lty = 2) +\n  geom_line(color = \"black\", alpha = .2) +\n  geom_line(data = filter(usl, team == \"PIT\"), \n            color = \"gold\", size = 2) +\n  geom_point(data = filter(usl, team == \"PIT\"),\n             aes(fill = coach),\n             shape = 21, size = 4) +\n  scale_fill_manual(values = c(\"grey\", \"gold\")) +\n  #coord_fixed(ratio = .5) +\n  labs(title = \"xG difference per game\",\n       x = \"Season\",\n       y = \"xG Difference\",\n       fill = \"Riverhounds Coach\",\n       caption = \"Grey lines show other USL teams\")\n\n\n\n\n\n\n\n\nLilley’s Riverhounds are consistently better than league average in terms of expected goals.\n\nusl %&gt;% \n  ggplot(aes(x_gd)) +\n  #geom_histogram(binwidth = .2) +\n  geom_vline(data = filter(usl, team == \"PIT\"), aes(xintercept = x_gd), size = 3) +\n  geom_vline(data = filter(usl, team == \"PIT\"), aes(xintercept = x_gd, color = coach),\n             size = 2.5, key_glyph = \"rect\") +\n  geom_density(aes(y = ..count.. * .2), fill = \"white\", alpha = 1) +\n  geom_vline(xintercept = 0, lty = 2) +\n  geom_hline(yintercept = 0) +\n  scale_color_manual(values = c(\"grey\", \"gold\")) +\n  scale_x_continuous(expand = c(0,0)) +\n  scale_y_continuous(expand = c(0,0)) +\n  coord_cartesian(ylim = c(0, 25)) +\n  #coord_fixed(ratio = .1) +\n  labs(title = \"xG Difference Per Game\",\n       subtitle = \"Distribution of all USL teams 2017-2020\",\n       x = \"xG\",\n       y = \"Number of teams\",\n       color = \"Riverhounds Coach\") +\n  theme(legend.key = element_rect(color = \"black\"))\n\n\n\n\n\n\n\n\nWhile the 2020 Riverhounds were a very good team, they were not quite as good as their plain goals for/against would show. This graph shows that they were fortunate to do as well as they did (which, again, was very well).\n\nusl %&gt;% \n  mutate(logo = case_when(team == \"PIT\" ~ \"post_data/pit_logo.png\",\n                          TRUE ~ NA_character_)) %&gt;% \n  ggplot(aes(x_gd, gd)) +\n  geom_abline(lty = 2) +\n  geom_point(alpha = .3) +\n  ggimage::geom_image(aes(image = logo)) +\n  geom_label_repel(data = filter(usl, team == \"PIT\"),\n                   aes(label = season, fill = coach),\n                   force = 5,\n                   key_glyph = \"rect\") +\n  annotate(\"text\", label = \"Under-performing\",\n           x = .75, y = -1.5) +\n  annotate(\"text\", label = \"Over-performing\",\n           x = -1, y = 1.5) +\n  tune::coord_obs_pred() +\n  scale_fill_manual(values = c(\"grey\", \"gold\")) +\n  labs(title = \"Goal and xG difference per game\",\n       x = \"xG Difference\",\n       y = \"Goal Difference\",\n       fill = \"Riverhounds Coach\") +\n  theme(legend.key = element_rect(color = \"black\"))\n\n\n\n\n\n\n\n\nThis shows that the 2020 Riverhounds were probably one of the most fortunate teams in the league, in addition to being very good.\n\nusl %&gt;% \n  ggplot(aes(season, gd_x_gd, group = team)) +\n  geom_hline(yintercept = 0, lty = 2) +\n  geom_line(color = \"black\", alpha = .2) +\n  geom_line(data = filter(usl, team == \"PIT\"),\n            color = \"gold\", size = 2) +\n  geom_point(data = filter(usl, team == \"PIT\"),\n             aes(fill = coach, group = team),\n             shape = 21, size = 4, color = \"black\") +\n  scale_fill_manual(values = c(\"grey\", \"gold\")) +\n  coord_cartesian(ylim = c(-1.5, 1.5)) +\n  #coord_fixed(ratio = .5) +\n  labs(title = \"Goal difference - xG difference\",\n       subtitle = \"Per game\",\n       x = \"Season\",\n       y = substitute(paste(\"\" %&lt;-% \"\", \"Under-performing\", \"  |  \", \"Over-performing\", \"\" %-&gt;% \"\")),\n       fill = \"Riverhounds Coach\",\n       caption = \"Grey lines show other USL teams\")\n\n\n\n\n\n\n\n\nIn FiveThirtyEights’ Global Soccer Power Index, the Riverhounds will begin the 2021 season ranked around #460 out of 639 teams."
  },
  {
    "objectID": "posts/cleaning-creatively-formatted-u-s-census-bureau-migration-data/index.html",
    "href": "posts/cleaning-creatively-formatted-u-s-census-bureau-migration-data/index.html",
    "title": "Cleaning Creatively Formatted US Census Bureau Migration Data",
    "section": "",
    "text": "I recently ran across the U.S. Census Bureau page for state-to-state migration flows. There’s a lot of interesting analyses that can be done with a dataset like this. Unfortunately, when I opened up the .xls file, I realized that the data is not formatted in a way that makes it easy to analyze.\n\nThe data is extremely wide and contains a header, merged cells, multiple layers of column names, duplicate rows, and a footer. I assume there is a good reason for this format, but it does not make it easy for analysts to use.\nThe readxl package makes working with Excel files and the attendant formatting issues very easy. It supports the legacy .xls format and has many useful functions that help you transform Excel sheets designed for presentation into data that is ready for analysis. The goal is to have from_state (where the migration flowed from), columns for summary statistics about the from_state, to_state (where the migration flowed to), and migration_amount (how many people migrated).\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(readxl)\nlibrary(janitor)\nlibrary(tidytext)\n\ntheme_set(theme_bw())\n\nThe first step is to read in the data with read_xls. I opened the .xls file to find the range that contained the data, and set Excel’s “N/A” value to be NA.\n\ndf &lt;- read_xls(\"post_data/State_to_State_Migrations_Table_2017.xls\", range = \"A7:DZ78\", na = \"N/A\")\n\n\nThe column headers are clearly malformed and there are rows of useless or blank values. The data is wide, and each from_state has its own column for Estimate and MOE (Margin of Error). For my purposes, I only want to keep the Estimate data for each from_state.\nNext, I identify the colums that contain the summary statistics I want to preserve. I identify the columns with their numeric index since the column names are mangled.\n\ndf &lt;- df %&gt;% \n    rename(state_from = 1,\n           state_population_same_one_year_plus = 2,\n           state_population_same_house_one_year = 4,\n           state_population_same_state_one_year = 6,\n           state_population_different_state_residence_one_year = 8)\n\n\nNext, I drop the columns that contain the MOEs and other duplicate columns. read_xls did a good job identifying those automatically, so I can just search for \"..\", the string that the function uses to name columns it didn’t find a name for.\n\ndf &lt;- df %&gt;% \n    select(-contains(\"..\")) %&gt;%\n    clean_names() %&gt;%\n    filter_all(any_vars(!str_detect(., \"Estimate\"))) %&gt;%\n    filter(!str_detect(state_from, \"residence\"))\n\n\nThis is much closer to a dataset that can be used for analysis. The remaining steps are pretty standard data munging activities. I turn the relevant columns from character to numeric type, filter out data I don’t need, and pivot the data from wide to long. I aso compute the total migration population by state.\n\ndf &lt;- df %&gt;%\n  mutate_at(vars(2:ncol(df)), as.numeric) %&gt;%\n  filter(!str_detect(state_from, \"United States\")) %&gt;%\n  pivot_longer(cols = 6:ncol(df), names_to = \"state_to\", values_to = \"migration\") %&gt;%\n  group_by(state_from) %&gt;%\n  mutate(total_migration_population = sum(migration, na.rm = TRUE)) %&gt;%\n  ungroup() \n\n\nFinally, I calculate what % of a from_state’s total outward migration goes to each to_state and clean up the to_state names.\n\ndf &lt;- df %&gt;% \n  mutate(pct_migrated = migration / total_migration_population,\n        state_to = str_replace_all(state_to, \"_\", \" \"),\n        state_to = str_to_title(state_to),\n        state_to = str_remove_all(state_to, \"[[:digit:]]\"))\n\nThis is the final dataset: \nI turned the above steps into a function called clean_census_migration_data. This lets me read in and clean the data in one line.\n\nclean_census_migration_data &lt;- function(data){\n  \n  message(str_c(\"Reading in:\", data, sep = \" \"))\n  \n  df &lt;- suppressMessages(read_xls(data, range = \"A7:DZ78\", na = \"N/A\")) %&gt;%\n    rename(state_from = 1,\n           state_population_same_one_year_plus = 2,\n           state_population_same_house_one_year = 4,\n           state_population_same_state_one_year = 6,\n           state_population_different_state_residence_one_year = 8) %&gt;%\n    select(-contains(\"..\")) %&gt;%\n    clean_names() %&gt;%\n    filter_all(any_vars(!str_detect(., \"Estimate\"))) %&gt;%\n    filter(!str_detect(state_from, \"residence\"))\n\n  message(str_c(\"Rows:\", nrow(df), sep = \" \"))\n  message(str_c(\"Columns:\", ncol(df), sep = \" \"))\n          \n\n  df &lt;- df %&gt;%\n    mutate_at(vars(2:ncol(df)), as.numeric) %&gt;%\n    filter(!str_detect(state_from, \"United States\")) %&gt;%\n    pivot_longer(cols = 6:ncol(df), names_to = \"state_to\", values_to = \"migration\") %&gt;%\n    group_by(state_from) %&gt;%\n    mutate(total_migration_population = sum(migration, na.rm = TRUE)) %&gt;%\n    ungroup() %&gt;%\n    mutate(pct_migrated = migration / total_migration_population,\n           state_to = str_replace_all(state_to, \"_\", \" \"),\n           state_to = str_to_title(state_to),\n           state_to = str_remove_all(state_to, \"[[:digit:]]\"),\n           state_to = str_replace(state_to, \" Of \", \" of \"))\n\n  return(df)\n}\n\nSince the Census Bureau provides a file for each year, I can map clean_census_migration_data to multiple files in a list and combine them. This allows me to compare the data longitudinally.\n\nmigration_files &lt;- list.files(\"post_data\", full.names = TRUE) %&gt;% \n  keep(str_detect(., \".xls$\"))\n\nmigration_files\n\n[1] \"post_data/state_to_state_migrations_table_2010.xls\"\n[2] \"post_data/state_to_state_migrations_table_2011.xls\"\n[3] \"post_data/state_to_state_migrations_table_2012.xls\"\n[4] \"post_data/state_to_state_migrations_table_2013.xls\"\n[5] \"post_data/State_to_State_Migrations_Table_2014.xls\"\n[6] \"post_data/State_to_State_Migrations_Table_2015.xls\"\n[7] \"post_data/State_to_State_Migrations_Table_2016.xls\"\n[8] \"post_data/State_to_State_Migrations_Table_2017.xls\"\n[9] \"post_data/State_to_State_Migrations_Table_2018.xls\"\n\n\n\ndf_migration &lt;- migration_files %&gt;% \n  set_names() %&gt;% \n  map_dfr(clean_census_migration_data, .id = \"file_name\") %&gt;% \n  mutate(year = str_extract(file_name, \"[[:digit:]]{4}\") %&gt;% as.numeric) %&gt;% \n  select(year, everything(), -file_name)\n\nReading in: post_data/state_to_state_migrations_table_2010.xls\n\n\nRows: 53\n\n\nColumns: 59\n\n\nReading in: post_data/state_to_state_migrations_table_2011.xls\n\n\nRows: 53\n\n\nColumns: 59\n\n\nReading in: post_data/state_to_state_migrations_table_2012.xls\n\n\nRows: 53\n\n\nColumns: 59\n\n\nReading in: post_data/state_to_state_migrations_table_2013.xls\n\n\nRows: 53\n\n\nColumns: 59\n\n\nReading in: post_data/State_to_State_Migrations_Table_2014.xls\n\n\nRows: 53\n\n\nColumns: 59\n\n\nReading in: post_data/State_to_State_Migrations_Table_2015.xls\n\n\nRows: 53\n\n\nColumns: 59\n\n\nReading in: post_data/State_to_State_Migrations_Table_2016.xls\n\n\nRows: 53\n\n\nColumns: 59\n\n\nReading in: post_data/State_to_State_Migrations_Table_2017.xls\n\n\nRows: 53\n\n\nColumns: 59\n\n\nReading in: post_data/State_to_State_Migrations_Table_2018.xls\n\n\nRows: 53\n\n\nColumns: 59\n\n\n\nNote: the above function will not work with the 2009 migration file because the Census Bureau did not include summary statistics about the from_state for that year. You can read in the 2009 file separately with a modified clean_census_migration_data function and join that back to the rest of the data."
  },
  {
    "objectID": "posts/mapping-boswash-commuter-patterns/index.html",
    "href": "posts/mapping-boswash-commuter-patterns/index.html",
    "title": "Mapping BosWash commuter patterns with Flowmap.blue",
    "section": "",
    "text": "This map shows the commuter patterns in the Northeast Megalopolis/Acela Corridor/BosWash metro area. I pulled the data from the Census Longitudinal Employer-Household Dynamics (LODES) system via the {lehdr} package. The map was created through the Flowmap.blue tool, which makes interactive maps of movement between areas. Flowmap.blue also exposes a bunch of cool features, like animating and clustering connections, among others.\nYou can view a full version of the map here."
  },
  {
    "objectID": "posts/mapping-boswash-commuter-patterns/index.html#code",
    "href": "posts/mapping-boswash-commuter-patterns/index.html#code",
    "title": "Mapping BosWash commuter patterns with Flowmap.blue",
    "section": "Code",
    "text": "Code\nThis code is what I used to query the LODES data and aggregate it. First, load the required libraries.\n\nlibrary(tidyverse)\nlibrary(tidycensus)\nlibrary(janitor)\nlibrary(lehdr)\nlibrary(tigris)\nlibrary(sf)\nlibrary(ggraph)\nlibrary(tidygraph)\nlibrary(googlesheets4)\nlibrary(googledrive)\n\noptions(tigris_use_cache = TRUE,\n        scipen = 999,\n        digits = 4)\n\nThis code gets the main and aux LODES data for each state that I name in the states object. I then combine the data into lodes_combined and check that there are no duplicate origin-destination pairs. Be warned that these files are large (100-500MB each), and can take a bit to read into R, depending on your machine.\n\n#get lodes\nstates &lt;- c(\"pa\", \"wv\", \"va\", \"dc\", \"de\",\n            \"md\", \"ny\", \"ri\", \"ct\", \"ma\", \"vt\", \"nh\", \"me\")\n\nlodes_od_main &lt;- grab_lodes(state = states, year = 2017, \n                            lodes_type = \"od\", job_type = \"JT00\", \n                            segment = \"S000\", state_part = \"main\", \n                            agg_geo = \"county\",\n                            use_cache = TRUE) %&gt;% \n  select(state, w_county, h_county, S000, year) %&gt;% \n  rename(commuters = S000)\n\nlodes_od_aux &lt;- grab_lodes(state = states, year = 2017, \n                           lodes_type = \"od\", job_type = \"JT00\", \n                           segment = \"S000\", state_part = \"aux\", \n                           agg_geo = \"county\",\n                           use_cache = TRUE) %&gt;% \n  select(state, w_county, h_county, S000, year) %&gt;% \n  rename(commuters = S000)\n\nlodes_combined &lt;- bind_rows(lodes_od_main, lodes_od_aux)\n\nThis code pulls the geometry for the states from the TIGER shapefile API:\n\ncounties_combined &lt;- tigris::counties(state = c(\"PA\", \"NY\", \"NJ\", \"MD\", \n                                                \"WV\", \"DE\", \"VA\", \n                                                \"DC\", \"MA\", \"CT\", \"VT\", \n                                                \"RI\", \"NH\", \"ME\"), \n                                      cb = TRUE) %&gt;% \n  arrange(STATEFP) %&gt;% \n  left_join(fips_codes %&gt;% distinct(state_code, state_name), by = c(\"STATEFP\" = \"state_code\"))\n\ncounties_combined %&gt;% \n  ggplot() +\n  geom_sf(aes(fill = state_name))\n\n\n\n\n\n\n\n\nThe next step is to calculate the centroid of each county that will be used in the final map.\n\nnode_pos &lt;- counties_combined %&gt;% \n  mutate(centroid = map(geometry, st_centroid),\n         x = map_dbl(centroid, 1),\n         y = map_dbl(centroid, 2)) %&gt;% \n  select(GEOID, NAME, x, y) %&gt;% \n  arrange(GEOID) %&gt;% \n  st_drop_geometry() %&gt;% \n  as_tibble() %&gt;% \n  select(-NAME) %&gt;%\n  rename(lon = x,\n         lat = y) %&gt;% \n  mutate(id = row_number()) %&gt;% \n  select(id, GEOID, lat, lon)\n\nThen I add the county and state name to the node positions so the name is intelligible.\n\nnode_pos &lt;- node_pos %&gt;% \n  left_join(st_drop_geometry(counties_combined), by = c(\"GEOID\" = \"GEOID\")) %&gt;% \n  mutate(county_name = str_c(NAME, \"County\", sep = \" \"),\n         name = str_c(county_name, state_name, sep = \", \"))\n\nnode_pos &lt;- node_pos %&gt;% \n  select(id, name, lat, lon, GEOID)\n\nnode_pos\n\n\n\nRows: 433\nColumns: 5\n$ id    &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 1…\n$ name  &lt;chr&gt; \"Fairfield County, Connecticut\", \"Hartford County, Connecticut\",…\n$ lat   &lt;dbl&gt; 41.27, 41.81, 41.79, 41.46, 41.41, 41.49, 41.86, 41.83, 39.09, 3…\n$ lon   &lt;dbl&gt; -73.39, -72.73, -73.25, -72.54, -72.93, -72.10, -72.34, -71.99, …\n$ GEOID &lt;chr&gt; \"09001\", \"09003\", \"09005\", \"09007\", \"09009\", \"09011\", \"09013\", \"…\n\n\nThis processes the LODES origin-destination data and creates the node-edge network graph object that will be fed into the Flowmap.blue service.\n\nnetwork_graph &lt;- lodes_combined %&gt;%\n  semi_join(counties_combined, by = c(\"w_county\" = \"GEOID\")) %&gt;%\n  semi_join(counties_combined, by = c(\"h_county\" = \"GEOID\")) %&gt;%\n  select(h_county, w_county, commuters) %&gt;% \n  as_tbl_graph(directed = TRUE) %&gt;% \n  activate(edges) %&gt;% \n  filter(commuters &gt;= 500,\n         #!edge_is_loop()\n  ) %&gt;%\n  activate(nodes) %&gt;%\n  arrange(name)\n\n\nnodes &lt;- network_graph %&gt;%\n  activate(nodes) %&gt;%\n  as_tibble()\n\nglimpse(nodes)\n\n\n\nRows: 433\nColumns: 1\n$ name &lt;chr&gt; \"09001\", \"09003\", \"09005\", \"09007\", \"09009\", \"09011\", \"09013\", \"0…\n\n\n\nedges &lt;- network_graph %&gt;% \n  activate(edges) %&gt;% \n  as_tibble() %&gt;% \n  rename(origin = from,\n         dest = to,\n         count = commuters) %&gt;% \n  arrange(desc(count))\n\nglimpse(edges)\n\n\n\nRows: 3,309\nColumns: 3\n$ origin &lt;dbl&gt; 128, 161, 121, 149, 61, 121, 138, 210, 112, 2, 127, 125, 138, 1…\n$ dest   &lt;dbl&gt; 128, 161, 128, 149, 61, 121, 128, 210, 112, 2, 127, 125, 138, 1…\n$ count  &lt;dbl&gt; 558308, 483764, 475073, 468885, 450975, 398535, 384941, 371075,…\n\n\nFinally, this code checks that the node position data matches up with the nodes from the network object. If these checks fail, the origin-destination pairs will be mapped to the wrong geographic coordinates.\n\n#check that nodes match up\nall(node_pos$GEOID == nodes$name)\n\n[1] TRUE\n\nidentical(node_pos$GEOID, nodes$name)\n\n[1] TRUE\n\nlength(node_pos$GEOID) == length(nodes$name)\n\n[1] TRUE\n\n\nThis code creates the metadata that Flowmap.blue requires and loads the data into Google Sheets.\n\nmy_properties &lt;- c(\n  \"title\"=\"BosWash regional US commuter flow\",\n  \"description\"=\"Miniumum 500 commuters per origin-destination pair\",\n  \"source.name\"=\"2017 US Census LODES\",\n  \"source.url\"=\"https://lehd.ces.census.gov/data/\",\n  \"createdBy.name\"=\"Conor Tompkins\",\n  \"createdBy.url\"=\"https://ctompkins.netlify.app/\",\n  \"mapbox.mapStyle\"=NA,\n  \"flows.sheets\" = \"flows\",\n  \"colors.scheme\"=\"interpolateViridis\",\n  \"colors.darkMode\"=\"yes\",\n  \"animate.flows\"=\"no\",\n  \"clustering\"=\"yes\"\n)\n\nproperties &lt;- tibble(property=names(my_properties)) %&gt;%\n  mutate(value=my_properties[property])\n\ngs4_auth()\n\ngoogledrive::drive_auth()\n\ndrive_trash(\"lodes_flowmapblue\")\n\nss &lt;- gs4_create(\"lodes_flowmapblue\", sheets = list(properties = properties,\n                                                    locations = node_pos,\n                                                    flows = edges))\n\nThe final step is to allow the Google Sheet to be read by anyone with the link, and copy the Sheet’s link to Flowmap.blue"
  },
  {
    "objectID": "posts/mapping-boswash-commuter-patterns/index.html#references",
    "href": "posts/mapping-boswash-commuter-patterns/index.html#references",
    "title": "Mapping BosWash commuter patterns with Flowmap.blue",
    "section": "References",
    "text": "References\n\nhttps://doodles.mountainmath.ca/blog/2020/01/06/flow-maps/\nhttps://jamgreen.github.io/lehdr/articles/getting_started.html"
  },
  {
    "objectID": "posts/healthy_ride_access_pittsburgh/index.html",
    "href": "posts/healthy_ride_access_pittsburgh/index.html",
    "title": "Bike rental access in Pittsburgh",
    "section": "",
    "text": "This is an interactive Leaflet map of Healthy Ride access in Pittsburgh. It counts how many Healthy Ride stations are within a 10 minute bike ride of a given location. This gives an estimation of how accessible the Healthy Ride service is in a given neighborhood (lighter green = more accessible). As you zoom in, individual bike stations will appear. Click the “full screen” button on the left to maximize your view.\nThere are some obvious cases (like the Wabash Tunnel) where the API doesn’t know that a bicyclist shouldn’t go in there, but overall it is accurate.\nThis map was built with the Mapbox API and was inspired by the Penn MUSA Masterclass 2020 talk that Kyle Walker gave."
  },
  {
    "objectID": "posts/healthy_ride_access_pittsburgh/index.html#build-one-isochrone",
    "href": "posts/healthy_ride_access_pittsburgh/index.html#build-one-isochrone",
    "title": "Bike rental access in Pittsburgh",
    "section": "Build one isochrone",
    "text": "Build one isochrone\nThis takes the first station in the dataframe and uses the Mapbox API to make a test isochrone that shows how far a bicyclist can go in a given period of time (5, 10, 15 minutes).\n\ntest_isochrone_data &lt;- stations %&gt;% \n  slice(1) %&gt;% \n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326) %&gt;% \n  mb_isochrone(profile = \"cycling\", time = c(5, 10, 15))\n\ntest_isochrone_map &lt;- mapbox_map %&gt;% \n  mapdeck() %&gt;% \n  add_polygon(data = test_isochrone_data,\n              fill_colour = \"time\",\n              fill_opacity = 0.5,\n              legend = TRUE) %&gt;% \n  add_scatterplot(data = stations %&gt;% \n                          slice(1) %&gt;% \n                          st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326),\n                  radius = 100,\n                  fill_colour = \"#ffffff\") %&gt;% \n  mapdeck_view(location = c(pgh_coords[1], pgh_coords[2]), zoom = 11)\n\ntest_isochrone_map\n\n\n\n\n\nThe same graph can be made in ggplot2:\n\ntest_isochrone_data %&gt;% \n  ggplot() +\n  geom_sf(aes(fill = time)) +\n  scale_fill_viridis_c() +\n  theme_void()\n\n\n\n\n\n\n\n\nThis calculates the isochrones for all the stations and transforms them into a projected coordinate system:\n\nstation_isochrone &lt;- stations %&gt;%\n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326) %&gt;%\n  mb_isochrone(profile = \"cycling\", time = c(10)) %&gt;%\n  st_transform(3857)\n\nThis shows the overlap between all the isochrones. Interesting to look at, but not very informative.\n\nstation_isochrone %&gt;% \n  ggplot() +\n  geom_sf(fill = \"black\", lwd = 0, alpha = .05) +\n  theme_void()"
  },
  {
    "objectID": "posts/healthy_ride_access_pittsburgh/index.html#build-raster",
    "href": "posts/healthy_ride_access_pittsburgh/index.html#build-raster",
    "title": "Bike rental access in Pittsburgh",
    "section": "Build Raster",
    "text": "Build Raster\nThis builds a raster object that calculates how many isochrones overlap on a given area:\n\n#raster\npolygons_proj &lt;- station_isochrone %&gt;%\n  mutate(test_id = 1) %&gt;% \n  filter(time == 10) %&gt;% \n  st_transform(3857)\n\ntemplate &lt;- raster(polygons_proj, resolution = 25)\n\nraster_surface &lt;- fasterize(polygons_proj, template, field = \"test_id\", fun = \"sum\")\n\nraster_values &lt;- tibble(values = values(raster_surface)) %&gt;% \n  filter(!is.na(values)) %&gt;% \n  distinct(values) %&gt;% \n  pull(values)\n\nplot(raster_surface)"
  },
  {
    "objectID": "posts/healthy_ride_access_pittsburgh/index.html#build-interactive-map",
    "href": "posts/healthy_ride_access_pittsburgh/index.html#build-interactive-map",
    "title": "Bike rental access in Pittsburgh",
    "section": "Build interactive Map",
    "text": "Build interactive Map\nThis builds out the interactive map using leaflet and Mapbox libraries:\n\ncustom_pal &lt;- colorNumeric(\"viridis\", \n                           #0:max_bike_stations, \n                           raster_values,\n                           na.color = \"transparent\")\n\npopup_labels &lt;- sprintf(\"%s \n                        &lt;br&gt;Number of bike racks: %s\",\n                        stations$station_name, stations$number_of_racks) %&gt;% \n  map(htmltools::HTML)\n\nhealth_ride_icon &lt;- makeIcon(\n  iconUrl = \"https://healthyridepgh.com/wp-content/uploads/sites/3/2019/05/NEXTBIKE-LOGO-01.png\",\n  #iconUrl = \"https://healthyridepgh.com/wp-content/uploads/sites/3/2016/09/Healthy-Ride-Logo.Stacked-01.png\",\n  iconWidth = 50, iconHeight = 50,\n  iconAnchorX = 0, iconAnchorY = 0\n)\n\nstation_heatmap &lt;- mapbox_map %&gt;%\n  addPolygons(data = city_boundary,\n              opacity = 1,\n              color = \"black\",\n              fillColor = \"#ffffff\",\n              group = \"City boundary\") %&gt;% \n  addRasterImage(raster_surface, colors = custom_pal, opacity = .75,\n                 group = \"Raster\") %&gt;% \n  addLegend(pal = custom_pal, \n            values = raster_values,\n            title = \"Number of stations&lt;br&gt;within 10-minute bike ride\") %&gt;% \n  addMarkers(data = stations, lng = ~longitude, lat = ~latitude,\n             popup = popup_labels,\n             icon = health_ride_icon,\n             clusterOptions = markerClusterOptions(),\n             group = \"Stations\") %&gt;% \n  addLayersControl(overlayGroups = c(\"City boundary\", \"Raster\", \"Stations\"),\n                   options = layersControlOptions(collapsed = FALSE)) %&gt;% \n  addFullscreenControl() %&gt;% \n  setView(lng = pgh_coords[1], lat = pgh_coords[2], zoom = 12)\n\nframeWidget(station_heatmap, options=frameOptions(allowfullscreen = TRUE))"
  },
  {
    "objectID": "posts/exploring-allegheny-county-with-census-data/index.html",
    "href": "posts/exploring-allegheny-county-with-census-data/index.html",
    "title": "Exploring Allegheny County With Census Data",
    "section": "",
    "text": "This post explores Allegheny County and Pennsylvania through census data. I use the tidycensus and sf packages to collect data from the census API and draw maps with the data."
  },
  {
    "objectID": "posts/exploring-allegheny-county-with-census-data/index.html#setup",
    "href": "posts/exploring-allegheny-county-with-census-data/index.html#setup",
    "title": "Exploring Allegheny County With Census Data",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(tidycensus)\nlibrary(tigris)\nlibrary(sf)\nlibrary(broom)\nlibrary(ggfortify)\nlibrary(viridis)\nlibrary(scales)\nlibrary(janitor)\n\noptions(tigris_use_cache = TRUE)\n\ntheme_set(theme_bw())\n\ncensus_vars &lt;- load_variables(2010, \"sf1\", cache = TRUE)"
  },
  {
    "objectID": "posts/exploring-allegheny-county-with-census-data/index.html#collect-data",
    "href": "posts/exploring-allegheny-county-with-census-data/index.html#collect-data",
    "title": "Exploring Allegheny County With Census Data",
    "section": "Collect data",
    "text": "Collect data\ntidycensus provides a wrapper for the U.S. Census API. You can request a wide variety of data, from economic measures to information about demography. The API also includes data about geographic regions.\nThis code creates a dataframe of some of the variables available through the census API.\n\nvars &lt;- load_variables(2016, \"acs5\", cache = TRUE)\n\nvars\n\n# A tibble: 22,816 × 4\n   name        label                                  concept          geography\n   &lt;chr&gt;       &lt;chr&gt;                                  &lt;chr&gt;            &lt;chr&gt;    \n 1 B00001_001  Estimate!!Total                        UNWEIGHTED SAMP… block gr…\n 2 B00002_001  Estimate!!Total                        UNWEIGHTED SAMP… block gr…\n 3 B01001A_001 Estimate!!Total                        SEX BY AGE (WHI… tract    \n 4 B01001A_002 Estimate!!Total!!Male                  SEX BY AGE (WHI… tract    \n 5 B01001A_003 Estimate!!Total!!Male!!Under 5 years   SEX BY AGE (WHI… tract    \n 6 B01001A_004 Estimate!!Total!!Male!!5 to 9 years    SEX BY AGE (WHI… tract    \n 7 B01001A_005 Estimate!!Total!!Male!!10 to 14 years  SEX BY AGE (WHI… tract    \n 8 B01001A_006 Estimate!!Total!!Male!!15 to 17 years  SEX BY AGE (WHI… tract    \n 9 B01001A_007 Estimate!!Total!!Male!!18 and 19 years SEX BY AGE (WHI… tract    \n10 B01001A_008 Estimate!!Total!!Male!!20 to 24 years  SEX BY AGE (WHI… tract    \n# ℹ 22,806 more rows\n\n\nThis code requests information about the median income of census tracts in Allegheny County. The “geography” argument sets the level of geographic granularity.\n\nallegheny &lt;- get_acs(state = \"PA\", \n                     county = \"Allegheny County\", \n                     geography = \"tract\", \n                     variables = c(median_income = \"B19013_001\"),\n                     year = 2018,\n                     geometry = TRUE,\n                     cb = FALSE)\n\nhead(allegheny)\n\nSimple feature collection with 6 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -80.16148 ymin: 40.41478 xmax: -79.88377 ymax: 40.57546\nGeodetic CRS:  NAD83\n        GEOID                                              NAME      variable\n1 42003563200 Census Tract 5632, Allegheny County, Pennsylvania median_income\n2 42003980000 Census Tract 9800, Allegheny County, Pennsylvania median_income\n3 42003564100 Census Tract 5641, Allegheny County, Pennsylvania median_income\n4 42003461000 Census Tract 4610, Allegheny County, Pennsylvania median_income\n5 42003437000 Census Tract 4370, Allegheny County, Pennsylvania median_income\n6 42003981800 Census Tract 9818, Allegheny County, Pennsylvania median_income\n  estimate   moe                       geometry\n1    29750  8141 MULTIPOLYGON (((-80.00469 4...\n2       NA    NA MULTIPOLYGON (((-79.90168 4...\n3   145179 11268 MULTIPOLYGON (((-80.09943 4...\n4    39063  6923 MULTIPOLYGON (((-80.16148 4...\n5   106250 11871 MULTIPOLYGON (((-80.12246 4...\n6       NA    NA MULTIPOLYGON (((-79.90822 4...\n\n\nThis code maps the data onto the census tracts.\n\nallegheny %&gt;%\n  ggplot(aes(fill = estimate, color = estimate)) + \n  geom_sf() + \n  scale_fill_viridis(\"Median household income\", option = \"magma\", labels = comma) +\n  scale_color_viridis(\"Median household income\", option = \"magma\", labels = comma) +\n  labs(title = \"Allegheny County\",\n       subtitle = \"American Community Survey\") +\n  theme(axis.text = element_blank())\n\n\n\n\n\n\n\n\nThis code requests information about the ethnicities within each census tract. Then, it calculates the percentage of the entire population of a tract that each ethnicity makes up.\n\nracevars &lt;- c(White = \"P005003\", \n              Black = \"P005004\", \n              Asian = \"P005006\", \n              Hispanic = \"P004003\")\n\nget_decennial(geography = \"tract\", \n              variables = racevars,\n              state = \"PA\", \n              county = \"Allegheny\", \n              geometry = TRUE,\n              summary_var = \"P001001\",\n              year = 2010,) %&gt;% \n  mutate(value = value / summary_value,\n         variable = str_c(\"percent_\", tolower(variable))) -&gt; allegheny_race\n\nhead(allegheny_race)\n\nSimple feature collection with 6 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -80.12431 ymin: 40.54225 xmax: -79.99058 ymax: 40.61431\nGeodetic CRS:  NAD83\n# A tibble: 6 × 6\n  GEOID       NAME      variable   value summary_value                  geometry\n  &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt;      &lt;dbl&gt;         &lt;dbl&gt;        &lt;MULTIPOLYGON [°]&gt;\n1 42003412002 Census T… percent… 0.916            4865 (((-80.07936 40.58043, -…\n2 42003412002 Census T… percent… 0.00843          4865 (((-80.07936 40.58043, -…\n3 42003412002 Census T… percent… 0.0580           4865 (((-80.07936 40.58043, -…\n4 42003412002 Census T… percent… 0.0103           4865 (((-80.07936 40.58043, -…\n5 42003413100 Census T… percent… 0.878            6609 (((-80.06788 40.60846, -…\n6 42003413100 Census T… percent… 0.0172           6609 (((-80.06788 40.60846, -…\n\n\nThis code maps that data. The facet_wrap function creates a map for each ethnicity.\n\n#allegheny_race &lt;- st_erase(allegheny_race, allegheny_water)\n\nallegheny_race %&gt;%\n  ggplot(aes(fill = value, color = value)) +\n  facet_wrap(~variable) +\n  geom_sf() +\n  scale_fill_viridis(\"Percent\", option = \"magma\", labels = percent) +\n  scale_color_viridis(\"Percent\", option = \"magma\", labels = percent) +\n  theme(axis.text = element_blank())\n\n\n\n\n\n\n\n\nYou can also request data for an entire state. This code requests the median income for each census tract in Pennsylvania.\n\npa &lt;- get_acs(state = \"PA\",\n              geography = \"tract\", \n              variables = c(median_income = \"B19013_001\"), \n              year = 2018,\n              geometry = TRUE)\n\nhead(pa)\n\nSimple feature collection with 6 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -80.14905 ymin: 40.32178 xmax: -75.6175 ymax: 41.07083\nGeodetic CRS:  NAD83\n        GEOID                                           NAME      variable\n1 42019910500 Census Tract 9105, Butler County, Pennsylvania median_income\n2 42019912200 Census Tract 9122, Butler County, Pennsylvania median_income\n3 42021000100   Census Tract 1, Cambria County, Pennsylvania median_income\n4 42021012600 Census Tract 126, Cambria County, Pennsylvania median_income\n5 42025020700  Census Tract 207, Carbon County, Pennsylvania median_income\n6 42027010800  Census Tract 108, Centre County, Pennsylvania median_income\n  estimate   moe                       geometry\n1       NA    NA MULTIPOLYGON (((-80.04897 4...\n2    93446 11356 MULTIPOLYGON (((-80.14905 4...\n3    12907  1274 MULTIPOLYGON (((-78.92583 4...\n4    47143  9880 MULTIPOLYGON (((-78.73584 4...\n5    57939  4427 MULTIPOLYGON (((-75.71378 4...\n6    53569  4123 MULTIPOLYGON (((-77.55509 4...\n\n\n\npa %&gt;%\n  ggplot(aes(fill = estimate, color = estimate)) + \n  geom_sf() + \n  scale_fill_viridis(\"Estimated median income\", option = \"magma\", label = comma) + \n  scale_color_viridis(\"Estimated median income\", option = \"magma\", label = comma) +\n  theme(axis.text = element_blank())\n\n\n\n\n\n\n\n\nThis code requests ethnicity data for each tract in Pennsylvania.\n\nracevars &lt;- c(White = \"P005003\", \n              Black = \"P005004\", \n              Asian = \"P005006\", \n              Hispanic = \"P004003\")\n\nget_decennial(geography = \"tract\", \n              variables = racevars,\n              state = \"PA\",\n              year = 2010,\n              geometry = TRUE,\n              summary_var = \"P001001\") %&gt;% \n  mutate(value = value / summary_value,\n         variable = str_c(\"percent_\", tolower(variable))) -&gt; pa_race\n\nhead(pa_race)\n\nSimple feature collection with 6 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -80.12431 ymin: 40.54225 xmax: -79.99058 ymax: 40.61431\nGeodetic CRS:  NAD83\n# A tibble: 6 × 6\n  GEOID       NAME      variable   value summary_value                  geometry\n  &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt;      &lt;dbl&gt;         &lt;dbl&gt;        &lt;MULTIPOLYGON [°]&gt;\n1 42003412002 Census T… percent… 0.916            4865 (((-80.07936 40.58043, -…\n2 42003412002 Census T… percent… 0.00843          4865 (((-80.07936 40.58043, -…\n3 42003412002 Census T… percent… 0.0580           4865 (((-80.07936 40.58043, -…\n4 42003412002 Census T… percent… 0.0103           4865 (((-80.07936 40.58043, -…\n5 42003413100 Census T… percent… 0.878            6609 (((-80.06788 40.60846, -…\n6 42003413100 Census T… percent… 0.0172           6609 (((-80.06788 40.60846, -…\n\n\n\npa_race %&gt;%\n  ggplot(aes(fill = value, color = value)) +\n  facet_wrap(~variable) +\n  geom_sf() +\n  labs(title = \"Major ethncities in Pennsylvania\",\n       subtitle = \"Census data\") +\n  scale_fill_viridis(\"Percent\", option = \"magma\", label = percent) +\n  scale_color_viridis(\"Percent\", option = \"magma\", label = percent) +\n  theme(axis.text = element_blank())\n\n\n\n\n\n\n\n\nResources used:\n\nhttp://strimas.com/r/tidy-sf/\nhttps://walkerke.github.io/tidycensus/articles/spatial-data.html\nhttps://walkerke.github.io/tidycensus/index.html\nhttps://walkerke.github.io/2017/06/comparing-metros/"
  },
  {
    "objectID": "posts/c-s-presentation-time-series-forecasting/index.html",
    "href": "posts/c-s-presentation-time-series-forecasting/index.html",
    "title": "C&S Presentation: Time Series Forecasting",
    "section": "",
    "text": "This material was presented at Code & Supply on 2024-11-07"
  },
  {
    "objectID": "posts/c-s-presentation-time-series-forecasting/index.html#what-is-time-series-forecasting",
    "href": "posts/c-s-presentation-time-series-forecasting/index.html#what-is-time-series-forecasting",
    "title": "C&S Presentation: Time Series Forecasting",
    "section": "What is time series forecasting?",
    "text": "What is time series forecasting?\nTime series forecasting is the practice of making predictions about the future value of some quantitative variable.\nPredictive accuracy is typically the focus, not inference for understanding underlying causes."
  },
  {
    "objectID": "posts/c-s-presentation-time-series-forecasting/index.html#what-is-a-time-series",
    "href": "posts/c-s-presentation-time-series-forecasting/index.html#what-is-a-time-series",
    "title": "C&S Presentation: Time Series Forecasting",
    "section": "What is a time series?",
    "text": "What is a time series?\nData collected about a quantitative variable sequentially over time.\n\nCustomer demand for a company’s product\nElectricity usage\nStock price\nNetwork latency\nPothole reports"
  },
  {
    "objectID": "posts/c-s-presentation-time-series-forecasting/index.html#what-is-a-time-series-1",
    "href": "posts/c-s-presentation-time-series-forecasting/index.html#what-is-a-time-series-1",
    "title": "C&S Presentation: Time Series Forecasting",
    "section": "What is a time series",
    "text": "What is a time series\n\ntsibbledata::vic_elec |&gt; \n  as_tibble() |&gt; \n  mutate(day = date(Time)) |&gt; \n  group_by(day) |&gt; \n  summarize(max_demand = max(Demand)) |&gt; \n  as_tsibble(index = day) |&gt; \n  autoplot() +\n  labs(title = \"Peak daily electricity demand in Victoria, Australia\",\n       y = \"Peak demand (MWh)\",\n       x = \"Date\") +\n  scale_y_continuous(labels = scales::comma_format())"
  },
  {
    "objectID": "posts/c-s-presentation-time-series-forecasting/index.html#what-can-be-forecasted",
    "href": "posts/c-s-presentation-time-series-forecasting/index.html#what-can-be-forecasted",
    "title": "C&S Presentation: Time Series Forecasting",
    "section": "What can be forecasted?",
    "text": "What can be forecasted?\nKey questions:\n\nDo we understand the underlying process that creates the time series (data generating process)?\nIs historical data is available?\nWill the future be similar to the past?\nWill the forecast affect the thing we are trying to forecast?\n\nFeedback loop"
  },
  {
    "objectID": "posts/c-s-presentation-time-series-forecasting/index.html#what-can-be-forecasted-1",
    "href": "posts/c-s-presentation-time-series-forecasting/index.html#what-can-be-forecasted-1",
    "title": "C&S Presentation: Time Series Forecasting",
    "section": "What can be forecasted?",
    "text": "What can be forecasted?\nFeedback loops AKA “efficient market hypothesis”\n\n“How much will it rain tomorrow”\n\nvs.\n\n“How much rain will fall on you tomorrow”"
  },
  {
    "objectID": "posts/c-s-presentation-time-series-forecasting/index.html#why-forecast",
    "href": "posts/c-s-presentation-time-series-forecasting/index.html#why-forecast",
    "title": "C&S Presentation: Time Series Forecasting",
    "section": "Why forecast?",
    "text": "Why forecast?\n\nKnowing more about the future means we can make better decisions today.\nTypically related to resource allocation."
  },
  {
    "objectID": "posts/c-s-presentation-time-series-forecasting/index.html#forecast-horizons",
    "href": "posts/c-s-presentation-time-series-forecasting/index.html#forecast-horizons",
    "title": "C&S Presentation: Time Series Forecasting",
    "section": "Forecast horizons",
    "text": "Forecast horizons\n\nShort term\n\nWhat will the peak demand for electricity be in the next hour?\n\nMedium term\n\nWhat will customer demand for flowers be next Valentine’s Day?\n\nLong term\n\nWhat will future demand for a company’s products be given changing population trends across geography?"
  },
  {
    "objectID": "posts/c-s-presentation-time-series-forecasting/index.html#how",
    "href": "posts/c-s-presentation-time-series-forecasting/index.html#how",
    "title": "C&S Presentation: Time Series Forecasting",
    "section": "How?",
    "text": "How?\n\nTypical regression model\nPothole reports as a function of explanatory variables\n\npothole_reports ~ year + month + weather + public_works_budget + error\n\n\n\nTime series model\nFuture value of pothole complaints as a function of the previous values of pothole complaints (plus explanatory variables)\n\npothole_reports(t+1) ~ pothole_reports(t) + pothole_reports(t-1) + ... + error"
  },
  {
    "objectID": "posts/c-s-presentation-time-series-forecasting/index.html#why-use-time-series-models",
    "href": "posts/c-s-presentation-time-series-forecasting/index.html#why-use-time-series-models",
    "title": "C&S Presentation: Time Series Forecasting",
    "section": "Why use time series models?",
    "text": "Why use time series models?\nTime series models can typically handle autocorrelation in the data.\n\npothole_reports(t) and pothole_reports(t-1) are correlated\nThis can cause correlated error in regression models"
  },
  {
    "objectID": "posts/c-s-presentation-time-series-forecasting/index.html#forecasting-process",
    "href": "posts/c-s-presentation-time-series-forecasting/index.html#forecasting-process",
    "title": "C&S Presentation: Time Series Forecasting",
    "section": "Forecasting Process",
    "text": "Forecasting Process\n\nExploratory data analysis\nModel selection\nForecast\nEvaluate forecast accuracy"
  },
  {
    "objectID": "posts/c-s-presentation-time-series-forecasting/index.html#code-walkthrough",
    "href": "posts/c-s-presentation-time-series-forecasting/index.html#code-walkthrough",
    "title": "C&S Presentation: Time Series Forecasting",
    "section": "Code walkthrough",
    "text": "Code walkthrough\n311 data from Western Pennsylvania Regional Data Center\n\n#read in pothole data\n#https://data.wprdc.org/datastore/dump/29462525-62a6-45bf-9b5e-ad2e1c06348d\nreport_data &lt;- read_csv(\"post_data/wprdc_311_2024_10_20.csv\") |&gt; \n  clean_names() |&gt;\n  mutate(create_date = yearmonth(create_date_et)) |&gt; \n  rename(request_type = request_type_name)"
  },
  {
    "objectID": "posts/c-s-presentation-time-series-forecasting/index.html#data-structure",
    "href": "posts/c-s-presentation-time-series-forecasting/index.html#data-structure",
    "title": "C&S Presentation: Time Series Forecasting",
    "section": "Data structure",
    "text": "Data structure\nTime series table data structure:\n\nKey: request_type\nIndex: create_date\nMeasured variable: report_count\n\n\n#create basic tsibble\npothole_df &lt;- report_data |&gt; \n  filter(request_type == \"Potholes\") |&gt; \n  summarize(report_count = n(),\n            .by = c(create_date, request_type)) |&gt; \n  ungroup() |&gt;\n  filter(year(create_date) &gt;= 2016) |&gt; \n  as_tsibble(key = request_type, index = create_date)\n\npothole_df\n\n# A tsibble: 106 x 3 [1M]\n# Key:       request_type [1]\n   create_date request_type report_count\n         &lt;mth&gt; &lt;chr&gt;               &lt;int&gt;\n 1    2016 Jan Potholes              208\n 2    2016 Feb Potholes              574\n 3    2016 Mar Potholes              936\n 4    2016 Apr Potholes              732\n 5    2016 May Potholes              789\n 6    2016 Jun Potholes              723\n 7    2016 Jul Potholes              554\n 8    2016 Aug Potholes              516\n 9    2016 Sep Potholes              357\n10    2016 Oct Potholes              313\n# ℹ 96 more rows"
  },
  {
    "objectID": "posts/c-s-presentation-time-series-forecasting/index.html#exploratory-data-analysis",
    "href": "posts/c-s-presentation-time-series-forecasting/index.html#exploratory-data-analysis",
    "title": "C&S Presentation: Time Series Forecasting",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\n\nautoplot(pothole_df)"
  },
  {
    "objectID": "posts/c-s-presentation-time-series-forecasting/index.html#exploratory-data-analysis-1",
    "href": "posts/c-s-presentation-time-series-forecasting/index.html#exploratory-data-analysis-1",
    "title": "C&S Presentation: Time Series Forecasting",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nSeasonal plot shows each year across months\n\ngg_season(pothole_df)"
  },
  {
    "objectID": "posts/c-s-presentation-time-series-forecasting/index.html#exploratory-data-analysis-2",
    "href": "posts/c-s-presentation-time-series-forecasting/index.html#exploratory-data-analysis-2",
    "title": "C&S Presentation: Time Series Forecasting",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nSeasonal subseries plot shows each month across years\n\ngg_subseries(pothole_df) +\n  facet_wrap(vars(month(create_date, label = TRUE)), ncol = 3)"
  },
  {
    "objectID": "posts/c-s-presentation-time-series-forecasting/index.html#exploratory-data-analysis-3",
    "href": "posts/c-s-presentation-time-series-forecasting/index.html#exploratory-data-analysis-3",
    "title": "C&S Presentation: Time Series Forecasting",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nTime series decomposition\n\ndcmp &lt;- pothole_df |&gt;\n  model(stl = STL(report_count, robust = TRUE))\n\ndcmp_components &lt;- components(dcmp)\n\nautoplot(dcmp_components)"
  },
  {
    "objectID": "posts/c-s-presentation-time-series-forecasting/index.html#exploratory-data-analysis-4",
    "href": "posts/c-s-presentation-time-series-forecasting/index.html#exploratory-data-analysis-4",
    "title": "C&S Presentation: Time Series Forecasting",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nOutliers\n\noutliers &lt;- dcmp_components |&gt;\n  filter(remainder &lt; quantile(remainder, 0.25) - 3*IQR(remainder) |\n           remainder &gt; quantile(remainder, 0.75) + 3*IQR(remainder))\n\npothole_df |&gt;\n  ggplot(aes(create_date, report_count)) +\n  geom_line() +\n  geom_point(data = outliers, color = \"red\")"
  },
  {
    "objectID": "posts/c-s-presentation-time-series-forecasting/index.html#traintest-split",
    "href": "posts/c-s-presentation-time-series-forecasting/index.html#traintest-split",
    "title": "C&S Presentation: Time Series Forecasting",
    "section": "Train/test split",
    "text": "Train/test split\nYou always want to validate a model’s performance with data it hasn’t seen already. This analysis uses the last 20% of observations as test data.\n\ndata_test &lt;- pothole_df |&gt; \n  slice_tail(prop = .2)\n\ndata_train &lt;- pothole_df |&gt; \n  anti_join(data_test, by = \"create_date\")"
  },
  {
    "objectID": "posts/c-s-presentation-time-series-forecasting/index.html#model-types",
    "href": "posts/c-s-presentation-time-series-forecasting/index.html#model-types",
    "title": "C&S Presentation: Time Series Forecasting",
    "section": "Model types",
    "text": "Model types\n\nNAIVE: play the last value forward over the forecast horizon\nSNAIVE: play the last seasonal value forward\nMEAN: take the average of the entire series and play it forward\n\nUse window() to apply a window for rolling averages"
  },
  {
    "objectID": "posts/c-s-presentation-time-series-forecasting/index.html#model-types-1",
    "href": "posts/c-s-presentation-time-series-forecasting/index.html#model-types-1",
    "title": "C&S Presentation: Time Series Forecasting",
    "section": "Model types",
    "text": "Model types\n\nTSLM*: fit a linear model along the trend()\n\nUse seasonal() to add dummy variables for seasonal effects\n\nETS: exponential smoothing\n\nUse a weighted average based on the recency of the observations\n\nARIMA*: Autoregressive Integrated Moving Average\n\nfable::ARIMA automatically determines optimal model parameters (PDQ, seasonal PDQ)\n\nAKA “autoarima”\nYou can also manually set these\n\n\nMore available in {fable}\n\n* These models can use exogenous variables to capture additional information\n\nNotes\nARIMA:\n\nP: number of autoregressive terms\nD: number of differences required to make it stationary\nQ: number of lagged forecast errors"
  },
  {
    "objectID": "posts/c-s-presentation-time-series-forecasting/index.html#fit-models",
    "href": "posts/c-s-presentation-time-series-forecasting/index.html#fit-models",
    "title": "C&S Presentation: Time Series Forecasting",
    "section": "Fit models",
    "text": "Fit models\nFit models on training data\n\nmodel_df &lt;- data_train |&gt; \n  model(naive = NAIVE(log(report_count + 1)),\n        naive_seasonal = SNAIVE(log(report_count + 1)),\n        mean = MEAN(log(report_count + 1)),\n        mean_moving_6 = MEAN(log(report_count + 1), window = 6),\n        lm = TSLM(log(report_count + 1) ~ trend()),\n        lm_seasonal = TSLM(log(report_count + 1) ~ trend() + season()),\n        arima = ARIMA(log(report_count + 1)),\n        ets = ETS(log(report_count + 1)))\n\nTransformations of target variable are automatically reversed in fable::forecast"
  },
  {
    "objectID": "posts/c-s-presentation-time-series-forecasting/index.html#fit-models-1",
    "href": "posts/c-s-presentation-time-series-forecasting/index.html#fit-models-1",
    "title": "C&S Presentation: Time Series Forecasting",
    "section": "Fit models",
    "text": "Fit models\n\nglimpse(model_df)\n\nRows: 1\nColumns: 9\nKey: request_type [1]\n$ request_type   &lt;chr&gt; \"Potholes\"\n$ naive          &lt;model&gt; [NAIVE]\n$ naive_seasonal &lt;model&gt; [SNAIVE]\n$ mean           &lt;model&gt; [MEAN]\n$ mean_moving_6  &lt;model&gt; [MEAN]\n$ lm             &lt;model&gt; [TSLM]\n$ lm_seasonal    &lt;model&gt; [TSLM]\n$ arima          &lt;model&gt; [ARIMA(1,0,2)(2,1,0)[12]]\n$ ets            &lt;model&gt; [ETS(M,N,A)]"
  },
  {
    "objectID": "posts/c-s-presentation-time-series-forecasting/index.html#model-summary",
    "href": "posts/c-s-presentation-time-series-forecasting/index.html#model-summary",
    "title": "C&S Presentation: Time Series Forecasting",
    "section": "Model summary",
    "text": "Model summary\nInspect model\n\nmodel_df |&gt; \n  select(arima) |&gt; \n  report()\n\nSeries: report_count \nModel: ARIMA(1,0,2)(2,1,0)[12] \nTransformation: log(report_count + 1) \n\nCoefficients:\n         ar1      ma1      ma2     sar1     sar2\n      0.9608  -0.3667  -0.2622  -0.6048  -0.7281\ns.e.  0.0360   0.1566   0.1544   0.0931   0.0953\n\nsigma^2 estimated as 0.1867:  log likelihood=-49.7\nAIC=111.4   AICc=112.7   BIC=125.1"
  },
  {
    "objectID": "posts/c-s-presentation-time-series-forecasting/index.html#evaluate-fit-on-training-data",
    "href": "posts/c-s-presentation-time-series-forecasting/index.html#evaluate-fit-on-training-data",
    "title": "C&S Presentation: Time Series Forecasting",
    "section": "Evaluate fit on training data",
    "text": "Evaluate fit on training data\nPlot forecast vs. training data\n\nmodel_df |&gt; \n  select(ets) |&gt; \n  augment() |&gt; \n  ggplot(aes(x = create_date)) +\n  geom_line(aes(y = report_count, color = \"observed\"), lwd = 1) +\n  geom_line(aes(y = .fitted, color = \"prediction\"), lwd = 1) +\n  scale_color_manual(values = c(\"observed\" = \"black\", \"prediction\" = \"orange\")) +\n  labs(title = \"ETS model\",\n       color = NULL) +\n  theme(legend.text = element_text(size = 16))"
  },
  {
    "objectID": "posts/c-s-presentation-time-series-forecasting/index.html#forecast-on-test-data",
    "href": "posts/c-s-presentation-time-series-forecasting/index.html#forecast-on-test-data",
    "title": "C&S Presentation: Time Series Forecasting",
    "section": "Forecast on test data",
    "text": "Forecast on test data\nMake forecast from fit models onto test data\n\npothole_fc &lt;- forecast(model_df, data_test)\n\n\npothole_fc\n\n# A fable: 168 x 5 [1M]\n# Key:     request_type, .model [8]\n   request_type .model create_date    report_count .mean\n   &lt;chr&gt;        &lt;chr&gt;        &lt;mth&gt;          &lt;dist&gt; &lt;dbl&gt;\n 1 Potholes     naive     2023 Feb t(N(6.1, 0.39))  521.\n 2 Potholes     naive     2023 Mar t(N(6.1, 0.77))  606.\n 3 Potholes     naive     2023 Apr  t(N(6.1, 1.2))  690.\n 4 Potholes     naive     2023 May  t(N(6.1, 1.5))  774.\n 5 Potholes     naive     2023 Jun  t(N(6.1, 1.9))  859.\n 6 Potholes     naive     2023 Jul  t(N(6.1, 2.3))  943.\n 7 Potholes     naive     2023 Aug  t(N(6.1, 2.7)) 1027.\n 8 Potholes     naive     2023 Sep  t(N(6.1, 3.1)) 1112.\n 9 Potholes     naive     2023 Oct  t(N(6.1, 3.5)) 1196.\n10 Potholes     naive     2023 Nov  t(N(6.1, 3.9)) 1280.\n# ℹ 158 more rows"
  },
  {
    "objectID": "posts/c-s-presentation-time-series-forecasting/index.html#accuracy",
    "href": "posts/c-s-presentation-time-series-forecasting/index.html#accuracy",
    "title": "C&S Presentation: Time Series Forecasting",
    "section": "Accuracy",
    "text": "Accuracy\nAccuracy metrics\n\nRoot Mean Squared Error (RMSE)\n\nOn average, how far off the forecast is from the actual observed value\n\nContinuous Ranked Probability Score (CRPS)\n\nMeasures how well the forecast distribution fits the test data\n“skill” measures CRPS compared to a naive benchmark model\n\nOthers available in {fabletools}"
  },
  {
    "objectID": "posts/c-s-presentation-time-series-forecasting/index.html#forecast-accuracy",
    "href": "posts/c-s-presentation-time-series-forecasting/index.html#forecast-accuracy",
    "title": "C&S Presentation: Time Series Forecasting",
    "section": "Forecast accuracy",
    "text": "Forecast accuracy\nEvaluate forecast accuracy based on full time series\n\nfc_acc &lt;- pothole_fc |&gt; \n  accuracy(pothole_df,\n           measures = list(point_accuracy_measures, \n                           distribution_accuracy_measures, \n                           skill_crps = skill_score(CRPS))) |&gt; \n  rename(rmse = RMSE) |&gt; \n  select(request_type, .model, .type, skill_crps, rmse) |&gt; \n  arrange(desc(skill_crps))"
  },
  {
    "objectID": "posts/c-s-presentation-time-series-forecasting/index.html#forecast-accuracy-1",
    "href": "posts/c-s-presentation-time-series-forecasting/index.html#forecast-accuracy-1",
    "title": "C&S Presentation: Time Series Forecasting",
    "section": "Forecast accuracy",
    "text": "Forecast accuracy\n\nfc_acc\n\n# A tibble: 8 × 5\n  request_type .model         .type skill_crps  rmse\n  &lt;chr&gt;        &lt;chr&gt;          &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;\n1 Potholes     lm_seasonal    Test       0.689  162.\n2 Potholes     arima          Test       0.635  206.\n3 Potholes     lm             Test       0.596  175.\n4 Potholes     ets            Test       0.305  377.\n5 Potholes     mean           Test       0.253  384.\n6 Potholes     mean_moving_6  Test       0.253  384.\n7 Potholes     naive_seasonal Test       0.129  595.\n8 Potholes     naive          Test      -1.01  1231."
  },
  {
    "objectID": "posts/c-s-presentation-time-series-forecasting/index.html#plot-forecast",
    "href": "posts/c-s-presentation-time-series-forecasting/index.html#plot-forecast",
    "title": "C&S Presentation: Time Series Forecasting",
    "section": "Plot forecast",
    "text": "Plot forecast\nPlot forecast vs. test data\n\nmodel_acc &lt;- fc_acc |&gt; \n  pull(.model)\n\npothole_fc &lt;- pothole_fc |&gt; \n  mutate(.model = factor(.model, levels = model_acc))\n\npothole_fc |&gt; \n  mutate(.model = factor(.model, levels = model_acc)) |&gt; \n  filter(.model %in% model_acc[1:3]) |&gt; \n  autoplot(data = pothole_df |&gt; filter(year(create_date) &gt;= 2021)) +\n  facet_wrap(vars(.model), ncol = 1) +\n  guides(fill_ramp = \"none\",\n         fill = \"none\",\n         color = \"none\") +\n  labs(title = \"Forecasts of top 3 models\",\n       subtitle = \"Sorted descending by accuracy\") +\n  theme(strip.text = element_text(size = 14))"
  },
  {
    "objectID": "posts/c-s-presentation-time-series-forecasting/index.html#refit-and-forecast",
    "href": "posts/c-s-presentation-time-series-forecasting/index.html#refit-and-forecast",
    "title": "C&S Presentation: Time Series Forecasting",
    "section": "Refit and forecast",
    "text": "Refit and forecast\nRefit top model on entire time series and make a true 12 month forecast\n\nfinal_model &lt;- pothole_df |&gt; \n  model(lm_seasonal = TSLM(log(report_count + 1) ~ trend() + season()))\n\nfinal_model |&gt; \n  forecast(h = 12) |&gt; \n  autoplot(pothole_df |&gt; filter(year(create_date) &gt;= 2021)) +\n  labs(title = \"Final 12 month forecast of pothole reports\",\n       x = \"Report create date\",\n       y = \"Report count\")\n\n\n\n\n\n\n\n\nfable::forecast automatically builds out a new dataframe of the specified horizon with the trend() and season() variables"
  },
  {
    "objectID": "posts/c-s-presentation-time-series-forecasting/index.html#references",
    "href": "posts/c-s-presentation-time-series-forecasting/index.html#references",
    "title": "C&S Presentation: Time Series Forecasting",
    "section": "References",
    "text": "References\nMaterial adapted from Forecasting: Principles and Practice (3rd ed) by Rob J Hyndman and George Athanasopoulos"
  },
  {
    "objectID": "posts/c-s-presentation-time-series-forecasting/index.html#questions",
    "href": "posts/c-s-presentation-time-series-forecasting/index.html#questions",
    "title": "C&S Presentation: Time Series Forecasting",
    "section": "Questions?",
    "text": "Questions?"
  },
  {
    "objectID": "posts/c-s-presentation-time-series-forecasting/index.html#appendix",
    "href": "posts/c-s-presentation-time-series-forecasting/index.html#appendix",
    "title": "C&S Presentation: Time Series Forecasting",
    "section": "Appendix",
    "text": "Appendix"
  },
  {
    "objectID": "posts/c-s-presentation-time-series-forecasting/index.html#autocorrelation",
    "href": "posts/c-s-presentation-time-series-forecasting/index.html#autocorrelation",
    "title": "C&S Presentation: Time Series Forecasting",
    "section": "Autocorrelation",
    "text": "Autocorrelation\nValues of report_count are correlated across time\n\nACF(pothole_df) |&gt; \n  autoplot()"
  },
  {
    "objectID": "posts/c-s-presentation-time-series-forecasting/index.html#autocorrelation-1",
    "href": "posts/c-s-presentation-time-series-forecasting/index.html#autocorrelation-1",
    "title": "C&S Presentation: Time Series Forecasting",
    "section": "Autocorrelation",
    "text": "Autocorrelation\nPartial autocorrelation measures correlation between gapped lags of report_count, accounting for the relationship between the intermediate lags\n\nPACF(pothole_df) |&gt; \n  autoplot()"
  },
  {
    "objectID": "posts/c-s-presentation-time-series-forecasting/index.html#cross-validation",
    "href": "posts/c-s-presentation-time-series-forecasting/index.html#cross-validation",
    "title": "C&S Presentation: Time Series Forecasting",
    "section": "Cross-validation",
    "text": "Cross-validation\nCreate multiple train/test sets with rolling origins\n\npothole_cv &lt;- stretch_tsibble(pothole_df, .step = 6, .init = 24)\n\n\npothole_cv\n\n# A tsibble: 882 x 4 [1M]\n# Key:       .id, request_type [14]\n   create_date request_type report_count   .id\n         &lt;mth&gt; &lt;chr&gt;               &lt;int&gt; &lt;int&gt;\n 1    2016 Jan Potholes              208     1\n 2    2016 Feb Potholes              574     1\n 3    2016 Mar Potholes              936     1\n 4    2016 Apr Potholes              732     1\n 5    2016 May Potholes              789     1\n 6    2016 Jun Potholes              723     1\n 7    2016 Jul Potholes              554     1\n 8    2016 Aug Potholes              516     1\n 9    2016 Sep Potholes              357     1\n10    2016 Oct Potholes              313     1\n# ℹ 872 more rows"
  },
  {
    "objectID": "posts/c-s-presentation-time-series-forecasting/index.html#cross-validation-1",
    "href": "posts/c-s-presentation-time-series-forecasting/index.html#cross-validation-1",
    "title": "C&S Presentation: Time Series Forecasting",
    "section": "Cross-validation",
    "text": "Cross-validation\n\n\n\nForecasting: Principles and Practice"
  },
  {
    "objectID": "posts/c-s-presentation-time-series-forecasting/index.html#time-series-features",
    "href": "posts/c-s-presentation-time-series-forecasting/index.html#time-series-features",
    "title": "C&S Presentation: Time Series Forecasting",
    "section": "Time series features",
    "text": "Time series features\n\nreport_df &lt;- report_data |&gt; \n  summarize(report_count = n(),\n            .by = c(create_date, request_type)) |&gt; \n  ungroup() |&gt;\n  filter(year(create_date) &gt;= 2016) |&gt; \n  as_tsibble(key = request_type, index = create_date)\n\n\ntop_request_type &lt;- report_df |&gt; \n  as_tibble() |&gt; \n  summarize(report_count = sum(report_count),\n            .by = c(request_type)) |&gt; \n  slice_max(n = 12, order_by = report_count)\n\nreport_df_top12 &lt;- report_df |&gt; \n  semi_join(top_request_type, by = \"request_type\")\n\nreport_df_top12 |&gt; \n  mutate(request_type = fct_reorder(request_type, report_count, sum, .desc = TRUE)) |&gt; \n  autoplot() +\n  facet_wrap(vars(request_type), scales = \"free_y\") +\n  guides(color = \"none\") +\n  theme(axis.text.x = element_text(size = 6))"
  },
  {
    "objectID": "posts/c-s-presentation-time-series-forecasting/index.html#time-series-features-1",
    "href": "posts/c-s-presentation-time-series-forecasting/index.html#time-series-features-1",
    "title": "C&S Presentation: Time Series Forecasting",
    "section": "Time series features",
    "text": "Time series features\n\nreport_features &lt;- report_df_top12 |&gt; \n  features(report_count, feature_set(pkgs = \"feasts\"))\n\nreport_features |&gt; \n  slice_head(n = 1) |&gt; \n  glimpse()\n\nRows: 1\nColumns: 49\n$ request_type           &lt;chr&gt; \"Abandoned Vehicle (parked on street)\"\n$ trend_strength         &lt;dbl&gt; 0.3949\n$ seasonal_strength_year &lt;dbl&gt; 0.3865\n$ seasonal_peak_year     &lt;dbl&gt; 8\n$ seasonal_trough_year   &lt;dbl&gt; 0\n$ spikiness              &lt;dbl&gt; 2158\n$ linearity              &lt;dbl&gt; 160.7\n$ curvature              &lt;dbl&gt; -42.04\n$ stl_e_acf1             &lt;dbl&gt; 0.1834\n$ stl_e_acf10            &lt;dbl&gt; 0.1054\n$ acf1                   &lt;dbl&gt; 0.5248\n$ acf10                  &lt;dbl&gt; 0.4291\n$ diff1_acf1             &lt;dbl&gt; -0.2125\n$ diff1_acf10            &lt;dbl&gt; 0.07291\n$ diff2_acf1             &lt;dbl&gt; -0.5601\n$ diff2_acf10            &lt;dbl&gt; 0.3479\n$ season_acf1            &lt;dbl&gt; 0.0387\n$ pacf5                  &lt;dbl&gt; 0.2901\n$ diff1_pacf5            &lt;dbl&gt; 0.1979\n$ diff2_pacf5            &lt;dbl&gt; 0.6507\n$ season_pacf            &lt;dbl&gt; -0.1747\n$ zero_run_mean          &lt;dbl&gt; 0\n$ nonzero_squared_cv     &lt;dbl&gt; 0.06562\n$ zero_start_prop        &lt;dbl&gt; 0\n$ zero_end_prop          &lt;dbl&gt; 0\n$ lambda_guerrero        &lt;dbl&gt; -0.8999\n$ kpss_stat              &lt;dbl&gt; 0.4661\n$ kpss_pvalue            &lt;dbl&gt; 0.04931\n$ pp_stat                &lt;dbl&gt; -5.633\n$ pp_pvalue              &lt;dbl&gt; 0.01\n$ ndiffs                 &lt;int&gt; 1\n$ nsdiffs                &lt;int&gt; 0\n$ bp_stat                &lt;dbl&gt; 29.19\n$ bp_pvalue              &lt;dbl&gt; 0.00000006553\n$ lb_stat                &lt;dbl&gt; 30.03\n$ lb_pvalue              &lt;dbl&gt; 0.00000004261\n$ var_tiled_var          &lt;dbl&gt; 1.133\n$ var_tiled_mean         &lt;dbl&gt; 0.2935\n$ shift_level_max        &lt;dbl&gt; 109.2\n$ shift_level_index      &lt;dbl&gt; 63\n$ shift_var_max          &lt;dbl&gt; 11807\n$ shift_var_index        &lt;dbl&gt; 70\n$ shift_kl_max           &lt;dbl&gt; 6.258\n$ shift_kl_index         &lt;dbl&gt; 69\n$ spectral_entropy       &lt;dbl&gt; 0.927\n$ n_crossing_points      &lt;int&gt; 40\n$ longest_flat_spot      &lt;int&gt; 5\n$ coef_hurst             &lt;dbl&gt; 0.9206\n$ stat_arch_lm           &lt;dbl&gt; 0.1287"
  },
  {
    "objectID": "posts/c-s-presentation-time-series-forecasting/index.html#time-series-features-2",
    "href": "posts/c-s-presentation-time-series-forecasting/index.html#time-series-features-2",
    "title": "C&S Presentation: Time Series Forecasting",
    "section": "Time series features",
    "text": "Time series features\nPrincipal Component Analysis\n\npcs &lt;- report_features |&gt;\n  select(-request_type, -contains(\"zero\")) |&gt;\n  prcomp(scale = TRUE) |&gt;\n  augment(report_features)\n\npcs |&gt;\n  ggplot(aes(x = .fittedPC1, y = .fittedPC2, col = request_type)) +\n  geom_point() +\n  geom_label_repel(aes(label = request_type)) +\n  scale_x_continuous(expand = expansion(mult = c(.2, .2))) +\n  scale_y_continuous(expand = expansion(mult = c(.2, .2))) +\n  theme(aspect.ratio = 1) +\n  guides(color = \"none\")"
  },
  {
    "objectID": "posts/forecasting-potholes-with-exogenous-variables/index.html",
    "href": "posts/forecasting-potholes-with-exogenous-variables/index.html",
    "title": "Forecasting potholes with exogenous variables",
    "section": "",
    "text": "Intro\nIn this post I will extend the modelling approach from the previous post with exogenous variables (variables not directly about the quantity being measured in the time series). These time series models will take into account the time series dynamics of the historical data and any relationship between pothole reports and weather. As I noted in the previous post, you can imagine a “physics” model of pothole creation driven by precipitation and the freeze/thaw cycle. These models will attempt to capture some of that process.\n\n\nSet up packages and environment\n\nlibrary(fpp3)\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(future)\nlibrary(hrbrthemes)\nlibrary(GSODR)\nlibrary(tictoc)\n\ntheme_set(theme_ipsum())\n\nplan(multisession)\n\noptions(scipen = 999, digits = 4)\n\nThis code reads in the pothole data used in the previous post, aggregates it by year + month, and turns it into a tsibble.\n\n#read in pothole data\npothole_data &lt;- read_csv(\"post_data/wprdc_311.csv\") |&gt; \n  clean_names() |&gt; \n  filter(request_type == \"Potholes\") |&gt; \n  mutate(created_yearmonth = yearmonth(created_on))\n\npothole_df &lt;- pothole_data |&gt; \n  group_by(created_yearmonth, request_type) |&gt; \n  summarize(report_count = n()) |&gt; \n  ungroup() |&gt; \n  as_tsibble()\n\npothole_df\n\n# A tsibble: 93 x 3 [1M]\n   created_yearmonth request_type report_count\n               &lt;mth&gt; &lt;chr&gt;               &lt;int&gt;\n 1          2015 Apr Potholes              906\n 2          2015 May Potholes             1493\n 3          2015 Jun Potholes             1236\n 4          2015 Jul Potholes             1288\n 5          2015 Aug Potholes              734\n 6          2015 Sep Potholes              526\n 7          2015 Oct Potholes              516\n 8          2015 Nov Potholes              890\n 9          2015 Dec Potholes              309\n10          2016 Jan Potholes              222\n# ℹ 83 more rows\n\n\n\n\nWeather data\nThis uses the {GSODR} package to get daily weather data from the USA National Centers for Environmental Information (‘NCEI’). Temperature is in Celsius and precipitation is in millimeters.\n\nload(system.file(\"extdata\", \"isd_history.rda\", package = \"GSODR\"))\n\nweather_raw &lt;- get_GSOD(years = c(2014:2023), station = \"725205-14762\") |&gt; \n  as_tibble() |&gt; \n  clean_names()\n\nweather_data &lt;- weather_raw |&gt; \n  select(stnid, name, date = yearmoda, min, temp, max, prcp)\n\n\nglimpse(weather_data)\n\nRows: 3,601\nColumns: 7\n$ stnid &lt;chr&gt; \"725205-14762\", \"725205-14762\", \"725205-14762\", \"725205-14762\", …\n$ name  &lt;chr&gt; \"ALLEGHENY COUNTY AIRPORT\", \"ALLEGHENY COUNTY AIRPORT\", \"ALLEGHE…\n$ date  &lt;date&gt; 2014-01-01, 2014-01-02, 2014-01-03, 2014-01-04, 2014-01-05, 201…\n$ min   &lt;dbl&gt; -5.0, -6.1, -13.9, -13.9, -2.8, -18.3, -22.8, -22.8, -6.0, -6.1,…\n$ temp  &lt;dbl&gt; -1.7, -2.1, -10.7, -6.6, 2.6, -2.1, -19.9, -11.8, -3.3, 3.4, 9.8…\n$ max   &lt;dbl&gt; 3.3, 3.3, -0.6, 3.9, 8.9, 10.6, -16.0, -4.4, 1.1, 8.9, 13.0, 12.…\n$ prcp  &lt;dbl&gt; 0.00, 0.00, 3.56, 0.00, 0.00, 7.37, 6.86, 0.00, 0.00, 0.25, 0.00…\n\n\nNext I summarize the data by year + month and calculate various lags for each variable.\n\nweather_data &lt;- weather_data |&gt; \n  mutate(date_ym = yearmonth(date)) |&gt; \n  group_by(date_ym) |&gt; \n  summarize(temp_min_avg = mean(min),\n            temp_avg = mean(temp),\n            temp_max_avg = mean(max),\n            prcp_sum = sum(prcp, na.rm = TRUE)) |&gt; #2023-07-30 is missing prcp\n  ungroup() |&gt; \n  mutate(temp_diff = temp_max_avg - temp_min_avg) |&gt; \n  mutate(across(c(temp_min_avg, temp_avg, temp_max_avg, temp_diff, prcp_sum), ~lag(.x, 1), .names = \"{.col}_lag1\")) |&gt; \n  mutate(across(c(temp_min_avg, temp_avg, temp_max_avg, temp_diff, prcp_sum), ~lag(.x, 2), .names = \"{.col}_lag2\")) |&gt; \n  mutate(across(c(temp_min_avg, temp_avg, temp_max_avg, temp_diff, prcp_sum), ~lag(.x, 3), .names = \"{.col}_lag3\")) |&gt; \n  select(date_ym, contains(\"temp_avg\"), contains(\"min\"), contains(\"max\"), contains(\"diff\"), contains(\"prcp\"))\n\nglimpse(weather_data)\n\nRows: 119\nColumns: 21\n$ date_ym           &lt;mth&gt; 2014 Jan, 2014 Feb, 2014 Mar, 2014 Apr, 2014 May, 20…\n$ temp_avg          &lt;dbl&gt; -4.668, -2.504, 2.194, 12.100, 17.574, 22.097, 21.69…\n$ temp_avg_lag1     &lt;dbl&gt; NA, -4.668, -2.504, 2.194, 12.100, 17.574, 22.097, 2…\n$ temp_avg_lag2     &lt;dbl&gt; NA, NA, -4.668, -2.504, 2.194, 12.100, 17.574, 22.09…\n$ temp_avg_lag3     &lt;dbl&gt; NA, NA, NA, -4.668, -2.504, 2.194, 12.100, 17.574, 2…\n$ temp_min_avg      &lt;dbl&gt; -10.2935, -6.8643, -4.5258, 5.3300, 11.0806, 16.4833…\n$ temp_min_avg_lag1 &lt;dbl&gt; NA, -10.2935, -6.8643, -4.5258, 5.3300, 11.0806, 16.…\n$ temp_min_avg_lag2 &lt;dbl&gt; NA, NA, -10.2935, -6.8643, -4.5258, 5.3300, 11.0806,…\n$ temp_min_avg_lag3 &lt;dbl&gt; NA, NA, NA, -10.2935, -6.8643, -4.5258, 5.3300, 11.0…\n$ temp_max_avg      &lt;dbl&gt; 1.7903, 2.9250, 9.9032, 19.9033, 24.2419, 28.2500, 2…\n$ temp_max_avg_lag1 &lt;dbl&gt; NA, 1.7903, 2.9250, 9.9032, 19.9033, 24.2419, 28.250…\n$ temp_max_avg_lag2 &lt;dbl&gt; NA, NA, 1.7903, 2.9250, 9.9032, 19.9033, 24.2419, 28…\n$ temp_max_avg_lag3 &lt;dbl&gt; NA, NA, NA, 1.7903, 2.9250, 9.9032, 19.9033, 24.2419…\n$ temp_diff         &lt;dbl&gt; 12.084, 9.789, 14.429, 14.573, 13.161, 11.767, 11.59…\n$ temp_diff_lag1    &lt;dbl&gt; NA, 12.084, 9.789, 14.429, 14.573, 13.161, 11.767, 1…\n$ temp_diff_lag2    &lt;dbl&gt; NA, NA, 12.084, 9.789, 14.429, 14.573, 13.161, 11.76…\n$ temp_diff_lag3    &lt;dbl&gt; NA, NA, NA, 12.084, 9.789, 14.429, 14.573, 13.161, 1…\n$ prcp_sum          &lt;dbl&gt; 47.25, 58.41, 56.13, 91.42, 149.08, 120.39, 83.81, 1…\n$ prcp_sum_lag1     &lt;dbl&gt; NA, 47.25, 58.41, 56.13, 91.42, 149.08, 120.39, 83.8…\n$ prcp_sum_lag2     &lt;dbl&gt; NA, NA, 47.25, 58.41, 56.13, 91.42, 149.08, 120.39, …\n$ prcp_sum_lag3     &lt;dbl&gt; NA, NA, NA, 47.25, 58.41, 56.13, 91.42, 149.08, 120.…\n\n\n\nExplore weather data\nThis shows average temperature, average minimum temperature, and average maximum temperature in Pittsburgh by year + month.\n\nweather_data |&gt; \n  ggplot(aes(date_ym, temp_avg)) +\n  geom_ribbon(aes(ymin = temp_min_avg, ymax = temp_max_avg), alpha = .3) +\n  geom_line()\n\n\n\n\n\n\n\n\nThis shows the sum of precipitation by year + month over time.\n\nweather_data |&gt; \n  ggplot(aes(date_ym, prcp_sum)) +\n  geom_line()\n\n\n\n\n\n\n\n\nThis compares precipitation vs the minimum temperature (below freezing highlighted).\n\nweather_data |&gt; \n  mutate(year = as.factor(year(date_ym))) |&gt; \n  ggplot(aes(temp_min_avg, prcp_sum)) +\n  geom_rect(aes(xmin = -Inf, xmax = 0, ymin = -Inf, ymax = Inf), color = \"grey\", alpha = .1) +\n  geom_point(aes(color = year)) +\n  geom_vline(xintercept = 0) +\n  facet_wrap(vars(year)) +\n  guides(color = \"none\")\n\n\n\n\n\n\n\n\n2017 and 2018 appear to have slightly more precipitation in below freezing temperatures, but not significantly.\n\n\nCompare weather data and pothole reports\nNext I do some EDA to visualize any connection between reports of potholes in the “current” month and weather.\n\npothole_df &lt;- pothole_df |&gt; \n  left_join(weather_data, by = c(\"created_yearmonth\" = \"date_ym\"))\n\n\npothole_df |&gt; \n  as_tibble() |&gt; \n  select(report_count, contains(\"temp_avg\")) |&gt; \n  pivot_longer(contains(\"temp\")) |&gt; \n  ggplot(aes(value, report_count)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  facet_wrap(vars(name), scales = \"free\") +\n  labs(title = \"Pothole reports vs. the average temperature\")\n\n\n\n\n\n\n\n\nThere is some positive relationship between lower average temperatures in previous months and pothole reports. The “current” average temperature does not appear to be related.\n\npothole_df |&gt; \n  as_tibble() |&gt; \n  select(report_count, contains(\"temp_diff\")) |&gt; \n  pivot_longer(contains(\"temp\")) |&gt; \n  ggplot(aes(value, report_count)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  facet_wrap(vars(name), scales = \"free\") +\n  labs(title = \"Pothole reports vs. the temperature difference\")\n\n\n\n\n\n\n\n\nThere is a weakly positive relationship between temperature difference in the current month and pothole reports. Longer lags develop a negative relationship.\n\npothole_df |&gt; \n  as_tibble() |&gt; \n  select(report_count, contains(\"min\")) |&gt; \n  pivot_longer(contains(\"min\")) |&gt; \n  ggplot(aes(value, report_count)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  facet_wrap(vars(name), scales = \"free\") +\n  labs(title = \"Pothole reports vs. the minimum temperature\")\n\n\n\n\n\n\n\n\nThere appears to be a positive relationship between lower minimum temperature in previous months and pothole reports.\n\npothole_df |&gt; \n  as_tibble() |&gt; \n  select(report_count, contains(\"max\")) |&gt; \n  pivot_longer(contains(\"max\")) |&gt; \n  ggplot(aes(value, report_count)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  facet_wrap(vars(name), scales = \"free\") +\n  labs(title = \"Pothole reports vs. the maximum temperature\")\n\n\n\n\n\n\n\n\nThere is some positive relationship between lower maximum temperature in previous months and pothole reports.\n\npothole_df |&gt; \n  as_tibble() |&gt; \n  select(report_count, contains(\"prcp\")) |&gt; \n  pivot_longer(contains(\"prcp\")) |&gt; \n  ggplot(aes(value, report_count)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  facet_wrap(vars(name), scales = \"free\") +\n  labs(title = \"Pothole reports vs. precipitation\")\n\n\n\n\n\n\n\n\nThere is a positive relationship between the total precipitation in the current month and pothole reports.\n\n\n\nCross-validate models\nNext I cross-validate models using various combinations of the weather data as exogenous variables. I also make benchmark models for comparison.\n\n#cv\npothole_cv &lt;- stretch_tsibble(pothole_df, .step = 1, .init = 24)\n\npothole_cv |&gt; \n  count(.id)\n\n# A tibble: 70 × 2\n     .id     n\n   &lt;int&gt; &lt;int&gt;\n 1     1    24\n 2     2    25\n 3     3    26\n 4     4    27\n 5     5    28\n 6     6    29\n 7     7    30\n 8     8    31\n 9     9    32\n10    10    33\n# ℹ 60 more rows\n\n\nAs in the previous post, report_count is transformed with log(x + 1) to force the predictions to be positive.\n\ntic()\nprogressr::with_progress(\n  \n  model_df_exo &lt;- pothole_cv |&gt; \n    model(ets = ETS(log(report_count + 1)),\n          ts_lm = TSLM(log(report_count + 1) ~ trend() + season()),\n          ts_lm_exo = TSLM(log(report_count + 1) ~ trend() + season() + temp_avg + temp_min_avg + temp_max_avg + prcp_sum),\n          ts_lm_exo_lag1 = TSLM(log(report_count + 1) ~ trend() + season() + temp_avg_lag1 + temp_min_avg_lag1 + temp_max_avg_lag1 + prcp_sum_lag1),\n          ts_lm_exo_lag2 = TSLM(log(report_count + 1) ~ trend() + season() + temp_avg_lag2 + temp_min_avg_lag2 + temp_max_avg_lag2 + prcp_sum_lag2),\n          ts_lm_exo_lag3 = TSLM(log(report_count + 1) ~ trend() + season() + temp_avg_lag3 + temp_min_avg_lag3 + temp_max_avg_lag3 + prcp_sum_lag3),\n          ts_lm_exo_custom = TSLM(log(report_count + 1) ~ trend() + season() + temp_avg_lag3 + temp_diff + temp_min_avg_lag3 + temp_max_avg_lag1 + prcp_sum),\n          arima = ARIMA(log(report_count + 1)),\n          arima_exo = ARIMA(log(report_count + 1) ~ temp_avg + temp_min_avg + temp_max_avg + prcp_sum),\n          arima_exo_lag1 = ARIMA(log(report_count + 1) ~ temp_avg_lag1 + temp_min_avg_lag1 + temp_max_avg_lag1 + prcp_sum_lag1),\n          arima_exo_lag2 = ARIMA(log(report_count + 1) ~ temp_avg_lag2 + temp_min_avg_lag2 + temp_max_avg_lag2 + prcp_sum_lag2),\n          arima_exo_lag3 = ARIMA(log(report_count + 1) ~ temp_avg_lag3 + temp_min_avg_lag3 + temp_max_avg_lag3 + prcp_sum_lag3),\n          arima_exo_custom = ARIMA(log(report_count + 1) ~ temp_avg_lag3 + temp_diff + temp_min_avg_lag3 + temp_max_avg_lag1 + prcp_sum)\n    )\n)\ntoc()\n\n376.773 sec elapsed\n\n\nThe “exo_custom” models represent a naive guess at what combinations of weather variables are most related, based on the previous graphs. A more methodological meteorological approach would probably be much better.\nI use new_data to generate 12 new future observations for each CV .id and make a forecast for each .id and .model.\n\nhorizon_data &lt;- new_data(pothole_cv, 12) |&gt; \n  left_join(pothole_df)\n\nhorizon_data\n\n# A tsibble: 840 x 24 [1M]\n# Key:       .id [70]\n   created_yearmonth   .id request_type report_count temp_avg temp_avg_lag1\n               &lt;mth&gt; &lt;int&gt; &lt;chr&gt;               &lt;int&gt;    &lt;dbl&gt;         &lt;dbl&gt;\n 1          2017 Apr     1 Potholes             1191    14.3           4.65\n 2          2017 May     1 Potholes             1088    15.8          14.3 \n 3          2017 Jun     1 Potholes              880    20.7          15.8 \n 4          2017 Jul     1 Potholes              801    22.9          20.7 \n 5          2017 Aug     1 Potholes              620    20.8          22.9 \n 6          2017 Sep     1 Potholes              369    18.9          20.8 \n 7          2017 Oct     1 Potholes              278    14.8          18.9 \n 8          2017 Nov     1 Potholes              207     5.82         14.8 \n 9          2017 Dec     1 Potholes              131    -1.24          5.82\n10          2018 Jan     1 Potholes             1995    -3.45         -1.24\n# ℹ 830 more rows\n# ℹ 18 more variables: temp_avg_lag2 &lt;dbl&gt;, temp_avg_lag3 &lt;dbl&gt;,\n#   temp_min_avg &lt;dbl&gt;, temp_min_avg_lag1 &lt;dbl&gt;, temp_min_avg_lag2 &lt;dbl&gt;,\n#   temp_min_avg_lag3 &lt;dbl&gt;, temp_max_avg &lt;dbl&gt;, temp_max_avg_lag1 &lt;dbl&gt;,\n#   temp_max_avg_lag2 &lt;dbl&gt;, temp_max_avg_lag3 &lt;dbl&gt;, temp_diff &lt;dbl&gt;,\n#   temp_diff_lag1 &lt;dbl&gt;, temp_diff_lag2 &lt;dbl&gt;, temp_diff_lag3 &lt;dbl&gt;,\n#   prcp_sum &lt;dbl&gt;, prcp_sum_lag1 &lt;dbl&gt;, prcp_sum_lag2 &lt;dbl&gt;, …\n\npothole_fc_exo &lt;- model_df_exo |&gt; \n  forecast(horizon_data)\n\n\nCompare accuracy\nThis code calculates the out of sample accuracy for each .id and .model, and then averages the accuracy by .model.\n\ntic()\nfc_exo_acc &lt;- pothole_fc_exo |&gt; \n  accuracy(pothole_df, measures = list(point_accuracy_measures, distribution_accuracy_measures, skill_crps = skill_score(CRPS))) |&gt; \n  select(.model, .type, RMSE, skill_crps) |&gt; \n  arrange(desc(skill_crps))\ntoc()\n\n7108.297 sec elapsed\n\nfc_exo_acc\n\n# A tibble: 13 × 4\n   .model           .type  RMSE skill_crps\n   &lt;chr&gt;            &lt;chr&gt; &lt;dbl&gt;      &lt;dbl&gt;\n 1 arima_exo_custom Test   610.      0.743\n 2 arima_exo_lag3   Test   596.      0.741\n 3 ts_lm_exo_custom Test   658.      0.728\n 4 arima_exo_lag2   Test   658.      0.710\n 5 arima_exo_lag1   Test   660.      0.707\n 6 ts_lm_exo_lag3   Test   711.      0.699\n 7 arima_exo        Test   713.      0.696\n 8 ts_lm_exo_lag1   Test   758.      0.696\n 9 ts_lm            Test   780.      0.672\n10 ts_lm_exo_lag2   Test   875.      0.669\n11 ts_lm_exo        Test   793.      0.669\n12 ets              Test  1901.      0.540\n13 arima            Test  1843.      0.516\n\n\nMy arima_exo_custom model slightly improves on the arima_exo_lag3 model.\nExcluding the worst two models:\n\nfc_exo_acc |&gt; \n  filter(!.model %in% c(\"ets\", \"arima\")) |&gt; \n  ggplot(aes(RMSE, skill_crps, label = .model)) +\n  geom_point() +\n  ggrepel::geom_label_repel(max.overlaps = 100) +\n  scale_x_reverse()\n\n\n\n\n\n\n\n\n\n\n\nScenario forecasting\nThis code simulates high and low scenarios of precipitation. I use these to create scenario forecasts based on varying levels of future precipitation and the temperature data. Then I forecast each scenario with the arima_exo_custom model.\n\n#extracts the 10%, 50%, and 90% percentiles of precipitation by month\nprcp_percentiles &lt;- pothole_df |&gt; \n  mutate(month = month(created_yearmonth, label = TRUE)) |&gt; \n  as_tibble() |&gt; \n  select(month, prcp_sum) |&gt; \n  group_by(month) |&gt; \n  reframe(pctiles = c(\"10\", \"50\", \"90\"),\n          prcp_sum = quantile(prcp_sum, probs = c(.1, .5, .9))) |&gt; \n  ungroup() |&gt; \n  pivot_wider(names_from = pctiles, values_from = prcp_sum, names_prefix = \"prcp_sum_\")\n\nprcp_percentiles\n\n# A tibble: 12 × 4\n   month prcp_sum_10 prcp_sum_50 prcp_sum_90\n   &lt;ord&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1 Jan          46.0        86.8        104.\n 2 Feb          57.1        94.2        150.\n 3 Mar          57.6        74.7        124.\n 4 Apr          63.8       106.         125.\n 5 May          72.1       135.         159.\n 6 Jun          56.5       149.         277.\n 7 Jul          70.9       118.         193.\n 8 Aug          91.4       117.         155.\n 9 Sep          39.5        69.3        200.\n10 Oct          79         113.         131.\n11 Nov          21.4        65.5        105.\n12 Dec          46.2        93.3        120.\n\n\n\ncreate_horizon_data &lt;- function(x, prcp_scenario, prcp_col){\n  \n  #drop the lagged weather variables from the input df containing historical weather data\n  x &lt;- x |&gt; \n    select(-contains(\"lag\"))\n  \n  #create a new dataframe with the next 12 future observations\n  new_df &lt;- new_data(x, 12) |&gt; \n    mutate(request_type = \"Potholes\")\n  \n  #find the monthly average for all the temperature variables\n  new_temp_data &lt;- x |&gt; \n    mutate(month = month(created_yearmonth, label = TRUE)) |&gt; \n    as_tibble() |&gt; \n    select(-contains(c(\"lag\", \"prcp\"))) |&gt; \n    group_by(month) |&gt; \n    summarize(across(where(is.numeric), mean)) |&gt; \n    ungroup() |&gt; \n    #add in percentile precipitation column\n    left_join(prcp_scenario |&gt; \n                select(month, {{ prcp_col }})) |&gt; \n    rename(prcp_sum = {{ prcp_col }})\n  \n  #join new temperature data\n  new_df &lt;- new_df |&gt; \n    mutate(month = month(created_yearmonth, label = TRUE)) |&gt; \n    left_join(new_temp_data)\n\n  #append new temperature data to historical data\n  x &lt;- x |&gt; \n    bind_rows(new_df)\n\n  #recalculate the lagged weather data based on the given percentile of precipitation\n  x |&gt;\n    mutate(across(c(temp_min_avg, temp_avg, temp_max_avg, temp_diff, prcp_sum), ~lag(.x, 1), .names = \"{.col}_lag1\")) |&gt;\n    mutate(across(c(temp_min_avg, temp_avg, temp_max_avg, temp_diff, prcp_sum), ~lag(.x, 2), .names = \"{.col}_lag2\")) |&gt;\n    mutate(across(c(temp_min_avg, temp_avg, temp_max_avg, temp_diff, prcp_sum), ~lag(.x, 3), .names = \"{.col}_lag3\")) |&gt;\n    semi_join(new_df, by = c(\"created_yearmonth\")) |&gt; \n    select(created_yearmonth, request_type, report_count, contains(\"temp_avg\"), contains(\"min\"), contains(\"max\"), contains(\"diff\"), contains(\"prcp\"))\n  \n}\n\nThis shows the future scenario with 10th percentile precipitation in each month:\n\ncreate_horizon_data(pothole_df, prcp_percentiles, prcp_sum_10) |&gt; \n  glimpse()\n\nRows: 12\nColumns: 23\n$ created_yearmonth &lt;mth&gt; 2023 Jan, 2023 Feb, 2023 Mar, 2023 Apr, 2023 May, 20…\n$ request_type      &lt;chr&gt; \"Potholes\", \"Potholes\", \"Potholes\", \"Potholes\", \"Pot…\n$ report_count      &lt;dbl&gt; 618.0, 1323.6, 1149.4, 1140.5, 1137.5, 874.4, 755.5,…\n$ temp_avg          &lt;dbl&gt; -1.001, 1.834, 5.820, 10.916, 17.100, 21.095, 23.503…\n$ temp_avg_lag1     &lt;dbl&gt; 1.887, -1.001, 1.834, 5.820, 10.916, 17.100, 21.095,…\n$ temp_avg_lag2     &lt;dbl&gt; 8.017, 1.887, -1.001, 1.834, 5.820, 10.916, 17.100, …\n$ temp_avg_lag3     &lt;dbl&gt; 11.287, 8.017, 1.887, -1.001, 1.834, 5.820, 10.916, …\n$ temp_min_avg      &lt;dbl&gt; -5.62304, -3.39310, 0.08664, 4.54833, 11.10927, 15.1…\n$ temp_min_avg_lag1 &lt;dbl&gt; -3.29032, -5.62304, -3.39310, 0.08664, 4.54833, 11.1…\n$ temp_min_avg_lag2 &lt;dbl&gt; 2.56333, -3.29032, -5.62304, -3.39310, 0.08664, 4.54…\n$ temp_min_avg_lag3 &lt;dbl&gt; 4.95161, 2.56333, -3.29032, -5.62304, -3.39310, 0.08…\n$ temp_max_avg      &lt;dbl&gt; 4.404, 7.953, 12.976, 18.346, 23.937, 27.482, 29.685…\n$ temp_max_avg_lag1 &lt;dbl&gt; 6.545, 4.404, 7.953, 12.976, 18.346, 23.937, 27.482,…\n$ temp_max_avg_lag2 &lt;dbl&gt; 13.753, 6.545, 4.404, 7.953, 12.976, 18.346, 23.937,…\n$ temp_max_avg_lag3 &lt;dbl&gt; 17.977, 13.753, 6.545, 4.404, 7.953, 12.976, 18.346,…\n$ temp_diff         &lt;dbl&gt; 10.027, 11.346, 12.889, 13.798, 12.827, 12.326, 11.7…\n$ temp_diff_lag1    &lt;dbl&gt; 9.835, 10.027, 11.346, 12.889, 13.798, 12.827, 12.32…\n$ temp_diff_lag2    &lt;dbl&gt; 11.190, 9.835, 10.027, 11.346, 12.889, 13.798, 12.82…\n$ temp_diff_lag3    &lt;dbl&gt; 13.026, 11.190, 9.835, 10.027, 11.346, 12.889, 13.79…\n$ prcp_sum          &lt;dbl&gt; 46.02, 57.14, 57.64, 63.77, 72.06, 56.53, 70.94, 91.…\n$ prcp_sum_lag1     &lt;dbl&gt; 50.28, 46.02, 57.14, 57.64, 63.77, 72.06, 56.53, 70.…\n$ prcp_sum_lag2     &lt;dbl&gt; 99.30, 50.28, 46.02, 57.14, 57.64, 63.77, 72.06, 56.…\n$ prcp_sum_lag3     &lt;dbl&gt; 66.54, 99.30, 50.28, 46.02, 57.14, 57.64, 63.77, 72.…\n\n\nNext I create the scenarios to be fed into the model.\n\n#create scenarios\nfc_scenarios &lt;- scenarios(\n  \n  scenario_low = create_horizon_data(pothole_df, prcp_percentiles, prcp_sum_10),\n  \n  scenario_median = create_horizon_data(pothole_df, prcp_percentiles, prcp_sum_50),\n  \n  scenario_high = create_horizon_data(pothole_df, prcp_percentiles, prcp_sum_90)\n  \n)\n\nstr(fc_scenarios, max.level = 1)\n\nList of 3\n $ scenario_low   : tbl_ts [12 × 23] (S3: tbl_ts/tbl_df/tbl/data.frame)\n  ..- attr(*, \"key\")= tibble [1 × 1] (S3: tbl_df/tbl/data.frame)\n  ..- attr(*, \"index\")= chr \"created_yearmonth\"\n  .. ..- attr(*, \"ordered\")= logi TRUE\n  ..- attr(*, \"index2\")= chr \"created_yearmonth\"\n  ..- attr(*, \"interval\")= interval [1:1] 1M\n $ scenario_median: tbl_ts [12 × 23] (S3: tbl_ts/tbl_df/tbl/data.frame)\n  ..- attr(*, \"key\")= tibble [1 × 1] (S3: tbl_df/tbl/data.frame)\n  ..- attr(*, \"index\")= chr \"created_yearmonth\"\n  .. ..- attr(*, \"ordered\")= logi TRUE\n  ..- attr(*, \"index2\")= chr \"created_yearmonth\"\n  ..- attr(*, \"interval\")= interval [1:1] 1M\n $ scenario_high  : tbl_ts [12 × 23] (S3: tbl_ts/tbl_df/tbl/data.frame)\n  ..- attr(*, \"key\")= tibble [1 × 1] (S3: tbl_df/tbl/data.frame)\n  ..- attr(*, \"index\")= chr \"created_yearmonth\"\n  .. ..- attr(*, \"ordered\")= logi TRUE\n  ..- attr(*, \"index2\")= chr \"created_yearmonth\"\n  ..- attr(*, \"interval\")= interval [1:1] 1M\n - attr(*, \"names_to\")= chr \".scenario\"\n\n\nThis shows the monthly precipitation in each scenario:\n\nfc_scenarios |&gt; \n  map(as_tibble) |&gt; \n  set_names(nm = c(\"scenario_low\", \"scenario_median\", \"scenario_high\")) |&gt; \n  bind_rows(.id = \".scenario\") |&gt; \n  select(.scenario, created_yearmonth, prcp_sum) |&gt; \n  mutate(.scenario = as.factor(.scenario)) |&gt; \n  ggplot(aes(created_yearmonth, prcp_sum, color = .scenario)) +\n  geom_line()\n\n\n\n\n\n\n\n\nFinally, I refit the model against the entire history and forecast against each scenario.\n\n#refit best model on total history\nfinal_exo_model &lt;- pothole_df |&gt; \n  model(arima_exo_custom = ARIMA(log(report_count + 1) ~ temp_avg_lag3 + temp_diff + temp_min_avg_lag3 + temp_max_avg_lag1 + prcp_sum))\n\nreport(final_exo_model)\n\nSeries: report_count \nModel: LM w/ ARIMA(0,1,1) errors \nTransformation: log(report_count + 1) \n\nCoefficients:\n          ma1  temp_avg_lag3  temp_diff  temp_min_avg_lag3  temp_max_avg_lag1\n      -0.6696         0.1409     0.1378            -0.1879            -0.0131\ns.e.   0.0895         0.0716     0.0378             0.0718             0.0072\n      prcp_sum\n        0.0025\ns.e.    0.0009\n\nsigma^2 estimated as 0.2233:  log likelihood=-58.77\nAIC=131.5   AICc=132.9   BIC=149.2\n\n\n\n#forecast scenarios\nscenerio_fc &lt;- final_exo_model |&gt; \n  forecast(fc_scenarios) |&gt; \n  mutate(.scenario = fct_relevel(.scenario, c(\"scenario_low\", \"scenario_median\", \"scenario_high\")))\n\nscenerio_fc |&gt; \n  mutate(.scenario = fct_rev(.scenario)) |&gt; \n  autoplot() +\n  facet_wrap(vars(.scenario), scales = \"fixed\", ncol = 1)\n\n\n\n\n\n\n\n\nThe model predicts that the scenario with more precipitation will have ~1,000 more pothole reports in the next 12 months than the scenario with less precipitation.\n\nscenerio_fc |&gt; \n  as_tibble() |&gt; \n  group_by(.scenario) |&gt; \n  summarize(total_pothole_fc = sum(.mean)) |&gt; \n  ggplot(aes(total_pothole_fc, .scenario)) +\n  geom_col() +\n  scale_x_comma()\n\n\n\n\n\n\n\n\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: x86_64-apple-darwin20\nRunning under: macOS 15.1.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-x86_64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-x86_64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n [1] tictoc_1.2.1      GSODR_4.1.3       hrbrthemes_0.8.7  future_1.34.0    \n [5] janitor_2.2.0     forcats_1.0.0     stringr_1.5.1     purrr_1.0.2      \n [9] readr_2.1.5       tidyverse_2.0.0   fable_0.4.1       feasts_0.4.1     \n[13] fabletools_0.5.0  tsibbledata_0.4.1 tsibble_1.1.5     ggplot2_3.5.1    \n[17] lubridate_1.9.3   tidyr_1.3.1       dplyr_1.1.4       tibble_3.2.1     \n[21] fpp3_1.0.1       \n\nloaded via a namespace (and not attached):\n [1] tidyselect_1.2.1        farver_2.1.2            fastmap_1.2.0          \n [4] fontquiver_0.2.1        digest_0.6.37           timechange_0.3.0       \n [7] lifecycle_1.0.4         ellipsis_0.3.2          magrittr_2.0.3         \n[10] compiler_4.4.1          rlang_1.1.4             tools_4.4.1            \n[13] utf8_1.2.4              yaml_2.3.10             data.table_1.16.0      \n[16] knitr_1.48              labeling_0.4.3          htmlwidgets_1.6.4      \n[19] bit_4.0.5               numDeriv_2016.8-1.1     withr_3.0.1            \n[22] grid_4.4.1              fansi_1.0.6             gdtools_0.4.0          \n[25] colorspace_2.1-1        progressr_0.14.0        extrafontdb_1.0        \n[28] globals_0.16.3          scales_1.3.0            cli_3.6.3              \n[31] anytime_0.3.9           rmarkdown_2.28          crayon_1.5.3           \n[34] generics_0.1.3          future.apply_1.11.2     rstudioapi_0.16.0      \n[37] tzdb_0.4.0              splines_4.4.1           parallel_4.4.1         \n[40] vctrs_0.6.5             Matrix_1.7-0            jsonlite_1.8.8         \n[43] fontBitstreamVera_0.1.1 hms_1.1.3               ggrepel_0.9.6          \n[46] bit64_4.0.5             listenv_0.9.1           systemfonts_1.1.0      \n[49] ggdist_3.3.2            glue_1.8.0              parallelly_1.38.0      \n[52] codetools_0.2-20        distributional_0.5.0    stringi_1.8.4          \n[55] gtable_0.3.5            extrafont_0.19          munsell_0.5.1          \n[58] pillar_1.9.0            rappdirs_0.3.3          htmltools_0.5.8.1      \n[61] R6_2.5.1                vroom_1.6.5             evaluate_0.24.0        \n[64] lattice_0.22-6          snakecase_0.11.1        renv_1.0.11            \n[67] fontLiberation_0.1.0    Rcpp_1.0.13             nlme_3.1-164           \n[70] Rttf2pt1_1.3.12         mgcv_1.9-1              xfun_0.49              \n[73] pkgconfig_2.0.3"
  },
  {
    "objectID": "posts/comparing-u-s-senators-casey-and-toomey/index.html",
    "href": "posts/comparing-u-s-senators-casey-and-toomey/index.html",
    "title": "Comparing US Senators Casey and Toomey",
    "section": "",
    "text": "As a follow-up to my post about how Pittsburgh Mayor Bill Peduto uses Twitter, I thought it would be useful to examine and compare how Pennsylvania’s U.S. senators use Twitter. I will use some of the same methods, in addition to some comparative methods. In this analysis, I exclude retweets and quote tweets.\nThis code loads the libraries requires for the analysis, and sets some prefences for plotting.\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(lubridate)\nlibrary(rtweet)\nlibrary(scales)\nlibrary(knitr)\nlibrary(kableExtra)\n\nset.seed(1234)\ntheme_set(theme_bw(base_size = 18))\n\ntitle &lt;- \"@SenBobCasey and @SenToomey tweets\"\ncaption &lt;- \"@conor_tompkins\"\nThis code uses the rtweet package to download the senator’s tweets via the Twitter API:\ndf_casey &lt;- get_timelines(\"SenBobCasey\", n = 3200) %&gt;% \n  mutate(senator = \"Casey\")\n\ndf_toomey &lt;- get_timelines(\"SenToomey\", n = 3200) %&gt;% \n  mutate(senator = \"Toomey\")\nI have already pulled the data from Twitter, so this code downloads the tweet data from my GitHub repo.\nsource(\"https://raw.githubusercontent.com/conorotompkins/pittsburgh_twitter/master/scripts/tidytext_functions.R\")\n\ndf_casey &lt;- read_csv(\"https://raw.githubusercontent.com/conorotompkins/pittsburgh_twitter/master/data/tweets_casey.tweets.csv\")\n\ndf_toomey &lt;- read_csv(\"https://raw.githubusercontent.com/conorotompkins/pittsburgh_twitter/master/data/tweets_toomey.tweets.csv\")"
  },
  {
    "objectID": "posts/comparing-u-s-senators-casey-and-toomey/index.html#bigram-analysis",
    "href": "posts/comparing-u-s-senators-casey-and-toomey/index.html#bigram-analysis",
    "title": "Comparing US Senators Casey and Toomey",
    "section": "Bigram analysis",
    "text": "Bigram analysis\nBigrams are two-word chunks pulled from text. For example, “Senator Casey”, “Casey is”, “is from”, and “from Pennsylvania” are all bigrams of the sentence “Senator Casey is from Pennsylania”. This code extracts the bigrams from Senator Casey’s tweets and counts how many times they occur. It also removes some artifacts of URLs and other Twitter metadata that are not relevant.\n\ncasey_stopwords &lt;- c(\"0085\", \"009f\", \"f0\", \"00a6\")\ncasey_replacers &lt;- c(\"'s\")\ntweets_casey &lt;- count_twitter_bigrams(df_casey, custom_stopwords = casey_stopwords)\n\ntweets_casey %&gt;% \n  rename(count = n) %&gt;% \n  head() %&gt;% \n  kable(\"html\") %&gt;% \n  kable_styling()\n\n\n\n\nword1\nword2\ncount\n\n\n\n\nhealth\ncare\n226\n\n\ncredits\npaying\n123\n\n\nmiddle\nclass\n94\n\n\nage\ntax\n80\n\n\n50\n64\n69\n\n\ntax\n50\n67\n\n\n\n\n\n\n\nThis network graph shows how the words are related:\n\nvisualize_bigrams(tweets_casey, 20,\n                  title = \"@SenBobCasey tweets\",\n                  subtitle = \"Bigram network\",\n                  caption = \"@conor_tompkins\")\n\n\n\n\n\n\n\n\nTakeaway: Senator Casey focused on the tax cut and ACA repeal bills.\nThis code extracts the bigrams from Senator Toomey’s tweets:\n\ntoomey_stopwords &lt;- c(\"0085\", \"009f\", \"f0\", \"00a6\")\ntweets_toomey &lt;- count_twitter_bigrams(df_toomey, custom_stopwords = toomey_stopwords)\n                                       \ntweets_toomey %&gt;% \n  rename(count = n) %&gt;% \n  head() %&gt;% \n  kable(\"html\") %&gt;% \n  kable_styling()\n\n\n\n\nword1\nword2\ncount\n\n\n\n\nhappy\nfriday\n45\n\n\nhappy\nbirthday\n39\n\n\nweekly\nupdate\n39\n\n\nhealth\ncare\n36\n\n\nfriday\npa\n32\n\n\ninbox\ndelivery\n24\n\n\n\n\n\n\n\nThis is the bigram network plot for Senator Toomey:\n\nvisualize_bigrams(tweets_toomey, 20,\n                  title = \"@SenToomey tweets\",\n                  subtitle = \"Bigram network\",\n                  caption = \"@conor_tompkins\")\n\n\n\n\n\n\n\n\nTakeaway: Senator Toomey’s bigrams reflect that he uses Twitter to issue weekly updates and notify constituents about his newsletter."
  },
  {
    "objectID": "posts/comparing-u-s-senators-casey-and-toomey/index.html#word-correlation",
    "href": "posts/comparing-u-s-senators-casey-and-toomey/index.html#word-correlation",
    "title": "Comparing US Senators Casey and Toomey",
    "section": "Word correlation",
    "text": "Word correlation\nWe can also calculate and graph how the words in the tweets are correlated with each other. This code also stems the words, which combines similar words for the sake of capturing the broader usage. For example, “county” and “counties” could be stemmed into “counti”.\nThis code calculates and plots the correlations:\n\ncasey_words &lt;- word_correlations(df_casey, minimum = 75, casey_stopwords)\n\nvisualize_word_correlations(casey_words, \n                            minimum_correlation = .2,\n                            title = \"@SenBobCasey tweets\",\n                            subtitle = \"Word correlation\",\n                            caption = \"@conor_tompkins\")\n\n\n\n\n\n\n\n\nTakeaway: The two main clusters in this graph show that Senator Casey used a consistent vocabulary to discuss the tax cut bill and the effort to repeal the ACA.\nThis code calculates and plots the correlations for Senator Toomey’s tweets:\n\ntoomey_words &lt;- word_correlations(df_toomey, minimum = 75, casey_stopwords)\n\nvisualize_word_correlations(toomey_words, \n                            minimum_correlation = .2,\n                            title = \"@SenToomey tweets\",\n                            subtitle = \"Word correlation\",\n                            caption = \"@conor_tompkins\")\n\n\n\n\n\n\n\n\nTakeaway: This plot shows that Senator Toomey used a consistent vocabulary to discuss his main policy goal, the tax cut bill."
  },
  {
    "objectID": "posts/comparing-u-s-senators-casey-and-toomey/index.html#word-frequency-comparison",
    "href": "posts/comparing-u-s-senators-casey-and-toomey/index.html#word-frequency-comparison",
    "title": "Comparing US Senators Casey and Toomey",
    "section": "Word frequency comparison",
    "text": "Word frequency comparison\nWe can also compare how frequently the senators use various words. To set a baseline, Senator Toomey tweeted around 800 more times than Senator Casey in this sample of their tweets.\n\ntweets &lt;- bind_rows(df_casey, df_toomey)\n\nreplace_reg &lt;- \"https://t.co/[A-Za-z\\\\d]+|http://[A-Za-z\\\\d]+|&amp;|&lt;|&gt;|RT|https|'s|'\"\nunnest_reg &lt;- \"([^A-Za-z_\\\\d#@']|'(?![A-Za-z_\\\\d#@]))\"\n\ntweets %&gt;% \n  select(senator, status_id, text, is_quote, is_retweet) %&gt;% \n  filter(is_quote == FALSE, is_retweet == FALSE) %&gt;% \n  mutate(text = str_replace_all(text, replace_reg, \"\"),\n         senator = factor(senator, levels = c(\"SenToomey\", \"SenBobCasey\"))) %&gt;% \n  count(senator) %&gt;% \n  rename(count = n) %&gt;% \n  head() %&gt;% \n  kable(\"html\") %&gt;% \n  kable_styling()\n\n\n\n\nsenator\ncount\n\n\n\n\nSenToomey\n2916\n\n\nSenBobCasey\n2180\n\n\n\n\n\n\n\nThis code breaks the tweets down into single words:\n\ntidy_tweets &lt;- tweets %&gt;% \n  select(senator, status_id, text, is_quote, is_retweet) %&gt;% \n  filter(is_quote == FALSE, is_retweet == FALSE) %&gt;% \n  mutate(text = str_replace_all(text, replace_reg, \"\"),\n         senator = factor(senator, levels = c(\"SenBobCasey\", \"SenToomey\"))) %&gt;%\n  unnest_tokens(word, text, token = \"regex\", pattern = unnest_reg) %&gt;%\n  filter(!word %in% stop_words$word,\n         !word %in% c(\"009f\", \"00a6\", \"f0\", \"http\", \".va\"),\n         str_detect(word, \"[a-z]\"))\n\nThis code calculates how frequently each word is used by each senator, given how many tweets each senator has:\n\nfrequency &lt;- tidy_tweets %&gt;% \n  group_by(senator) %&gt;% \n  count(word, sort = TRUE) %&gt;% \n  left_join(tidy_tweets %&gt;% \n              group_by(senator) %&gt;% \n              summarise(total = n())) %&gt;%\n  mutate(freq = n/total)\n\nfrequency %&gt;% \n  rename(count = n) %&gt;% \n  head() %&gt;% \n  kable(\"html\") %&gt;% \n  kable_styling()\n\n\n\n\nsenator\nword\ncount\ntotal\nfreq\n\n\n\n\nSenToomey\npa\n344\n27773\n0.0123861\n\n\nSenBobCasey\ntax\n336\n20002\n0.0167983\n\n\nSenBobCasey\nhealth\n299\n20002\n0.0149485\n\n\nSenBobCasey\ncare\n282\n20002\n0.0140986\n\n\nSenBobCasey\n@realdonaldtrump\n227\n20002\n0.0113489\n\n\nSenToomey\nhappy\n189\n27773\n0.0068052\n\n\n\n\n\n\n\nThis code spits the frequency into two columns, one for each senator. The data is sorted by how often Senator Casey used the word.\n\nfrequency &lt;- frequency %&gt;% \n  select(senator, word, freq) %&gt;% \n  spread(senator, freq) %&gt;%\n  arrange(desc(SenBobCasey))\n\nfrequency %&gt;% \n  head() %&gt;% \n  kable(\"html\") %&gt;% \n  kable_styling()\n\n\n\n\nword\nSenBobCasey\nSenToomey\n\n\n\n\ntax\n0.0167983\n0.0044648\n\n\nhealth\n0.0149485\n0.0017643\n\n\ncare\n0.0140986\n0.0018363\n\n\n@realdonaldtrump\n0.0113489\n0.0004321\n\n\nfamilies\n0.0088991\n0.0026645\n\n\nlose\n0.0083992\n0.0001440\n\n\n\n\n\n\n\nThis plot compares how often the senators use each word. I think it gives a somewhat fuzzy view of the policy issues each senator focuses on.\n\nggplot(frequency, aes(SenBobCasey, SenToomey)) +\n  geom_jitter(alpha = 0.1, size = 2.5, width = 0.25, height = 0.25) +\n  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +\n  scale_x_log10(labels = percent_format()) +\n  scale_y_log10(labels = percent_format()) +\n  geom_abline(color = \"red\") +\n  labs(title = title,\n       x = \"Used more by @SenBobCasey\",\n       y = \"Used more by @SenToomey\",\n       caption = caption)\n\n\n\n\n\n\n\n\nTakeaway: Senator Toomey was more likely to use words like “economy”, “safety”, “reform”, and “prayers”. Senator Casey was more likely to use words like “deficit”, “premiums”, “medicaid”, “disabilities”, “credits”, and “corportations”."
  },
  {
    "objectID": "posts/comparing-u-s-senators-casey-and-toomey/index.html#word-ratios",
    "href": "posts/comparing-u-s-senators-casey-and-toomey/index.html#word-ratios",
    "title": "Comparing US Senators Casey and Toomey",
    "section": "Word ratios",
    "text": "Word ratios\nWe can also directly compare how often each senator used a word. This code calculates that differece. The higher the log ratio, the greater the difference in how often each senator used the word.\n\nword_ratios &lt;- tidy_tweets %&gt;%\n  filter(!str_detect(word, \"^@\")) %&gt;%\n  count(word, senator) %&gt;%\n  filter(sum(n) &gt;= 10) %&gt;%\n  ungroup() %&gt;%\n  spread(senator, n, fill = 0) %&gt;%\n  mutate_if(is.numeric, funs((. + 1) / sum(. + 1))) %&gt;%\n  mutate(logratio = log(SenBobCasey / SenToomey)) %&gt;%\n  arrange(desc(logratio))\n\nword_ratios %&gt;% \n  arrange(desc(abs(logratio))) %&gt;%\n  head() %&gt;% \n  kable(\"html\") %&gt;% \n  kable_styling()\n\n\n\n\nword\nSenBobCasey\nSenToomey\nlogratio\n\n\n\n\n#trumpcare\n0.0055571\n0.0000283\n5.279218\n\n\nscheme\n0.0034820\n0.0000283\n4.811743\n\n\nolds\n0.0024268\n0.0000283\n4.450729\n\n\n#noagetax\n0.0023565\n0.0000283\n4.421315\n\n\n#taxreform\n0.0000352\n0.0024356\n-4.237725\n\n\n#obamacare\n0.0000352\n0.0022091\n-4.140086\n\n\n\n\n\n\n\nSenator Toomey often references wife Kris in his tweets, usually while offering condolences or prayers.\nThis plot shows which words are more uniquely used by each senator.\n\nword_ratios %&gt;%\n  group_by(logratio &lt; 0) %&gt;%\n  top_n(15, abs(logratio)) %&gt;%\n  ungroup() %&gt;%\n  mutate(word = reorder(word, logratio)) %&gt;%\n  ggplot(aes(word, logratio, fill = logratio &lt; 0)) +\n  geom_col(alpha = .8) +\n  coord_flip() +\n  labs(x = \"\",\n       y = \"Log odds ratio (Casey/Toomey)\") +\n  scale_fill_manual(name = \"\", \n                    values = c(\"blue\", \"red\"),\n                    breaks = c(FALSE, TRUE), \n                    labels = c(\"@SenBobCasey\", \"@SenToomey\")) +\n  theme(panel.grid.major.y = element_blank())\n\n\n\n\n\n\n\n\nTakeaway: This plot shows the stark divide between how each senator views some of the major policy issues. Senator Casey directly criticized the problems he saw during the tax cut and ACA fight (“trumpcare”, “scheme”, “lose”, “giveaways”, and “#noagetax”). Senator Toomey referenced “#philly” and “#obamacare” much more, and framed the tax cut bill as “#taxreform”."
  },
  {
    "objectID": "posts/modeling-pittsburgh-house-sales-linear/index.html",
    "href": "posts/modeling-pittsburgh-house-sales-linear/index.html",
    "title": "Modeling Pittsburgh House Sales Linear",
    "section": "",
    "text": "In this post I will be modeling house (land parcel) sales in Pittsburgh. The data is from the WPRDC’s Parcels n’at dashboard or here.\nThe goal is to use linear modeling to predict the sale price of a house using features of the house and the property.\nThis code sets up the environment and loads the libraries I will use.\n\n#load libraries\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(caret)\nlibrary(broom)\nlibrary(modelr)\nlibrary(rsample)\nlibrary(janitor)\nlibrary(vroom)\n\n#set up environment\noptions(scipen = 999, digits = 5)\n\ntheme_set(theme_bw())\n\nThis reads the data and engineers some features.\n\n#read in data\ndf &lt;- vroom(\"post_data/allegheny_county_master_file.csv\", col_types = cols(.default = \"c\")) %&gt;% \n  clean_names() %&gt;% \n  mutate(across(.cols = c(saleprice, finishedlivingarea, lotarea, yearblt,\n                          bedrooms, fullbaths, halfbaths), parse_number))\n\n#glimpse(df)\n\n\n# df %&gt;% \n#   select(saleprice, finishedlivingarea, lotarea, yearblt, bedrooms, fullbaths, halfbaths) %&gt;% \n#   glimpse()\n# \n# df %&gt;% \n#   select(contains(\"muni\"))\n\n\ndf &lt;- df %&gt;% \n  mutate(munidesc = str_replace(munidesc, \" - PITTSBURGH\", \"\")) %&gt;% \n  mutate(finishedlivingarea_log10 = log10(finishedlivingarea),\n         lotarea_log10 = log10(lotarea),\n         saleprice_log10 = log10(saleprice)) %&gt;% \n  select(parid, classdesc, munidesc, schooldesc, neighdesc, taxdesc,\n         usedesc, homesteadflag, farmsteadflag, styledesc,\n         yearblt, extfinish_desc, roofdesc,  basementdesc,\n         gradedesc, conditiondesc, stories, totalrooms, bedrooms,\n         fullbaths, halfbaths, heatingcoolingdesc, fireplaces, \n         bsmtgarage, finishedlivingarea, finishedlivingarea_log10,\n         lotarea, lotarea_log10, saledate,\n         saleprice, saleprice_log10)\n\n#create grade vectors\ngrades_standard &lt;- c(\"average -\", \"average\", \"average +\",\n                     \"good -\", \"good\", \"good +\",\n                     \"very good -\", \"very good\", \"very good +\")\n\ngrades_below_average_or_worse &lt;- c(\"poor -\", \"poor\", \"poor +\",\n                                   \"below average -\", \"below average\", \"below average +\")\n\ngrades_excellent_or_better &lt;- c(\"excellent -\", \"excellent\", \"excellent +\",\n                                \"highest cost -\", \"highest cost\", \"highest cost +\")\n\n#subset data and engineer features\ndf &lt;- df %&gt;% \n  filter(classdesc == \"RESIDENTIAL\",\n         saleprice &gt; 100,\n         str_detect(munidesc, \"Ward\"),\n         finishedlivingarea &gt; 0,\n         lotarea &gt; 0) %&gt;% \n  select(parid, munidesc, schooldesc, neighdesc, taxdesc,\n         usedesc, homesteadflag, farmsteadflag, styledesc,\n         yearblt, extfinish_desc, roofdesc,  basementdesc, \n         heatingcoolingdesc, gradedesc, conditiondesc, stories, \n         totalrooms, bedrooms, fullbaths, halfbaths, fireplaces, \n         bsmtgarage, finishedlivingarea_log10, lotarea_log10, \n         saleprice_log10, saledate) %&gt;% \n  mutate(usedesc = fct_lump(usedesc, n = 5),\n         styledesc = fct_lump(styledesc, n = 10),\n         #clean up and condense gradedesc\n         gradedesc = str_to_lower(gradedesc),\n         gradedesc = case_when(gradedesc %in% grades_below_average_or_worse ~ \"below average + or worse\",\n                                    gradedesc %in% grades_excellent_or_better ~ \"excellent - or better\",\n                                    gradedesc %in% grades_standard ~ gradedesc),\n         gradedesc = fct_relevel(gradedesc, c(\"below average + or worse\", \"average -\", \"average\", \"average +\",\n                                                        \"good -\", \"good\", \"good +\",\n                                                        \"very good -\", \"very good\", \"very good +\", \"excellent - or better\")))\n\n#replace missing character rows with \"missing\", change character columns to factor\ndf &lt;- df %&gt;% \n  mutate_if(is.character, replace_na, \"missing\") %&gt;% \n  mutate_if(is.character, as.factor)\n\n#select response and features\ndf &lt;- df %&gt;% \n  select(munidesc, usedesc, styledesc, conditiondesc, gradedesc,\n         finishedlivingarea_log10, lotarea_log10, yearblt, bedrooms, \n         fullbaths, halfbaths, saleprice_log10) %&gt;% \n  na.omit()\n\n#muni_desc_levels &lt;- levels(df$munidesc)\n\n#view data\nglimpse(df)\n\nRows: 74,687\nColumns: 12\n$ munidesc                 &lt;fct&gt; 1st Ward , 1st Ward , 1st Ward , 1st Ward , 1…\n$ usedesc                  &lt;fct&gt; Other, SINGLE FAMILY, Other, Other, Other, Ot…\n$ styledesc                &lt;fct&gt; Other, TOWNHOUSE, Other, Other, Other, Other,…\n$ conditiondesc            &lt;fct&gt; AVERAGE, EXCELLENT, GOOD, AVERAGE, AVERAGE, G…\n$ gradedesc                &lt;fct&gt; very good +, excellent - or better, very good…\n$ finishedlivingarea_log10 &lt;dbl&gt; 3.0993, 3.6170, 3.1844, 3.2057, 3.1173, 3.159…\n$ lotarea_log10            &lt;dbl&gt; 3.0993, 3.0461, 3.2355, 3.1584, 3.1173, 3.159…\n$ yearblt                  &lt;dbl&gt; 2007, 2012, 2007, 2007, 2015, 1905, 1905, 190…\n$ bedrooms                 &lt;dbl&gt; 2, 3, 2, 2, 2, 1, 2, 2, 1, 2, 4, 3, 3, 4, 4, …\n$ fullbaths                &lt;dbl&gt; 2, 3, 2, 1, 2, 1, 2, 2, 1, 2, 2, 2, 2, 3, 2, …\n$ halfbaths                &lt;dbl&gt; 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, …\n$ saleprice_log10          &lt;dbl&gt; 6.0414, 6.2544, 5.8543, 5.8876, 5.6767, 5.531…\n\n\nAs shown in the data above, the model uses the following features to predict sale price:\n\nmunicipality name\nprimary use of the parcel\nstyle of building\ncondition of the structure\ngrade of construction\nliving area in square feet\nlot area in square feet\nyear the house was built\nnumber of bedrooms\nnumber of full baths\nnumber of half-baths\n\nThis code sets up the data for cross validation.\n\n#create initial split object\ndf_split &lt;- initial_split(df, prop = .75)\n\n#extract training dataframe\ntraining_data_full &lt;- training(df_split)\n\n#extract testing dataframe\ntesting_data &lt;- testing(df_split)\n\ndistinct(training_data_full, munidesc)|&gt; \n  anti_join(distinct(testing_data, munidesc))\n\n# A tibble: 1 × 1\n  munidesc              \n  &lt;fct&gt;                 \n1 1st Ward  - McKEESPORT\n\n#find dimensions of training_data_full and testing_data\ndim(training_data_full)\n\n[1] 56015    12\n\ndim(testing_data)\n\n[1] 18672    12\n\n\nThis code divides the data into training and testing sets.\n\nset.seed(42)\n\n#prep the df with the cross validation partitions\ncv_split &lt;- vfold_cv(training_data_full, v = 5)\n\ncv_data &lt;- cv_split %&gt;% \n  mutate(\n    #extract train dataframe for each split\n    train = map(splits, ~training(.x)), \n    #extract validate dataframe for each split\n    validate = map(splits, ~testing(.x))\n  )\n\n#view df\ncv_data\n\n#  5-fold cross-validation \n# A tibble: 5 × 4\n  splits                id    train                  validate              \n  &lt;list&gt;                &lt;chr&gt; &lt;list&gt;                 &lt;list&gt;                \n1 &lt;split [44812/11203]&gt; Fold1 &lt;tibble [44,812 × 12]&gt; &lt;tibble [11,203 × 12]&gt;\n2 &lt;split [44812/11203]&gt; Fold2 &lt;tibble [44,812 × 12]&gt; &lt;tibble [11,203 × 12]&gt;\n3 &lt;split [44812/11203]&gt; Fold3 &lt;tibble [44,812 × 12]&gt; &lt;tibble [11,203 × 12]&gt;\n4 &lt;split [44812/11203]&gt; Fold4 &lt;tibble [44,812 × 12]&gt; &lt;tibble [11,203 × 12]&gt;\n5 &lt;split [44812/11203]&gt; Fold5 &lt;tibble [44,812 × 12]&gt; &lt;tibble [11,203 × 12]&gt;\n\n\nThis builds the model to predict house sale price.\n\n#build model using the train data for each fold of the cross validation\ncv_models_lm &lt;- cv_data %&gt;% \n  mutate(model = map(train, ~lm(formula = saleprice_log10 ~ ., data = .x)))\n\ncv_models_lm\n\n#  5-fold cross-validation \n# A tibble: 5 × 5\n  splits                id    train                  validate model \n  &lt;list&gt;                &lt;chr&gt; &lt;list&gt;                 &lt;list&gt;   &lt;list&gt;\n1 &lt;split [44812/11203]&gt; Fold1 &lt;tibble [44,812 × 12]&gt; &lt;tibble&gt; &lt;lm&gt;  \n2 &lt;split [44812/11203]&gt; Fold2 &lt;tibble [44,812 × 12]&gt; &lt;tibble&gt; &lt;lm&gt;  \n3 &lt;split [44812/11203]&gt; Fold3 &lt;tibble [44,812 × 12]&gt; &lt;tibble&gt; &lt;lm&gt;  \n4 &lt;split [44812/11203]&gt; Fold4 &lt;tibble [44,812 × 12]&gt; &lt;tibble&gt; &lt;lm&gt;  \n5 &lt;split [44812/11203]&gt; Fold5 &lt;tibble [44,812 × 12]&gt; &lt;tibble&gt; &lt;lm&gt;  \n\n#problem with factors split across training/validation\n#https://stats.stackexchange.com/questions/235764/new-factors-levels-not-present-in-training-data\n\nThis is where I begin to calculate metrics to judge how well my model is doing.\n\ncv_prep_lm &lt;- cv_models_lm %&gt;% \n  mutate(\n    #extract actual sale price for the records in the validate dataframes\n    validate_actual = map(validate, ~.x$saleprice_log10),\n    #predict response variable for each validate set using its corresponding model\n    validate_predicted = map2(.x = model, .y = validate, ~predict(.x, .y))\n  )\n\n#View data\ncv_prep_lm\n\n#  5-fold cross-validation \n# A tibble: 5 × 7\n  splits                id    train    validate model  validate_actual\n  &lt;list&gt;                &lt;chr&gt; &lt;list&gt;   &lt;list&gt;   &lt;list&gt; &lt;list&gt;         \n1 &lt;split [44812/11203]&gt; Fold1 &lt;tibble&gt; &lt;tibble&gt; &lt;lm&gt;   &lt;dbl [11,203]&gt; \n2 &lt;split [44812/11203]&gt; Fold2 &lt;tibble&gt; &lt;tibble&gt; &lt;lm&gt;   &lt;dbl [11,203]&gt; \n3 &lt;split [44812/11203]&gt; Fold3 &lt;tibble&gt; &lt;tibble&gt; &lt;lm&gt;   &lt;dbl [11,203]&gt; \n4 &lt;split [44812/11203]&gt; Fold4 &lt;tibble&gt; &lt;tibble&gt; &lt;lm&gt;   &lt;dbl [11,203]&gt; \n5 &lt;split [44812/11203]&gt; Fold5 &lt;tibble&gt; &lt;tibble&gt; &lt;lm&gt;   &lt;dbl [11,203]&gt; \n# ℹ 1 more variable: validate_predicted &lt;list&gt;\n\n\n\n#calculate fit metrics for each validate fold       \ncv_eval_lm &lt;- cv_prep_lm %&gt;% \n  mutate(validate_rmse = map2_dbl(model, validate, modelr::rmse),\n         validate_mae = map2_dbl(model, validate, modelr::mae))\n\ncv_eval_lm &lt;- cv_eval_lm %&gt;% \n  mutate(fit = map(model, ~glance(.x))) %&gt;% \n  unnest(fit)\n\n\n#view data\ncv_eval_lm %&gt;% \n  select(id, validate_mae, validate_rmse, adj.r.squared)\n\n# A tibble: 5 × 4\n  id    validate_mae validate_rmse adj.r.squared\n  &lt;chr&gt;        &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n1 Fold1        0.328         0.447         0.428\n2 Fold2        0.330         0.450         0.428\n3 Fold3        0.329         0.448         0.430\n4 Fold4        0.328         0.448         0.431\n5 Fold5        0.326         0.442         0.428\n\n\nFinally, this calculates how well the model did on the validation set.\n\n#summarize fit metrics on cross-validated dfs\ncv_eval_lm %&gt;% \n  select(validate_mae, validate_rmse, adj.r.squared) %&gt;% \n  summarize_all(mean)\n\n# A tibble: 1 × 3\n  validate_mae validate_rmse adj.r.squared\n         &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n1        0.328         0.447         0.429\n\n\n\n#fit model on full training set\ntrain_df &lt;- cv_data %&gt;% \n  select(train) %&gt;% \n  unnest(train)\n\nmodel_train &lt;- lm(formula = saleprice_log10 ~ ., data = train_df)\n\nmodel_train %&gt;% \n  glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df   logLik     AIC     BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     0.430         0.430 0.446     1941.       0    87 -137142. 274462. 275381.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nThis is the RMSE on the training set\n\n#calculate rmse on training set\nrmse(model_train, train_df)\n\n[1] 0.44626\n\n\nThis shows the impact each term of the model has on the response variable. This is for the training data.\n\n#visualize estimates for terms\nmodel_train %&gt;% \n  tidy() %&gt;% \n  filter(term != \"(Intercept)\") %&gt;% \n  mutate(term = fct_reorder(term, estimate)) %&gt;% \n  ggplot(aes(term, estimate)) +\n  geom_hline(yintercept = 0, linetype = 2, color = \"red\") +\n  geom_point() +\n  coord_flip()\n\n\n\n\n\n\n\n\nNext, I apply the model to the testing data to see how the model does out-of-sample.\n\n#create dfs for train_data_small and validate_data\n#train_data_small &lt;- cv_prep_lm %&gt;% \n#  unnest(train) %&gt;% \n#  select(-id)\n\nvalidate_df &lt;- cv_prep_lm %&gt;% \n  select(validate) %&gt;% \n  unnest()\n\nThis creates the augmented dataframe and plots the actual price vs. the fitted price.\n\n#visualize model on validate data\naugment_validate &lt;- augment(model_train, newdata = validate_df) %&gt;% \n  mutate(.resid = saleprice_log10 - .fitted)\n\n#actual vs. fitted\ncv_prep_lm %&gt;% \n  unnest(validate_actual, validate_predicted) %&gt;% \n  ggplot(aes(validate_actual, validate_predicted)) +\n  geom_abline() +\n  stat_density_2d(aes(fill = stat(level)), geom = \"polygon\") +\n  geom_smooth(method = \"lm\") +\n  scale_x_continuous(limits = c(2, 7)) +\n  scale_y_continuous(limits = c(2, 7)) +\n  coord_equal() +\n  scale_fill_viridis_c()\n\n\n\n\n\n\n\n\nThis distribution shows that the model overestimates the prices on many houses.\n\n#distribution of residuals\naugment_validate %&gt;% \n  ggplot(aes(.resid)) +\n  geom_density() +\n  geom_vline(xintercept = 0, color = \"red\", linetype = 2)\n\n\n\n\n\n\n\n\nThis shows that the residuals are correlated with the actual price, which indicates that the model is failing to account for some dynamic in the sale process.\n\n#sale price vs. residuals\naugment_validate %&gt;% \n  ggplot(aes(.resid, saleprice_log10)) +\n  stat_density_2d(aes(fill = stat(level)), geom = \"polygon\") +\n  geom_vline(xintercept = 0, color = \"red\", linetype = 2) +\n  scale_fill_viridis_c()\n\n\n\n\n\n\n\n\nThis calculates how well the model predicted sale price on out-of-sample testing data.\n\n#calculate fit of model on test data\nrmse(model_train, validate_df)\n\n[1] 0.44626\n\nmae(model_train, validate_df)\n\n[1] 0.32743\n\nrsquare(model_train, validate_df)\n\n[1] 0.42988"
  },
  {
    "objectID": "posts/networking-usl-clubs-with-euclidean-distance/index.html",
    "href": "posts/networking-usl-clubs-with-euclidean-distance/index.html",
    "title": "Networking USL Club Similarity With Euclidean Distance",
    "section": "",
    "text": "Euclidean distance is a simple way to measure the distance between two points. It can also be used to measure how similar two sports teams are, given a set of variables. In this post, I use Euclidean distance to calculate the similarity between USL clubs and map that data to a network graph. I will use the 538 Soccer Power Index data to calculate the distance."
  },
  {
    "objectID": "posts/networking-usl-clubs-with-euclidean-distance/index.html#setup",
    "href": "posts/networking-usl-clubs-with-euclidean-distance/index.html#setup",
    "title": "Networking USL Club Similarity With Euclidean Distance",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(ggraph)\nlibrary(tidygraph)\nlibrary(viridis)\n\nset_graph_style()\n\nset.seed(1234)"
  },
  {
    "objectID": "posts/networking-usl-clubs-with-euclidean-distance/index.html#download-data",
    "href": "posts/networking-usl-clubs-with-euclidean-distance/index.html#download-data",
    "title": "Networking USL Club Similarity With Euclidean Distance",
    "section": "Download data",
    "text": "Download data\nThis code downloads the data from 538’s GitHub repo and does some light munging.\n\nread_csv(\"https://projects.fivethirtyeight.com/soccer-api/club/spi_global_rankings.csv\", progress = FALSE) %&gt;% \n  filter(league == \"United Soccer League\") %&gt;% \n  mutate(name = str_replace(name, \"Arizona United\", \"Phoenix Rising\")) -&gt; df\n\ndf"
  },
  {
    "objectID": "posts/networking-usl-clubs-with-euclidean-distance/index.html#calculate-euclidean-distance",
    "href": "posts/networking-usl-clubs-with-euclidean-distance/index.html#calculate-euclidean-distance",
    "title": "Networking USL Club Similarity With Euclidean Distance",
    "section": "Calculate Euclidean distance",
    "text": "Calculate Euclidean distance\nThis is the code that measures the distance between the clubs. It uses the 538 offensive and defensive ratings.\n\ndf %&gt;% \n  select(name, off, def) %&gt;% \n  column_to_rownames(var = \"name\") -&gt; df_dist\n\n#df_dist\n#rownames(df_dist) %&gt;% \n#  head()\n\ndf_dist &lt;- dist(df_dist, \"euclidean\", upper = FALSE, diag = FALSE)\n#head(df_dist)\n\ndf_dist %&gt;% \n  tidy() %&gt;% \n  arrange(desc(distance)) -&gt; df_dist\n\n#df_dist %&gt;% \n#  count(item1, sort = TRUE) %&gt;% \n#  ggplot(aes(item1, n)) +\n#  geom_point() +\n#  coord_flip() +\n#  theme_bw()"
  },
  {
    "objectID": "posts/networking-usl-clubs-with-euclidean-distance/index.html#network-graph",
    "href": "posts/networking-usl-clubs-with-euclidean-distance/index.html#network-graph",
    "title": "Networking USL Club Similarity With Euclidean Distance",
    "section": "Network graph",
    "text": "Network graph\nIn this snippet I set a threshhold for how similar clubs need to be to warrant a connection. Then I graph it using tidygraph and ggraph. Teams that are closer together on the graph are more similar. Darker and thicker lines indicate higher similarity.\n\ndistance_filter &lt;- .5\n\ndf_dist %&gt;% \n  mutate(distance = distance^2) %&gt;% \n  filter(distance &lt;= distance_filter) %&gt;%\n  as_tbl_graph(directed = FALSE) %&gt;% \n  mutate(community = as.factor(group_edge_betweenness())) %&gt;%\n  ggraph(layout = \"kk\", maxiter = 1000) +\n    geom_edge_fan(aes(edge_alpha = distance, edge_width = distance)) + \n    geom_node_label(aes(label = name, color = community), size = 5) +\n    scale_color_discrete(\"Group\") +\n    scale_edge_alpha_continuous(\"Euclidean distance ^2\", range = c(.2, 0)) +\n    scale_edge_width_continuous(\"Euclidean distance ^2\", range = c(2, 0)) +\n    labs(title = \"United Soccer League clubs\",\n       subtitle = \"Euclidean distance (offensive rating, defensive rating)^2\",\n       x = NULL,\n       y = NULL,\n       caption = \"538 data, @conor_tompkins\")"
  },
  {
    "objectID": "posts/ac_driving_commuter_routes/index.html",
    "href": "posts/ac_driving_commuter_routes/index.html",
    "title": "Analyzing major commuter routes in Allegheny County",
    "section": "",
    "text": "In this post I will use the Mapbox API to calculate metrics for major commuter routes in Allegheny County. The API will provide the distance and duration of the trip, as well as turn-by-turn directions. The route duration should be considered a “minimum duration” because it does not consider traffic. Then I will estimate the duration of the trips with a linear model and compare that to the actual duration from the Mapbox API. I will use the difference between the actual and estimated duration to identify neighborhoods that experience longer or shorter commutes than expected.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(mapboxapi)\nlibrary(tidycensus)\nlibrary(janitor)\nlibrary(lehdr)\nlibrary(tigris)\nlibrary(sf)\nlibrary(hrbrthemes)\n\noptions(tigris_use_cache = TRUE,\n        scipen = 999,\n        digits = 4)\n\ntheme_set(theme_ipsum())\n\nsf_use_s2(FALSE)"
  },
  {
    "objectID": "posts/ac_driving_commuter_routes/index.html#intro",
    "href": "posts/ac_driving_commuter_routes/index.html#intro",
    "title": "Analyzing major commuter routes in Allegheny County",
    "section": "",
    "text": "In this post I will use the Mapbox API to calculate metrics for major commuter routes in Allegheny County. The API will provide the distance and duration of the trip, as well as turn-by-turn directions. The route duration should be considered a “minimum duration” because it does not consider traffic. Then I will estimate the duration of the trips with a linear model and compare that to the actual duration from the Mapbox API. I will use the difference between the actual and estimated duration to identify neighborhoods that experience longer or shorter commutes than expected.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(mapboxapi)\nlibrary(tidycensus)\nlibrary(janitor)\nlibrary(lehdr)\nlibrary(tigris)\nlibrary(sf)\nlibrary(hrbrthemes)\n\noptions(tigris_use_cache = TRUE,\n        scipen = 999,\n        digits = 4)\n\ntheme_set(theme_ipsum())\n\nsf_use_s2(FALSE)"
  },
  {
    "objectID": "posts/ac_driving_commuter_routes/index.html#gather-data",
    "href": "posts/ac_driving_commuter_routes/index.html#gather-data",
    "title": "Analyzing major commuter routes in Allegheny County",
    "section": "Gather data",
    "text": "Gather data\nThe first step is to download the census tract shapefiles for the county:\n\n#get tracts\nallegheny_county_tracts &lt;- tracts(state = \"PA\", county = \"Allegheny\", cb = TRUE) %&gt;% \n  select(GEOID)\n\nst_erase &lt;- function(x, y) {\n  st_difference(x, st_union(y))\n}\n\nac_water &lt;- area_water(\"PA\", \"Allegheny\", class = \"sf\")\n\nallegheny_county_tracts &lt;- st_erase(allegheny_county_tracts, ac_water)\n\nThen I download the “Origin-Destination” LODES file from the Census for Pennsylvania in 2017 and subset it to commuters within Allegheny County.\n\n#load od tract-level data\nlodes_od_ac_main &lt;- grab_lodes(state = \"pa\", year = 2017, \n                               lodes_type = \"od\", job_type = \"JT00\", \n                               segment = \"S000\", state_part = \"main\", \n                               agg_geo = \"tract\", use_cache = TRUE) %&gt;%\n  select(state, w_tract, h_tract, S000, year) %&gt;% \n  rename(commuters = S000) %&gt;% \n  mutate(intra_tract_commute = h_tract == w_tract) %&gt;% \n  semi_join(allegheny_county_tracts, by = c(\"w_tract\" = \"GEOID\")) %&gt;% \n  semi_join(allegheny_county_tracts, by = c(\"h_tract\" = \"GEOID\"))\n\nThis analysis only considers routes where the commuter changed census tracts. 96% of commuters in Allegheny County change census tracts.\n\nlodes_od_ac_main %&gt;% \n  group_by(intra_tract_commute) %&gt;% \n  summarize(commuters = sum(commuters)) %&gt;% \n  ungroup() %&gt;% \n  mutate(pct = commuters / sum(commuters))\n\n# A tibble: 2 × 3\n  intra_tract_commute commuters    pct\n  &lt;lgl&gt;                   &lt;dbl&gt;  &lt;dbl&gt;\n1 FALSE                  465637 0.963 \n2 TRUE                    18127 0.0375"
  },
  {
    "objectID": "posts/ac_driving_commuter_routes/index.html#get-directions",
    "href": "posts/ac_driving_commuter_routes/index.html#get-directions",
    "title": "Analyzing major commuter routes in Allegheny County",
    "section": "Get directions",
    "text": "Get directions\nThis is the code that identifies the center of each tract, geocodes those centroids to get an address, and gets the turn-by-turn directions and route data for each pair of home and work addresses. I will focus on the top 20% of these routes (in terms of cumulative percent of commuters) because the Mapbox API is not designed for the size of query I would need to get directions for all combinations of census tracts.\nNote that I manually replaced the geocoded address for the Wexford and Swissvale areas because the geocoder returned results outside of the county, probably because the center of those tracts intersect with highways.\n\n#filter out rows where commuter doesn't change tracts\ncombined_tract_sf &lt;- lodes_od_ac_main %&gt;%\n  arrange(desc(commuters)) %&gt;% \n  filter(w_tract != h_tract)\n\n#calculate cumulative pct of commuters, keep only top 20%\ncombined_tract_sf_small &lt;- combined_tract_sf %&gt;% \n  select(h_tract, w_tract, commuters) %&gt;% \n  arrange(desc(commuters)) %&gt;% \n  mutate(id = row_number(),\n         pct_commuters = commuters / sum(commuters),\n         cumulative_pct_commuters = cumsum(pct_commuters)) %&gt;%\n  filter(cumulative_pct_commuters &lt; .2) %&gt;%\n  select(h_tract, w_tract, commuters)\n\n#add census centroid geometry\ncombined_tract_sf_small &lt;- combined_tract_sf_small %&gt;% \n  left_join(st_centroid(allegheny_county_tracts), by = c(\"h_tract\" = \"GEOID\")) %&gt;% \n  rename(h_tract_geo = geometry) %&gt;% \n  left_join(st_centroid(allegheny_county_tracts), by = c(\"w_tract\" = \"GEOID\")) %&gt;% \n  rename(w_tract_geo = geometry) %&gt;% \n  select(h_tract, h_tract_geo, w_tract, w_tract_geo, commuters)\n\ncombined_tract_sf_small |&gt; \n  st_sf() |&gt; \n  ggplot() +\n  geom_sf(data = allegheny_county_tracts) +\n  geom_sf()\n\n#get addresses for tract centroids\ntract_od_directions &lt;- combined_tract_sf_small %&gt;%\n  mutate(home_address = map_chr(h_tract_geo, mb_reverse_geocode),\n         work_address = map_chr(w_tract_geo, mb_reverse_geocode))\n\n#replace bad address with good address\nwexford_good_address &lt;- \"3321 Wexford Rd, Gibsonia, PA 15044\"\nswissvale_good_address &lt;- \"1118 S Braddock Ave, Swissvale, PA 15218\"\n\ntract_od_directions &lt;- tract_od_directions %&gt;% \n  #fix wexford address\n  mutate(home_address = case_when(h_tract == \"42003409000\" ~ wexford_good_address,\n                                  h_tract != \"42003409000\" ~ home_address),\n         work_address = case_when(w_tract == \"42003409000\" ~ wexford_good_address,\n                                  w_tract != \"42003409000\" ~ work_address)) |&gt; \n  #fix swissvale address\n  mutate(home_address = case_when(h_tract == \"42003515401\" ~ swissvale_good_address,\n                                  TRUE ~ home_address))\n\n#define error-safe mb_directions function\nmb_directions_possibly &lt;- possibly(mb_directions, otherwise = NA)\n\n#geocode addresses, get directions\ntract_od_directions &lt;- tract_od_directions %&gt;% \n  mutate(home_address_location_geocoded = map(home_address, mb_geocode),\n         work_address_location_geocoded = map(work_address, mb_geocode)) %&gt;% \n  mutate(directions = map2(home_address, work_address, ~ mb_directions_possibly(origin = .x,\n                                                                       destination = .y,\n                                                                       steps = TRUE,\n                                                                       profile = \"driving\"))) %&gt;% \n  select(h_tract, h_tract_geo, home_address, home_address_location_geocoded,\n         w_tract, w_tract_geo, work_address, work_address_location_geocoded,\n         directions, commuters)\n\nThe core of the above code is combining map2 and mb_directions_possibly. This maps the mb_directions_possibly function against two inputs (the home address and work address).\nThe result is a dataframe with a row per turn-by-turn direction for each commuter route.\nThis summarizes the data so there is one row per commuter route and creates summarized route data.\n\n#summarize direction data\ntract_od_stats &lt;- tract_od_directions %&gt;% \n  group_by(h_tract, home_address, w_tract, work_address) %&gt;%\n  summarize(duration = sum(duration),\n            distance = sum(distance),\n            steps = n(),\n            commuters = unique(commuters)) %&gt;% \n  ungroup()\n\nAs expected, route duration and distance are highly correlated. The median duration of a trip is 16.7 minutes.\n\n#graph od stats\ntract_od_stats %&gt;% \n  ggplot(aes(distance, duration, size = commuters)) +\n  geom_point(alpha = .3) +\n  geom_abline(linetype = 2, color = \"red\") +\n  coord_equal() +\n  theme_ipsum() +\n  labs(title = \"Commutes between census tracts\",\n       subtitle = \"Allegheny County, PA\",\n       x = \"Distance in KM\",\n       y = \"Duration in minutes\",\n       size = \"Commuters\")\n\n\n\n\n\n\n\n\n\nmedian_duration &lt;- tract_od_stats %&gt;% \n  uncount(weights = commuters) %&gt;% \n  summarize(median_duration = median(duration)) %&gt;% \n  pull(median_duration)\n\ntract_od_stats %&gt;% \n  uncount(weights = commuters) %&gt;% \n  ggplot(aes(duration)) +\n  geom_density(fill = \"grey\") +\n  geom_vline(xintercept = median_duration, lty = 2, color = \"red\") +\n  annotate(\"text\", x = 21, y = .05, label = \"median\", color = \"red\") +\n  theme_ipsum() +\n  labs(title = \"Trip duration\",\n       x = \"Duration in minutes\",\n       y = \"Density of observations\")\n\n\n\n\n\n\n\n\nThis map shows the main roads that commuter routes use I-376, I-279, and Route 28 are major arteries, as expected.\n\n#map routes\ntract_od_stats %&gt;% \n  ggplot() +\n  geom_sf(data = allegheny_county_tracts, linewidth = .1, fill = \"black\") +\n  geom_sf(aes(alpha = commuters, linewidth = commuters), color = \"#ffcc01\", alpha = .1) +\n  guides(linewidth = guide_legend(override.aes= list(alpha = 1))) +\n  scale_linewidth_continuous(range = c(.1, 5)) +\n  theme_void() +\n  labs(title = \"Commuter routes between Allegheny County census tracts\",\n       subtitle = \"Driving routes\",\n       linewidth = \"Commuters\")\n\n\n\n\n\n\n\n\nA high-resolution image of this map is available here. An animation of the routes is here.\nPeople that live closer to downtown Pittsburgh have shorter commutes, on average.\n\nallegheny_county_tracts %&gt;% \n  st_drop_geometry() %&gt;% \n  left_join(tract_od_stats %&gt;% \n              st_drop_geometry() |&gt; \n              select(h_tract, w_tract, duration) %&gt;% \n              pivot_longer(contains(\"tract\")) %&gt;% \n              group_by(name, value) %&gt;% \n              summarize(avg_duration = mean(duration)) %&gt;% \n              ungroup(),\n            by = c(\"GEOID\" = \"value\")) %&gt;% \n  complete(GEOID, name) %&gt;% \n  filter(!is.na(name)) %&gt;% \n  left_join(allegheny_county_tracts, by = \"GEOID\") %&gt;%\n  mutate(name = case_when(name == \"h_tract\" ~ \"Origin tract\",\n                          name == \"w_tract\" ~ \"Destination tract\"),\n         name = as.factor(name) %&gt;% fct_rev()) %&gt;% \n  st_sf() %&gt;% \n  ggplot() +\n  geom_sf(aes(fill = avg_duration), size = .1) +\n  facet_wrap(~name, ncol = 1) +\n  scale_fill_viridis_c(na.value = \"grey90\") +\n  labs(title = \"Average commute duration\",\n       fill = \"Minutes\") +\n  theme_void()"
  },
  {
    "objectID": "posts/ac_driving_commuter_routes/index.html#model",
    "href": "posts/ac_driving_commuter_routes/index.html#model",
    "title": "Analyzing major commuter routes in Allegheny County",
    "section": "Model",
    "text": "Model\nThe next step is to create a model that estimates the duration of a given commute. I will use the number of steps in the turn-by-turn directions and the distance as predictors. Additionally, I will calculate which rivers a commute route crosses and use those as logical variables in the model.\nThis collects the geometry for the main rivers in the county.\n\nmain_rivers &lt;- ac_water %&gt;% \n  group_by(FULLNAME) %&gt;% \n  summarize(AWATER = sum(AWATER)) %&gt;% \n  arrange(desc(AWATER)) %&gt;% \n  slice(1:4)\n\nThis code calculates whether a given commuter route crosses a river.\n\ntract_od_stats_rivers &lt;- tract_od_stats %&gt;% \n  mutate(intersects_ohio = st_intersects(., main_rivers %&gt;% \n                                           filter(FULLNAME == \"Ohio Riv\")) %&gt;% as.logical(),\n         intersects_allegheny = st_intersects(., main_rivers %&gt;% \n                                                filter(FULLNAME == \"Allegheny Riv\")) %&gt;% as.logical(),\n         intersects_monongahela = st_intersects(., main_rivers %&gt;% \n                                                  filter(FULLNAME == \"Monongahela Riv\")) %&gt;% as.logical(),\n         intersects_youghiogheny = st_intersects(., main_rivers %&gt;% \n                                                   filter(FULLNAME == \"Youghiogheny Riv\")) %&gt;% as.logical()) %&gt;% \n  replace_na(list(intersects_ohio = FALSE,\n                  intersects_allegheny = FALSE,\n                  intersects_monongahela = FALSE,\n                  intersects_youghiogheny = FALSE)) %&gt;% \n  st_drop_geometry()\n\nglimpse(tract_od_stats_rivers)\n\nRows: 780\nColumns: 12\n$ h_tract                 &lt;chr&gt; \"42003020100\", \"42003020300\", \"42003030500\", \"…\n$ home_address            &lt;chr&gt; \"445 Wood Street, Pittsburgh, Pennsylvania 152…\n$ w_tract                 &lt;chr&gt; \"42003982200\", \"42003020100\", \"42003020100\", \"…\n$ work_address            &lt;chr&gt; \"4215 Fifth Avenue, Pittsburgh, Pennsylvania 1…\n$ duration                &lt;dbl&gt; 14.087, 9.225, 7.460, 10.951, 2.510, 10.517, 1…\n$ distance                &lt;dbl&gt; 5.6640, 2.6518, 1.5922, 3.4968, 0.7169, 4.2762…\n$ steps                   &lt;int&gt; 8, 7, 7, 5, 6, 6, 9, 7, 10, 11, 14, 10, 10, 8,…\n$ commuters               &lt;dbl&gt; 58, 106, 287, 68, 81, 129, 87, 121, 116, 59, 1…\n$ intersects_ohio         &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS…\n$ intersects_allegheny    &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS…\n$ intersects_monongahela  &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS…\n$ intersects_youghiogheny &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS…\n\ntract_od_stats_rivers &lt;- tract_od_stats_rivers %&gt;% \n  mutate(od_id = str_c(\"h_tract: \", h_tract, \", \", \"w_tract: \", w_tract, sep = \"\"))\n\nFirst I set the seed and split the data into training and testing sets.\n\nset.seed(1234)\n\n#split data\nsplits &lt;- initial_split(tract_od_stats_rivers, prop = .75)\n\ntraining_data &lt;- training(splits)\ntesting_data &lt;- testing(splits)\n\nThen I use {tidymodels} to define a linear model, cross-validate it, and extract the coefficients.\n\n#recipe\nmodel_recipe &lt;- recipe(duration ~ ., \n                       data = training_data) %&gt;% \n  update_role(od_id, new_role = \"id\") %&gt;%\n  step_rm(h_tract, home_address, w_tract, work_address, commuters) %&gt;% \n  step_normalize(distance, steps) %&gt;% \n  step_zv(all_predictors())\n\nmodel_recipe %&gt;% \n  prep() %&gt;% \n  summary()\n\n# A tibble: 8 × 4\n  variable                type      role      source  \n  &lt;chr&gt;                   &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n1 distance                &lt;chr [2]&gt; predictor original\n2 steps                   &lt;chr [2]&gt; predictor original\n3 intersects_ohio         &lt;chr [1]&gt; predictor original\n4 intersects_allegheny    &lt;chr [1]&gt; predictor original\n5 intersects_monongahela  &lt;chr [1]&gt; predictor original\n6 intersects_youghiogheny &lt;chr [1]&gt; predictor original\n7 od_id                   &lt;chr [3]&gt; id        original\n8 duration                &lt;chr [2]&gt; outcome   original\n\nmodel_recipe_prep &lt;- model_recipe %&gt;% \n  prep()\n\n\n#apply cv to training data\ntraining_vfold &lt;- vfold_cv(training_data, v = 10, repeats = 2)\n\n\n#model specification\nlm_model &lt;- linear_reg(mode = \"regression\") %&gt;% \n  set_engine(\"lm\")\n\n#linear regression workflow\nlm_workflow &lt;- workflow() %&gt;% \n  add_recipe(model_recipe) %&gt;% \n  add_model(lm_model)\n\n#fit against training resamples\nkeep_pred &lt;- control_resamples(save_pred = TRUE)\n\nlm_training_fit &lt;- lm_workflow %&gt;% \n  fit_resamples(training_vfold, control = keep_pred) %&gt;% \n  mutate(model = \"lm\")\n\n#get results from training cv\nlm_training_fit %&gt;% \n  collect_metrics()\n\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   2.88     20 0.0590  Preprocessor1_Model1\n2 rsq     standard   0.870    20 0.00571 Preprocessor1_Model1\n\n\nThe model averaged an R-squared of .82 on the training data, which is pretty good.\nThe predictions from the training set fit the actual duration pretty well.\n\n#graph predictions from assessment sets\nlm_training_fit %&gt;% \n  collect_predictions() %&gt;% \n  ggplot(aes(duration, .pred)) +\n  geom_point(alpha = .3) +\n  geom_abline(linetype = 2, color = \"red\") +\n  coord_equal() +\n  labs(x = \"Actual duration\",\n       y = \"Predicted duration\")\n\n\n\n\n\n\n\n\nNext I fit the model against the test data to extract the coefficients. Holding the other variables constant, distance is by far the most influential variable in the model. For every kilometer increase in distance, the duration of the commute can be expected to increase by around 5 minutes. Crossing the Monongahela will add around 2 minutes to a commute, while crossing the Allegheny and Ohio actually decrease commute times. This is probably related to the bridge that the commuter uses.\n\n#variable importance\nlm_workflow %&gt;% \n  fit(testing_data) %&gt;% \n  pull_workflow_fit() %&gt;% \n  tidy() %&gt;% \n  filter(term != \"(Intercept)\") %&gt;% \n  mutate(term = fct_reorder(term, estimate)) %&gt;% \n  ggplot(aes(estimate, term)) +\n  geom_col(fill = \"grey\", color = \"black\")\n\n\n\n\n\n\n\n\nThis fits the model to the full dataset and plots the predicted duration against the actual duration. The fit is tighter than just plotting distance vs. duration.\n\n#final model\ntract_od_pred &lt;- lm_workflow %&gt;% \n  fit(testing_data) %&gt;% \n  predict(tract_od_stats_rivers) %&gt;% \n  bind_cols(tract_od_stats_rivers) %&gt;% \n  select(h_tract, w_tract, distance, steps, duration, .pred, commuters)\n\ntract_od_pred %&gt;% \n  ggplot(aes(duration, .pred, size = commuters)) +\n  geom_point(alpha = .3) +\n  geom_abline(lty = 2, color = \"red\") +\n  coord_equal() +\n  labs(x = \"Duration in minutes\",\n       y = \"Predicted duration\",\n       size = \"Number of commuters\")\n\n\n\n\n\n\n\n\nThis calculates how far off the model’s estimation of duration was for each census tract in the dataset (origin and destination). Commuters originating from neighborhoods between State Route 51 and the Monongahela River experience longer than expected commutes.\n\nallegheny_county_tracts %&gt;% \n  st_drop_geometry() %&gt;% \n  left_join(tract_od_pred %&gt;% \n              mutate(.resid = duration - .pred) %&gt;% \n              select(h_tract, w_tract, .resid) %&gt;% \n              pivot_longer(contains(\"tract\")) %&gt;% \n              group_by(name, value) %&gt;% \n              summarize(avg_resid = mean(.resid)) %&gt;% \n              ungroup(),\n            by = c(\"GEOID\" = \"value\")) %&gt;% \n  complete(GEOID, name) %&gt;% \n  filter(!is.na(name)) %&gt;% \n  left_join(allegheny_county_tracts) %&gt;%\n  mutate(name = case_when(name == \"h_tract\" ~ \"Origin tract\",\n                          name == \"w_tract\" ~ \"Destination tract\"),\n         name = as.factor(name) %&gt;% fct_rev()) %&gt;% \n  st_sf() %&gt;% \n  ggplot() +\n  geom_sf(aes(fill = avg_resid), size = .1) +\n  facet_wrap(~name, ncol = 1) +\n  scale_fill_viridis_c(na.value = \"grey90\") +\n  labs(title = \"Commute duration above/below expected\",\n       fill = \"Minutes\") +\n  theme_void()"
  },
  {
    "objectID": "posts/car-crashes-in-allegheny-county/index.html",
    "href": "posts/car-crashes-in-allegheny-county/index.html",
    "title": "Car Crashes in Allegheny County",
    "section": "",
    "text": "WPRDC has published a dataset on car crashes in Allegheny County from 2004-2017. I was interested to see if there were any patterns or interesting trends in the data."
  },
  {
    "objectID": "posts/car-crashes-in-allegheny-county/index.html#load-data",
    "href": "posts/car-crashes-in-allegheny-county/index.html#load-data",
    "title": "Car Crashes in Allegheny County",
    "section": "Load data",
    "text": "Load data\nThe data was difficult to work with, so I condensed my data munging and cleansing workflow into the following scripts. I may write a post about that process in the future.\n\nsource(\"https://raw.githubusercontent.com/conorotompkins/allegheny_crashes/master/scripts/02_factorize_columns.R\")\nsource(\"https://raw.githubusercontent.com/conorotompkins/allegheny_crashes/master/scripts/03_clean_data.R\")\n\ndf &lt;- data %&gt;% \n  mutate(casualty_count = injury_count + fatal_count)\n\nrm(\"data\", \"df_combined_allegheny_county_crash_data_2004_2017_factorized\", \"df_dictionary\")\n\nThis graph shows that the number of crashes per year is stable, with some year-to-year variation.\n\ndf %&gt;% \n  mutate(crash_year = factor(crash_year)) %&gt;% \n  count(crash_year) %&gt;% \n  ggplot(aes(crash_year, n, group = 1)) +\n  geom_line() +\n  geom_point() +\n  scale_y_continuous(limits = c(0, 13000),\n                     label=comma) +\n  labs(title = \"Crashes per year\",\n       subtitle = my_subtitle,\n       x = NULL,\n       y = \"Number of crashes\",\n       caption = my_caption) +\n    theme(axis.text.x = element_text(angle = 75, hjust = 1))\n\n\nThis shows that the number of crashes per month has varied similarly over the years:\n\ndf %&gt;% \n  mutate(crash_year = factor(crash_year)) %&gt;% \n  count(crash_year, crash_month) %&gt;% \n  ggplot(aes(crash_month, n)) +\n    geom_smooth(aes(group = 1)) +\n    geom_jitter(aes(color = crash_month),\n                height = 0,\n                width = .25,\n                alpha = .5,\n                show.legend = F) +\n    scale_y_continuous(label = comma) +\n    scale_color_viridis(\"Month\",\n                        discrete = TRUE) +\n    labs(title = \"Crashes per month\",\n         subtitle = my_subtitle,\n         x = \"1 dot = Month/Year. Jitter applied\",\n         y = \"Number of crashes\",\n         caption = my_caption) +\n  theme(axis.title.x = element_text(size = 12))\n\n\nThis shows that there is much greater variation between weekdays, though there is still a perceptible pattern.\n\ndf %&gt;% \n  count(crash_year, crash_month, day_of_week) -&gt; df_months_year_dow\n\ndf_months_year_dow %&gt;% \n  group_by(day_of_week) %&gt;% \n  summarize(median = median(n)) -&gt; df_dow\n\ndf_months_year_dow %&gt;% \n  ggplot(aes(day_of_week, n)) +\n  geom_jitter(aes(color = day_of_week), \n              height = 0,\n              alpha = .3,\n              show.legend = F) +\n  geom_point(data = df_dow,\n             aes(x = day_of_week,\n                 y = median,\n                 fill = day_of_week),\n             color = \"black\",\n             size = 4,\n             shape = 21,\n             show.legend = F) +\n  scale_color_viridis(discrete = TRUE) +\n  scale_fill_viridis(discrete = TRUE) +\n  scale_y_continuous(label = comma) +\n  labs(title = \"Crashes per weekday\",\n         subtitle = my_subtitle,\n         x = \"Large dot = median, small dot = Weekday/Month/Year. Jitter applied\",\n         y = \"Number of crashes\",\n         caption = my_caption) +\n  theme(axis.title.x = element_text(size = 12),\n        axis.text.x = element_text(size = 12))\n\n\nThis shows that the number of crashes increases in the fall and winter.\n\ndf %&gt;% \n  mutate(crash_month = fct_rev(crash_month)) %&gt;% \n  count(crash_year, crash_month) %&gt;% \n  ggplot(aes(crash_year, crash_month, fill = n)) +\n    geom_tile() +\n    coord_equal() +\n    scale_x_continuous(expand = c(0,0),\n                       breaks = c(2004:2017)) +\n    scale_y_discrete(expand = c(0,0)) +\n    scale_fill_viridis(\"Number of crashes\",\n                       labels = comma) +\n    labs(title = \"Crash heatmap\",\n         subtitle = my_subtitle,\n         x = NULL,\n         y = NULL,\n         caption = my_caption) +\n    theme(panel.grid = element_blank(),\n          axis.text.x = element_text(angle = 75, hjust = 1))\n\n\nThese plots show how the number of crashes changes throughout the day.\n\ndf %&gt;% \n  mutate(day_of_week = fct_rev(day_of_week)) %&gt;% \n  filter(!hour_of_day &gt; 24,\n         !is.na(day_of_week)) %&gt;% \n  count(day_of_week, hour_of_day) %&gt;% \n  ggplot(aes(hour_of_day, day_of_week, fill = n)) +\n  geom_tile() +\n  coord_equal() +\n  labs(title = \"Crash heatmap\",\n       subtitle = my_subtitle,\n       x = \"Hour of day\",\n       y = \"\",\n       caption = my_caption) +\n  scale_y_discrete(expand = c(0,0)) +\n  scale_x_continuous(expand = c(0,0)) +\n  scale_fill_viridis(labels = comma,\n                     \"Number of crashes\") +\n  theme(legend.position = \"bottom\",\n        legend.direction = \"horizontal\",\n        legend.text = element_text(size = 8, \n                                   angle = 300))\n\n\nThis shows a more granular view:\n\ndf %&gt;% \n  select(day_of_week, time_of_day) %&gt;% \n  filter(!time_of_day &gt; 2400,\n         !is.na(day_of_week)) %&gt;% \n  mutate(day_of_week = fct_rev(day_of_week),\n         hour = time_of_day %/% 100,\n         minute = time_of_day %% 100) %&gt;% \n  count(day_of_week, hour, minute) %&gt;% \n  complete(day_of_week, hour = 0:23, minute = 0:60) %&gt;% \n  replace_na(list(n = 0)) %&gt;% \n  mutate(time = make_datetime(hour = hour, min = minute),\n         time = round_date(time, unit = \"15 minutes\")) %&gt;%\n  group_by(day_of_week, time) %&gt;% \n  summarize(n = sum(n)) -&gt; df_time_rounded\n\ndf_time_rounded %&gt;% \n  ggplot(aes(time, day_of_week, fill = n)) +\n  geom_tile() +\n  scale_y_discrete(expand = c(0,0)) +\n  scale_x_datetime(date_labels = (\"%H:%M\"),\n                   expand = c(0,0)) +\n  scale_fill_viridis(\"Number of crashes\") +\n  labs(title = \"Crash heatmap\",\n       subtitle = my_subtitle,\n       x = \"Time (rounded to nearest 15 minutes)\",\n       y = \"\",\n       caption = my_caption) +\n  theme(legend.position = \"bottom\",\n        legend.direction = \"horizontal\",\n        legend.text = element_text(size = 8, angle = 300))\n\n\nThis is a different veiew of the same data. Saturday and Sunday behave differently than the weekdays.\n\ndf %&gt;%\n  select(time_of_day, day_of_week) %&gt;% \n  filter(!time_of_day &gt; 2400,\n         !is.na(day_of_week)) %&gt;% \n  mutate(hour = time_of_day %/% 100,\n         minute = time_of_day %% 100,\n         time = make_datetime(hour = hour, min = minute),\n         time = round_date(time, unit = \"15 minutes\")) %&gt;% \n  ggplot(aes(time, color = day_of_week)) +\n  geom_freqpoly(size = 2) +\n  scale_color_viridis(\"Weekday\", \n                      discrete = TRUE) +\n  scale_x_datetime(labels = date_format(\"%H:%M\")) +\n  scale_y_continuous(labels = comma) +\n  labs(title = \"Number of crashes\",\n       subtitle = my_subtitle,\n       x = \"Time (rounded to nearest 15 minutes)\",\n       y = \"Number of crashes\",\n       caption = my_caption)\n\n\nThis shows that there are more casualties (injuries and fatalities) per person involved in the crash in the early morning.\n\ndf %&gt;%\n  mutate(day_of_week = fct_rev(day_of_week)) %&gt;% \n  filter(hour_of_day &lt; 24) %&gt;% \n  group_by(day_of_week, hour_of_day) %&gt;% \n  summarize(person_sum = sum(person_count, na.rm = TRUE),\n            casualties_sum = sum(casualty_count, na.rm = TRUE),\n            casualties_per_person = casualties_sum / person_sum) %&gt;% \n  ggplot(aes(hour_of_day, day_of_week, fill = casualties_per_person)) +\n  geom_tile() +\n  coord_equal() +\n  scale_y_discrete(expand = c(0,0)) +\n  scale_x_continuous(expand = c(0,0)) +\n  scale_fill_viridis(\"Casualties per person\") +\n  labs(title = \"Casualties per person\",\n       subtitle = my_subtitle,\n       x = \"Hour\",\n       y = \"\",\n       caption = my_caption) +\n  theme(legend.direction = \"horizontal\",\n        legend.position = \"bottom\",\n        legend.text = element_text(size = 8, \n                                   angle = 300))\n\n\nThe number of injuries and fatalities follow the same general pattern, but it is less pronunced in the fatality data.\n\ndf %&gt;% \n  select(crash_year, crash_month, injury_count, fatal_count) %&gt;% \n  gather(measure, value, -c(crash_year, crash_month)) %&gt;% \n  mutate(measure = factor(measure, \n                          levels = c(\"injury_count\", \"fatal_count\"),\n                          labels = c(\"Injuries\", \"Fatalities\"))) %&gt;% \n  group_by(crash_year, crash_month, measure) %&gt;% \n  summarize(value = sum(value, na.rm = TRUE)) %&gt;% \n  ggplot(aes(crash_month, value, color = crash_month)) +\n    geom_jitter(alpha = .75,\n                height = 0,\n                width = .25,\n                show.legend = FALSE) +\n    facet_wrap(~measure,\n             ncol = 1,\n             scales = \"free\") +\n    labs(title = \"Injuries and fatalities\",\n       subtitle = my_subtitle,\n       x = \"Jitter applied\",\n       y = \"Sum\")\n\n\nThis shows the number of pedestrian fatalities by month.\n\ndf %&gt;% \n  select(crash_year, crash_month, ped_death_count) %&gt;%\n  group_by(crash_year, crash_month) %&gt;% \n  summarize(ped_death_count = sum(ped_death_count, na.rm = TRUE)) %&gt;% \n  ggplot(aes(crash_month, ped_death_count, color = crash_month)) +\n    geom_jitter(height = .15,\n                width = .25,\n                show.legend = FALSE) +\n    scale_color_viridis(\"Month\",\n                      discrete = TRUE) +\n    labs(title = \"Pedestrian fatalities\",\n       subtitle = my_subtitle,\n       x = \"One dot = Month/Year. Jitter applied\",\n       y = \"Sum\")\n\n\nThe rate of increase in the number of fatalities among belted vehicle occupants has been decreasing, while the rate among unbelted occupants has been increasing.\n\ndf %&gt;% \n  select(crash_year, crash_month, belted_death_count, unb_death_count) %&gt;% \n  arrange(crash_year, crash_month) %&gt;%\n  mutate(time_period = make_date(year = crash_year, month = crash_month)) %&gt;%\n  group_by(time_period, crash_year, crash_month) %&gt;% \n  summarize(belted_death_count = sum(belted_death_count),\n            unb_death_count = sum(unb_death_count)) %&gt;% \n  gather(death_type, death_count, -c(time_period, crash_year, crash_month)) %&gt;% \n  arrange(death_type, time_period) %&gt;% \n  group_by(death_type) %&gt;% \n  mutate(death_count_cum = cumsum(death_count)) %&gt;% \n  ungroup() %&gt;% \n  mutate(death_type = factor(death_type,\n                             levels = c(\"belted_death_count\", \"unb_death_count\"),\n                             labels = c(\"Belted deaths\", \"Unbelted deaths\"))) -&gt; df_belted_unbelted\n\ndf_belted_unbelted %&gt;% \n  ggplot(aes(time_period, death_count_cum, color = death_type, group = death_type)) +\n  geom_line(size = 2) +\n  scale_color_viridis(\"\", discrete = TRUE) +\n  scale_y_continuous(label = comma) +\n  labs(title = \"Car occupant fatalities\",\n       subtitle = my_subtitle,\n       x = \"\",\n       y = \"Cumulative sum of deaths\",\n       caption = my_caption) +\n  theme(panel.grid = element_blank())"
  },
  {
    "objectID": "posts/map-census-data-with-r/index.html",
    "href": "posts/map-census-data-with-r/index.html",
    "title": "Map Census Data With R",
    "section": "",
    "text": "This talk was presented on May 30th, 2019 at Code For Pittsburgh.\nBefore we dive in, this presentation assumes that the user has basic familiarity with tidyverse, mainly dplyr. Knowing how to use %&gt;% will be very helpful.\nHow to install packages:\ninstall.packages(\"package_name\")\nGet your census API key: https://api.census.gov/data/key_signup.html\nConfigure environment:\nlibrary(tidycensus)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tigris)\nlibrary(ggmap)\nlibrary(janitor)\n\ntheme_set(theme_bw())\n\noptions(tigris_use_cache = TRUE,\n        scipen = 4,\n        digits = 3)"
  },
  {
    "objectID": "posts/map-census-data-with-r/index.html#packages",
    "href": "posts/map-census-data-with-r/index.html#packages",
    "title": "Map Census Data With R",
    "section": "Packages",
    "text": "Packages\n\n{tidycensus}\ntidycensus gives access to the Census API and makes it easy to plot data on a map.\nData\n\nDemographics\n\nDecennial Census\nAmerican Community Survey (ACS)\nerror estimates\n\nGeometries\n\ncountry\ncounty\nzip code\nblocks\ntracts\nand more\n\n\n\n\n{sf}\nsimple features makes it easy to work with polygon data in R. It uses the familiar tidyverse framework: everything is a tibble, and it uses %&gt;%.\nggplot2::geom_sf() makes it easy to plot sf polygons.\nsf can also do spatial calculations such as st_contains, st_intersects, and st_boundary.\n\n\n{ggmap}\nUses Google Maps API to get basemaps. The API now requires a credit card, but it has a fairly generous “free” tier."
  },
  {
    "objectID": "posts/map-census-data-with-r/index.html#using-tidycensus",
    "href": "posts/map-census-data-with-r/index.html#using-tidycensus",
    "title": "Map Census Data With R",
    "section": "Using {tidycensus}",
    "text": "Using {tidycensus}\n\ncensus_api_key(\"your_key_here\")\n\nThis loads the variables from the Decennial Census in 2010:\n\nvariables_dec &lt;- load_variables(year = 2010, dataset = \"sf1\", cache = TRUE)\n\n\n\n# A tibble: 8,959 × 3\n   name    label                                concept         \n   &lt;chr&gt;   &lt;chr&gt;                                &lt;chr&gt;           \n 1 H001001 Total                                HOUSING UNITS   \n 2 H002001 Total                                URBAN AND RURAL \n 3 H002002 Total!!Urban                         URBAN AND RURAL \n 4 H002003 Total!!Urban!!Inside urbanized areas URBAN AND RURAL \n 5 H002004 Total!!Urban!!Inside urban clusters  URBAN AND RURAL \n 6 H002005 Total!!Rural                         URBAN AND RURAL \n 7 H002006 Total!!Not defined for this file     URBAN AND RURAL \n 8 H003001 Total                                OCCUPANCY STATUS\n 9 H003002 Total!!Occupied                      OCCUPANCY STATUS\n10 H003003 Total!!Vacant                        OCCUPANCY STATUS\n# ℹ 8,949 more rows\n\n\nThis loads the ACS variables for 2017:\n\nvariables_acs &lt;- load_variables(year = 2017, dataset = \"acs5\", cache = TRUE)\n\n\n\n# A tibble: 25,071 × 4\n   name        label                                  concept          geography\n   &lt;chr&gt;       &lt;chr&gt;                                  &lt;chr&gt;            &lt;chr&gt;    \n 1 B00001_001  Estimate!!Total                        UNWEIGHTED SAMP… block gr…\n 2 B00002_001  Estimate!!Total                        UNWEIGHTED SAMP… block gr…\n 3 B01001A_001 Estimate!!Total                        SEX BY AGE (WHI… tract    \n 4 B01001A_002 Estimate!!Total!!Male                  SEX BY AGE (WHI… tract    \n 5 B01001A_003 Estimate!!Total!!Male!!Under 5 years   SEX BY AGE (WHI… tract    \n 6 B01001A_004 Estimate!!Total!!Male!!5 to 9 years    SEX BY AGE (WHI… tract    \n 7 B01001A_005 Estimate!!Total!!Male!!10 to 14 years  SEX BY AGE (WHI… tract    \n 8 B01001A_006 Estimate!!Total!!Male!!15 to 17 years  SEX BY AGE (WHI… tract    \n 9 B01001A_007 Estimate!!Total!!Male!!18 and 19 years SEX BY AGE (WHI… tract    \n10 B01001A_008 Estimate!!Total!!Male!!20 to 24 years  SEX BY AGE (WHI… tract    \n# ℹ 25,061 more rows\n\n\n\nMap total population in the U.S.\nUse View() to browse the variables\n\nvariables_dec %&gt;% \n  filter(str_detect(concept, \"POPULATION\")) %&gt;% \n  View()\n\nP001001 has the data we are looking for.\nQuery the total population of the continental U.S. states:\n\nstates &lt;- get_decennial(geography = \"state\",\n                        variables = c(total_pop = \"P001001\"),\n                        geometry = TRUE,\n                        output = \"wide\",\n                        year = 2010)\n\nThe states tibble contains the census data and the polygons for the geometries.\n\n\nSimple feature collection with 52 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -179 ymin: 17.9 xmax: 180 ymax: 71.4\nGeodetic CRS:  NAD83\n# A tibble: 52 × 4\n   GEOID NAME           total_pop                                       geometry\n   &lt;chr&gt; &lt;chr&gt;              &lt;dbl&gt;                             &lt;MULTIPOLYGON [°]&gt;\n 1 23    Maine            1328361 (((-67.6 44.5, -67.6 44.5, -67.6 44.5, -67.6 …\n 2 25    Massachusetts    6547629 (((-70.8 41.6, -70.8 41.6, -70.8 41.6, -70.8 …\n 3 26    Michigan         9883640 (((-88.7 48.1, -88.7 48.1, -88.7 48.1, -88.7 …\n 4 30    Montana           989415 (((-104 45, -104 45, -104 45, -104 45, -105 4…\n 5 32    Nevada           2700551 (((-114 37, -114 37, -114 36.8, -114 36.7, -1…\n 6 34    New Jersey       8791894 (((-75.5 39.7, -75.5 39.7, -75.5 39.7, -75.5 …\n 7 36    New York        19378102 (((-71.9 41.3, -71.9 41.3, -71.9 41.3, -71.9 …\n 8 37    North Carolina   9535483 (((-82.6 36, -82.6 36, -82.6 36, -82.6 36, -8…\n 9 39    Ohio            11536504 (((-82.8 41.7, -82.8 41.7, -82.8 41.7, -82.8 …\n10 42    Pennsylvania    12702379 (((-75.4 39.8, -75.4 39.8, -75.5 39.8, -75.5 …\n# ℹ 42 more rows\n\n\nMake a bar graph with the data:\n\nstates %&gt;% \n  mutate(NAME = fct_reorder(NAME, total_pop)) %&gt;% \n  ggplot(aes(NAME, total_pop)) +\n    geom_col() +\n    coord_flip()\n\n\n\n\n\n\n\n\nPlot the same data on a map:\n\nstates %&gt;% \n  filter(NAME != \"Alaska\",\n         NAME != \"Hawaii\",\n         !str_detect(NAME, \"Puerto\")) %&gt;% \n  ggplot(aes(fill = total_pop)) +\n    geom_sf() +\n    scale_fill_viridis_c(\"Total Population\")\n\n\n\n\n\n\n\n\nPull the total population of each county in PA and plot it:\n\npennsylvania &lt;- get_decennial(geography = \"county\",\n                              variables = c(total_pop = \"P001001\"),\n                              state = \"PA\",\n                              geometry = TRUE,\n                              output = \"wide\",\n                              year = 2010)\npennsylvania %&gt;% \n  ggplot(aes(fill = total_pop)) +\n    geom_sf() +\n    scale_fill_viridis_c()\n\n\n\n\n\n\n\n\nggplot2 intelligently handles cases when we don’t have data for a certain polygon:\n\npennsylvania %&gt;% \n  mutate(total_pop = case_when(NAME == \"Allegheny County, Pennsylvania\" ~ NA_real_,\n                               NAME != \"Allegheny County, Pennsylvania\" ~ total_pop)) %&gt;% \n  ggplot(aes(fill = total_pop)) +\n    geom_sf() +\n    scale_fill_viridis_c()\n\n\n\n\n\n\n\n\nWe can stack multiple polygons in the same graph to highlight Allegheny County:\n\nallegheny &lt;- pennsylvania %&gt;% \n  filter(str_detect(NAME, \"Allegheny\"))\n\npennsylvania %&gt;% \n  ggplot() +\n    geom_sf(aes(fill = total_pop)) +\n    geom_sf(data = allegheny, color = \"white\", linetype = 2, size = 1, alpha = 0) +\n    scale_fill_viridis_c()\n\n\n\n\n\n\n\n\nWe can also use tidycensus to download demographic data for census tracts.\nSet the variables we want to use first:\n\nracevars &lt;- c(White = \"P005003\", \n              Black = \"P005004\", \n              Asian = \"P005006\", \n              Hispanic = \"P004003\")\n#note that this data is long, not wide\nallegheny_tracts &lt;- get_decennial(geography = \"tract\", \n                                  variables = racevars, \n                                  state = \"PA\", \n                                  county = \"Allegheny County\", \n                                  geometry = TRUE,\n                                  summary_var = \"P001001\",\n                                  year = 2010) \n\nCalculate as a percentage of tract population:\n\nallegheny_tracts &lt;- allegheny_tracts %&gt;% \n  mutate(pct = 100 * value / summary_value)\n\nFacet by variable and map the data:\n\nallegheny_tracts %&gt;% \n  ggplot(aes(fill = pct)) +\n    geom_sf(color = NA) +\n    facet_wrap(~variable) +\n    scale_fill_viridis_c()\n\n\n\n\n\n\n\n\nWe can overlay the boundaries of Pittsburgh over the same graph.\nDownload the boundary shapefile and use sf::st_read to read it into R:\n\ncity_pgh &lt;- st_read(\"post_data/Pittsburgh_City_Boundary-shp/City_Boundary.shp\")\n\nReading layer `City_Boundary' from data source \n  `/Users/conorotompkins/Documents/github_repos/ctompkins_quarto_blog/posts/map-census-data-with-r/post_data/Pittsburgh_City_Boundary-shp/City_Boundary.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 8 features and 8 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 1320000 ymin: 382000 xmax: 1380000 ymax: 433000\nProjected CRS: NAD83 / Pennsylvania South (ftUS)\n\nallegheny_tracts %&gt;% \n  ggplot() +\n    geom_sf(aes(fill = pct), color = NA) +\n    geom_sf(data = city_pgh, color = \"white\", linetype = 2, size = 1, alpha = 0) +\n    facet_wrap(~variable) + \n    scale_fill_viridis_c()\n\n\n\n\n\n\n\n\n\n\nWorking with other data\n\nWPRDC 311 data and city wards\nWe can also download the shapefile for the City of Pittsburgh wards. The 311 dataset is tagged with the ward the request originated from, so we can use that to aggregate and map the total number of 311 requests per ward.\n\ndf_311 &lt;- read_csv(\"https://data.wprdc.org/datastore/dump/76fda9d0-69be-4dd5-8108-0de7907fc5a4\") %&gt;% \n  clean_names()\n\n\n\n\n\n\nrequest_id\ncreated_on\nrequest_type\nrequest_origin\nstatus\ndepartment\nneighborhood\ncouncil_district\nward\n\n\n\n\n203364\n2017-12-15 14:53:00\nStreet Obstruction/Closure\nCall Center\n1\nDOMI - Permits\nCentral Northside\n1\n22\n\n\n200800\n2017-11-29 09:54:00\nGraffiti\nControl Panel\n1\nPolice - Zones 1-6\nSouth Side Flats\n3\n16\n\n\n201310\n2017-12-01 13:23:00\nLitter\nCall Center\n1\nDPW - Street Maintenance\nTroy Hill\n1\n24\n\n\n200171\n2017-11-22 14:54:00\nWater Main Break\nCall Center\n1\nPittsburgh Water and Sewer Authority\nBanksville\n2\n20\n\n\n193043\n2017-10-12 12:46:00\nGuide Rail\nCall Center\n1\nDPW - Construction Division\nEast Hills\n9\n13\n\n\n196521\n2017-10-31 15:17:00\nGuide Rail\nCall Center\n1\nDPW - Construction Division\nEast Hills\n9\n13\n\n\n193206\n2017-10-13 09:18:00\nSidewalk/Curb/HC Ramp Maintenance\nCall Center\n1\nDOMI - Permits\nMount Washington\n4\n19\n\n\n195917\n2017-10-27 10:23:00\nManhole Cover\nCall Center\n1\nDOMI - Permits\nBluff\n6\n1\n\n\n179176\n2017-08-14 14:00:00\nNeighborhood Issues\nControl Panel\n0\nNA\nMiddle Hill\n6\n5\n\n\n190422\n2017-09-29 11:46:00\nMayor's Office\nWebsite\n1\n311\nNorth Oakland\n8\n4\n\n\n\n\n\n\n\n\n\nSimple feature collection with 32 features and 1 field\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 1320000 ymin: 382000 xmax: 1380000 ymax: 433000\nProjected CRS: NAD83 / Pennsylvania South (ftUS)\n# A tibble: 32 × 2\n    ward                                                                geometry\n   &lt;int&gt;                                              &lt;POLYGON [US_survey_foot]&gt;\n 1     1 ((1344377 410658, 1344401 410655, 1344473 410650, 1344497 410648, 1344…\n 2     2 ((1349657 415566, 1349615 415539, 1349572 415508, 1349487 415433, 1349…\n 3     3 ((1348490 410322, 1348432 410326, 1348333 410334, 1348260 410339, 1348…\n 4     4 ((1357003 413341, 1357009 413322, 1357024 413281, 1357058 413173, 1357…\n 5     5 ((1354794 418150, 1354861 418062, 1354918 417978, 1354931 417956, 1355…\n 6     6 ((1354713 418633, 1354629 418544, 1354609 418522, 1354575 418487, 1354…\n 7     7 ((1364905 417408, 1364974 417343, 1365180 417147, 1365249 417082, 1365…\n 8     8 ((1357423 420022, 1357491 420008, 1357693 419965, 1357760 419952, 1357…\n 9     9 ((1357423 420022, 1357369 420033, 1357314 420045, 1357208 420069, 1357…\n10    10 ((1365337 428177, 1365357 428160, 1365370 428141, 1365383 428126, 1365…\n# ℹ 22 more rows\n\n\nPlot the ward polygons:\n\nwards %&gt;% \n  ggplot() +\n  geom_sf()\n\n\n\n\n\n\n\n\nCalculate the center of each ward. We will use this to label the wards on the map:\n\nward_labels &lt;- wards %&gt;% \n  st_centroid() %&gt;% \n  st_coordinates() %&gt;%\n  as_tibble() %&gt;%\n  clean_names() %&gt;%\n  mutate(ward = wards$ward)\n\nward_labels_transformed &lt;- wards %&gt;% \n  st_transform(4326) |&gt; \n  st_centroid() %&gt;% \n  st_coordinates() %&gt;%\n  as_tibble() %&gt;%\n  clean_names() %&gt;%\n  mutate(ward = wards$ward)\n\n\n\n\n\n\nx\ny\nward\n\n\n\n\n1343990\n410154\n1\n\n\n1345190\n413807\n2\n\n\n1346380\n411764\n3\n\n\n1353781\n410344\n4\n\n\n1351582\n414257\n5\n\n\n\n\n\n\n\nCount the number of requests per ward:\n\ndf_311_count &lt;- df_311 %&gt;% \n  count(ward, sort = TRUE)\n\nUse left_join to join the count data with the coordinates:\n\nward_311 &lt;- wards %&gt;% \n  left_join(df_311_count) %&gt;%\n  mutate(ward_label = ward_labels$ward)\n\n\n\nSimple feature collection with 32 features and 3 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 1320000 ymin: 382000 xmax: 1380000 ymax: 433000\nProjected CRS: NAD83 / Pennsylvania South (ftUS)\n# A tibble: 32 × 4\n    ward                                               geometry     n ward_label\n * &lt;dbl&gt;                             &lt;POLYGON [US_survey_foot]&gt; &lt;int&gt;      &lt;int&gt;\n 1     1 ((1344377 410658, 1344401 410655, 1344473 410650, 134…  3821          1\n 2     2 ((1349657 415566, 1349615 415539, 1349572 415508, 134…  7188          2\n 3     3 ((1348490 410322, 1348432 410326, 1348333 410334, 134…  2327          3\n 4     4 ((1357003 413341, 1357009 413322, 1357024 413281, 135… 12187          4\n 5     5 ((1354794 418150, 1354861 418062, 1354918 417978, 135…  8514          5\n 6     6 ((1354713 418633, 1354629 418544, 1354609 418522, 135…  6672          6\n 7     7 ((1364905 417408, 1364974 417343, 1365180 417147, 136…  6896          7\n 8     8 ((1357423 420022, 1357491 420008, 1357693 419965, 135…  9128          8\n 9     9 ((1357423 420022, 1357369 420033, 1357314 420045, 135…  8321          9\n10    10 ((1365337 428177, 1365357 428160, 1365370 428141, 136… 14032         10\n# ℹ 22 more rows\n\n\nPlot the data:\n\nward_311 %&gt;% \n  ggplot() +\n    geom_sf(aes(fill = n), color = NA) +\n    geom_label(data = ward_labels, aes(x, y, label = ward), size = 3, inherit.aes = FALSE) +\n    scale_fill_viridis_c(\"Number of 311 requests\")\n\n\n\n\n\n\n\n\n\n\nWPRDC overdose data\nWe can use the census data to adjust other data for per capita rates. For example, the WPRDC’s overdose data has the zip code that the overdose occurred in.\nFirst, download the overdose dataset and pull the population data for each zip code:\n\ndf_overdose &lt;- read_csv(\"https://data.wprdc.org/datastore/dump/1c59b26a-1684-4bfb-92f7-205b947530cf\") %&gt;% \n  clean_names() %&gt;% \n  mutate(incident_zip = str_sub(incident_zip, 1, 5))\n\n\nall_zips &lt;- get_acs(geography = \"zip code tabulation area\",\n                    variables = c(total_pop = \"B01003_001\"),\n                    geometry = TRUE,\n                    output = \"wide\",\n                    year = 2018)\n\n\ndf_overdose &lt;- read_csv(\"post_data/1c59b26a-1684-4bfb-92f7-205b947530cf.csv\") %&gt;% \n  clean_names() %&gt;% \n  mutate(incident_zip = str_sub(incident_zip, 1, 5)) |&gt; \n  filter(death_date_and_time &lt;= \"2019-06-01\")\n\nThen, aggregate the overdose data to the zip code and join the datasets:\n\ndf_overdose &lt;- df_overdose %&gt;% \n  count(incident_zip, sort = TRUE)\n\nattempt1 &lt;- all_zips %&gt;%\n  semi_join(df_overdose, by = c(\"GEOID\" = \"incident_zip\")) %&gt;% \n  left_join(df_overdose, by = c(\"GEOID\" = \"incident_zip\"))\n\nattempt1 %&gt;% \n  ggplot() +\n    geom_sf()\n\n\n\n\n\n\n\n\nUnfortunately the data is kind of messy and includes zip codes that aren’t in Allegheny County.\nWe can use st_intersection to exclude all of the zip code polygons that do not fall within the allegheny county tibble we made earlier:\n\nallegheny %&gt;% \n  ggplot() +\n    geom_sf()\n\n\n\n\n\n\n\n\nThen, join the aggregated overdose data with left_join:\n\ndf_allegheny_overdose &lt;- st_intersection(allegheny, all_zips) %&gt;% \n  left_join(df_overdose, by = c(\"GEOID.1\" = \"incident_zip\"))\n\nNow we can calculate the per 1,000 overdose rate and plot the data:\n\ndf_allegheny_overdose %&gt;% \n  filter(total_popE &gt;= 400) %&gt;% \n  mutate(overdoses_per_capita = n / total_popE * 1000) %&gt;% \n  ggplot(aes(fill = overdoses_per_capita)) +\n    geom_sf(color = NA) +\n    scale_fill_viridis_c()\n\n\n\n\n\n\n\n\n\n\n\n{ggmap} basemaps\nWe can use ggmap to request a basemap from the Google Maps API. Get your API key here\n\nregister_google(key = \"Your key here\")\n\n\npgh_map &lt;- get_map(location = c(lat = 40.445315, lon = -79.977104), zoom = 12)\n\nggmap(pgh_map)\n\n\n\n\n\n\n\n\nThere are multiple basemap styles available:\n\nget_map(location = c(lat = 40.445315, lon = -79.977104), zoom = 12, maptype = \"satellite\", source = \"google\") %&gt;% \n  ggmap()\n\n\n\n\n\n\n\n\n\nget_map(location = c(lat = 40.445315, lon = -79.977104), zoom = 12, maptype = \"roadmap\", source = \"google\") %&gt;% \n  ggmap()\n\n\n\n\n\n\n\n\nCombining maps from different systems requires us to use the same map projection. Google uses 4326. Use coord_sf to set the projection:\n\nward_labels_sf &lt;- ward_labels\n\nggmap(pgh_map) +\n  geom_sf(data = ward_311, aes(fill = n), inherit.aes = FALSE, color = NA, alpha = .7) +\n  geom_label(data = ward_labels_transformed, aes(x, y, label = ward), size = 3) +\n  coord_sf(crs = st_crs(4326)) +\n  scale_fill_viridis_c(\"Number of 311 requests\")"
  },
  {
    "objectID": "posts/map-census-data-with-r/index.html#links",
    "href": "posts/map-census-data-with-r/index.html#links",
    "title": "Map Census Data With R",
    "section": "Links",
    "text": "Links\n\nhttps://walkerke.github.io/tidycensus/articles/basic-usage.html\nhttps://walkerke.github.io/tidycensus/reference/get_acs.html\nhttps://walkerke.github.io/tidycensus/articles/spatial-data.html\nhttps://walkerke.github.io/tidycensus/articles/other-datasets.html\nhttps://cengel.github.io/R-spatial/mapping.html\nhttps://www.r-spatial.org/r/2018/10/25/ggplot2-sf.html\nhttps://www.r-spatial.org/r/2018/10/25/ggplot2-sf-2.html\nhttps://www.r-spatial.org/r/2018/10/25/ggplot2-sf-3.html\ngoogle maps API key: https://cloud.google.com/maps-platform/\nhttps://lucidmanager.org/geocoding-with-ggmap/\nhttps://github.com/rstudio/cheatsheets/blob/master/sf.pdf"
  },
  {
    "objectID": "posts/forecasting-healthy-ride-ridership-with-prophet/index.html",
    "href": "posts/forecasting-healthy-ride-ridership-with-prophet/index.html",
    "title": "Forecasting Healthy Ride Ridership With Prophet",
    "section": "",
    "text": "This post is about predicting demand for the Healthy Ride bike system in Pittsburgh. I wanted to try out Facebook’s prophet package and try to do some time series forecasting.\nAs usual, load the required packages and set up the environment:\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(prophet)\nlibrary(janitor)\nlibrary(hrbrthemes)\n\noptions(scipen = 999)\n\ntheme_set(theme_bw())\ndf &lt;- read_csv(\"post_data/combined_ride_data.csv\") %&gt;%\n  filter(date &lt; \"2019-08-01\") %&gt;% \n  count(date)\nThis code loads the data and formats the date column so the prophet package can interface with it. I use dir() to find all the CSV files in the folder and then set_names() and map_df(read_csv()) to read each of the CSV files into memory.\nfiles &lt;- dir(\"data\", \".csv\")\n\ndata &lt;- str_c(\"data/\", files) %&gt;% \n  set_names() %&gt;% \n  map_df(read_csv) %&gt;% \n  clean_names()\n\ndf &lt;- data %&gt;% \n  filter(date &lt; \"2019-08-01\") %&gt;% \n  count(date)\n\n# df %&gt;% \n#   ggplot(aes(date, n)) +\n#   geom_point()\n# \n# last(df$date)\nThe data I will use contains the number of rides per day and also includes the month and year. prophet will identify the time series patterns (“seasonality”) in the data and identify the “true” pattern\nprophet has a plug-and-play workflow that is easy to use, but it has more stringent requirements for how the data has to be shaped. The date data has to be named ds and the target variable has to be named y. I set the floor to zero because there cannot be fewer than 0 rides in a day. prophet requires a cap\nmax_rides &lt;- df %&gt;% \n  summarize(max_rides = max(n) * 3) %&gt;% \n  pull()\n\ndf &lt;- df %&gt;% \n  mutate(n = log(n),\n         cap = log(max_rides)) %&gt;% \n  filter(!is.na(date)) %&gt;% \n  rename(ds = date,\n         y = n) %&gt;% \n  mutate(floor = 0)\n\n\n\ndf %&gt;% \n  filter(is.na(ds))\n\n# A tibble: 0 × 4\n# ℹ 4 variables: ds &lt;date&gt;, y &lt;dbl&gt;, cap &lt;dbl&gt;, floor &lt;dbl&gt;\nglimpse(df)\n\nRows: 1,518\nColumns: 4\n$ ds    &lt;date&gt; 2015-05-31, 2015-06-01, 2015-06-02, 2015-06-03, 2015-06-04, 201…\n$ y     &lt;dbl&gt; 6.173786, 4.836282, 4.934474, 4.875197, 5.361292, 5.613128, 5.94…\n$ cap   &lt;dbl&gt; 8.013343, 8.013343, 8.013343, 8.013343, 8.013343, 8.013343, 8.01…\n$ floor &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\nThis creates the set of holidays I use in the model.\nus_holidays &lt;- prophet::generated_holidays %&gt;% \n  as_tibble() %&gt;% \n  filter(country == \"US\") %&gt;% \n  mutate(ds = as.Date(ds))\nThis code fits a model to the dataset.\nm &lt;- prophet(df, growth = 'logistic', holidays = us_holidays)\nmake_future_dataframe() creates the dataframe that prophet uses to make its forecast. In this case, I have it create a dataframe with 365 days of additional rows to predict onto.\nfuture &lt;- make_future_dataframe(m, periods = 365, freq = \"day\") %&gt;% \n  mutate(floor = 0,\n         cap = unique(df$cap))\nThis code performs the forecast on the future dataset.\nforecast &lt;- predict(m, future) %&gt;% \n  as_tibble()\nThe output is a dataframe with the date, the predicted ridership, and the upper and lower bounds of the prediction.\nforecast\n\n# A tibble: 1,883 × 66\n   ds                  trend   cap floor additive_terms additive_terms_lower\n   &lt;dttm&gt;              &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;          &lt;dbl&gt;                &lt;dbl&gt;\n 1 2015-05-31 00:00:00  5.13  8.01     0          0.720                0.720\n 2 2015-06-01 00:00:00  5.13  8.01     0          0.658                0.658\n 3 2015-06-02 00:00:00  5.13  8.01     0          0.724                0.724\n 4 2015-06-03 00:00:00  5.13  8.01     0          0.766                0.766\n 5 2015-06-04 00:00:00  5.13  8.01     0          0.733                0.733\n 6 2015-06-05 00:00:00  5.13  8.01     0          0.784                0.784\n 7 2015-06-06 00:00:00  5.13  8.01     0          0.844                0.844\n 8 2015-06-07 00:00:00  5.13  8.01     0          0.754                0.754\n 9 2015-06-08 00:00:00  5.13  8.01     0          0.691                0.691\n10 2015-06-09 00:00:00  5.13  8.01     0          0.755                0.755\n# ℹ 1,873 more rows\n# ℹ 60 more variables: additive_terms_upper &lt;dbl&gt;, `Christmas Day` &lt;dbl&gt;,\n#   `Christmas Day_lower` &lt;dbl&gt;, `Christmas Day_upper` &lt;dbl&gt;,\n#   `Christmas Day (Observed)` &lt;dbl&gt;, `Christmas Day (Observed)_lower` &lt;dbl&gt;,\n#   `Christmas Day (Observed)_upper` &lt;dbl&gt;, `Columbus Day` &lt;dbl&gt;,\n#   `Columbus Day_lower` &lt;dbl&gt;, `Columbus Day_upper` &lt;dbl&gt;, holidays &lt;dbl&gt;,\n#   holidays_lower &lt;dbl&gt;, holidays_upper &lt;dbl&gt;, `Independence Day` &lt;dbl&gt;, …\nplot automatically plots the forecast data:\nplot(m, forecast)\nprophet also decomposes the various seasonal effects.\nprophet_plot_components(m, forecast)\nWe can of course use ggplot to manually plot the data.\ndf_aug &lt;- forecast %&gt;% \n  mutate(ds = ymd(ds)) %&gt;% \n  left_join(df) %&gt;% \n  mutate(yhat = exp(yhat),\n         yhat_lower = exp(yhat_lower),\n         yhat_upper = exp(yhat_upper),\n         y = exp(y))\n\ndf_aug %&gt;% \n  ggplot(aes(x = ds)) +\n    geom_ribbon(data = df_aug %&gt;% filter(ds &gt; last(df$ds)), \n                aes(ymin = yhat_lower, ymax = yhat_upper), alpha = .2, fill = \"blue\") +\n    geom_line(data = df_aug %&gt;% filter(ds &gt; last(df$ds)), \n              aes(y = yhat), color = \"blue\") +\n    geom_point(aes(y = y), alpha = .5) +\n    geom_hline(aes(yintercept = unique(floor)), linetype = 2) +\n    labs(x = NULL,\n         y = \"Number of rides\") +\n    scale_y_comma() +\n  theme_bw(base_size = 20)\nprophet also provides functions for cross-validation.\ndf_cv &lt;- cross_validation(m, horizon = 30, units = 'days')\n\nperformance_metrics(df_cv) %&gt;% \n  as_tibble() %&gt;% \n  gather(metric, measure, -horizon) %&gt;% \n  ggplot(aes(horizon, measure)) +\n  geom_line() +\n  facet_wrap(~metric, scales = \"free_y\",\n             ncol = 1) +\n  labs(x = \"Horizon\",\n       y = \"Measure\") +\n  theme_bw()\nWe can also inspect the impact of holidays on the prediction.\ndf_holiday_impact &lt;- forecast %&gt;% \n  clean_names() %&gt;% \n  select(ds, christmas_day:washingtons_birthday) %&gt;% \n  select(-contains(\"upper\"), -contains(\"lower\")) %&gt;% \n  pivot_longer(-ds, names_to = \"holiday\", values_to = \"value\") %&gt;% \n  mutate(year = as.factor(year(ds))) %&gt;% \n  filter(holiday != \"holidays\",\n         value != 0)\n\ndf_holiday_impact %&gt;% \n  arrange(ds) %&gt;% \n  mutate(holiday = as.factor(holiday)) %&gt;% \n  ggplot(aes(holiday, value, color = year)) +\n  geom_hline(yintercept = 0, linetype = 2) +\n  geom_jitter() +\n  coord_flip() +\n  labs(x = \"Holiday\",\n       y = \"Impact\") +\n  scale_color_discrete(\"Year\") +\n  theme_ipsum()"
  },
  {
    "objectID": "posts/forecasting-healthy-ride-ridership-with-prophet/index.html#documentation-and-references",
    "href": "posts/forecasting-healthy-ride-ridership-with-prophet/index.html#documentation-and-references",
    "title": "Forecasting Healthy Ride Ridership With Prophet",
    "section": "Documentation and references",
    "text": "Documentation and references\n\nhttps://facebook.github.io/prophet/\nhttps://data.wprdc.org/dataset/healthyride-trip-data"
  },
  {
    "objectID": "posts/house_price_estimator/index.html",
    "href": "posts/house_price_estimator/index.html",
    "title": "House Price Estimator Dashboard",
    "section": "",
    "text": "Click here to view the full dashboard"
  },
  {
    "objectID": "posts/house_price_estimator/index.html#training-set-metrics-75-of-total-observations",
    "href": "posts/house_price_estimator/index.html#training-set-metrics-75-of-total-observations",
    "title": "House Price Estimator Dashboard",
    "section": "Training set metrics (75% of total observations)",
    "text": "Training set metrics (75% of total observations)\nI used 10-fold cross-validation to assess model performance against the training set:\n\ntrain_metrics %&gt;% \n  select(model_name, id, .metric, .estimate) %&gt;% \n  pivot_wider(names_from = .metric, values_from = .estimate) %&gt;% \n  ggplot(aes(rmse, rsq, color = model_name)) +\n  geom_point() +\n  scale_x_continuous(label = dollar) +\n  labs(x = \"Root Mean Squared Error\",\n       y = \"R^2\")"
  },
  {
    "objectID": "posts/house_price_estimator/index.html#test-set-metrics-25-of-total-observations",
    "href": "posts/house_price_estimator/index.html#test-set-metrics-25-of-total-observations",
    "title": "House Price Estimator Dashboard",
    "section": "Test set metrics (25% of total observations)",
    "text": "Test set metrics (25% of total observations)\n\ntest_metrics %&gt;% \n  select(.metric, .estimate)\n\n# A tibble: 3 × 2\n  .metric   .estimate\n  &lt;chr&gt;         &lt;dbl&gt;\n1 rmse     251406.   \n2 rsq           0.607\n3 mape    3724488.   \n\n\n\nmodel_results %&gt;% \n  ggplot(aes(.resid)) +\n  geom_density() +\n  geom_vline(xintercept = 0, lty = 2) +\n  scale_x_continuous(label = label_dollar())\n\n\n\n\n\n\n\n\n\nmodel_results %&gt;% \n  ggplot(aes(sale_price_adj, .pred_dollar)) +\n  geom_density_2d_filled(contour_var = \"count\") +\n  scale_x_log10(label = label_dollar()) +\n  scale_y_log10(label = label_dollar()) +\n  guides(fill = guide_coloursteps()) +\n  labs(x = \"Inflation-adjusted sale price log10 scale\",\n       y = \"Prediction\",\n       fill = \"Sales\")\n\n\n\n\n\n\n\n\nThe model becomes less effective as the actual sale price increases.\n\nmodel_results %&gt;% \n  ggplot(aes(sale_price_adj, .resid)) +\n  geom_point(alpha = .01) +\n  scale_x_log10(label = dollar) +\n  scale_y_continuous(label = dollar) +\n  labs(x = \"Inflation-adjusted sale price log10 scale\",\n       y = \"Residual\")\n\n\n\n\n\n\n\n\n\ngeo_ids &lt;- st_read(\"post_data/unified_geo_ids/unified_geo_ids.shp\",\n                   quiet = T)\n\ngeo_id_median_resid &lt;- model_results %&gt;% \n  group_by(geo_id) %&gt;% \n  summarize(median_resid = median(.resid))\n\npal &lt;- colorNumeric(\n  palette = \"viridis\",\n  domain = geo_id_median_resid$median_resid)\n\ngeo_ids %&gt;% \n  left_join(geo_id_median_resid) %&gt;% \n  leaflet() %&gt;% \n  addProviderTiles(providers$OpenStreetMap.Mapnik,\n                   options = providerTileOptions(noWrap = TRUE,\n                                                 minZoom = 9\n                                                 #maxZoom = 8\n                   )) %&gt;%\n  addPolygons(popup = ~ str_c(geo_id, \" \", \"median residual: \", round(median_resid, 2), sep = \"\"),\n              fillColor = ~pal(median_resid),\n              fillOpacity = .7,\n              color = \"black\",\n              weight = 3) %&gt;% \n  addLegend(\"bottomright\", pal = pal, values = ~median_resid,\n            title = \"Median of residual\",\n            opacity = 1)\n\nJoining with `by = join_by(geo_id)`\n\n\n\n\n\n\n\nmodel_results %&gt;% \n  add_count(geo_id) %&gt;% \n  mutate(geo_id = fct_reorder(geo_id, .resid, .fun = median)) %&gt;% \n  ggplot(aes(.resid, geo_id, fill = n)) +\n  geom_boxplot(color = \"grey\",\n               outlier.alpha = 0) +\n  geom_vline(xintercept = 0, lty = 2, color = \"red\") +\n  scale_fill_viridis_c() +\n  coord_cartesian(xlim = c(-10^5, 10^5)) +\n  labs(fill = \"Sales\")\n\n\n\n\n\n\n\n\n\nmodel_results %&gt;% \n  add_count(style_desc) %&gt;% \n  mutate(style_desc = fct_reorder(style_desc, .resid, .fun = median)) %&gt;% \n  ggplot(aes(.resid, style_desc, fill = n)) +\n  geom_boxplot(color = \"grey\",\n               outlier.alpha = 0) +\n  geom_vline(xintercept = 0, lty = 2, color = \"red\") +\n  coord_cartesian(xlim = c(-10.5^5, 10.5^5)) +\n  scale_x_continuous(labels = label_dollar()) +\n  scale_fill_viridis_c() +\n  labs(fill = \"Sales\",\n       x = \"Residual\",\n       y = \"House style\")\n\n\n\n\n\n\n\n\n\nmodel_results %&gt;% \n  add_count(grade_desc) %&gt;% \n  mutate(grade_desc = fct_reorder(grade_desc, .resid, .fun = median)) %&gt;% \n  ggplot(aes(.resid, grade_desc, fill = n)) +\n  geom_boxplot(color = \"grey\",\n               outlier.alpha = 0) +\n  scale_fill_viridis_c() +\n  scale_x_continuous(labels = label_dollar()) +\n  coord_cartesian(xlim = c(-10^5, 10.5^6)) +\n  labs(x = \"Residual\",\n       y = \"Grade\",\n       fill = \"Sales\")\n\n\n\n\n\n\n\n\n\nmodel_results %&gt;% \n  add_count(condition_desc) %&gt;% \n  mutate(condition_desc = fct_explicit_na(condition_desc),\n         condition_desc = fct_reorder(condition_desc, .resid, .fun = median)) %&gt;% \n  ggplot(aes(.resid, condition_desc, fill = n)) +\n  geom_boxplot(color = \"grey\",\n               outlier.alpha = 0) +\n  geom_vline(xintercept = 0, lty = 2, color = \"red\") +\n  scale_fill_viridis_c() +\n  scale_x_continuous(labels = label_dollar()) +\n  coord_cartesian(xlim = c(-10^5, 10.5^5)) +\n  labs(x = \"Residual\",\n       y = \"Condition\",\n       fill = \"Sales\")\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `condition_desc = fct_explicit_na(condition_desc)`.\nCaused by warning:\n! `fct_explicit_na()` was deprecated in forcats 1.0.0.\nℹ Please use `fct_na_value_to_level()` instead.\n\n\n\n\n\n\n\n\n\n\nmodel_results %&gt;% \n  ggplot(aes(finished_living_area, .resid)) +\n  geom_point(alpha = .1) +\n  scale_x_log10() +\n  scale_y_continuous(label = dollar) +\n  labs(x = \"Finished Living Area sq. ft. log10 scale\",\n       y = \"Residual\")\n\n\n\n\n\n\n\n\n\nmodel_results %&gt;% \n  ggplot(aes(lot_area, .resid)) +\n  geom_point(alpha = .1) +\n  scale_x_log10(labels = label_comma()) +\n  scale_y_continuous(labels = label_dollar()) +\n  labs(x = \"Lot Area sq. ft. log10 scale\",\n       y = \"Residual\")\n\nWarning: Transformation introduced infinite values in continuous x-axis\n\n\n\n\n\n\n\n\n\n\nmodel_results %&gt;% \n  group_by(house_age_at_sale) %&gt;% \n  rmse(truth = sale_price_adj, estimate = .pred_dollar) %&gt;% \n  ggplot(aes(house_age_at_sale, .estimate)) +\n  geom_point(alpha = .5) +\n  scale_y_continuous(labels = label_dollar()) +\n  labs(x = \"House age at sale\",\n       y = \"RMSE\")\n\n\n\n\n\n\n\n\nThe model is best at predicting the sale price of houses built in the 1940s to 1980s. This is when most of the houses in the county were built.\n\nmodel_results %&gt;% \n  group_by(year_built) %&gt;% \n  rmse(truth = sale_price_adj, estimate = .pred_dollar) %&gt;% \n  ggplot(aes(year_built, .estimate)) +\n  geom_point(alpha = .5) +\n  scale_y_continuous(labels = label_dollar()) +\n  labs(x = \"Year Built\",\n       y = \"RMSE\")\n\n\n\n\n\n\n\n\n\nmodel_results %&gt;% \n  add_count(bedrooms) %&gt;% \n  ggplot(aes(.resid, bedrooms, group = bedrooms, fill = n)) +\n  geom_boxplot(color = \"grey\",\n               outlier.alpha = 0) +\n  geom_vline(xintercept = 0, lty = 2, color = \"red\") +\n  scale_y_continuous(breaks = c(0:15)) +\n  scale_fill_viridis_c() +\n  scale_x_continuous(labels = label_dollar()) +\n  coord_cartesian(xlim = c(-10^5, 10^5)) +\n  labs(x = \"Residual\",\n       y = \"Bedrooms\",\n       fill = \"Sales\")\n\nWarning: Removed 2 rows containing missing values (`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\nmodel_results %&gt;% \n  add_count(full_baths) %&gt;% \n  ggplot(aes(.resid, full_baths, group = full_baths, fill = n)) +\n  geom_boxplot(color = \"grey\",\n               outlier.alpha = 0) +\n  geom_vline(xintercept = 0, lty = 2, color = \"red\") +\n  scale_y_continuous(breaks = c(0:12)) +\n  scale_fill_viridis_c() +\n  scale_x_continuous(label = dollar) +\n  coord_cartesian(xlim = c(-10^5, 750000)) +\n  labs(x = \"Residual\",\n       y = \"Full bathrooms\",\n       fill = \"Sales\")\n\nWarning: Removed 14 rows containing missing values (`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\nmodel_results %&gt;% \n  add_count(half_baths) %&gt;% \n  ggplot(aes(.resid, half_baths, group = half_baths, fill = n)) +\n  geom_boxplot(color = \"grey\",\n               outlier.alpha = 0) +\n  geom_vline(xintercept = 0, lty = 2, color = \"red\") +\n  scale_y_continuous(breaks = c(0:8)) +\n  scale_x_continuous(labels = label_dollar()) +\n  scale_fill_viridis_c() +\n  coord_cartesian(xlim = c(-10^5, 10^5)) +\n  labs(x = \"Residual\",\n       y = \"Half bathrooms\",\n       fill = \"Sales\")\n\nWarning: Removed 785 rows containing missing values (`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\nmodel_results %&gt;% \n  group_by(sale_year) %&gt;% \n  rmse(truth = sale_price_adj, estimate = .pred_dollar) %&gt;% \n  ggplot(aes(sale_year, .estimate)) +\n  geom_line() +\n  scale_y_continuous(label = dollar) +\n  labs(x = \"Sale year\",\n       y = \"RMSE\")"
  },
  {
    "objectID": "posts/remodeling-the-office/index.html",
    "href": "posts/remodeling-the-office/index.html",
    "title": "(re)Modeling the Office",
    "section": "",
    "text": "The goal for this analysis is to determine which characters, directors, and writers from The Office most influence an episode’s IMDB rating. My hypothesis is that IMDB rating is largely driven by a few show personnel. I also briefly walk through the data cleaning and modeling processes. This analysis is based on code from Julia Silge’s Tidy Tuesdays writeup She does a very good job of explaining the modeling aspects of this. She uses LASSO regression, which is very similar to the ridge regression I use.\nThe steps in the analysis are:\nI use these variables in the model:"
  },
  {
    "objectID": "posts/remodeling-the-office/index.html#basic-eda",
    "href": "posts/remodeling-the-office/index.html#basic-eda",
    "title": "(re)Modeling the Office",
    "section": "Basic EDA",
    "text": "Basic EDA\nThe boxplot shows that season may have an impact on the rating, so I will include that in the model.\n\ndf %&gt;% \n  distinct(air_date, season, imdb_rating) %&gt;% \n  ggplot(aes(air_date, imdb_rating, fill = as.factor(season))) +\n    geom_boxplot() +\n    labs(x = \"Air date\",\n         y = \"IMDB rating\",\n         fill = \"Season\") +\n    theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nThis creates a table with IMDB ratings and season premier/finale flags. This will be the table I join the personnel data to.\n\ndf_imdb &lt;- df %&gt;% \n  distinct(season, episode, imdb_rating) %&gt;% \n  group_by(season) %&gt;% \n  mutate(flag_premier = episode == first(episode),\n         flag_finale = episode == last(episode)) %&gt;% \n  ungroup() %&gt;% \n  mutate(across(contains(\"flag\"), as.numeric))\n\nglimpse(df_imdb)\n\nRows: 186\nColumns: 5\n$ season       &lt;int&gt; 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ episode      &lt;int&gt; 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, …\n$ imdb_rating  &lt;dbl&gt; 7.6, 8.3, 7.9, 8.1, 8.4, 7.8, 8.7, 8.2, 8.4, 8.4, 8.2, 8.…\n$ flag_premier &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ flag_finale  &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n\n\n\nDirectors\nSome episodes have more than one director, so I move them into separate rows.\n\ndf_directors &lt;- df %&gt;% \n  distinct(season, episode, director) %&gt;% \n  separate_rows(director, sep = \";\")\n\nThe original data contains misspellings of director names, which will cause issues when I filter out less common directors. This code fixes those misspellings.\n\ndf_director_fix &lt;- tibble(director_good = c(\"Charles McDougall\",\n                                            \"Claire Scanlon\",\n                                            \"Greg Daniels\",\n                                            \"Ken Whittingham\",\n                                            \"Paul Lieberstein\"),\n                          director_bad = c(\"Charles McDougal\",\n                                           \"Claire Scanlong\",\n                                           \"Greg Daneils\",\n                                           \"Ken Wittingham\",\n                                           \"Paul Lieerstein\"))\n\ndf_directors &lt;- df_directors %&gt;% \n  left_join(df_director_fix, by = c(\"director\" = \"director_bad\")) %&gt;% \n  mutate(director = case_when(!is.na(director_good) ~ director_good,\n                              is.na(director_good) ~ director)) %&gt;% \n  mutate(director = str_c(\"director\", director, sep = \"_\")) %&gt;% \n  select(-director_good)\n\nThis cleans up the director names and selects only directors that were involved in more than 2 episodes.\n\ndf_directors &lt;- df_directors %&gt;%  \n  mutate(director = str_remove_all(director, \"\\\\.\"),\n         director = str_replace_all(director, \"\\\\-\", \"_\"),\n         director = str_replace_all(director, \" \", \"_\")) %&gt;% \n  add_count(director) %&gt;% \n  filter(n &gt; 2) %&gt;% \n  select(-n)\n\nThis pivots the data wide so it can be used with the regression model.\n\ndf_directors &lt;- df_directors %&gt;% \n  mutate(flag = 1) %&gt;% \n  pivot_wider(id_cols = c(season, episode), names_from = director, values_from = flag, values_fill = list(flag = 0))\n\ndf_directors %&gt;% \n  select(1:20) %&gt;% \n  glimpse()\n\nRows: 139\nColumns: 20\n$ season                     &lt;int&gt; 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ episode                    &lt;int&gt; 1, 2, 3, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, …\n$ director_Ken_Kwapis        &lt;dbl&gt; 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1…\n$ director_Ken_Whittingham   &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ director_Greg_Daniels      &lt;dbl&gt; 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0…\n$ director_Paul_Feig         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0…\n$ director_Charles_McDougall &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0…\n$ director_Randall_Einhorn   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ director_Tucker_Gates      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ director_Jeffrey_Blitz     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ director_Harold_Ramis      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ director_Paul_Lieberstein  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ director_Jennifer_Celotta  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ director_David_Rogers      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ director_Steve_Carell      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ director_Brent_Forrester   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ director_BJ_Novak          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ director_John_Krasinski    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ director_Matt_Sohn         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ director_Rainn_Wilson      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n\n\n\n\nWriters\nThis separates out where more than one writer was involved in an episode, filters on writers that were involved in more than 2 episodes, and pivots the data wide.\n\ndf_writers &lt;- df %&gt;% \n  distinct(season, episode, writer) %&gt;% \n  separate_rows(writer, sep = \";\") %&gt;% \n  add_count(writer) %&gt;% \n  filter(n &gt; 2)\n\ndf_writers &lt;- df_writers %&gt;% \n  mutate(writer = str_remove_all(writer, \"\\\\.\"),\n         writer = str_replace_all(writer, \"\\\\-\", \"_\"),\n         writer = str_replace_all(writer, \" \", \"_\")) %&gt;% \n  mutate(writer = str_c(\"writer\", writer, sep = \"_\"))\n\ndf_writers &lt;- df_writers %&gt;% \n  mutate(flag = 1) %&gt;% \n  pivot_wider(id_cols = c(season, episode), names_from = writer, values_from = flag, values_fill = list(flag = 0))\n\ndf_writers %&gt;% \n  select(1:20) %&gt;% \n  glimpse()\n\nRows: 157\nColumns: 20\n$ season                    &lt;int&gt; 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2,…\n$ episode                   &lt;int&gt; 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 9, 10…\n$ writer_Greg_Daniels       &lt;dbl&gt; 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,…\n$ writer_BJ_Novak           &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,…\n$ writer_Paul_Lieberstein   &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,…\n$ writer_Michael_Schur      &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,…\n$ writer_Mindy_Kaling       &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ writer_Gene_Stupnitsky    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,…\n$ writer_Lee_Eisenberg      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,…\n$ writer_Jennifer_Celotta   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,…\n$ writer_Brent_Forrester    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ writer_Justin_Spitzer     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ writer_Aaron_Shure        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ writer_Charlie_Grandy     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ writer_Warren_Lieberstein &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ writer_Halsted_Sullivan   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ writer_Daniel_Chun        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ writer_Carrie_Kemper      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ writer_Steve_Hely         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ writer_Robert_Padnick     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n\n\n\n\nCharacters\nSome of the characters are named inconsistently, so this fixes the cases I identified.\n\ndf_characters &lt;- df %&gt;% \n  select(season, episode, character) %&gt;% \n  mutate(character = case_when(season == 7 & episode == 18 & character == \"Todd\" ~ \"Todd Packer\",\n                               TRUE ~ character)) %&gt;% \n  mutate(character = case_when(season == 7 & episode == 14 & character == \"David\" ~ character,\n                               character == \"David\" ~ \"David Wallace\",\n                               TRUE ~ character)) %&gt;% \n  mutate(character = case_when(character == \"DeAngelo\" ~ \"Deangelo\",\n                               TRUE ~ character))\n\nSome of the values contain odd characters that need to be removed. This also counts how many lines a character had in an episode.\n\ndf_characters &lt;- df_characters %&gt;%\n  mutate(character = str_replace_all(character, \" & \", \" and \"),\n         character = str_replace_all(character, \"/\", \" and \"),\n         character = str_replace_all(character, \",\", \" and \"),\n         character = str_trim(character),\n         character = str_remove_all(character, \"#\"),\n         character = str_remove_all(character, \"-\"),\n         character = str_remove_all(character, \"'\"),\n         character = str_remove_all(character, '\"'),\n         character = str_remove_all(character, \"\\\\[\"),\n         character = str_remove_all(character, \"\\\\]\"),\n         character = str_remove_all(character, \"\\\\(\"),\n         character = str_remove_all(character, \"\\\\)\"),\n         character = str_replace_all(character, \" \", \"_\")) %&gt;%\n  count(season, episode, character, name = \"line_count\")\n\nThis selects only the characters that were involved in more than 20 episodes and pivots the data wide. The value in each character column shows how many lines they had in the episode.\n\ndf_top_characters &lt;- df_characters %&gt;% \n  count(character, sort = TRUE) %&gt;% \n  filter(n &gt;= 20) %&gt;% \n  select(character)\n\ndf_characters_main &lt;- df_characters %&gt;% \n  semi_join(df_top_characters) %&gt;% \n  pivot_wider(id_cols = c(season, episode), \n              names_from = character, \n              names_prefix = \"cast_\", \n              values_from = line_count, \n              values_fill = list(line_count = 0))\n\ndf_characters_main %&gt;% \n  select(1:20) %&gt;% \n  glimpse()\n\nRows: 186\nColumns: 20\n$ season        &lt;int&gt; 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,…\n$ episode       &lt;int&gt; 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,…\n$ cast_Angela   &lt;int&gt; 1, 4, 5, 7, 3, 3, 1, 2, 6, 17, 13, 3, 0, 5, 13, 9, 1, 5,…\n$ cast_Dwight   &lt;int&gt; 29, 17, 62, 47, 25, 28, 32, 11, 55, 65, 33, 64, 22, 42, …\n$ cast_Jan      &lt;int&gt; 12, 0, 18, 0, 0, 8, 9, 11, 0, 0, 0, 0, 46, 52, 0, 0, 0, …\n$ cast_Jim      &lt;int&gt; 36, 25, 42, 49, 21, 55, 32, 16, 55, 51, 30, 49, 40, 26, …\n$ cast_Kevin    &lt;int&gt; 1, 8, 6, 3, 1, 5, 1, 6, 9, 5, 2, 3, 1, 4, 8, 11, 0, 2, 8…\n$ cast_Michael  &lt;int&gt; 81, 75, 56, 68, 104, 106, 96, 100, 83, 69, 108, 85, 73, …\n$ cast_Oscar    &lt;int&gt; 3, 13, 9, 14, 2, 1, 2, 0, 10, 4, 7, 0, 4, 1, 6, 8, 1, 3,…\n$ cast_Pam      &lt;int&gt; 41, 12, 32, 22, 14, 45, 41, 27, 33, 22, 27, 25, 32, 30, …\n$ cast_Phyllis  &lt;int&gt; 2, 0, 0, 5, 4, 0, 10, 6, 2, 6, 3, 0, 4, 1, 4, 5, 4, 3, 0…\n$ cast_Roy      &lt;int&gt; 5, 0, 0, 3, 12, 14, 6, 14, 0, 6, 0, 0, 1, 0, 0, 8, 11, 0…\n$ cast_Ryan     &lt;int&gt; 8, 4, 1, 4, 8, 12, 2, 1, 5, 40, 1, 18, 6, 1, 2, 15, 2, 1…\n$ cast_Stanley  &lt;int&gt; 5, 5, 6, 2, 3, 3, 8, 1, 3, 5, 3, 3, 0, 4, 2, 5, 8, 4, 3,…\n$ cast_Kelly    &lt;int&gt; 0, 2, 0, 0, 0, 0, 7, 0, 0, 4, 3, 3, 1, 2, 1, 8, 4, 1, 5,…\n$ cast_Toby     &lt;int&gt; 0, 2, 0, 4, 0, 7, 0, 26, 0, 0, 0, 5, 1, 1, 0, 3, 0, 7, 3…\n$ cast_Meredith &lt;int&gt; 0, 0, 3, 10, 0, 0, 0, 1, 1, 4, 0, 0, 0, 0, 0, 10, 3, 1, …\n$ cast_Darryl   &lt;int&gt; 0, 0, 0, 0, 15, 0, 1, 9, 0, 0, 0, 0, 0, 0, 0, 11, 3, 0, …\n$ cast_Everyone &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,…\n$ cast_Creed    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 16, 0, 0, 1, 0, 4, 0, 1, 3…"
  },
  {
    "objectID": "posts/remodeling-the-office/index.html#prepare-data-for-modeling",
    "href": "posts/remodeling-the-office/index.html#prepare-data-for-modeling",
    "title": "(re)Modeling the Office",
    "section": "Prepare data for modeling",
    "text": "Prepare data for modeling\nThis combines all the personnel tables and creates an episode_id variable. I also replace missing values with 0.\n\ndf_office &lt;- df_imdb %&gt;% \n  left_join(df_directors) %&gt;% \n  left_join(df_writers) %&gt;% \n  left_join(df_characters_main) %&gt;% \n  mutate(episode_id = str_c(season, episode, sep = \"_\")) %&gt;% \n  mutate(across(contains(\"director\"), coalesce, 0),\n         across(contains(\"writer\"), coalesce, 0)) %&gt;% \n  select(-episode)\n\ndf_office %&gt;% \n  glimpse()\n\nRows: 186\nColumns: 72\n$ season                     &lt;int&gt; 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ imdb_rating                &lt;dbl&gt; 7.6, 8.3, 7.9, 8.1, 8.4, 7.8, 8.7, 8.2, 8.4…\n$ flag_premier               &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0…\n$ flag_finale                &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ director_Ken_Kwapis        &lt;dbl&gt; 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0…\n$ director_Ken_Whittingham   &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ director_Greg_Daniels      &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0…\n$ director_Paul_Feig         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1…\n$ director_Charles_McDougall &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ director_Randall_Einhorn   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ director_Tucker_Gates      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ director_Jeffrey_Blitz     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ director_Harold_Ramis      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ director_Paul_Lieberstein  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ director_Jennifer_Celotta  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ director_David_Rogers      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ director_Steve_Carell      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ director_Brent_Forrester   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ director_BJ_Novak          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ director_John_Krasinski    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ director_Matt_Sohn         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ director_Rainn_Wilson      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ director_Troy_Miller       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ writer_Greg_Daniels        &lt;dbl&gt; 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0…\n$ writer_BJ_Novak            &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0…\n$ writer_Paul_Lieberstein    &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0…\n$ writer_Michael_Schur       &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0…\n$ writer_Mindy_Kaling        &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0…\n$ writer_Gene_Stupnitsky     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0…\n$ writer_Lee_Eisenberg       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0…\n$ writer_Jennifer_Celotta    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1…\n$ writer_Brent_Forrester     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ writer_Justin_Spitzer      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ writer_Aaron_Shure         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ writer_Charlie_Grandy      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ writer_Warren_Lieberstein  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ writer_Halsted_Sullivan    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ writer_Daniel_Chun         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ writer_Carrie_Kemper       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ writer_Steve_Hely          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ writer_Robert_Padnick      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ writer_Allison_Silverman   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ writer_Owen_Ellickson      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ cast_Angela                &lt;int&gt; 1, 4, 5, 7, 3, 3, 1, 2, 6, 17, 13, 3, 0, 5,…\n$ cast_Dwight                &lt;int&gt; 29, 17, 62, 47, 25, 28, 32, 11, 55, 65, 33,…\n$ cast_Jan                   &lt;int&gt; 12, 0, 18, 0, 0, 8, 9, 11, 0, 0, 0, 0, 46, …\n$ cast_Jim                   &lt;int&gt; 36, 25, 42, 49, 21, 55, 32, 16, 55, 51, 30,…\n$ cast_Kevin                 &lt;int&gt; 1, 8, 6, 3, 1, 5, 1, 6, 9, 5, 2, 3, 1, 4, 8…\n$ cast_Michael               &lt;int&gt; 81, 75, 56, 68, 104, 106, 96, 100, 83, 69, …\n$ cast_Oscar                 &lt;int&gt; 3, 13, 9, 14, 2, 1, 2, 0, 10, 4, 7, 0, 4, 1…\n$ cast_Pam                   &lt;int&gt; 41, 12, 32, 22, 14, 45, 41, 27, 33, 22, 27,…\n$ cast_Phyllis               &lt;int&gt; 2, 0, 0, 5, 4, 0, 10, 6, 2, 6, 3, 0, 4, 1, …\n$ cast_Roy                   &lt;int&gt; 5, 0, 0, 3, 12, 14, 6, 14, 0, 6, 0, 0, 1, 0…\n$ cast_Ryan                  &lt;int&gt; 8, 4, 1, 4, 8, 12, 2, 1, 5, 40, 1, 18, 6, 1…\n$ cast_Stanley               &lt;int&gt; 5, 5, 6, 2, 3, 3, 8, 1, 3, 5, 3, 3, 0, 4, 2…\n$ cast_Kelly                 &lt;int&gt; 0, 2, 0, 0, 0, 0, 7, 0, 0, 4, 3, 3, 1, 2, 1…\n$ cast_Toby                  &lt;int&gt; 0, 2, 0, 4, 0, 7, 0, 26, 0, 0, 0, 5, 1, 1, …\n$ cast_Meredith              &lt;int&gt; 0, 0, 3, 10, 0, 0, 0, 1, 1, 4, 0, 0, 0, 0, …\n$ cast_Darryl                &lt;int&gt; 0, 0, 0, 0, 15, 0, 1, 9, 0, 0, 0, 0, 0, 0, …\n$ cast_Everyone              &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0…\n$ cast_Creed                 &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 16, 0, 0, 1, …\n$ cast_All                   &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ cast_David_Wallace         &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ cast_Andy                  &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ cast_Karen                 &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ cast_Pete                  &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ cast_Erin                  &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ cast_Gabe                  &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ cast_Clark                 &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ cast_Robert                &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ cast_Nellie                &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ episode_id                 &lt;chr&gt; \"1_1\", \"1_2\", \"1_3\", \"1_4\", \"1_5\", \"1_6\", \"…\n\n\nThis splits the data into the training and testing sets that will be used to model the data. I stratify by season because it may have an effect on imdb_rating that I want to capture.\n\noffice_split &lt;- initial_split(df_office, strata = season)\noffice_train &lt;- training(office_split)\noffice_test &lt;- testing(office_split)\n\nThis creates a tidymodels recipe that removes zero-variance variables and normalizes the predictor variables.\n\noffice_rec &lt;- recipe(imdb_rating ~ ., data = office_train) %&gt;%\n  update_role(episode_id, new_role = \"ID\") %&gt;%\n  step_zv(all_numeric(), -all_outcomes()) %&gt;%\n  step_normalize(all_numeric(), -all_outcomes())\n\noffice_prep &lt;- office_rec %&gt;%\n  prep(strings_as_factors = FALSE)"
  },
  {
    "objectID": "posts/remodeling-the-office/index.html#modeling",
    "href": "posts/remodeling-the-office/index.html#modeling",
    "title": "(re)Modeling the Office",
    "section": "Modeling",
    "text": "Modeling\nI will use a linear model with ridge regression to penalize extreme coefficients. I bootstrap the training data and use tune() to find the optimal value for penalty.\n\nwf &lt;- workflow() %&gt;%\n  add_recipe(office_rec)\n\noffice_boot &lt;- bootstraps(office_train, strata = season)\n\ntune_spec &lt;- linear_reg(penalty = tune(), mixture = 0) %&gt;%\n  set_engine(\"glmnet\")\n\nlambda_grid &lt;- grid_regular(penalty(), levels = 50)\n\nridge_grid &lt;- tune_grid(\n  wf %&gt;% add_model(tune_spec),\n  resamples = office_boot,\n  grid = lambda_grid)\n\nlowest_rmse searches through the bootstrapped models to find the penalty that gives the lowest RMSE (root mean squared error). This graph shows that increasing the penalty increases performance, but has diminishing returns.\n\nlowest_rmse &lt;- ridge_grid %&gt;%\n  select_best(\"rmse\")\n\n#graph metrics\nridge_grid %&gt;%\n  collect_metrics() %&gt;%\n  ggplot(aes(penalty, mean, color = .metric, fill = .metric)) +\n  geom_ribbon(aes(ymin = mean - std_err,\n                  ymax = mean + std_err),\n              alpha = 0.5) +\n  geom_line(size = 1.5) +\n  geom_vline(xintercept = lowest_rmse$penalty, linetype = 2) +\n  facet_wrap(~.metric, scales = \"free\", nrow = 2) +\n  scale_x_log10() +\n  labs(title = \"Ridge regression lambda values\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nThis fits the model with the best value for penalty.\n\nfinal_ridge &lt;- finalize_workflow(wf %&gt;% add_model(tune_spec), lowest_rmse)\n\n\nAssess model\nThe model generally overrates episodes with low ratings and underrates episodes with high ratings.\n\nfinal_ridge %&gt;% \n  fit(office_train) %&gt;% \n  predict(office_train) %&gt;% \n  bind_cols(office_train) %&gt;% \n  ggplot(aes(imdb_rating, .pred)) +\n    geom_abline(linetype = 2) +\n    geom_point(alpha = .2) +\n    geom_smooth() +\n    coord_equal() +\n    labs(x = \"IMDB rating\",\n         y = \"Predicted rating\")\n\n\n\n\n\n\n\n\nExamining the data by season shows that the model predicted imdb_rating reasonably well for most seasons. It overestimated season 1 and underestimated season 3.\n\nfinal_ridge %&gt;% \n  fit(office_train) %&gt;% \n  predict(office_train) %&gt;% \n  bind_cols(office_train) %&gt;% \n  separate(episode_id, into = c(\"season\", \"episode\"), sep = \"_\") %&gt;% \n  mutate(.resid = imdb_rating - .pred) %&gt;% \n  select(season, episode, .resid) %&gt;% \n  ggplot(aes(season, .resid)) +\n    geom_boxplot() +\n    geom_hline(yintercept = 0, linetype = 2, color = \"red\") +\n    labs(y = \"Residual\",\n         title = \"Actual minus predicted rating\")\n\n\n\n\n\n\n\n\nThis graph shows the variable importance, split by role:\n\ndf_vi &lt;- final_ridge %&gt;%\n  fit(office_train) %&gt;%\n  pull_workflow_fit() %&gt;%\n  vi(lambda = lowest_rmse$penalty) %&gt;%\n  mutate(Importance = case_when(Sign == \"NEG\" ~ Importance * -1,\n                                TRUE ~ Importance)) %&gt;%\n  mutate(Variable = case_when(str_detect(Variable, \"writer|director|cast\") ~ Variable,\n                              TRUE ~ str_c(\"other_\", Variable))) %&gt;% \n  mutate(Variable = fct_reorder(Variable, Importance)) %&gt;%\n  separate(Variable, sep = \"_\", into = c(\"role\", \"person\"), extra = \"merge\") %&gt;% \n  mutate(person = str_replace_all(person, \"_\", \" \"))\n\ndf_vi %&gt;% \n  mutate(person = tidytext::reorder_within(x = person, by = Importance, within = role)) %&gt;% \n  ggplot(aes(x = Importance, y = person, fill = Importance)) +\n  geom_col(color = \"black\") +\n  facet_wrap(~role, scales = \"free_y\") +\n  scale_fill_viridis_c() +\n  scale_y_reordered() +\n  labs(y = NULL)\n\n\n\n\n\n\n\n\nThe importance of characters is much more evenly distributed than I thought it would be. Stanley is the cast MVP (non-Michael division) based on this model. The character isn’t usually the focus of an episode, but when he has a lot of lines, the episode gets better ratings.\n\n\n\n\n\nThe high values for “All” show that scenes where the entire office is involved are highly associated with increased ratings.\n\n\n\n\n\nI’m impressed by the high ratings for Michael and Jim, who carried a lot of the workload in terms of lines delivered. Despite this, the model still considers the number of lines they deliver to be important.\nCarell’s directorship is significantly more important than the other directors. I was definitely surprised by this, since Carell only directed a few episodes, and I expected the ridge regression to penalize his director coefficient heavily.\nThe model has a dim view of Nellie and Robert, who were brought in fill the gap left by Carell’s departure from the show.\n\n\n\n\n\nIn the “other” variables, the model thinks the show gets lower ratings as the seasons go on. Finales and season premiers are positive influences.\nSplitting the model inputs by role means I can compare how impactful a person was across roles. For example, the showrunner Greg Daniels was relatively more important as as writer than a director.\n\ndf_vi %&gt;%  \n  filter(person == \"Greg Daniels\")\n\n# A tibble: 2 × 4\n  role     person       Importance Sign \n  &lt;chr&gt;    &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;\n1 writer   Greg Daniels     0.0328 POS  \n2 director Greg Daniels     0.0123 POS  \n\n\nRainn Wilson was much more important as a cast member than as a director.\n\ndf_vi %&gt;% \n  filter(person == \"Dwight\" | person == \"Rainn Wilson\")\n\n# A tibble: 2 × 4\n  role     person       Importance Sign \n  &lt;chr&gt;    &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;\n1 cast     Dwight           0.0197 POS  \n2 director Rainn Wilson    -0.0189 NEG  \n\n\n\n\n\n\n\nFinally, this tests how the model performs on test data that it has not seen. I think this is reasonably good, considering that TV show quality is driven by chemistry between the cast, which is hard to quantify.\n\nlast_fit(final_ridge, office_split) %&gt;%\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       0.430 Preprocessor1_Model1\n2 rsq     standard       0.324 Preprocessor1_Model1"
  },
  {
    "objectID": "posts/classifying-pittsburgh-city-boundary/index.html",
    "href": "posts/classifying-pittsburgh-city-boundary/index.html",
    "title": "Modeling the Pittsburgh City Boundary",
    "section": "",
    "text": "My goal is to create a classification model that can distinguish between census tracts that are inside the city or outside the City of Pittsburgh. The border is interrupted by rivers, has an enclave, and is very irregular in general, which made this an interesting intellectual exercise.\n\n\nWhile Pittsburgh was founded in 1758, the city’s borders have changed many times due to annexation of surrounding municipalities. This map shows that what we call the North Side was previously Allegheny City, and was annexed into the city in 1907.\n\nMt. Oliver is a geographic enclave that is completely surrounded by the City of Pittsburgh, but is a separate municipality. The borough has resisted multiple annexation attempts.\n\n\n\n\nThis code loads the packages I need, configures some options, and sets the seed.\n\n#set up environment\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(janitor)\nlibrary(tidycensus)\nlibrary(sf)\nlibrary(hrbrthemes)\nlibrary(GGally)\n\ntheme_set(theme_ipsum(base_size = 15))\n\noptions(scipen = 999, digits = 4, tigris_use_cache = TRUE)\n\nset.seed(1234)\n\nI created a small Shiny app that let me select which tracts are inside the city borders. I will go over that in a future post. This loads the tracts from the Census API and pulls the results from the Shiny app.\n\n#load data about census tracts\ntracts &lt;- get_decennial(year = 2010, state = \"PA\", county = \"Allegheny County\", \n                        variables = \"P001001\",\n                        geography = \"tract\", geometry = TRUE)\n\ncity_tracts &lt;- read_csv(\"post_data/selected_tracts.csv\", col_types = cols(\"c\", \"l\")) %&gt;% \n  filter(selected == TRUE)\n\nglimpse(city_tracts)\n\nRows: 137\nColumns: 2\n$ GEOID    &lt;chr&gt; \"42003563000\", \"42003562800\", \"42003563100\", \"42003281500\", \"…\n$ selected &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, T…\n\n\nThis reads in the boundary shapefile and graphs it to show which tracts are in the city.\n\npgh_official_boundary &lt;- st_read(\"post_data/Pittsburgh_City_Boundary-shp\") %&gt;% \n  mutate(geography = \"City boundary\") %&gt;% \n  st_transform(crs = \"NAD83\") %&gt;% \n  st_cast(\"POLYGON\") %&gt;% \n  filter(FID != 7)\n\nReading layer `City_Boundary' from data source \n  `/Users/conorotompkins/Documents/github_repos/ctompkins_quarto_blog/posts/classifying-pittsburgh-city-boundary/post_data/Pittsburgh_City_Boundary-shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 8 features and 8 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 1316000 ymin: 381900 xmax: 1380000 ymax: 433400\nProjected CRS: NAD83 / Pennsylvania South (ftUS)\n\ntracts %&gt;% \n  left_join(city_tracts) %&gt;% \n  mutate(type = case_when(selected == TRUE ~ \"city\",\n                          is.na(selected) ~ \"non_city\")) %&gt;% \n  ggplot() +\n  geom_sf(aes(fill = type), size = .1) +\n  geom_sf(data = pgh_official_boundary, color = \"white\", linetype = 2, alpha = 0) +\n  theme_void()\n\n\n\n\n\n\n\n\nThis reads in the data I will consider for the model. All the data is from the 2010 Census.\n\nall_data &lt;- read_csv(\"post_data/combined_census_data_tract.csv\", col_types = cols(.default = \"c\")) %&gt;%\n  left_join(city_tracts) %&gt;% \n  mutate(type = case_when(selected == TRUE ~ \"city\",\n                          is.na(selected) ~ \"non_city\")) %&gt;% \n  mutate(across(pct_units_owned_loan:housed_population_density_pop_per_square_km, as.numeric)) %&gt;% \n  select(GEOID, type, everything()) %&gt;%\n  select(-c(selected, total_population_housed))\n\nglimpse(all_data)\n\nRows: 402\nColumns: 13\n$ GEOID                                       &lt;chr&gt; \"42003010300\", \"4200302010…\n$ type                                        &lt;chr&gt; \"city\", \"city\", \"city\", \"c…\n$ pct_units_owned_loan                        &lt;dbl&gt; 0.137755, 0.119779, 0.0762…\n$ pct_units_owned_entire                      &lt;dbl&gt; 0.18367, 0.06619, 0.02110,…\n$ pct_units_rented                            &lt;dbl&gt; 0.6786, 0.8140, 0.9026, 0.…\n$ total_population                            &lt;dbl&gt; 6600, 3629, 616, 2256, 260…\n$ pct_white                                   &lt;dbl&gt; 0.658333, 0.745660, 0.7564…\n$ pct_black                                   &lt;dbl&gt; 0.31167, 0.15982, 0.15584,…\n$ pct_asian                                   &lt;dbl&gt; 0.0110606, 0.0471204, 0.06…\n$ pct_hispanic                                &lt;dbl&gt; 0.024394, 0.032791, 0.0162…\n$ workers                                     &lt;dbl&gt; 0, 963, 408, 1056, 537, 47…\n$ jobs                                        &lt;dbl&gt; 0, 79639, 8301, 4633, 1962…\n$ housed_population_density_pop_per_square_km &lt;dbl&gt; 682.7, 1511.3, 386.7, 3205…\n\n\nThis plot compares all of the variables against each other. I used this to identify covariance and determine which variables should be excluded.\n\n#eda\npairwise_plot &lt;- all_data %&gt;% \n  select(-c(GEOID)) %&gt;% \n  ggpairs(aes(color = type)) +\n  theme(axis.text = element_text(size = 8))\n\n After reviewing this graph and considering things like covariance and zero-variance, I will use these variables in the model:\n\nHousing\n\nPercent of housing units owned outright\nPercent of housing units owned with a mortgage\nPercent of housing units rented\n\nDemographics\n\nPercent of people in the tract that are\n\nWhite\nBlack\n\n\nPopulation density\nEconomic data\n\nNumber of workers that live in the tract\nNumber of jobs in the tract\n\n\nNote that I am intentionally excluding any geographic data about the tracts. I am more interested in how “city-like” a given tract is than how close it is to the geographic center of the city.\nThis finalizes the data I will use to build the model.\n\ncensus_combined &lt;- all_data %&gt;% \n  select(GEOID, type, \n         pct_units_owned_loan, pct_units_owned_entire, pct_units_rented,\n         housed_population_density_pop_per_square_km,\n         pct_white, pct_black,\n         workers, jobs)\n\nglimpse(census_combined)\n\nRows: 402\nColumns: 10\n$ GEOID                                       &lt;chr&gt; \"42003010300\", \"4200302010…\n$ type                                        &lt;chr&gt; \"city\", \"city\", \"city\", \"c…\n$ pct_units_owned_loan                        &lt;dbl&gt; 0.137755, 0.119779, 0.0762…\n$ pct_units_owned_entire                      &lt;dbl&gt; 0.18367, 0.06619, 0.02110,…\n$ pct_units_rented                            &lt;dbl&gt; 0.6786, 0.8140, 0.9026, 0.…\n$ housed_population_density_pop_per_square_km &lt;dbl&gt; 682.7, 1511.3, 386.7, 3205…\n$ pct_white                                   &lt;dbl&gt; 0.658333, 0.745660, 0.7564…\n$ pct_black                                   &lt;dbl&gt; 0.31167, 0.15982, 0.15584,…\n$ workers                                     &lt;dbl&gt; 0, 963, 408, 1056, 537, 47…\n$ jobs                                        &lt;dbl&gt; 0, 79639, 8301, 4633, 1962…\n\n\n34% of the tracts are in the city, which is a slightly unbalanced dataset.\n\ncensus_combined %&gt;% \n  tabyl(type)\n\n     type   n percent\n     city 137  0.3408\n non_city 265  0.6592\n\n\nSince the total amount of data available is small, I will bootstrap the data to try to achieve more stable results from the model. Bootstrapping resamples the data with replacement, which creates multiple replicates of the original dataset with some variation due to sampling. I created a meme of my dog Quincy to illustrate the effect: \nI stratify by type so that each bootstrap has ~34% city tracts. This generates 50 sets of data for the model to work with.\n\ntract_boot &lt;- bootstraps(census_combined, strata = type, times = 50)\n\ntract_boot\n\n# Bootstrap sampling using stratification \n# A tibble: 50 × 2\n   splits            id         \n   &lt;list&gt;            &lt;chr&gt;      \n 1 &lt;split [402/152]&gt; Bootstrap01\n 2 &lt;split [402/145]&gt; Bootstrap02\n 3 &lt;split [402/147]&gt; Bootstrap03\n 4 &lt;split [402/150]&gt; Bootstrap04\n 5 &lt;split [402/144]&gt; Bootstrap05\n 6 &lt;split [402/148]&gt; Bootstrap06\n 7 &lt;split [402/152]&gt; Bootstrap07\n 8 &lt;split [402/149]&gt; Bootstrap08\n 9 &lt;split [402/143]&gt; Bootstrap09\n10 &lt;split [402/156]&gt; Bootstrap10\n# ℹ 40 more rows\n\n\nThis code chunk prepares the data to be modeled. I define the formula and scale all the numeric variables to have a mean of 0 and standard deviation of 1.\n\n#recipe\nmodel_recipe &lt;- recipe(type ~ ., data = census_combined) %&gt;% \n  update_role(GEOID, new_role = \"id\") %&gt;% \n  step_normalize(all_predictors())\n\nmodel_recipe_prep &lt;- model_recipe %&gt;%\n  prep(strings_as_factors = FALSE)\n\nmodel_recipe %&gt;% \n  summary()\n\n# A tibble: 10 × 4\n   variable                                    type      role      source  \n   &lt;chr&gt;                                       &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n 1 GEOID                                       &lt;chr [3]&gt; id        original\n 2 pct_units_owned_loan                        &lt;chr [2]&gt; predictor original\n 3 pct_units_owned_entire                      &lt;chr [2]&gt; predictor original\n 4 pct_units_rented                            &lt;chr [2]&gt; predictor original\n 5 housed_population_density_pop_per_square_km &lt;chr [2]&gt; predictor original\n 6 pct_white                                   &lt;chr [2]&gt; predictor original\n 7 pct_black                                   &lt;chr [2]&gt; predictor original\n 8 workers                                     &lt;chr [2]&gt; predictor original\n 9 jobs                                        &lt;chr [2]&gt; predictor original\n10 type                                        &lt;chr [3]&gt; outcome   original\n\n\nThis creates the model specifications for the two types of models I will use.\n\n#logistic regression\nlm_model &lt;- logistic_reg(mode = \"classification\") %&gt;% \n  set_engine(\"glm\")\n\n#random forest\nranger_model &lt;- rand_forest(trees = 1000, mode = \"classification\") %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\")\n\nThis sets up a workflow object to fit a logistic regression model against the bootstrap resamples I created earlier.\n\n#logistic regression\nlm_workflow &lt;- workflow() %&gt;% \n  add_recipe(model_recipe) %&gt;% \n  add_model(lm_model)\n\nlm_res &lt;- lm_workflow %&gt;% \n  fit_resamples(resamples = tract_boot) %&gt;% \n  mutate(model = \"lm\")\n\nlm_res %&gt;% \n  collect_metrics()\n\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary     0.783    49 0.00383 Preprocessor1_Model1\n2 roc_auc  binary     0.856    49 0.00394 Preprocessor1_Model1\n\n\nThe logistic regression gets ~76% accuracy, which is pretty good, but I want to know if a random forest could do better. This creates a workflow to fit a random forest model, and saves the predictions so I can use them later.\n\n#rf\nrf_workflow &lt;- workflow() %&gt;% \n  add_recipe(model_recipe_prep) %&gt;% \n  add_model(ranger_model)\n\nrf_res &lt;- rf_workflow %&gt;% \n  fit_resamples(resamples = tract_boot,\n                control = control_resamples(save_pred = TRUE)) %&gt;% \n  mutate(model = \"rf\")\n\nrf_res %&gt;% \n  collect_metrics()\n\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary     0.816    50 0.00360 Preprocessor1_Model1\n2 roc_auc  binary     0.894    50 0.00289 Preprocessor1_Model1\n\n\nIf you compare the results of the two models, the random forest model does much better than the logistic regression model.\n\ncombined_res &lt;- bind_rows(rf_res, lm_res)\n\ncombined_res %&gt;% \n  unnest(.metrics) %&gt;% \n  ggplot(aes(.estimate, color = model, fill = model)) +\n  geom_density(alpha = .5) +\n  facet_wrap(~.metric)\n\n\n\n\n\n\n\n\nThis graph shows that the random forest model’s false negative and false positive rates are about the same.\n\nrf_res %&gt;% \n  collect_predictions() %&gt;% \n  count(type, .pred_class) %&gt;% \n  ggplot(aes(type, .pred_class, fill = n)) +\n  geom_tile() +\n  labs(x = \"Truth\",\n       y = \"Prediction\",\n       fill = \"Number of observations\") +\n  scale_fill_continuous(label = scales::comma) +\n  coord_equal() +\n  theme(panel.grid.major = element_blank())\n\n\n\n\n\n\n\n\nThis fits the random forest model against the entire dataset to extract the variable importance metrics.\n\n#variable importance\nvar_imp &lt;- rf_workflow %&gt;% \n  fit(juice(model_recipe_prep)) %&gt;% \n  pull_workflow_fit() %&gt;% \n  vip::vi()\n\nvar_imp %&gt;%\n  mutate(Variable = fct_reorder(Variable, Importance)) %&gt;% \n  ggplot(aes(Importance, Variable)) +\n  geom_point()\n\n\n\n\n\n\n\n\nThe top 3 variables are the percent of a tract’s population that is White, the percent of housing units that are owned with a loan, and the population density. This matches my subjective model of city vs. non-city characteristics. Areas that are low density with majority White demographics where people own their homes are typically outside of the city. This dynamic is probably connected to the history of segregation and redlining in majority African American communities in Pittsburgh.\nSince the random forest model was fit against multiple bootstraps, I have multiple predictions per tract. I stratified the bootstraps by type, so the city and non_city tracts were sampled about the same number of times.\n\n#extract probabilities from bootstrap resamples\nfull_predictions &lt;- rf_res %&gt;% \n  collect_predictions() %&gt;% \n  mutate(correct = type == .pred_class) %&gt;%\n  left_join(census_combined %&gt;%\n              mutate(.row = row_number()))\n\nfull_predictions %&gt;% \n  count(type, GEOID) %&gt;% \n  ggplot(aes(n, fill = type, color = type)) +\n  geom_density(alpha = .3) +\n  labs(x = \"Number of observations of a tract\")\n\n\n\n\n\n\n\n\nI am not solely interested in the top-line accuracy of the model. Since the city border is a geographic phenomenon, there may be interesting patterns in the geographic distribution of the model’s predictions that can be shown on a map.\nTo map the data, I calculate the following metrics per tract:\n\nthe percent of predictions that were correct\naverage city classification %\naverage non-city classification %\nthe number of times the tract was sampled\n\n\nfull_predictions_pct &lt;- full_predictions %&gt;% \n  group_by(GEOID) %&gt;% \n  summarize(pct_correct = mean(correct),\n            mean_city = mean(.pred_city),\n            mean_non_city = mean(.pred_non_city),\n            n = n())\n\nglimpse(full_predictions_pct)\n\nRows: 402\nColumns: 5\n$ GEOID         &lt;chr&gt; \"42003010300\", \"42003020100\", \"42003020300\", \"4200303050…\n$ pct_correct   &lt;dbl&gt; 1.00000, 1.00000, 1.00000, 1.00000, 1.00000, 1.00000, 1.…\n$ mean_city     &lt;dbl&gt; 0.7656, 0.7912, 0.8174, 0.7668, 0.7078, 0.8055, 0.9276, …\n$ mean_non_city &lt;dbl&gt; 0.23436, 0.20878, 0.18263, 0.23318, 0.29219, 0.19454, 0.…\n$ n             &lt;int&gt; 23, 13, 15, 21, 19, 17, 16, 19, 11, 20, 22, 21, 18, 22, …\n\n\nThis shows the % of correct predictions per tract. The model was very successful with the outlying tracts in the county, but struggled in Mt. Washington/Beechview/Brookline, the Hazelwood/Greenfield area, and Forest Hills towards Monroeville.\n\ntracts %&gt;% \n  left_join(full_predictions_pct) %&gt;% \n  ggplot() +\n  geom_sf(aes(fill = pct_correct), lwd = 0) +\n  geom_sf(data = pgh_official_boundary, alpha = 0, color = \"black\", lwd = 2) +\n  geom_sf(data = pgh_official_boundary, alpha = 0, color = \"yellow\", lwd = .3) +\n  scale_fill_viridis_c(labels = scales::percent) +\n  labs(fill = \"% predictions correct\") +\n  theme_void()\n\n\n\n\n\n\n\n\nThis shows the average % that the model classified a tract as being in the city. The model was very confident that Oakland, Shadyside, and Squirrel Hill are in the city. The model also thought that many communities to the east and communities along the Monongahela River are in the city, specifically McKeesport and West Mifflin.\n\ntracts %&gt;% \n  left_join(full_predictions_pct) %&gt;% \n  ggplot() +\n  geom_sf(aes(fill = mean_city), lwd = 0) +\n  geom_sf(data = pgh_official_boundary, alpha = 0, color = \"black\", lwd = 2) +\n  geom_sf(data = pgh_official_boundary, alpha = 0, color = \"yellow\", lwd = .3) +\n  scale_fill_viridis_c(labels = scales::percent) +\n  labs(fill = \"Average city classification %\") +\n  theme_void()\n\n\n\n\n\n\n\n\nTo review, I think it would be difficult to create a model that almost perfectly captures the current city border, which is a result of political decisions, court cases, and other non-deterministic phenomena. In addition, white flight and the relative expansion of the suburbs during the collapse of the steel industry reshaped the Pittsburgh metro area. City borders are defined by people and politicians, not a clustering algorithm based on Census data (although that would be interesting). My experience is that many people that don’t technically live within the border consider themselves to be Pittsburghers. So what is a border, anyways?"
  },
  {
    "objectID": "posts/classifying-pittsburgh-city-boundary/index.html#introduction",
    "href": "posts/classifying-pittsburgh-city-boundary/index.html#introduction",
    "title": "Modeling the Pittsburgh City Boundary",
    "section": "",
    "text": "My goal is to create a classification model that can distinguish between census tracts that are inside the city or outside the City of Pittsburgh. The border is interrupted by rivers, has an enclave, and is very irregular in general, which made this an interesting intellectual exercise.\n\n\nWhile Pittsburgh was founded in 1758, the city’s borders have changed many times due to annexation of surrounding municipalities. This map shows that what we call the North Side was previously Allegheny City, and was annexed into the city in 1907.\n\nMt. Oliver is a geographic enclave that is completely surrounded by the City of Pittsburgh, but is a separate municipality. The borough has resisted multiple annexation attempts.\n\n\n\n\nThis code loads the packages I need, configures some options, and sets the seed.\n\n#set up environment\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(janitor)\nlibrary(tidycensus)\nlibrary(sf)\nlibrary(hrbrthemes)\nlibrary(GGally)\n\ntheme_set(theme_ipsum(base_size = 15))\n\noptions(scipen = 999, digits = 4, tigris_use_cache = TRUE)\n\nset.seed(1234)\n\nI created a small Shiny app that let me select which tracts are inside the city borders. I will go over that in a future post. This loads the tracts from the Census API and pulls the results from the Shiny app.\n\n#load data about census tracts\ntracts &lt;- get_decennial(year = 2010, state = \"PA\", county = \"Allegheny County\", \n                        variables = \"P001001\",\n                        geography = \"tract\", geometry = TRUE)\n\ncity_tracts &lt;- read_csv(\"post_data/selected_tracts.csv\", col_types = cols(\"c\", \"l\")) %&gt;% \n  filter(selected == TRUE)\n\nglimpse(city_tracts)\n\nRows: 137\nColumns: 2\n$ GEOID    &lt;chr&gt; \"42003563000\", \"42003562800\", \"42003563100\", \"42003281500\", \"…\n$ selected &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, T…\n\n\nThis reads in the boundary shapefile and graphs it to show which tracts are in the city.\n\npgh_official_boundary &lt;- st_read(\"post_data/Pittsburgh_City_Boundary-shp\") %&gt;% \n  mutate(geography = \"City boundary\") %&gt;% \n  st_transform(crs = \"NAD83\") %&gt;% \n  st_cast(\"POLYGON\") %&gt;% \n  filter(FID != 7)\n\nReading layer `City_Boundary' from data source \n  `/Users/conorotompkins/Documents/github_repos/ctompkins_quarto_blog/posts/classifying-pittsburgh-city-boundary/post_data/Pittsburgh_City_Boundary-shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 8 features and 8 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 1316000 ymin: 381900 xmax: 1380000 ymax: 433400\nProjected CRS: NAD83 / Pennsylvania South (ftUS)\n\ntracts %&gt;% \n  left_join(city_tracts) %&gt;% \n  mutate(type = case_when(selected == TRUE ~ \"city\",\n                          is.na(selected) ~ \"non_city\")) %&gt;% \n  ggplot() +\n  geom_sf(aes(fill = type), size = .1) +\n  geom_sf(data = pgh_official_boundary, color = \"white\", linetype = 2, alpha = 0) +\n  theme_void()\n\n\n\n\n\n\n\n\nThis reads in the data I will consider for the model. All the data is from the 2010 Census.\n\nall_data &lt;- read_csv(\"post_data/combined_census_data_tract.csv\", col_types = cols(.default = \"c\")) %&gt;%\n  left_join(city_tracts) %&gt;% \n  mutate(type = case_when(selected == TRUE ~ \"city\",\n                          is.na(selected) ~ \"non_city\")) %&gt;% \n  mutate(across(pct_units_owned_loan:housed_population_density_pop_per_square_km, as.numeric)) %&gt;% \n  select(GEOID, type, everything()) %&gt;%\n  select(-c(selected, total_population_housed))\n\nglimpse(all_data)\n\nRows: 402\nColumns: 13\n$ GEOID                                       &lt;chr&gt; \"42003010300\", \"4200302010…\n$ type                                        &lt;chr&gt; \"city\", \"city\", \"city\", \"c…\n$ pct_units_owned_loan                        &lt;dbl&gt; 0.137755, 0.119779, 0.0762…\n$ pct_units_owned_entire                      &lt;dbl&gt; 0.18367, 0.06619, 0.02110,…\n$ pct_units_rented                            &lt;dbl&gt; 0.6786, 0.8140, 0.9026, 0.…\n$ total_population                            &lt;dbl&gt; 6600, 3629, 616, 2256, 260…\n$ pct_white                                   &lt;dbl&gt; 0.658333, 0.745660, 0.7564…\n$ pct_black                                   &lt;dbl&gt; 0.31167, 0.15982, 0.15584,…\n$ pct_asian                                   &lt;dbl&gt; 0.0110606, 0.0471204, 0.06…\n$ pct_hispanic                                &lt;dbl&gt; 0.024394, 0.032791, 0.0162…\n$ workers                                     &lt;dbl&gt; 0, 963, 408, 1056, 537, 47…\n$ jobs                                        &lt;dbl&gt; 0, 79639, 8301, 4633, 1962…\n$ housed_population_density_pop_per_square_km &lt;dbl&gt; 682.7, 1511.3, 386.7, 3205…\n\n\nThis plot compares all of the variables against each other. I used this to identify covariance and determine which variables should be excluded.\n\n#eda\npairwise_plot &lt;- all_data %&gt;% \n  select(-c(GEOID)) %&gt;% \n  ggpairs(aes(color = type)) +\n  theme(axis.text = element_text(size = 8))\n\n After reviewing this graph and considering things like covariance and zero-variance, I will use these variables in the model:\n\nHousing\n\nPercent of housing units owned outright\nPercent of housing units owned with a mortgage\nPercent of housing units rented\n\nDemographics\n\nPercent of people in the tract that are\n\nWhite\nBlack\n\n\nPopulation density\nEconomic data\n\nNumber of workers that live in the tract\nNumber of jobs in the tract\n\n\nNote that I am intentionally excluding any geographic data about the tracts. I am more interested in how “city-like” a given tract is than how close it is to the geographic center of the city.\nThis finalizes the data I will use to build the model.\n\ncensus_combined &lt;- all_data %&gt;% \n  select(GEOID, type, \n         pct_units_owned_loan, pct_units_owned_entire, pct_units_rented,\n         housed_population_density_pop_per_square_km,\n         pct_white, pct_black,\n         workers, jobs)\n\nglimpse(census_combined)\n\nRows: 402\nColumns: 10\n$ GEOID                                       &lt;chr&gt; \"42003010300\", \"4200302010…\n$ type                                        &lt;chr&gt; \"city\", \"city\", \"city\", \"c…\n$ pct_units_owned_loan                        &lt;dbl&gt; 0.137755, 0.119779, 0.0762…\n$ pct_units_owned_entire                      &lt;dbl&gt; 0.18367, 0.06619, 0.02110,…\n$ pct_units_rented                            &lt;dbl&gt; 0.6786, 0.8140, 0.9026, 0.…\n$ housed_population_density_pop_per_square_km &lt;dbl&gt; 682.7, 1511.3, 386.7, 3205…\n$ pct_white                                   &lt;dbl&gt; 0.658333, 0.745660, 0.7564…\n$ pct_black                                   &lt;dbl&gt; 0.31167, 0.15982, 0.15584,…\n$ workers                                     &lt;dbl&gt; 0, 963, 408, 1056, 537, 47…\n$ jobs                                        &lt;dbl&gt; 0, 79639, 8301, 4633, 1962…\n\n\n34% of the tracts are in the city, which is a slightly unbalanced dataset.\n\ncensus_combined %&gt;% \n  tabyl(type)\n\n     type   n percent\n     city 137  0.3408\n non_city 265  0.6592\n\n\nSince the total amount of data available is small, I will bootstrap the data to try to achieve more stable results from the model. Bootstrapping resamples the data with replacement, which creates multiple replicates of the original dataset with some variation due to sampling. I created a meme of my dog Quincy to illustrate the effect: \nI stratify by type so that each bootstrap has ~34% city tracts. This generates 50 sets of data for the model to work with.\n\ntract_boot &lt;- bootstraps(census_combined, strata = type, times = 50)\n\ntract_boot\n\n# Bootstrap sampling using stratification \n# A tibble: 50 × 2\n   splits            id         \n   &lt;list&gt;            &lt;chr&gt;      \n 1 &lt;split [402/152]&gt; Bootstrap01\n 2 &lt;split [402/145]&gt; Bootstrap02\n 3 &lt;split [402/147]&gt; Bootstrap03\n 4 &lt;split [402/150]&gt; Bootstrap04\n 5 &lt;split [402/144]&gt; Bootstrap05\n 6 &lt;split [402/148]&gt; Bootstrap06\n 7 &lt;split [402/152]&gt; Bootstrap07\n 8 &lt;split [402/149]&gt; Bootstrap08\n 9 &lt;split [402/143]&gt; Bootstrap09\n10 &lt;split [402/156]&gt; Bootstrap10\n# ℹ 40 more rows\n\n\nThis code chunk prepares the data to be modeled. I define the formula and scale all the numeric variables to have a mean of 0 and standard deviation of 1.\n\n#recipe\nmodel_recipe &lt;- recipe(type ~ ., data = census_combined) %&gt;% \n  update_role(GEOID, new_role = \"id\") %&gt;% \n  step_normalize(all_predictors())\n\nmodel_recipe_prep &lt;- model_recipe %&gt;%\n  prep(strings_as_factors = FALSE)\n\nmodel_recipe %&gt;% \n  summary()\n\n# A tibble: 10 × 4\n   variable                                    type      role      source  \n   &lt;chr&gt;                                       &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n 1 GEOID                                       &lt;chr [3]&gt; id        original\n 2 pct_units_owned_loan                        &lt;chr [2]&gt; predictor original\n 3 pct_units_owned_entire                      &lt;chr [2]&gt; predictor original\n 4 pct_units_rented                            &lt;chr [2]&gt; predictor original\n 5 housed_population_density_pop_per_square_km &lt;chr [2]&gt; predictor original\n 6 pct_white                                   &lt;chr [2]&gt; predictor original\n 7 pct_black                                   &lt;chr [2]&gt; predictor original\n 8 workers                                     &lt;chr [2]&gt; predictor original\n 9 jobs                                        &lt;chr [2]&gt; predictor original\n10 type                                        &lt;chr [3]&gt; outcome   original\n\n\nThis creates the model specifications for the two types of models I will use.\n\n#logistic regression\nlm_model &lt;- logistic_reg(mode = \"classification\") %&gt;% \n  set_engine(\"glm\")\n\n#random forest\nranger_model &lt;- rand_forest(trees = 1000, mode = \"classification\") %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\")\n\nThis sets up a workflow object to fit a logistic regression model against the bootstrap resamples I created earlier.\n\n#logistic regression\nlm_workflow &lt;- workflow() %&gt;% \n  add_recipe(model_recipe) %&gt;% \n  add_model(lm_model)\n\nlm_res &lt;- lm_workflow %&gt;% \n  fit_resamples(resamples = tract_boot) %&gt;% \n  mutate(model = \"lm\")\n\nlm_res %&gt;% \n  collect_metrics()\n\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary     0.783    49 0.00383 Preprocessor1_Model1\n2 roc_auc  binary     0.856    49 0.00394 Preprocessor1_Model1\n\n\nThe logistic regression gets ~76% accuracy, which is pretty good, but I want to know if a random forest could do better. This creates a workflow to fit a random forest model, and saves the predictions so I can use them later.\n\n#rf\nrf_workflow &lt;- workflow() %&gt;% \n  add_recipe(model_recipe_prep) %&gt;% \n  add_model(ranger_model)\n\nrf_res &lt;- rf_workflow %&gt;% \n  fit_resamples(resamples = tract_boot,\n                control = control_resamples(save_pred = TRUE)) %&gt;% \n  mutate(model = \"rf\")\n\nrf_res %&gt;% \n  collect_metrics()\n\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary     0.816    50 0.00360 Preprocessor1_Model1\n2 roc_auc  binary     0.894    50 0.00289 Preprocessor1_Model1\n\n\nIf you compare the results of the two models, the random forest model does much better than the logistic regression model.\n\ncombined_res &lt;- bind_rows(rf_res, lm_res)\n\ncombined_res %&gt;% \n  unnest(.metrics) %&gt;% \n  ggplot(aes(.estimate, color = model, fill = model)) +\n  geom_density(alpha = .5) +\n  facet_wrap(~.metric)\n\n\n\n\n\n\n\n\nThis graph shows that the random forest model’s false negative and false positive rates are about the same.\n\nrf_res %&gt;% \n  collect_predictions() %&gt;% \n  count(type, .pred_class) %&gt;% \n  ggplot(aes(type, .pred_class, fill = n)) +\n  geom_tile() +\n  labs(x = \"Truth\",\n       y = \"Prediction\",\n       fill = \"Number of observations\") +\n  scale_fill_continuous(label = scales::comma) +\n  coord_equal() +\n  theme(panel.grid.major = element_blank())\n\n\n\n\n\n\n\n\nThis fits the random forest model against the entire dataset to extract the variable importance metrics.\n\n#variable importance\nvar_imp &lt;- rf_workflow %&gt;% \n  fit(juice(model_recipe_prep)) %&gt;% \n  pull_workflow_fit() %&gt;% \n  vip::vi()\n\nvar_imp %&gt;%\n  mutate(Variable = fct_reorder(Variable, Importance)) %&gt;% \n  ggplot(aes(Importance, Variable)) +\n  geom_point()\n\n\n\n\n\n\n\n\nThe top 3 variables are the percent of a tract’s population that is White, the percent of housing units that are owned with a loan, and the population density. This matches my subjective model of city vs. non-city characteristics. Areas that are low density with majority White demographics where people own their homes are typically outside of the city. This dynamic is probably connected to the history of segregation and redlining in majority African American communities in Pittsburgh.\nSince the random forest model was fit against multiple bootstraps, I have multiple predictions per tract. I stratified the bootstraps by type, so the city and non_city tracts were sampled about the same number of times.\n\n#extract probabilities from bootstrap resamples\nfull_predictions &lt;- rf_res %&gt;% \n  collect_predictions() %&gt;% \n  mutate(correct = type == .pred_class) %&gt;%\n  left_join(census_combined %&gt;%\n              mutate(.row = row_number()))\n\nfull_predictions %&gt;% \n  count(type, GEOID) %&gt;% \n  ggplot(aes(n, fill = type, color = type)) +\n  geom_density(alpha = .3) +\n  labs(x = \"Number of observations of a tract\")\n\n\n\n\n\n\n\n\nI am not solely interested in the top-line accuracy of the model. Since the city border is a geographic phenomenon, there may be interesting patterns in the geographic distribution of the model’s predictions that can be shown on a map.\nTo map the data, I calculate the following metrics per tract:\n\nthe percent of predictions that were correct\naverage city classification %\naverage non-city classification %\nthe number of times the tract was sampled\n\n\nfull_predictions_pct &lt;- full_predictions %&gt;% \n  group_by(GEOID) %&gt;% \n  summarize(pct_correct = mean(correct),\n            mean_city = mean(.pred_city),\n            mean_non_city = mean(.pred_non_city),\n            n = n())\n\nglimpse(full_predictions_pct)\n\nRows: 402\nColumns: 5\n$ GEOID         &lt;chr&gt; \"42003010300\", \"42003020100\", \"42003020300\", \"4200303050…\n$ pct_correct   &lt;dbl&gt; 1.00000, 1.00000, 1.00000, 1.00000, 1.00000, 1.00000, 1.…\n$ mean_city     &lt;dbl&gt; 0.7656, 0.7912, 0.8174, 0.7668, 0.7078, 0.8055, 0.9276, …\n$ mean_non_city &lt;dbl&gt; 0.23436, 0.20878, 0.18263, 0.23318, 0.29219, 0.19454, 0.…\n$ n             &lt;int&gt; 23, 13, 15, 21, 19, 17, 16, 19, 11, 20, 22, 21, 18, 22, …\n\n\nThis shows the % of correct predictions per tract. The model was very successful with the outlying tracts in the county, but struggled in Mt. Washington/Beechview/Brookline, the Hazelwood/Greenfield area, and Forest Hills towards Monroeville.\n\ntracts %&gt;% \n  left_join(full_predictions_pct) %&gt;% \n  ggplot() +\n  geom_sf(aes(fill = pct_correct), lwd = 0) +\n  geom_sf(data = pgh_official_boundary, alpha = 0, color = \"black\", lwd = 2) +\n  geom_sf(data = pgh_official_boundary, alpha = 0, color = \"yellow\", lwd = .3) +\n  scale_fill_viridis_c(labels = scales::percent) +\n  labs(fill = \"% predictions correct\") +\n  theme_void()\n\n\n\n\n\n\n\n\nThis shows the average % that the model classified a tract as being in the city. The model was very confident that Oakland, Shadyside, and Squirrel Hill are in the city. The model also thought that many communities to the east and communities along the Monongahela River are in the city, specifically McKeesport and West Mifflin.\n\ntracts %&gt;% \n  left_join(full_predictions_pct) %&gt;% \n  ggplot() +\n  geom_sf(aes(fill = mean_city), lwd = 0) +\n  geom_sf(data = pgh_official_boundary, alpha = 0, color = \"black\", lwd = 2) +\n  geom_sf(data = pgh_official_boundary, alpha = 0, color = \"yellow\", lwd = .3) +\n  scale_fill_viridis_c(labels = scales::percent) +\n  labs(fill = \"Average city classification %\") +\n  theme_void()\n\n\n\n\n\n\n\n\nTo review, I think it would be difficult to create a model that almost perfectly captures the current city border, which is a result of political decisions, court cases, and other non-deterministic phenomena. In addition, white flight and the relative expansion of the suburbs during the collapse of the steel industry reshaped the Pittsburgh metro area. City borders are defined by people and politicians, not a clustering algorithm based on Census data (although that would be interesting). My experience is that many people that don’t technically live within the border consider themselves to be Pittsburghers. So what is a border, anyways?"
  },
  {
    "objectID": "posts/classifying-pittsburgh-city-boundary/index.html#references",
    "href": "posts/classifying-pittsburgh-city-boundary/index.html#references",
    "title": "Modeling the Pittsburgh City Boundary",
    "section": "References",
    "text": "References\n\nhttps://juliasilge.com/blog/multinomial-volcano-eruptions/\nhttp://www.rebeccabarter.com/blog/2020-03-25_machine_learning/#split-into-traintest\nhttps://www.brodrigues.co/blog/2018-11-25-tidy_cv/\nhttps://agailloty.rbind.io/en/post/tidymodels/\nhttps://alison.rbind.io/post/2020-02-27-better-tidymodels/\nhttps://hansjoerg.me/2020/02/09/tidymodels-for-machine-learning/\nhttps://towardsdatascience.com/modelling-with-tidymodels-and-parsnip-bae2c01c131c\nhttps://www.benjaminsorensen.me/post/modeling-with-parsnip-and-tidymodels/\nhttps://rviews.rstudio.com/2019/06/19/a-gentle-intro-to-tidymodels/\nhttps://en.wikipedia.org/wiki/Allegheny,_Pennsylvania"
  },
  {
    "objectID": "posts/volatile-zillow-house-value/index.html",
    "href": "posts/volatile-zillow-house-value/index.html",
    "title": "Graphing volatile home values in U.S. metro areas",
    "section": "",
    "text": "Zillow publishes a variety of cool data that I haven’t explored much yet. The first dataset that caught my eye was the Zillow Home Value Index (ZHVI). Zillow describes it as the “smoothed, seasonally adjusted measure of the typical home value and market changes across a given region and housing type”. In this post I will make a quick gganimate plot of the ZHVI of various metro areas in the U.S.\nThe code for this post was re-ran in 2024. The data was filtered to match the date of the original post.\nThe first thing I noticed about the data is that it is aggressively wide. There is a column for each year-month in the dataset. 293 columns is a lot to work with.\n\n#download housing data from https://www.zillow.com/research/data/\nzillow_house_value &lt;- read_csv(\"post_data/Metro_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv\")\n\ndim(zillow_house_value)\n\n[1] 895 293\n\n\nTo make the data more tidy, I use a regex to identify the columns that have a date in the name and pivot those longer. Now each row represents the ZHVI for a given region area on a given year-month.\n\nzillow_house_value &lt;- zillow_house_value %&gt;% \n  pivot_longer(cols = matches(\"\\\\d{4}-\\\\d{2}-\\\\d{2}\"),\n               names_to = \"date\", values_to = \"zhvi\") %&gt;% \n  clean_names() %&gt;% \n  mutate(date = ymd(date),\n         region_name = str_squish(region_name))\n\nglimpse(zillow_house_value)\n\nRows: 257,760\nColumns: 7\n$ region_id   &lt;dbl&gt; 102001, 102001, 102001, 102001, 102001, 102001, 102001, 10…\n$ size_rank   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ region_name &lt;chr&gt; \"United States\", \"United States\", \"United States\", \"United…\n$ region_type &lt;chr&gt; \"country\", \"country\", \"country\", \"country\", \"country\", \"co…\n$ state_name  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ date        &lt;date&gt; 2000-01-31, 2000-02-29, 2000-03-31, 2000-04-30, 2000-05-3…\n$ zhvi        &lt;dbl&gt; 118707, 118916, 119175, 119730, 120370, 121055, 121781, 12…\n\n\nOnce the data is tidy, it is easy to plot with ggplot2. In this graph, each line represents one metro area.\n\nzillow_house_value %&gt;% \n  ggplot(aes(date, zhvi, group = region_name)) +\n  geom_line(alpha = .1, size = .5)\n\n\n\n\n\n\n\n\nWhat struck me is that while most metro areas in the dataset start with ZHVI &lt; $300,000, many increase to 3x that, with many wild swings along the way due to housing bubbles, economic crashes, and housing scarcity. I will rank the metro areas by volatility (standard deviation of ZHVI) and use ggplot2 and gganimate to highlight the most volatile metro areas.\n\n#find most volatile regions\ndf_top_regions &lt;- zillow_house_value %&gt;% \n  group_by(region_name) %&gt;% \n  summarize(sd = sd(zhvi)) %&gt;% \n  ungroup() %&gt;% \n  arrange(desc(sd)) %&gt;% \n  slice(1:25) %&gt;% \n  mutate(region_name_rank = str_c(\"#\", row_number(), \" \", region_name, sep = \"\"))\n  \nregion_name_highlight_fct &lt;- df_top_regions %&gt;% \n  pull(region_name)\n\nregion_name_rank_fct &lt;- df_top_regions %&gt;% \n  pull(region_name_rank)\n\n\n#create highlight df\ndf_highlights &lt;- zillow_house_value %&gt;% \n  inner_join(df_top_regions) %&gt;% \n  mutate(region_name_highlight = region_name,\n         region_name_highlight = factor(region_name_highlight, levels = region_name_highlight_fct),\n         region_name_rank = factor(region_name_rank, levels = region_name_rank_fct))\n\nhousing_animation &lt;- zillow_house_value %&gt;% \n  ggplot() +\n  geom_line(aes(date, zhvi, group = region_name), alpha = .1, size = .5) +\n  geom_line(data = df_highlights,\n            aes(date, zhvi),\n            color = \"red\", size = 1.5) +\n  scale_y_continuous(labels = scales::dollar_format()) +\n  transition_manual(region_name_rank) +\n  labs(title = \"Top 25 most volatile housing markets 1996-2020\",\n       subtitle = \"Region: {current_frame}\",\n       x = NULL,\n       y = \"Zillow Housing Value Index\") +\n  theme(plot.subtitle = element_text(size = 15),\n        axis.title.y = element_text(size = 15))\n\nhousing_animation &lt;- animate(housing_animation, duration = 10, fps = 40)\n\nhousing_animation"
  },
  {
    "objectID": "posts/compare-pre-post-covid/index.html",
    "href": "posts/compare-pre-post-covid/index.html",
    "title": "Comparing Healthy Ride Usage Pre And “Post” COVID-19",
    "section": "",
    "text": "Lawrence Andrews asked me on Twitter if there had been a change in Health Ride usage after COVID-19.\n\n\nWould be interested to see this @healthyridepgh data to compare pre-covid (2019) and during (2020)\n\n— Lawrence Andrews (@lawrenceandrews) August 13, 2020\n\n\nThe {tidyverts} universe of packages from Rob Hyndman provides a lot of tools that let you interrogate time series data. I will use some of these tools to decompose the Healthy Ride time series and see if there was a change.\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(janitor)\nlibrary(tsibble)\nlibrary(feasts)\nlibrary(hrbrthemes)\n\noptions(scipen = 999, digits = 4)\n\ntheme_set(theme_ipsum(base_size = 20, \n                      strip_text_size = 18, \n                      axis_title_size = 18))\n\nI had already combined the usage data from the WPRDC with list.files and map_df(read_csv), so I can just read in the combined CSV file:\n\ndata &lt;- read_csv(\"post_data/combined_ride_data.csv\")\n\nSummarizing the number of rides per day shows that the data is very seasonal. The red line is on March 6, which is the date of the first known positive COVID-19 case in the state.\n\ndata %&gt;% \n  count(date, name = \"number_of_rides\", sort = TRUE) %&gt;% \n  filter(!is.na(date)) %&gt;% \n  ggplot(aes(date, number_of_rides)) +\n  geom_point(alpha = .5, size = .5) +\n  geom_vline(xintercept = ymd(\"2020-03-06\"), color = \"red\")\n\n\n\n\n\n\n\n\nI use the {tsibble} package to make a time series tibble and fill in a few gaps in the data. Then I create 3 different models to decompose the time series. I will compare these 3 models to see which strips away the seasonality the best.\n\ndcmp &lt;- data %&gt;%\n  mutate(time = date) %&gt;% \n  count(time, name = \"number_of_rides\") %&gt;%\n  as_tsibble(index = time) %&gt;%\n  tsibble::fill_gaps(number_of_rides = 0) %&gt;% \n  model(STL(number_of_rides),\n        STL(number_of_rides ~ season(window = Inf)),\n        STL(number_of_rides ~ trend(window=7) + season(window='periodic'),\n            robust = TRUE))\n\ncomponents(dcmp) %&gt;% \n  glimpse()\n\nRows: 5,850\nColumns: 8\nKey: .model [3]\n: number_of_rides = trend + season_year + season_week + remainder\n$ .model          &lt;chr&gt; \"STL(number_of_rides)\", \"STL(number_of_rides)\", \"STL(n…\n$ time            &lt;date&gt; 2015-05-31, 2015-06-01, 2015-06-02, 2015-06-03, 2015-…\n$ number_of_rides &lt;dbl&gt; 480, 126, 139, 131, 213, 274, 380, 424, 124, 255, 267,…\n$ trend           &lt;dbl&gt; 249.2, 249.1, 249.1, 249.0, 248.9, 248.9, 248.8, 248.8…\n$ season_week     &lt;dbl&gt; 120.53, -87.43, -67.74, -17.65, -50.30, 16.87, 87.83, …\n$ season_year     &lt;dbl&gt; 132.54, 101.70, 103.87, 105.32, 131.12, 84.99, 20.29, …\n$ remainder       &lt;dbl&gt; -22.2488, -137.3844, -146.1889, -205.6720, -116.7592, …\n$ season_adjust   &lt;dbl&gt; 226.93, 111.73, 102.87, 43.33, 132.18, 172.13, 271.89,…\n\n\nThis code pivots the data long and plots the true number of rides per day and the estimate of the underlying trend per model. The “season_adjust” panel shows the number of rides adjusted for seasonal effects, the “trend” panel shows the underlying trend, and the “remainder” panel shows how much the seasonal adjustment missed by.\n\ncomponents(dcmp) %&gt;% \n  pivot_longer(cols = number_of_rides:season_adjust) %&gt;% \n  mutate(name = factor(name, levels = c(\"number_of_rides\", \"season_adjust\",\n                                        \"trend\", \"seasonal\",\n                                        \"season_year\", \"season_week\",\n                                        \"random\", \"remainder\"))) %&gt;% \n  filter(!is.na(value)) %&gt;% \n  filter(name == \"trend\" | name == \"season_adjust\" | name == \"number_of_rides\" | name == \"remainder\") %&gt;% \n  ggplot(aes(time, value, color = .model)) +\n  geom_point(alpha = .6, size = .6) +\n  annotate(geom = \"rect\", \n           xmin = ymd(\"2020-03-06\"), xmax = ymd(\"2020-12-31\"),\n           ymin = -Inf, ymax = Inf, \n           fill = \"red\", alpha = .1) +\n  facet_grid(name ~ .model, scales = \"free_y\", labeller = label_wrap_gen()) +\n  guides(color = FALSE)\n\n\n\n\n\n\n\n\nI am not a time series expert, but it appears that the most basic STL model STL(number_of_rides) does the best job because that model’s “trend” panel shows the least seasonality.\n\ncomponents(dcmp) %&gt;% \n  pivot_longer(cols = number_of_rides:season_adjust) %&gt;% \n  mutate(name = factor(name, levels = c(\"number_of_rides\", \"season_adjust\",\n                                        \"trend\", \"seasonal\",\n                                        \"season_year\", \"season_week\",\n                                        \"random\", \"remainder\"))) %&gt;% \n  filter(!is.na(value)) %&gt;% \n  filter(name == \"trend\" | name == \"season_adjust\" | name == \"number_of_rides\" | name == \"remainder\") %&gt;% \n  filter(.model == \"STL(number_of_rides)\") %&gt;% \n  ggplot(aes(time, value, color = .model)) +\n  geom_point(alpha = .6, size = .6, color = \"#619CFF\") +\n  annotate(geom = \"rect\", \n           xmin = ymd(\"2016-03-06\"), xmax = ymd(\"2016-03-30\"),\n           ymin = -Inf, ymax = Inf, \n           fill = \"black\", alpha = .3) +\n  annotate(geom = \"rect\", \n           xmin = ymd(\"2017-03-06\"), xmax = ymd(\"2017-03-30\"),\n           ymin = -Inf, ymax = Inf, \n           fill = \"black\", alpha = .3) +\n  annotate(geom = \"rect\", \n           xmin = ymd(\"2018-03-06\"), xmax = ymd(\"2018-03-30\"),\n           ymin = -Inf, ymax = Inf, \n           fill = \"black\", alpha = .3) +\n  annotate(geom = \"rect\", \n           xmin = ymd(\"2019-03-06\"), xmax = ymd(\"2019-03-30\"),\n           ymin = -Inf, ymax = Inf, \n           fill = \"black\", alpha = .3) +\n  annotate(geom = \"rect\", \n           xmin = ymd(\"2020-03-06\"), xmax = ymd(\"2020-03-06\") + 60,\n           ymin = -Inf, ymax = Inf, \n           fill = \"red\", alpha = .3) +\n  facet_grid(name ~ .model, scales = \"free_y\", labeller = label_wrap_gen()) +\n  guides(color = FALSE)\n\n\n\n\n\n\n\n\nFocusing on that model, it appears that the trend dropped in mid-March, but rebounded to normal levels quickly. I highlighted the data from previous Marches to see if there was a recurring dip in March."
  },
  {
    "objectID": "posts/clustering-bird-species-with-seasonality/index.html",
    "href": "posts/clustering-bird-species-with-seasonality/index.html",
    "title": "Clustering Bird Species with Seasonality",
    "section": "",
    "text": "In this post, I use k-means clustering to identify clusters of bird species based on frequency of observations per month. I use bird sightings in Allegheny County from eBird.\nLoad the relevant libraries:\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(janitor)\nlibrary(vroom)\nlibrary(broom)\nlibrary(hrbrthemes)\n\ntheme_set(theme_bw())\n\nset.seed(1234)\n\nLoad and filter the data:\n\ndf &lt;- vroom(\"post_data/ebd_US-PA-003_201001_202003_relFeb-2020.zip\", delim = \"\\t\") %&gt;% \n  clean_names() %&gt;% \n  mutate_at(vars(observer_id, locality, observation_date, time_observations_started, protocol_type), str_replace_na, \"NA\") %&gt;% \n  mutate(observation_count = as.numeric(str_replace(observation_count, \"X\", as.character(NA))),\n         observation_event_id = str_c(observer_id, locality, observation_date, time_observations_started, sep = \"-\"),\n         observation_date = ymd(observation_date)) %&gt;%\n  filter(all_species_reported == 1)\n\n\ndf_top_protocols &lt;- df %&gt;% \n  count(protocol_type, sort = TRUE) %&gt;% \n  slice(1:2)\n\ndf &lt;- df %&gt;% \n  semi_join(df_top_protocols) %&gt;% \n  filter(year(observation_date) &gt;= 2016)\n\n\ndf %&gt;% \n  select(common_name, observation_date, observation_count) %&gt;% \n  glimpse()\n\nRows: 533,493\nColumns: 3\n$ common_name       &lt;chr&gt; \"American Black Duck\", \"American Black Duck\", \"Ameri…\n$ observation_date  &lt;date&gt; 2016-01-31, 2016-01-24, 2016-01-30, 2016-01-31, 201…\n$ observation_count &lt;dbl&gt; 2, 2, 2, 3, 2, 3, 57, 7, 2, 4, 1, 5, 8, 1, 1, 4, 1, …\n\n\nThis graph shows general seasonality in bird observations:\n\ndf %&gt;% \n  count(observation_date) %&gt;% \n  ggplot(aes(observation_date, n)) +\n    geom_line() +\n    labs(x = \"Observation date\",\n         y = \"Observation events\") +\n    scale_y_comma()\n\n\n\n\n\n\n\n\nThis code chunk calculates the average number of observations by species and month. Then, it interpolates a value of 0 for birds where there were no sightings in a given month:\n\nmonths &lt;- df %&gt;% \n  mutate(observation_month = month(observation_date, label = TRUE)) %&gt;% \n  distinct(observation_month) %&gt;% \n  pull(observation_month)\n\ndf_seasonality &lt;- df %&gt;% \n  mutate(observation_month = month(observation_date, label = TRUE),\n         observation_year = year(observation_date)) %&gt;% \n  group_by(common_name, observation_year, observation_month) %&gt;% \n  summarize(observation_count = sum(observation_count, na.rm = TRUE)) %&gt;% \n  group_by(common_name, observation_month) %&gt;% \n  summarize(observation_count_mean = mean(observation_count) %&gt;% round(1)) %&gt;% \n  ungroup() %&gt;% \n  complete(common_name, observation_month = months) %&gt;% \n  replace_na(list(observation_count_mean = 0)) %&gt;% \n  arrange(common_name, observation_month)\n\nglimpse(df_seasonality)\n\nRows: 3,672\nColumns: 3\n$ common_name            &lt;chr&gt; \"Acadian Flycatcher\", \"Acadian Flycatcher\", \"Ac…\n$ observation_month      &lt;ord&gt; Jan, Feb, Mar, Apr, May, Jun, Jul, Aug, Sep, Oc…\n$ observation_count_mean &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 184.8, 98.5, 67.0, 19.0, 22…\n\n\nThis transforms the mean monthly observation into log10:\n\ndf_seasonality &lt;- df_seasonality %&gt;% \n  mutate(observation_count_mean_log10 = log10(observation_count_mean),\n         observation_count_mean_log10 = case_when(is.infinite(observation_count_mean_log10) ~ 0,\n                                                  TRUE ~ observation_count_mean_log10)) %&gt;% \n  select(-observation_count_mean)\n\nThese graphs show that observations generally increase in the spring and fall, but there is wide variation:\n\ndf_seasonality %&gt;% \n  ggplot(aes(observation_month, observation_count_mean_log10)) +\n    geom_boxplot() +\n    labs(x = \"Observation month\",\n         y = \"Mean observation count (log10)\")\n\n\n\n\n\n\n\n\nThis tile graph shows the seasonality trends per species. I sort the birds by ascending mean observation count by month. It shows there are birds that appear year-round, some that appear seasonally, and some that only appear sporadically:\n\nvec_common_name &lt;- df_seasonality %&gt;% \n  pivot_wider(names_from = observation_month, values_from = observation_count_mean_log10, names_prefix = \"month_\") %&gt;% \n  clean_names() %&gt;% \n  arrange(month_jan, month_feb, month_mar, month_apr, month_may, month_jun, month_jul, month_aug, month_sep, month_oct, month_nov, month_dec) %&gt;% \n  pull(common_name)\n\n\ndf_seasonality %&gt;%\n  mutate(common_name = factor(common_name, levels = vec_common_name)) %&gt;%\n  ggplot(aes(observation_month, common_name, fill = observation_count_mean_log10)) +\n    geom_tile() +\n    scale_fill_viridis_c(\"Mean observation count (log10)\") +\n    scale_x_discrete(expand = c(0,0)) +\n    scale_y_discrete(expand = c(0,0)) +\n    labs(x = \"Observation month\",\n         y = \"Species\") +\n    theme(panel.grid = element_blank(),\n          axis.text.y = element_blank(),\n          axis.ticks.y = element_blank())\n\n\n\n\n\n\n\n\nYou can subjectively see clusters of bird types in the above graph. I will use k-means to attempt to find those clusters.\nThis code chunk pivots the data wide to prepare it for clustering:\n\ndf_seasonality_wide &lt;- df_seasonality %&gt;% \n  select(common_name, observation_month, observation_count_mean_log10) %&gt;% \n  pivot_wider(names_from = observation_month, values_from = observation_count_mean_log10, names_prefix = \"month_\") %&gt;% \n  clean_names()\n\nglimpse(df_seasonality_wide)\n\nRows: 306\nColumns: 13\n$ common_name &lt;chr&gt; \"Acadian Flycatcher\", \"Accipiter sp.\", \"Alder Flycatcher\",…\n$ month_jan   &lt;dbl&gt; 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0…\n$ month_feb   &lt;dbl&gt; 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000…\n$ month_mar   &lt;dbl&gt; 0.0000000, 0.1760913, 0.0000000, 0.0000000, 0.0000000, 0.0…\n$ month_apr   &lt;dbl&gt; 0.0000000, 0.3010300, 0.0000000, 0.0000000, 0.0000000, 0.6…\n$ month_may   &lt;dbl&gt; 2.2667020, 0.3010300, 0.1760913, 0.4771213, 0.0000000, 0.0…\n$ month_jun   &lt;dbl&gt; 1.993436, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000…\n$ month_jul   &lt;dbl&gt; 1.8260748, 0.0000000, 0.0000000, 0.4771213, 0.0000000, 0.0…\n$ month_aug   &lt;dbl&gt; 1.2787536, 0.0000000, 0.0000000, 0.3979400, 0.9030900, 0.0…\n$ month_sep   &lt;dbl&gt; 1.3424227, 0.0000000, 0.0000000, 1.0086002, 1.8450980, 0.0…\n$ month_oct   &lt;dbl&gt; 0.0000000, 0.3010300, 0.0000000, 0.0000000, 0.0000000, 0.0…\n$ month_nov   &lt;dbl&gt; 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0…\n$ month_dec   &lt;dbl&gt; 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0…\n\n\nThis uses purrr to cluster the data with varying numbers of clusters (1 to 9):\n\nkclusts &lt;- tibble(k = 1:9) %&gt;%\n  mutate(\n    kclust = map(k, ~kmeans(df_seasonality_wide %&gt;% select(-common_name), .x)),\n    tidied = map(kclust, tidy),\n    glanced = map(kclust, glance),\n    augmented = map(kclust, augment, df_seasonality_wide %&gt;% select(-common_name))\n  )\n\nkclusts\n\n# A tibble: 9 × 5\n      k kclust   tidied            glanced          augmented          \n  &lt;int&gt; &lt;list&gt;   &lt;list&gt;            &lt;list&gt;           &lt;list&gt;             \n1     1 &lt;kmeans&gt; &lt;tibble [1 × 15]&gt; &lt;tibble [1 × 4]&gt; &lt;tibble [306 × 13]&gt;\n2     2 &lt;kmeans&gt; &lt;tibble [2 × 15]&gt; &lt;tibble [1 × 4]&gt; &lt;tibble [306 × 13]&gt;\n3     3 &lt;kmeans&gt; &lt;tibble [3 × 15]&gt; &lt;tibble [1 × 4]&gt; &lt;tibble [306 × 13]&gt;\n4     4 &lt;kmeans&gt; &lt;tibble [4 × 15]&gt; &lt;tibble [1 × 4]&gt; &lt;tibble [306 × 13]&gt;\n5     5 &lt;kmeans&gt; &lt;tibble [5 × 15]&gt; &lt;tibble [1 × 4]&gt; &lt;tibble [306 × 13]&gt;\n6     6 &lt;kmeans&gt; &lt;tibble [6 × 15]&gt; &lt;tibble [1 × 4]&gt; &lt;tibble [306 × 13]&gt;\n7     7 &lt;kmeans&gt; &lt;tibble [7 × 15]&gt; &lt;tibble [1 × 4]&gt; &lt;tibble [306 × 13]&gt;\n8     8 &lt;kmeans&gt; &lt;tibble [8 × 15]&gt; &lt;tibble [1 × 4]&gt; &lt;tibble [306 × 13]&gt;\n9     9 &lt;kmeans&gt; &lt;tibble [9 × 15]&gt; &lt;tibble [1 × 4]&gt; &lt;tibble [306 × 13]&gt;\n\n\n\nclusters &lt;- kclusts %&gt;%\n  unnest(tidied)\n\nassignments &lt;- kclusts %&gt;% \n  unnest(augmented)\n\nclusterings &lt;- kclusts %&gt;%\n  unnest(glanced)\n\nThis scree plot shows that 2 clusters is probably optimal, but 4 could also be useful:\n\nggplot(clusterings, aes(k, tot.withinss)) +\n  geom_line() +\n  geom_vline(xintercept = 2, linetype = 2) +\n  geom_vline(xintercept = 4, linetype = 2) +\n  scale_x_continuous(breaks = seq(1:9)) +\n  labs(x = \"Number of clusters\")\n\n\n\n\n\n\n\n\nThis graph shows how the clustering performed by comparing the observation value in January to the value in the other months, for each k cluster value 1 through 4:\n\nassignments %&gt;% \n  select(k, .cluster, contains(\"month_\")) %&gt;% \n  mutate(id = row_number()) %&gt;% \n  pivot_longer(cols = contains(\"month_\"), names_to = \"observation_month\", values_to = \"observation_count_mean_log10\") %&gt;% \n  mutate(month_jan = case_when(observation_month == \"month_jan\" ~ observation_count_mean_log10,\n                               TRUE ~ as.numeric(NA))) %&gt;% \n  group_by(k, .cluster, id) %&gt;% \n  fill(month_jan, .direction = c(\"down\")) %&gt;% \n  ungroup() %&gt;% \n  filter(observation_month != \"month_jan\",\n         k &lt;= 4) %&gt;% \n  mutate(k = str_c(k, \"cluster(s)\", sep = \" \")) %&gt;% \n  ggplot(aes(observation_count_mean_log10, month_jan, color = .cluster)) +\n    geom_point() +\n    facet_grid(k ~ observation_month) +\n    labs(x = \"Observation month\",\n         y = \"January\")\n\n\n\n\n\n\n\n\nSubjectively, I think the optimal number of clusters is 4. It is noiser, but could show more interesting granularity in seasonality.\nThis clusters the data using 4 clusters:\n\ndf_kmeans &lt;- df_seasonality_wide %&gt;% \n  select(-common_name) %&gt;% \n  kmeans(centers = 4)\n\n\ndf_clustered &lt;- augment(df_kmeans, df_seasonality_wide) %&gt;% \n  select(common_name, .cluster)\n\ndf_clustered\n\n# A tibble: 306 × 2\n   common_name                                   .cluster\n   &lt;chr&gt;                                         &lt;fct&gt;   \n 1 Acadian Flycatcher                            4       \n 2 Accipiter sp.                                 3       \n 3 Alder Flycatcher                              3       \n 4 Alder/Willow Flycatcher (Traill's Flycatcher) 3       \n 5 American Avocet                               3       \n 6 American Bittern                              3       \n 7 American Black Duck                           2       \n 8 American Coot                                 2       \n 9 American Crow                                 1       \n10 American Golden-Plover                        3       \n# ℹ 296 more rows\n\n\nThis shows the same style of tile graph as shown previously, but facets it by cluster.\n\nvec_common_name_cluster &lt;- df_seasonality %&gt;%\n  left_join(df_clustered) %&gt;% \n  pivot_wider(names_from = observation_month, values_from = observation_count_mean_log10, names_prefix = \"month_\") %&gt;% \n  clean_names() %&gt;% \n  arrange(cluster, month_jan, month_feb, month_mar, month_apr, month_may, month_jun, month_jul, month_aug, month_sep, month_oct, month_nov, month_dec) %&gt;% \n  pull(common_name)\n\n\ndf_seasonality_clustered &lt;-  df_seasonality %&gt;%\n  left_join(df_clustered) %&gt;% \n  mutate(common_name = factor(common_name, levels = vec_common_name_cluster))\n\ndf_seasonality_clustered %&gt;% \n  mutate(.cluster = str_c(\"Cluster\", .cluster, sep = \" \")) %&gt;% \n  ggplot(aes(observation_month, common_name, fill = observation_count_mean_log10)) +\n    geom_tile() +\n    facet_wrap(~.cluster, scales = \"free_y\") +\n    scale_fill_viridis_c(\"Mean observation count (log10)\") +\n    scale_x_discrete(expand = c(0,0)) +\n    scale_y_discrete(expand = c(0,0)) +\n    labs(x = \"Observation month\",\n         y = \"Species\") +\n    theme(panel.grid = element_blank(),\n          axis.text.y = element_blank(),\n          axis.ticks.y = element_blank())\n\n\n\n\n\n\n\n\nCluster 1 shows birds that only appear sporadically. I think these are birds that migrate through Allegheny County, but do not stick around. Cluster 2 shows birds that are generally around all year. Cluster 3 shows birds that are seen mostly during the summer, and cluster 4 contains birds that appear in the winter.\nThis shows a sample of each cluster:\n\ndf_cluster_sample &lt;- df_clustered %&gt;% \n  group_by(.cluster) %&gt;% \n  sample_n(10, replace = FALSE) %&gt;% \n  ungroup()\n\ndf_seasonality_clustered %&gt;%\n  semi_join(df_cluster_sample) %&gt;% \n  mutate(.cluster = str_c(\"Cluster\", .cluster, sep = \" \")) %&gt;% \n  ggplot(aes(observation_month, common_name, fill = observation_count_mean_log10)) +\n    geom_tile() +\n    facet_wrap(~.cluster, scales = \"free_y\") +\n    scale_fill_viridis_c(\"Mean observation count (log10)\") +\n    scale_x_discrete(expand = c(0,0)) +\n    scale_y_discrete(expand = c(0,0)) +\n    labs(x = \"Observation month\",\n         y = NULL) +\n    theme(panel.grid = element_blank())"
  },
  {
    "objectID": "posts/visualizing-transit-connections-between-pittsburgh-census-tracts/index.html",
    "href": "posts/visualizing-transit-connections-between-pittsburgh-census-tracts/index.html",
    "title": "Visualizing Transit Connections Between Pittsburgh Census Tracts",
    "section": "",
    "text": "In this post I will use transit line and stop data from the WPRDC to map connections between census tracts. I access the census data via {tidycensus}, which contains information about the commuter connections between census tracts.\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tigris)\nlibrary(janitor)\nlibrary(tidycensus)\nlibrary(leaflet)\n\noptions(tigris_use_cache = TRUE,\n        scipen = 999,\n        digits = 2)\n\nThis code loads the transit line data from the WPRDC. I create the full_route_name_id column and set the coordinate reference system to 4326.\n\n##load transit data\ntransit_lines &lt;- st_read(\"post_data/shapefiles/transit_lines/PAAC_Routes_1909.shp\") %&gt;%\n  clean_names() %&gt;%\n  mutate_at(vars(-all_of(c(\"geometry\"))), as.character) %&gt;%\n  rename(route_id = route,\n         service_type = type_serv) %&gt;% \n  distinct(service_type, route_id, route_name, geometry) %&gt;%\n  mutate(full_route_name_id = str_c(route_id, route_name, sep = \" \")) %&gt;% \n  st_transform(3488)\n\nReading layer `PAAC_Routes_1909' from data source \n  `/Users/conorotompkins/Documents/github_repos/ctompkins_quarto_blog/posts/visualizing-transit-connections-between-pittsburgh-census-tracts/post_data/shapefiles/transit_lines/PAAC_Routes_1909.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 102 features and 13 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 1300000 ymin: 350000 xmax: 1400000 ymax: 490000\nProjected CRS: NAD83(2011) / Pennsylvania South (ftUS)\n\ntransit_lines\n\nSimple feature collection with 102 features and 4 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 3300000 ymin: 960000 xmax: 3300000 ymax: 1000000\nProjected CRS: NAD83(NSRS2007) / California Albers\nFirst 10 features:\n   service_type route_id               route_name\n1         Local       26                Chartiers\n2         Local       27                Fairywood\n3         Local       40           Mt. Washington\n4  Key Corridor      61C   McKeesport - Homestead\n5       Express       65            Squirrel Hill\n6  Key Corridor      71A                   Negley\n7  Key Corridor      71D                 Hamilton\n8         Local       74 Homewood - Squirrel Hill\n9         Local       83             Bedford Hill\n10        Local       89         Garfield Commons\n                         geometry          full_route_name_id\n1  MULTILINESTRING ((3293319 9...                26 Chartiers\n2  MULTILINESTRING ((3293319 9...                27 Fairywood\n3  MULTILINESTRING ((3294223 9...           40 Mt. Washington\n4  MULTILINESTRING ((3293614 9...  61C McKeesport - Homestead\n5  MULTILINESTRING ((3300806 9...            65 Squirrel Hill\n6  MULTILINESTRING ((3293614 9...                  71A Negley\n7  MULTILINESTRING ((3293614 9...                71D Hamilton\n8  MULTILINESTRING ((3298497 9... 74 Homewood - Squirrel Hill\n9  MULTILINESTRING ((3293794 9...             83 Bedford Hill\n10 MULTILINESTRING ((3298392 9...         89 Garfield Commons\n\n\nThis is what the transit lines look like on a basic map:\n\ntransit_lines %&gt;% \n  ggplot(aes(color = route_id)) + \n    geom_sf() +\n    guides(color = FALSE) +\n    theme_minimal()\n\n\n\n\n\n\n\n\nThis creates a table of route_id and service_type that I will join against later.\n\ndf_service_type &lt;- transit_lines %&gt;% \n  distinct(service_type, route_id, full_route_name_id) %&gt;% \n  st_drop_geometry()\n\nThis code loads the transit stop shapefile from the WPRDC:\n\ntransit_stops &lt;- st_read(\"post_data/shapefiles/transit_stops/PAAC_Stops_1909.shp\") %&gt;%\n  st_transform(3488) %&gt;% \n  clean_names() %&gt;% \n  mutate_at(vars(-all_of(c(\"geometry\", \"routes_cou\"))), as.character) %&gt;%\n  select(stop_name, routes_served = routes_ser, routes_cou, geometry) %&gt;% \n  distinct(stop_name, routes_served = routes_served, routes_cou, geometry)\n\nReading layer `PAAC_Stops_1909' from data source \n  `/Users/conorotompkins/Documents/github_repos/ctompkins_quarto_blog/posts/visualizing-transit-connections-between-pittsburgh-census-tracts/post_data/shapefiles/transit_stops/PAAC_Stops_1909.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 6946 features and 17 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 1300000 ymin: 350000 xmax: 1400000 ymax: 490000\nProjected CRS: NAD83(2011) / Pennsylvania South (ftUS)\n\ntransit_stops\n\nSimple feature collection with 6946 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3300000 ymin: 960000 xmax: 3300000 ymax: 1000000\nProjected CRS: NAD83(NSRS2007) / California Albers\nFirst 10 features:\n                             stop_name routes_served routes_cou\n1  26TH ST AT PENN AVE FS (SPRING WAY)    54, 88, 91          3\n2               28TH ST AT LIBERTY AVE            54          1\n3                32ND ST AT SPRING WAY    54, 88, 91          3\n4                 40TH ST AT BUTLER ST            93          1\n5            40TH ST AT DAVIDSON ST FS            93          1\n6              40TH ST OPP DAVIDSON ST        64, 93          2\n7                  4TH ST AT COREY AVE            59          1\n8               FIFTH AVE AT AIKEN AVE 28X, 71B, 71D          3\n9            FIFTH AVE AT AMBERSON AVE 28X, 71B, 71D          3\n10         FIFTH AVE AT BEECHWOOD BLVD      28X, 71D          2\n                 geometry\n1  POINT (3294762 976882)\n2  POINT (3294959 977172)\n3  POINT (3295178 977765)\n4  POINT (3295366 978861)\n5  POINT (3295502 978803)\n6  POINT (3295495 978788)\n7  POINT (3305461 976046)\n8  POINT (3298449 977796)\n9  POINT (3298126 977556)\n10 POINT (3299655 978942)\n\n\nMuch of the important data is stored in the routes_served column. This code pivots the data longer to make it easier to work with.\n\n#identify maximum number of routes served by a stop\nmax_routes_served &lt;- transit_stops %&gt;% \n  summarize(max_routes = max(routes_cou)) %&gt;% \n  pull(max_routes)\n\ntransit_stops %&gt;% \n  filter(routes_cou == max_routes_served)\n\nSimple feature collection with 1 feature and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3300000 ymin: 980000 xmax: 3300000 ymax: 980000\nProjected CRS: NAD83(NSRS2007) / California Albers\n                      stop_name\n1 EAST BUSWAY AT PENN STATION C\n                                                                                routes_served\n1 1, 11, 15, 19L, 39, 40, 44, 6, P1, P10, P12, P16, P17, P2, P67, P68, P69, P7, P71, P76, P78\n  routes_cou               geometry\n1         21 POINT (3294168 975311)\n\n\n\n#separate routes_served into multiple columns, one per route\ntransit_stops &lt;- transit_stops %&gt;% \n  separate(routes_served, sep = \", \", into = str_c(\"route_\", 1:max_routes_served), extra = \"merge\", fill = \"right\")\n\ntransit_stops\n\nSimple feature collection with 6946 features and 23 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3300000 ymin: 960000 xmax: 3300000 ymax: 1000000\nProjected CRS: NAD83(NSRS2007) / California Albers\nFirst 10 features:\n                             stop_name route_1 route_2 route_3 route_4 route_5\n1  26TH ST AT PENN AVE FS (SPRING WAY)      54      88      91    &lt;NA&gt;    &lt;NA&gt;\n2               28TH ST AT LIBERTY AVE      54    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;\n3                32ND ST AT SPRING WAY      54      88      91    &lt;NA&gt;    &lt;NA&gt;\n4                 40TH ST AT BUTLER ST      93    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;\n5            40TH ST AT DAVIDSON ST FS      93    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;\n6              40TH ST OPP DAVIDSON ST      64      93    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;\n7                  4TH ST AT COREY AVE      59    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;\n8               FIFTH AVE AT AIKEN AVE     28X     71B     71D    &lt;NA&gt;    &lt;NA&gt;\n9            FIFTH AVE AT AMBERSON AVE     28X     71B     71D    &lt;NA&gt;    &lt;NA&gt;\n10         FIFTH AVE AT BEECHWOOD BLVD     28X     71D    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;\n   route_6 route_7 route_8 route_9 route_10 route_11 route_12 route_13 route_14\n1     &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;\n2     &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;\n3     &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;\n4     &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;\n5     &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;\n6     &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;\n7     &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;\n8     &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;\n9     &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;\n10    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;\n   route_15 route_16 route_17 route_18 route_19 route_20 route_21 routes_cou\n1      &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;          3\n2      &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;          1\n3      &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;          3\n4      &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;          1\n5      &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;          1\n6      &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;          2\n7      &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;          1\n8      &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;          3\n9      &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;          3\n10     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;          2\n                 geometry\n1  POINT (3294762 976882)\n2  POINT (3294959 977172)\n3  POINT (3295178 977765)\n4  POINT (3295366 978861)\n5  POINT (3295502 978803)\n6  POINT (3295495 978788)\n7  POINT (3305461 976046)\n8  POINT (3298449 977796)\n9  POINT (3298126 977556)\n10 POINT (3299655 978942)\n\n\n\ntransit_stops %&gt;% \n  filter(routes_cou == max_routes_served)\n\nSimple feature collection with 1 feature and 23 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3300000 ymin: 980000 xmax: 3300000 ymax: 980000\nProjected CRS: NAD83(NSRS2007) / California Albers\n                      stop_name route_1 route_2 route_3 route_4 route_5 route_6\n1 EAST BUSWAY AT PENN STATION C       1      11      15     19L      39      40\n  route_7 route_8 route_9 route_10 route_11 route_12 route_13 route_14 route_15\n1      44       6      P1      P10      P12      P16      P17       P2      P67\n  route_16 route_17 route_18 route_19 route_20 route_21 routes_cou\n1      P68      P69       P7      P71      P76      P78         21\n                geometry\n1 POINT (3294168 975311)\n\n#pivot data longer\ntransit_stops &lt;- transit_stops %&gt;% \n  pivot_longer(cols = starts_with(\"route_\"), names_to = \"route_number\", values_to = \"route_id\") %&gt;% \n  st_as_sf()\n  \ntransit_stops\n\nSimple feature collection with 145866 features and 4 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3300000 ymin: 960000 xmax: 3300000 ymax: 1000000\nProjected CRS: NAD83(NSRS2007) / California Albers\n# A tibble: 145,866 × 5\n   stop_name          routes_cou         geometry route_number route_id\n   &lt;chr&gt;                   &lt;dbl&gt;      &lt;POINT [m]&gt; &lt;chr&gt;        &lt;chr&gt;   \n 1 26TH ST AT PENN A…          3 (3294762 976882) route_1      54      \n 2 26TH ST AT PENN A…          3 (3294762 976882) route_2      88      \n 3 26TH ST AT PENN A…          3 (3294762 976882) route_3      91      \n 4 26TH ST AT PENN A…          3 (3294762 976882) route_4      &lt;NA&gt;    \n 5 26TH ST AT PENN A…          3 (3294762 976882) route_5      &lt;NA&gt;    \n 6 26TH ST AT PENN A…          3 (3294762 976882) route_6      &lt;NA&gt;    \n 7 26TH ST AT PENN A…          3 (3294762 976882) route_7      &lt;NA&gt;    \n 8 26TH ST AT PENN A…          3 (3294762 976882) route_8      &lt;NA&gt;    \n 9 26TH ST AT PENN A…          3 (3294762 976882) route_9      &lt;NA&gt;    \n10 26TH ST AT PENN A…          3 (3294762 976882) route_10     &lt;NA&gt;    \n# ℹ 145,856 more rows\n\n\n\ntransit_stops &lt;- transit_stops %&gt;%\n  filter(!is.na(route_id)) %&gt;% \n  left_join(df_service_type)\n\nThis code loads the census tract data via {tidycensus}. I choose the census tracts 42003020100 (Downtown) and 42003070300 (Shadyside).\n\n#load tract data\nallegheny_tracts &lt;- get_decennial(geography = \"tract\",\n                                  variables = c(total_pop = \"P001001\"),\n                                  state = \"PA\",\n                                  county = \"Allegheny County\",\n                                  geometry = TRUE,\n                                  output = \"wide\",\n                                  year = 2010) %&gt;%\n  mutate(name = case_when(GEOID == \"42003020100\" ~ \"Downtown\",\n                          GEOID == \"42003070300\" ~ \"Shadyside\")) %&gt;% \n  st_transform(3488)\n\n#calculate centers of the tracts\nallegheny_tracts_centroid &lt;- allegheny_tracts %&gt;%\n  mutate(name = case_when(GEOID == \"42003020100\" ~ \"Downtown\",\n                          GEOID == \"42003070300\" ~ \"Shadyside\")) %&gt;% \n  mutate(lon = map_dbl(geometry, ~st_centroid(.x)[[1]]),\n         lat = map_dbl(geometry, ~st_centroid(.x)[[2]])) %&gt;% \n  st_drop_geometry() %&gt;% \n  st_as_sf(coords = c(\"lon\", \"lat\"), crs = 3488) %&gt;% \n  st_transform(3488)\n\n#creates table with geometry of the county border\nallegheny &lt;- allegheny_tracts %&gt;% \n  summarize()\n\n\ncommute_tracts &lt;- allegheny_tracts %&gt;% \n  filter(!is.na(name))\n\ncommute_centroids &lt;- allegheny_tracts_centroid %&gt;% \n  filter(!is.na(name))\n\nThis code uses st_is_within_distance to find the transit stops that are within 700 meters of the center of the Downtown AND Shadyside census tracts.\n\ndf_stops_joined_distance &lt;- transit_stops %&gt;% \n  st_join(commute_centroids, st_is_within_distance, dist = 700, left = TRUE) %&gt;% \n  arrange(route_id)\n\ndf_stops_joined_distance\n\nSimple feature collection with 12087 features and 10 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3300000 ymin: 960000 xmax: 3300000 ymax: 1000000\nProjected CRS: NAD83(NSRS2007) / California Albers\n# A tibble: 12,087 × 11\n   stop_name          routes_cou         geometry route_number route_id\n   &lt;chr&gt;                   &lt;dbl&gt;      &lt;POINT [m]&gt; &lt;chr&gt;        &lt;chr&gt;   \n 1 WEST MIFFLIN GARA…          4 (3302630 969221) route_2      \"\"      \n 2 LAYOVER PENN PARK…         12 (3294348 975649) route_2      \"\"      \n 3 LAYOVER PENN PARK…         12 (3294348 975649) route_6      \"\"      \n 4 LAYOVER PENN PARK…         12 (3294348 975649) route_8      \"\"      \n 5 LAYOVER PENN PARK…         12 (3294348 975649) route_11     \"\"      \n 6 4TH AVE AT 7TH ST           1 (3306225 995240) route_1      \"1\"     \n 7 E 4TH AVE AT BOYD…          2 (3305254 999390) route_1      \"1\"     \n 8 4TH AVE AT CENTRA…          1 (3306153 995343) route_1      \"1\"     \n 9 E 4TH AVE AT LOCK…          2 (3305377 999550) route_1      \"1\"     \n10 E 4TH AVE AT WOOD…          2 (3305323 999479) route_1      \"1\"     \n# ℹ 12,077 more rows\n# ℹ 6 more variables: service_type &lt;chr&gt;, full_route_name_id &lt;chr&gt;,\n#   GEOID &lt;chr&gt;, NAME &lt;chr&gt;, total_pop &lt;dbl&gt;, name &lt;chr&gt;\n\ndf_stops_joined_distance &lt;- df_stops_joined_distance %&gt;% \n  filter(!is.na(name))\n\ndf_stops_joined_distance\n\nSimple feature collection with 736 features and 10 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3300000 ymin: 970000 xmax: 3300000 ymax: 980000\nProjected CRS: NAD83(NSRS2007) / California Albers\n# A tibble: 736 × 11\n   stop_name          routes_cou         geometry route_number route_id\n * &lt;chr&gt;                   &lt;dbl&gt;      &lt;POINT [m]&gt; &lt;chr&gt;        &lt;chr&gt;   \n 1 7TH ST AT FT DUQU…          9 (3293481 975066) route_1      1       \n 2 7TH ST AT PENN AV…          4 (3293586 974931) route_1      1       \n 3 LIBERTY AVE AT 7T…          8 (3293638 974904) route_1      1       \n 4 LIBERTY AVE AT SM…         19 (3293846 975073) route_1      1       \n 5 LIBERTY AVE AT WI…         18 (3293943 975161) route_1      1       \n 6 LIBERTY AVE OPP 9…         20 (3293761 974994) route_1      1       \n 7 LIBERTY AVE OPP S…          8 (3293856 975107) route_1      1       \n 8 7TH ST AT FT DUQU…          9 (3293481 975066) route_2      11      \n 9 7TH ST AT PENN AV…          4 (3293586 974931) route_2      11      \n10 LIBERTY AVE AT 7T…          8 (3293638 974904) route_2      11      \n# ℹ 726 more rows\n# ℹ 6 more variables: service_type &lt;chr&gt;, full_route_name_id &lt;chr&gt;,\n#   GEOID &lt;chr&gt;, NAME &lt;chr&gt;, total_pop &lt;dbl&gt;, name &lt;chr&gt;\n\ndf_route_filter &lt;- df_stops_joined_distance %&gt;% \n  st_drop_geometry() %&gt;% \n  distinct(route_id, name) %&gt;% \n  group_by(route_id) %&gt;% \n  filter(n() &gt;= 2) %&gt;% \n  ungroup() %&gt;% \n  distinct(route_id)\n\ndf_route_filter\n\n# A tibble: 22 × 1\n   route_id\n   &lt;chr&gt;   \n 1 28X     \n 2 67      \n 3 69      \n 4 71A     \n 5 71B     \n 6 71C     \n 7 71D     \n 8 82      \n 9 86      \n10 P1      \n# ℹ 12 more rows\n\ndf_stops_joined_distance &lt;- df_stops_joined_distance %&gt;% \n  semi_join(df_route_filter, by = c(\"route_id\" = \"route_id\")) %&gt;% \n  left_join(df_service_type)\n\ndf_stops_joined_distance &lt;- df_stops_joined_distance %&gt;% \n  mutate(stop_name_route_id_route_name = str_c(full_route_name_id, str_to_title(stop_name), sep = \" - \"))\n\nTo help visualize how the join works, imagine a buffer around each point (transit stop). This shows the stops from 71A with a buffer:\n\nst_crs(transit_stops)$units\n\n[1] \"m\"\n\ntransit_stops %&gt;% \n  mutate(geometry_buffered = st_buffer(geometry, dist = 700)) %&gt;% \n  #st_transform(crs = 3488) %&gt;% \n  filter(route_id == \"71A\") %&gt;% \n  ggplot() +\n    geom_sf(data = commute_centroids,\n            color = \"blue\",\n            size = 3) +\n    geom_sf(color = \"red\", size = 1) +\n    geom_sf(aes(geometry = geometry_buffered),\n            fill = NA\n            ) +\n    #coord_sf(crs = 4326) +\n    theme_minimal()\n\n\n\n\n\n\n\n\nst_is_within_distance identifies which red points are within 700 meters of the blue points.\nThis shows the tracts, the tract centroids, and the transit stops within 700 meters of the centroids.\n\ndf_stops_joined_distance %&gt;% \n  ggplot() +\n    geom_sf(data = commute_tracts) +\n    geom_sf(data = commute_centroids, color = \"red\") +\n    geom_sf() +\n    theme_minimal()\n\n\n\n\n\n\n\n\nThis filters on the transit lines served by the stops that successfully joined against the Downtown and Shadyside centroids:\n\ncommuter_transit_lines &lt;- transit_lines %&gt;% \n  semi_join(df_route_filter, by = c(\"route_id\" = \"route_id\"))\n\ncommuter_transit_lines\n\nSimple feature collection with 22 features and 4 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 3300000 ymin: 970000 xmax: 3300000 ymax: 1000000\nProjected CRS: NAD83(NSRS2007) / California Albers\nFirst 10 features:\n   service_type route_id              route_name         full_route_name_id\n1  Key Corridor      71A                  Negley                 71A Negley\n2  Key Corridor      71D                Hamilton               71D Hamilton\n3         Rapid       P2       East Busway Short       P2 East Busway Short\n4       Express      P78           Oakmont Flyer          P78 Oakmont Flyer\n5         Rapid       P1 East Busway - All Stops P1 East Busway - All Stops\n6  Key Corridor      71B           Highland Park          71B Highland Park\n7         Local       69                Trafford                69 Trafford\n8         Local       86                 Liberty                 86 Liberty\n9       Express      P68    Braddock Hills Flyer   P68 Braddock Hills Flyer\n10      Express      P69          Trafford Flyer         P69 Trafford Flyer\n                         geometry\n1  MULTILINESTRING ((3293614 9...\n2  MULTILINESTRING ((3293614 9...\n3  MULTILINESTRING ((3293856 9...\n4  MULTILINESTRING ((3294045 9...\n5  MULTILINESTRING ((3294045 9...\n6  MULTILINESTRING ((3295547 9...\n7  MULTILINESTRING ((3310878 9...\n8  MULTILINESTRING ((3294134 9...\n9  MULTILINESTRING ((3294049 9...\n10 MULTILINESTRING ((3294049 9...\n\n\nThese are the transit lines that serve the two tracts:\n\ncommute_centroids %&gt;% \n  ggplot() +\n    geom_sf(size = 3) +\n    geom_sf(data = commuter_transit_lines, aes(color = route_id)) +\n    theme_void()\n\n\n\n\n\n\n\n\nThis sets the bounding box for the final static map:\n\ncommute_zoom &lt;- commute_tracts %&gt;% \n  st_buffer(dist = 700) %&gt;% \n  st_bbox()\n\nThis crops the transit lines to only include the parts within the bounding box:\n\ncommuter_transit_lines %&gt;% \n  st_crop(commute_zoom)\n\nSimple feature collection with 22 features and 4 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 3300000 ymin: 970000 xmax: 3300000 ymax: 980000\nProjected CRS: NAD83(NSRS2007) / California Albers\nFirst 10 features:\n   service_type route_id              route_name         full_route_name_id\n1  Key Corridor      71A                  Negley                 71A Negley\n2  Key Corridor      71D                Hamilton               71D Hamilton\n3         Rapid       P2       East Busway Short       P2 East Busway Short\n4       Express      P78           Oakmont Flyer          P78 Oakmont Flyer\n5         Rapid       P1 East Busway - All Stops P1 East Busway - All Stops\n6  Key Corridor      71B           Highland Park          71B Highland Park\n7         Local       69                Trafford                69 Trafford\n8         Local       86                 Liberty                 86 Liberty\n9       Express      P68    Braddock Hills Flyer   P68 Braddock Hills Flyer\n10      Express      P69          Trafford Flyer         P69 Trafford Flyer\n                         geometry\n1  MULTILINESTRING ((3293614 9...\n2  MULTILINESTRING ((3293614 9...\n3  MULTILINESTRING ((3293856 9...\n4  LINESTRING (3294045 975240,...\n5  MULTILINESTRING ((3294045 9...\n6  MULTILINESTRING ((3295547 9...\n7  MULTILINESTRING ((3299435 9...\n8  MULTILINESTRING ((3294134 9...\n9  MULTILINESTRING ((3294049 9...\n10 MULTILINESTRING ((3294049 9...\n\n\nThis plots the Downtown and Shadyside census tracts and the transit lines and stops that serve them:\n\np &lt;- commuter_transit_lines %&gt;% \n  st_crop(commute_zoom) %&gt;% \n  ggplot() +\n    geom_sf(data = allegheny, fill = NA) +\n    geom_sf(data = commute_tracts, aes(fill = name), size = 1, alpha = .5) +\n    geom_sf_label(data = commute_centroids, aes(label = name)) +\n    geom_sf(aes(color = route_id)) +\n    geom_sf(data = st_jitter(df_stops_joined_distance), aes(color = route_id), shape = 21, size = 3) +\n    geom_sf_label(aes(color = route_id, label = route_id)) +\n    coord_sf(xlim = c(commute_zoom[1], commute_zoom[3]),\n             ylim = c(commute_zoom[2], commute_zoom[4])) +\n    facet_wrap(~service_type) +\n    guides(color = FALSE,\n           fill = FALSE) +\n    theme_void() +\n    theme(panel.border = element_rect(color = \"black\", fill = NA))\n\np\n\n\n\n\n\n\n\n\nYou can use this interactive map made with leaflet to explore the transit lines and stops that connect Downtown and Shadyside:\n\nlibrary(widgetframe)\n#transform geometries to crs 4326\n\nallegheny &lt;- st_transform(allegheny, crs = 4326)\ncommute_tracts &lt;- st_transform(commute_tracts, crs = 4326)\ncommuter_transit_lines &lt;- st_transform(commuter_transit_lines, crs = 4326)\ndf_stops_joined_distance &lt;- st_transform(df_stops_joined_distance, crs = 4326)\ncommute_tracts &lt;- st_transform(commute_tracts, crs = 4326)\n\ncommute_zoom &lt;- commute_tracts %&gt;% \n  st_buffer(dist = .01) %&gt;% \n  st_bbox()\n\nnames(commute_zoom) &lt;- NULL\n\n###leaflet\ntransit_lines_palette &lt;- colorFactor(palette = \"Set1\", domain = commuter_transit_lines$full_route_name_id)\ntract_palette &lt;- colorFactor(palette = \"Set1\", domain = commute_tracts$GEOID)\n\ninteractive_map &lt;- leaflet() %&gt;% \n  addProviderTiles(providers$CartoDB.Positron) %&gt;% \n  addPolygons(data = allegheny,\n              color = \"#444444\",\n              stroke = TRUE,\n              fillOpacity = 0,\n              opacity = 1,\n              weight = 2,\n              group = \"Census tracts\") %&gt;%\n  addPolygons(data = commute_tracts,\n              #color\n              color = NA,\n              #fill\n              fillColor = ~tract_palette(GEOID),\n              fillOpacity = .3,\n              \n              #label\n              label = commute_tracts$name,\n              group = \"Census tracts\") %&gt;% \n  addPolylines(data = commuter_transit_lines,\n              color = ~transit_lines_palette(full_route_name_id),\n              label = commuter_transit_lines$full_route_name_id,\n              \n              #highlight\n              highlightOptions = highlightOptions(weight = 10, bringToFront = TRUE),\n              group = \"Transit lines and stops\"\n               ) %&gt;% \n  addCircles(data = df_stops_joined_distance,\n             radius = 3,\n             color = ~transit_lines_palette(full_route_name_id),\n             \n             #highlight\n             highlightOptions = highlightOptions(weight = 10, bringToFront = TRUE),\n             \n             #label\n             label = str_to_title(df_stops_joined_distance$stop_name_route_id_route_name),\n             group = \"Transit lines and stops\") %&gt;% \n  addLayersControl(overlayGroups = c(\"Census tracts\",\"Transit lines and stops\"), position = \"topleft\", \n                   options = layersControlOptions(collapsed = FALSE)) %&gt;% \n  addMiniMap() %&gt;% \n  fitBounds(lng1 = commute_zoom[[1]], lat1 = commute_zoom[[2]], lng2 = commute_zoom[[3]], lat2 = commute_zoom[[4]])\n\nframeWidget(interactive_map)"
  },
  {
    "objectID": "posts/exploring-311-data-with-pca/index.html",
    "href": "posts/exploring-311-data-with-pca/index.html",
    "title": "Exploring 311 Data With PCA",
    "section": "",
    "text": "Principal Component Analysis is an unsupervised method that reduces the number of dimensions in a dataset and highlights where the data varies. We will use PCA to analyze the 311 dataset from the WPRDC.\n\n\n\n\n\ninstall.packages(c(\"tidyverse\", \"lubridate\", \"broom\", \"ggfortify\", \"ggrepel\", \"janitor\"))\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(broom)\nlibrary(ggfortify)\nlibrary(ggrepel)\nlibrary(janitor)\n\noptions(scipen = 999, digits = 4)\nset.seed(1234)\n\ntheme_set(theme_bw())\n\n\n\n\n\n\nread_csv(\"https://raw.githubusercontent.com/conorotompkins/pittsburgh_311/master/data/pittsburgh_311.csv\", progress = FALSE) %&gt;% \n  clean_names() %&gt;% \n  mutate(date = ymd(str_sub(created_on, 1, 10)),\n         month = month(date, label = TRUE)) %&gt;% \n  filter(date &lt; \"2018-07-19\") -&gt; df\n\n\n\n\nCreate a dataframe of the top request types\n\n(df %&gt;% \n  count(request_type, sort = TRUE) %&gt;% \n  filter(n &gt; 400)-&gt; df_top_requests)\n\n# A tibble: 84 × 2\n   request_type                             n\n   &lt;chr&gt;                                &lt;int&gt;\n 1 Potholes                             25202\n 2 Weeds/Debris                         16503\n 3 Building Maintenance                 10469\n 4 Snow/Ice removal                      7006\n 5 Refuse Violations                     6515\n 6 Abandoned Vehicle (parked on street)  5877\n 7 Missed Pick Up                        4689\n 8 Replace/Repair a Sign                 4445\n 9 Building Without a Permit             4404\n10 Litter                                4198\n# ℹ 74 more rows\n\n\nCount the number of requests per month by request type, filter for the top request types, and fill in gaps in the data\n\n(df %&gt;%\n  semi_join(df_top_requests) %&gt;% \n  group_by(request_type, month) %&gt;% \n  summarize(n = n()) %&gt;% \n  ungroup() %&gt;%\n  complete(request_type, month) %&gt;% \n  replace_na(replace = list(n = 0)) -&gt; df_months)\n\n# A tibble: 1,008 × 3\n   request_type                         month     n\n   &lt;chr&gt;                                &lt;ord&gt; &lt;int&gt;\n 1 Abandoned Vehicle (parked on street) Jan     523\n 2 Abandoned Vehicle (parked on street) Feb     427\n 3 Abandoned Vehicle (parked on street) Mar     452\n 4 Abandoned Vehicle (parked on street) Apr     417\n 5 Abandoned Vehicle (parked on street) May     488\n 6 Abandoned Vehicle (parked on street) Jun     466\n 7 Abandoned Vehicle (parked on street) Jul     457\n 8 Abandoned Vehicle (parked on street) Aug     596\n 9 Abandoned Vehicle (parked on street) Sep     525\n10 Abandoned Vehicle (parked on street) Oct     571\n# ℹ 998 more rows\n\n\nCalculate the percentage of a request type for each month\n\n(df_months %&gt;% \n  group_by(request_type) %&gt;% \n  mutate(request_type_total = sum(n),\n         month_percentage = n / request_type_total) -&gt; df_months)\n\n# A tibble: 1,008 × 5\n# Groups:   request_type [84]\n   request_type                  month     n request_type_total month_percentage\n   &lt;chr&gt;                         &lt;ord&gt; &lt;int&gt;              &lt;int&gt;            &lt;dbl&gt;\n 1 Abandoned Vehicle (parked on… Jan     523               5877           0.0890\n 2 Abandoned Vehicle (parked on… Feb     427               5877           0.0727\n 3 Abandoned Vehicle (parked on… Mar     452               5877           0.0769\n 4 Abandoned Vehicle (parked on… Apr     417               5877           0.0710\n 5 Abandoned Vehicle (parked on… May     488               5877           0.0830\n 6 Abandoned Vehicle (parked on… Jun     466               5877           0.0793\n 7 Abandoned Vehicle (parked on… Jul     457               5877           0.0778\n 8 Abandoned Vehicle (parked on… Aug     596               5877           0.101 \n 9 Abandoned Vehicle (parked on… Sep     525               5877           0.0893\n10 Abandoned Vehicle (parked on… Oct     571               5877           0.0972\n# ℹ 998 more rows\n\n\nCheck for bad data\n\ndf_months %&gt;% \n  filter(is.na(month_percentage) | is.nan(month_percentage))\n\n# A tibble: 0 × 5\n# Groups:   request_type [0]\n# ℹ 5 variables: request_type &lt;chr&gt;, month &lt;ord&gt;, n &lt;int&gt;,\n#   request_type_total &lt;int&gt;, month_percentage &lt;dbl&gt;\n\n\nSpread the data to turn the months into the columns\n\n(df_months %&gt;% \n  select(request_type, month, month_percentage) %&gt;% \n  spread(month, month_percentage) %&gt;% \n  ungroup() -&gt; df_months)\n\n# A tibble: 84 × 13\n   request_type     Jan     Feb    Mar    Apr    May    Jun    Jul    Aug    Sep\n   &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 Abandoned V… 0.0890  0.0727  0.0769 0.0710 0.0830 0.0793 0.0778 0.101  0.0893\n 2 Barking Dog  0.0563  0.0608  0.0608 0.0631 0.104  0.101  0.0788 0.113  0.124 \n 3 Board Up (P… 0.0395  0.0482  0.0658 0.0943 0.114  0.0899 0.110  0.123  0.0855\n 4 Broken Side… 0.0337  0.155   0.148  0.0872 0.105  0.0964 0.0696 0.0735 0.0528\n 5 Building Ma… 0.0708  0.0919  0.103  0.0739 0.0842 0.0829 0.0725 0.0919 0.0776\n 6 Building Wi… 0.0842  0.0697  0.0636 0.0577 0.105  0.0883 0.0924 0.0815 0.0829\n 7 Catch Basin… 0.0636  0.0377  0.0778 0.0748 0.0984 0.132  0.0825 0.127  0.105 \n 8 City Source… 0.00527 0.00246 0.0105 0.0428 0.196  0.213  0.195  0.164  0.0808\n 9 City Steps,… 0.0443  0.0180  0.0148 0.0197 0.116  0.216  0.203  0.146  0.118 \n10 City Steps,… 0.0265  0.0305  0.0713 0.0509 0.128  0.120  0.136  0.128  0.108 \n# ℹ 74 more rows\n# ℹ 3 more variables: Oct &lt;dbl&gt;, Nov &lt;dbl&gt;, Dec &lt;dbl&gt;\n\n\nCheck that they all add up to 1 across the rows\n\n(df_months %&gt;% \n  select(Jan:Dec) %&gt;% \n  mutate(row_sum = rowSums(.)) %&gt;% \n  select(row_sum, everything()) -&gt; test)\n\n# A tibble: 84 × 13\n   row_sum     Jan     Feb    Mar    Apr    May    Jun    Jul    Aug    Sep\n     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1       1 0.0890  0.0727  0.0769 0.0710 0.0830 0.0793 0.0778 0.101  0.0893\n 2       1 0.0563  0.0608  0.0608 0.0631 0.104  0.101  0.0788 0.113  0.124 \n 3       1 0.0395  0.0482  0.0658 0.0943 0.114  0.0899 0.110  0.123  0.0855\n 4       1 0.0337  0.155   0.148  0.0872 0.105  0.0964 0.0696 0.0735 0.0528\n 5       1 0.0708  0.0919  0.103  0.0739 0.0842 0.0829 0.0725 0.0919 0.0776\n 6       1 0.0842  0.0697  0.0636 0.0577 0.105  0.0883 0.0924 0.0815 0.0829\n 7       1 0.0636  0.0377  0.0778 0.0748 0.0984 0.132  0.0825 0.127  0.105 \n 8       1 0.00527 0.00246 0.0105 0.0428 0.196  0.213  0.195  0.164  0.0808\n 9       1 0.0443  0.0180  0.0148 0.0197 0.116  0.216  0.203  0.146  0.118 \n10       1 0.0265  0.0305  0.0713 0.0509 0.128  0.120  0.136  0.128  0.108 \n# ℹ 74 more rows\n# ℹ 3 more variables: Oct &lt;dbl&gt;, Nov &lt;dbl&gt;, Dec &lt;dbl&gt;\n\n\n\n\n\ndf_months %&gt;% \n  ggplot(aes(Jan, Jul)) +\n  geom_point()\n\n\n\n\n\n\n\n\nRemember that each dot represents a request type, and the month shows what % of that request type occurred that month\n\ndf_months %&gt;% \n  ggplot(aes(Apr, Oct)) +\n  geom_point()\n\n\n\n\n\n\n\n\nIt is not feasible to plot all the months against each other. PCA can help by condensing the columns and increasing the variance. PCA creates eigenvectors that represents the data in a concentrated way. Eigenvectors and eigenvalues do not represent observed data. They are calculated representations of the data. We will refer to eigenvectors as “principal components”.\nIn this case, where our data is measured by months in a year, each principal component could loosely be compared to a season.\n\n\n\n\nThe PCA function requires an all-numeric dataframe, so drop the request types into the dataframe metadata\n\n(df_months %&gt;% \n  ungroup() %&gt;% \n  remove_rownames() %&gt;% \n  column_to_rownames(var = \"request_type\") -&gt; df_months_pca1)\n\n                                                 Jan      Feb      Mar      Apr\nAbandoned Vehicle (parked on street)        0.088991 0.072656 0.076910 0.070955\nBarking Dog                                 0.056306 0.060811 0.060811 0.063063\nBoard Up (PLI referral to DPW)              0.039474 0.048246 0.065789 0.094298\nBroken Sidewalk                             0.033665 0.154552 0.147666 0.087223\nBuilding Maintenance                        0.070780 0.091890 0.103353 0.073933\nBuilding Without a Permit                   0.084242 0.069709 0.063579 0.057675\nCatch Basin, Clogged                        0.063642 0.037714 0.077784 0.074838\nCity Source (CDBG)                          0.005267 0.002458 0.010534 0.042837\nCity Steps, Need Cleared                    0.044262 0.018033 0.014754 0.019672\nCity Steps, Need Repaired                   0.026477 0.030550 0.071283 0.050916\nCollapsed Catch Basin                       0.064220 0.053899 0.061927 0.075688\nCommercial Refuse/Dumpsters                 0.079430 0.079430 0.089613 0.077393\nCurb /Broken/Deteriorated                   0.042373 0.048729 0.072034 0.110169\nCurb/Request for Asphalt Windrow            0.038660 0.020619 0.028351 0.085052\nDead Animal                                 0.038181 0.032566 0.043234 0.076923\nDead tree (Public property)                 0.034516 0.028763 0.049856 0.066155\nDrainage/Leak                               0.141304 0.050000 0.035870 0.083696\nDrug Enforcement                            0.077085 0.049755 0.065172 0.079187\nDumping, Private Property                   0.064315 0.076763 0.120332 0.093361\nDumpster (on Street)                        0.070866 0.048819 0.042520 0.088189\nEarly Set Out                               0.072444 0.069736 0.062288 0.063643\nExcessive Noise/Disturbances                0.057377 0.047131 0.056011 0.085383\nField                                       0.016432 0.014085 0.042254 0.110329\nFire Safety System Not Working              0.093750 0.185547 0.128906 0.077474\nGraffiti, Documentation                     0.057116 0.054307 0.103933 0.102060\nGraffiti, Removal                           0.088710 0.111290 0.098387 0.046774\nHydrant                                     0.121771 0.062731 0.075646 0.053506\nIllegal Dumping                             0.065672 0.057214 0.076617 0.106965\nIllegal Parking                             0.095682 0.075074 0.081943 0.079735\nJunk Vehicles                               0.079384 0.093602 0.114929 0.068720\nLeak                                        0.171456 0.097418 0.048709 0.044812\nLeaves/Street Cleaning                      0.028967 0.031486 0.030227 0.059194\nLitter                                      0.064316 0.064555 0.085755 0.093378\nLitter Can, Public                          0.064777 0.049393 0.060729 0.069636\nMaintenance Issue                           0.026455 0.031746 0.039153 0.078307\nMayor's Office                              0.158455 0.033289 0.023968 0.027963\nMissed Blue Bag                             0.094002 0.042077 0.051925 0.068935\nMissed Pick Up                              0.076775 0.048198 0.047345 0.058861\nNeed Potable Water                          0.002398 0.914868 0.001199 0.003597\nOperating Without a License                 0.041215 0.021692 0.149675 0.114967\nOvergrowth                                  0.005058 0.007867 0.006462 0.019106\nParking Authority                           0.086022 0.075269 0.064516 0.105376\nPatrol                                      0.063164 0.047816 0.071429 0.081464\nPaving Concern/Problem                      0.054819 0.043324 0.042440 0.071618\nPaving Request                              0.052950 0.047504 0.108321 0.114675\nPermit Parking (Residential Parking Permit) 0.107062 0.075171 0.063781 0.079727\nPlayground                                  0.015038 0.024436 0.043233 0.093985\nPotholes                                    0.123324 0.052972 0.105230 0.112570\nPruning (city tree)                         0.024019 0.025372 0.044317 0.057510\nPublic Right of Way                         0.033397 0.029580 0.020992 0.057252\nQuestion                                    0.079824 0.064739 0.043997 0.024513\nReferral                                    0.099161 0.050725 0.046148 0.049962\nRefuse Violations                           0.079202 0.059708 0.065848 0.084728\nReplace/Repair a Sign                       0.067492 0.053093 0.080315 0.084814\nRequest New Sign                            0.069169 0.049768 0.059047 0.097005\nRetaining Wall Maintenance                  0.066239 0.091880 0.115385 0.096154\nRodent control                              0.040957 0.033594 0.041417 0.052462\nRoot prune                                  0.022321 0.038690 0.053571 0.098214\nSidewalk Obstruction                        0.052799 0.042621 0.044529 0.052163\nSidewalk, Lack of Snow/Ice Removal          0.767726 0.090465 0.002445 0.002445\nSinkhole                                    0.103995 0.058973 0.062143 0.066582\nSmoke detectors                             0.118421 0.064145 0.062500 0.092105\nSnow/Ice removal                            0.681273 0.135027 0.005995 0.002712\nSpeeding                                    0.063973 0.060606 0.084175 0.094276\nStreet Cleaning/Sweeping                    0.027306 0.026790 0.035033 0.102009\nStreet Light - Repair                       0.078803 0.055112 0.067830 0.044888\nStreet Obstruction/Closure                  0.126273 0.040733 0.081466 0.061100\nThank you - DPW                             0.136264 0.046154 0.032967 0.065934\nTraffic                                     0.065356 0.053666 0.061637 0.073326\nTraffic or Pedestrian Signal, Repair        0.089659 0.047221 0.069934 0.069337\nTraffic or Pedestrian Signal, Request       0.056641 0.029297 0.099609 0.087891\nTree Fallen Across Road                     0.042589 0.031516 0.051959 0.034923\nTree Fallen Across Sidewalk                 0.034125 0.028190 0.044510 0.044510\nTree Issues                                 0.038384 0.056566 0.076768 0.048485\nTree Removal                                0.042949 0.036507 0.071582 0.074445\nUnpermitted Electrical Work                 0.145055 0.012088 0.030769 0.019780\nUnpermitted HVAC Work                       0.108516 0.045330 0.064560 0.048077\nUtility Cut - Other                         0.114889 0.067995 0.052755 0.059789\nUtility Cut - PWSA                          0.202261 0.050251 0.057789 0.075377\nUtility Pole                                0.065728 0.075117 0.049296 0.075117\nVacant Building                             0.088199 0.083230 0.096066 0.048861\nWeeds/Debris                                0.029631 0.024965 0.035751 0.045083\nWires                                       0.060651 0.071006 0.060651 0.078402\nZoning Issue                                0.055000 0.065000 0.080833 0.082500\n                                                  May       Jun      Jul\nAbandoned Vehicle (parked on street)        0.0830356 0.0792922 0.077761\nBarking Dog                                 0.1036036 0.1013514 0.078829\nBoard Up (PLI referral to DPW)              0.1140351 0.0899123 0.109649\nBroken Sidewalk                             0.1048202 0.0964040 0.069625\nBuilding Maintenance                        0.0841532 0.0829115 0.072500\nBuilding Without a Permit                   0.1049046 0.0883288 0.092416\nCatch Basin, Clogged                        0.0984090 0.1319976 0.082499\nCity Source (CDBG)                          0.1955758 0.2134831 0.195225\nCity Steps, Need Cleared                    0.1163934 0.2163934 0.203279\nCity Steps, Need Repaired                   0.1283096 0.1201629 0.136456\nCollapsed Catch Basin                       0.1100917 0.0917431 0.083716\nCommercial Refuse/Dumpsters                 0.0529532 0.1038697 0.105906\nCurb /Broken/Deteriorated                   0.1525424 0.1122881 0.116525\nCurb/Request for Asphalt Windrow            0.1430412 0.2113402 0.155928\nDead Animal                                 0.0713083 0.1021898 0.139809\nDead tree (Public property)                 0.1246405 0.1447747 0.154362\nDrainage/Leak                               0.0902174 0.1043478 0.102174\nDrug Enforcement                            0.0988087 0.1023125 0.088998\nDumping, Private Property                   0.0746888 0.0622407 0.064315\nDumpster (on Street)                        0.0787402 0.1354331 0.105512\nEarly Set Out                               0.0886933 0.0873392 0.111713\nExcessive Noise/Disturbances                0.0887978 0.0758197 0.075137\nField                                       0.1854460 0.1384977 0.150235\nFire Safety System Not Working              0.1139323 0.0572917 0.047526\nGraffiti, Documentation                     0.1207865 0.1254682 0.073970\nGraffiti, Removal                           0.0338710 0.0596774 0.091935\nHydrant                                     0.0922509 0.0571956 0.064576\nIllegal Dumping                             0.1000000 0.1228856 0.113930\nIllegal Parking                             0.0765456 0.0691855 0.059863\nJunk Vehicles                               0.0864929 0.0710900 0.104265\nLeak                                        0.0491963 0.0526059 0.057964\nLeaves/Street Cleaning                      0.0629723 0.0541562 0.021411\nLitter                                      0.0826584 0.0855169 0.098380\nLitter Can, Public                          0.0923077 0.0995951 0.127126\nMaintenance Issue                           0.1417989 0.1185185 0.135450\nMayor's Office                              0.0319574 0.1824234 0.065246\nMissed Blue Bag                             0.0841540 0.1020591 0.087735\nMissed Pick Up                              0.1027938 0.1123907 0.118789\nNeed Potable Water                          0.0011990 0.0023981 0.000000\nOperating Without a License                 0.4338395 0.0542299 0.028200\nOvergrowth                                  0.1219444 0.2489463 0.234335\nParking Authority                           0.0860215 0.0838710 0.081720\nPatrol                                      0.1015348 0.0879575 0.095041\nPaving Concern/Problem                      0.1114058 0.1255526 0.085765\nPaving Request                              0.1397882 0.1458396 0.118306\nPermit Parking (Residential Parking Permit) 0.0569476 0.0706150 0.079727\nPlayground                                  0.1184211 0.1691729 0.159774\nPotholes                                    0.1346322 0.1150702 0.107095\nPruning (city tree)                         0.1234777 0.1742219 0.168133\nPublic Right of Way                         0.1316794 0.1650763 0.154580\nQuestion                                    0.0483972 0.0936518 0.122564\nReferral                                    0.0362319 0.0846682 0.129291\nRefuse Violations                           0.0784344 0.0983883 0.100844\nReplace/Repair a Sign                       0.1196850 0.1113611 0.094713\nRequest New Sign                            0.0932096 0.0927879 0.097427\nRetaining Wall Maintenance                  0.0961538 0.0982906 0.085470\nRodent control                              0.0745513 0.1099862 0.141279\nRoot prune                                  0.1190476 0.1264881 0.163690\nSidewalk Obstruction                        0.0807888 0.1075064 0.123410\nSidewalk, Lack of Snow/Ice Removal          0.0097800 0.0000000 0.002445\nSinkhole                                    0.0786303 0.1122384 0.128725\nSmoke detectors                             0.0871711 0.1348684 0.046053\nSnow/Ice removal                            0.0008564 0.0001427 0.000000\nSpeeding                                    0.0976431 0.0909091 0.104377\nStreet Cleaning/Sweeping                    0.1257084 0.1298300 0.123132\nStreet Light - Repair                       0.0498753 0.0675810 0.097257\nStreet Obstruction/Closure                  0.0549898 0.0509165 0.087576\nThank you - DPW                             0.0769231 0.0967033 0.105495\nTraffic                                     0.0887354 0.0600425 0.054729\nTraffic or Pedestrian Signal, Repair        0.0854752 0.1040048 0.086671\nTraffic or Pedestrian Signal, Request       0.1074219 0.1191406 0.085938\nTree Fallen Across Road                     0.1345826 0.2206133 0.137990\nTree Fallen Across Sidewalk                 0.1424332 0.1958457 0.126113\nTree Issues                                 0.1010101 0.1010101 0.129293\nTree Removal                                0.1295634 0.1410165 0.118826\nUnpermitted Electrical Work                 0.1054945 0.1230769 0.085714\nUnpermitted HVAC Work                       0.1689560 0.0879121 0.085165\nUtility Cut - Other                         0.0797186 0.0738570 0.082063\nUtility Cut - PWSA                          0.1005025 0.0967337 0.095477\nUtility Pole                                0.1126761 0.1244131 0.107981\nVacant Building                             0.0683230 0.0608696 0.064182\nWeeds/Debris                                0.1373690 0.1666364 0.157062\nWires                                       0.0961538 0.1434911 0.087278\nZoning Issue                                0.0958333 0.0866667 0.103333\n                                                  Aug      Sep       Oct\nAbandoned Vehicle (parked on street)        0.1014123 0.089331 0.0971584\nBarking Dog                                 0.1126126 0.123874 0.1126126\nBoard Up (PLI referral to DPW)              0.1228070 0.085526 0.0877193\nBroken Sidewalk                             0.0734507 0.052793 0.0849273\nBuilding Maintenance                        0.0918903 0.077562 0.0787086\nBuilding Without a Permit                   0.0815168 0.082879 0.1128520\nCatch Basin, Clogged                        0.1266942 0.104891 0.0931055\nCity Source (CDBG)                          0.1639747 0.080758 0.0582865\nCity Steps, Need Cleared                    0.1459016 0.118033 0.0557377\nCity Steps, Need Repaired                   0.1283096 0.107943 0.0855397\nCollapsed Catch Basin                       0.1238532 0.113532 0.0986239\nCommercial Refuse/Dumpsters                 0.1344196 0.071283 0.0712831\nCurb /Broken/Deteriorated                   0.1186441 0.084746 0.0572034\nCurb/Request for Asphalt Windrow            0.1082474 0.079897 0.0605670\nDead Animal                                 0.1235261 0.112296 0.1274565\nDead tree (Public property)                 0.1236817 0.102589 0.0882071\nDrainage/Leak                               0.1076087 0.058696 0.0782609\nDrug Enforcement                            0.1135249 0.117730 0.0946041\nDumping, Private Property                   0.1327801 0.076763 0.1016598\nDumpster (on Street)                        0.1070866 0.086614 0.0881890\nEarly Set Out                               0.1570752 0.080569 0.0663507\nExcessive Noise/Disturbances                0.0881148 0.090164 0.1038251\nField                                       0.1197183 0.098592 0.0610329\nFire Safety System Not Working              0.0449219 0.050781 0.0651042\nGraffiti, Documentation                     0.0608614 0.067416 0.1207865\nGraffiti, Removal                           0.1048387 0.125806 0.0935484\nHydrant                                     0.1254613 0.077491 0.0867159\nIllegal Dumping                             0.0651741 0.068657 0.0726368\nIllegal Parking                             0.0969087 0.103042 0.0991168\nJunk Vehicles                               0.0912322 0.072275 0.0758294\nLeak                                        0.1066732 0.080857 0.0681929\nLeaves/Street Cleaning                      0.0440806 0.012594 0.0642317\nLitter                                      0.1202954 0.095760 0.0824202\nLitter Can, Public                          0.1246964 0.110121 0.0850202\nMaintenance Issue                           0.1174603 0.113228 0.1047619\nMayor's Office                              0.1118509 0.114514 0.0892144\nMissed Blue Bag                             0.1056401 0.089526 0.0957923\nMissed Pick Up                              0.1106846 0.098315 0.0835999\nNeed Potable Water                          0.0731415 0.000000 0.0000000\nOperating Without a License                 0.0390456 0.021692 0.0455531\nOvergrowth                                  0.1944366 0.104805 0.0396179\nParking Authority                           0.1032258 0.090323 0.0731183\nPatrol                                      0.1097993 0.103306 0.1151122\nPaving Concern/Problem                      0.0813439 0.089302 0.1432361\nPaving Request                              0.0928896 0.075340 0.0490166\nPermit Parking (Residential Parking Permit) 0.1321185 0.102506 0.0956720\nPlayground                                  0.1672932 0.090226 0.0733083\nPotholes                                    0.0759860 0.050155 0.0442425\nPruning (city tree)                         0.1586604 0.082206 0.0801759\nPublic Right of Way                         0.1832061 0.094466 0.0620229\nQuestion                                    0.1646763 0.065996 0.1451917\nReferral                                    0.1525553 0.089245 0.1060259\nRefuse Violations                           0.1122026 0.085035 0.0968534\nReplace/Repair a Sign                       0.0899888 0.088189 0.0758155\nRequest New Sign                            0.1100801 0.113454 0.0889920\nRetaining Wall Maintenance                  0.0769231 0.055556 0.1089744\nRodent control                              0.1385182 0.125633 0.1214910\nRoot prune                                  0.1056548 0.096726 0.0937500\nSidewalk Obstruction                        0.1246819 0.117048 0.0966921\nSidewalk, Lack of Snow/Ice Removal          0.0000000 0.002445 0.0000000\nSinkhole                                    0.1230184 0.088142 0.0798985\nSmoke detectors                             0.0756579 0.041118 0.1118421\nSnow/Ice removal                            0.0001427 0.000000 0.0001427\nSpeeding                                    0.1144781 0.112795 0.0791246\nStreet Cleaning/Sweeping                    0.1215868 0.098403 0.0963421\nStreet Light - Repair                       0.1057357 0.105237 0.1134663\nStreet Obstruction/Closure                  0.1201629 0.120163 0.0916497\nThank you - DPW                             0.1252747 0.112088 0.0879121\nTraffic                                     0.1046759 0.162062 0.1232731\nTraffic or Pedestrian Signal, Repair        0.1165571 0.105798 0.0854752\nTraffic or Pedestrian Signal, Request       0.0742188 0.111328 0.1035156\nTree Fallen Across Road                     0.1831346 0.064736 0.0442930\nTree Fallen Across Sidewalk                 0.2121662 0.063798 0.0474777\nTree Issues                                 0.1454545 0.094949 0.0848485\nTree Removal                                0.1460272 0.085183 0.0787402\nUnpermitted Electrical Work                 0.0571429 0.065934 0.1318681\nUnpermitted HVAC Work                       0.0824176 0.075549 0.0879121\nUtility Cut - Other                         0.1160610 0.110199 0.0937866\nUtility Cut - PWSA                          0.0716080 0.062814 0.0690955\nUtility Pole                                0.1197183 0.077465 0.0563380\nVacant Building                             0.0749482 0.077847 0.0815735\nWeeds/Debris                                0.1616070 0.097922 0.0718657\nWires                                       0.1316568 0.091716 0.0532544\nZoning Issue                                0.0908333 0.079167 0.0875000\n                                                 Nov      Dec\nAbandoned Vehicle (parked on street)        0.086098 0.076400\nBarking Dog                                 0.074324 0.051802\nBoard Up (PLI referral to DPW)              0.076754 0.065789\nBroken Sidewalk                             0.064269 0.030604\nBuilding Maintenance                        0.093419 0.078900\nBuilding Without a Permit                   0.086285 0.075613\nCatch Basin, Clogged                        0.064820 0.043606\nCity Source (CDBG)                          0.025632 0.005969\nCity Steps, Need Cleared                    0.022951 0.024590\nCity Steps, Need Repaired                   0.075356 0.038697\nCollapsed Catch Basin                       0.068807 0.053899\nCommercial Refuse/Dumpsters                 0.069246 0.065173\nCurb /Broken/Deteriorated                   0.046610 0.038136\nCurb/Request for Asphalt Windrow            0.047680 0.020619\nDead Animal                                 0.083661 0.048849\nDead tree (Public property)                 0.051774 0.030681\nDrainage/Leak                               0.070652 0.077174\nDrug Enforcement                            0.067274 0.045550\nDumping, Private Property                   0.074689 0.058091\nDumpster (on Street)                        0.083465 0.064567\nEarly Set Out                               0.069059 0.071090\nExcessive Noise/Disturbances                0.092213 0.140027\nField                                       0.032864 0.030516\nFire Safety System Not Working              0.051432 0.083333\nGraffiti, Documentation                     0.072097 0.041199\nGraffiti, Removal                           0.066129 0.079032\nHydrant                                     0.090406 0.092251\nIllegal Dumping                             0.083582 0.066667\nIllegal Parking                             0.088077 0.074828\nJunk Vehicles                               0.091232 0.050948\nLeak                                        0.097418 0.124696\nLeaves/Street Cleaning                      0.430730 0.159950\nLitter                                      0.070272 0.056694\nLitter Can, Public                          0.068826 0.047773\nMaintenance Issue                           0.053968 0.039153\nMayor's Office                              0.083888 0.077230\nMissed Blue Bag                             0.091316 0.086840\nMissed Pick Up                              0.070164 0.072084\nNeed Potable Water                          0.000000 0.001199\nOperating Without a License                 0.026030 0.023861\nOvergrowth                                  0.013768 0.003653\nParking Authority                           0.068817 0.081720\nPatrol                                      0.070838 0.052538\nPaving Concern/Problem                      0.108753 0.042440\nPaving Request                              0.034493 0.020877\nPermit Parking (Residential Parking Permit) 0.077449 0.059226\nPlayground                                  0.041353 0.003759\nPotholes                                    0.053289 0.025434\nPruning (city tree)                         0.041272 0.020636\nPublic Right of Way                         0.044847 0.022901\nQuestion                                    0.084852 0.061596\nReferral                                    0.081998 0.073989\nRefuse Violations                           0.067536 0.071220\nReplace/Repair a Sign                       0.070191 0.064342\nRequest New Sign                            0.076339 0.052720\nRetaining Wall Maintenance                  0.070513 0.038462\nRodent control                              0.071330 0.048780\nRoot prune                                  0.043155 0.038690\nSidewalk Obstruction                        0.086514 0.071247\nSidewalk, Lack of Snow/Ice Removal          0.000000 0.122249\nSinkhole                                    0.058339 0.039315\nSmoke detectors                             0.088816 0.077303\nSnow/Ice removal                            0.003711 0.169997\nSpeeding                                    0.052189 0.045455\nStreet Cleaning/Sweeping                    0.082947 0.030912\nStreet Light - Repair                       0.115960 0.098254\nStreet Obstruction/Closure                  0.089613 0.075356\nThank you - DPW                             0.065934 0.048352\nTraffic                                     0.088735 0.063762\nTraffic or Pedestrian Signal, Repair        0.084877 0.054991\nTraffic or Pedestrian Signal, Request       0.068359 0.056641\nTree Fallen Across Road                     0.036627 0.017036\nTree Fallen Across Sidewalk                 0.040059 0.020772\nTree Issues                                 0.070707 0.052525\nTree Removal                                0.047960 0.027201\nUnpermitted Electrical Work                 0.101099 0.121978\nUnpermitted HVAC Work                       0.074176 0.071429\nUtility Cut - Other                         0.069168 0.079719\nUtility Cut - PWSA                          0.081658 0.036432\nUtility Pole                                0.075117 0.061033\nVacant Building                             0.132091 0.123810\nWeeds/Debris                                0.045386 0.026722\nWires                                       0.076923 0.048817\nZoning Issue                                0.092500 0.080833\n\n\nCreate the PCA object\n\n(df_months_pca1 %&gt;% \n  prcomp(scale = TRUE) -&gt; pc)\n\nStandard deviations (1, .., p=12):\n [1] 2.0313544303132338165 1.5112299607905637089 1.3677583442481686671\n [4] 1.0647449915481708160 0.9373153843502739502 0.6612690017981475155\n [7] 0.6319678449167122070 0.5732234023111666410 0.4666060722915733039\n[10] 0.4192405535100036107 0.3847717270238655840 0.0000000000000002152\n\nRotation (n x k) = (12 x 12):\n        PC1     PC2       PC3      PC4      PC5        PC6      PC7      PC8\nJan -0.3509  0.2377  0.036588 -0.40554 -0.40259 -0.0878343  0.04648  0.10077\nFeb -0.2189  0.2230 -0.226391  0.69886  0.14610  0.1005237  0.15306 -0.04311\nMar -0.0235 -0.4858 -0.323760  0.11519 -0.19761 -0.1724337 -0.62037  0.31077\nApr  0.1329 -0.4686 -0.301500 -0.04306 -0.07313 -0.5014973  0.54199 -0.09929\nMay  0.2339 -0.1448 -0.445597 -0.33914  0.15507  0.5723879 -0.10942 -0.40042\nJun  0.4049  0.1866  0.002112 -0.22765  0.21210 -0.0386388  0.13438  0.42038\nJul  0.4322  0.1697  0.095923 -0.10888  0.01800 -0.1443989  0.02032  0.13081\nAug  0.3866  0.1805  0.189907  0.17598  0.04242 -0.2554247 -0.44835 -0.13536\nSep  0.2944 -0.1580  0.365255  0.16856 -0.42595 -0.0005222  0.04916 -0.53195\nOct  0.1150 -0.4130  0.389922  0.16708 -0.17452  0.5136274  0.18989  0.43757\nNov -0.1323 -0.3291  0.333424 -0.05855  0.69855 -0.1384698 -0.02866 -0.09659\nDec -0.3720 -0.1311  0.333522 -0.25017  0.03408 -0.0524049 -0.15824 -0.15902\n          PC9     PC10      PC11   PC12\nJan -0.119474 -0.19207  0.310620 0.5699\nFeb  0.085692  0.10149 -0.130098 0.5209\nMar  0.259371  0.01955  0.035606 0.1620\nApr -0.264116  0.01396 -0.143422 0.1379\nMay -0.106727  0.02849 -0.028027 0.2717\nJun  0.373786 -0.36939 -0.413396 0.2471\nJul  0.103405  0.77442  0.256091 0.2205\nAug -0.607417 -0.20278 -0.123033 0.1983\nSep  0.454920 -0.16862  0.035475 0.1519\nOct -0.304459  0.02271 -0.003209 0.1456\nNov  0.104258 -0.14115  0.387838 0.2472\nDec  0.008971  0.35391 -0.678722 0.1743\n\n\nInspect the PCA object with tidier functions from the broom library. These functions turn the PCA object into a tidy dataframe\n\npc %&gt;% \n  tidy() %&gt;% \n  head()\n\n# A tibble: 6 × 3\n  row                                     PC   value\n  &lt;chr&gt;                                &lt;dbl&gt;   &lt;dbl&gt;\n1 Abandoned Vehicle (parked on street)     1 -0.844 \n2 Abandoned Vehicle (parked on street)     2 -0.844 \n3 Abandoned Vehicle (parked on street)     3  0.383 \n4 Abandoned Vehicle (parked on street)     4  0.311 \n5 Abandoned Vehicle (parked on street)     5 -0.206 \n6 Abandoned Vehicle (parked on street)     6  0.0620\n\n\n\npc %&gt;% \n  tidy(\"pcs\")\n\n# A tibble: 12 × 4\n      PC  std.dev percent cumulative\n   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n 1     1 2.03e+ 0  0.344       0.344\n 2     2 1.51e+ 0  0.190       0.534\n 3     3 1.37e+ 0  0.156       0.690\n 4     4 1.06e+ 0  0.0945      0.785\n 5     5 9.37e- 1  0.0732      0.858\n 6     6 6.61e- 1  0.0364      0.894\n 7     7 6.32e- 1  0.0333      0.927\n 8     8 5.73e- 1  0.0274      0.955\n 9     9 4.67e- 1  0.0181      0.973\n10    10 4.19e- 1  0.0146      0.988\n11    11 3.85e- 1  0.0123      1    \n12    12 2.15e-16  0           1    \n\n\n\npc %&gt;% \n  augment(data = df_months) -&gt; au\n\nau %&gt;% \n  head()\n\n# A tibble: 6 × 26\n  .rownames request_type    Jan    Feb    Mar    Apr    May    Jun    Jul    Aug\n  &lt;chr&gt;     &lt;chr&gt;         &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 1         Abandoned V… 0.0890 0.0727 0.0769 0.0710 0.0830 0.0793 0.0778 0.101 \n2 2         Barking Dog  0.0563 0.0608 0.0608 0.0631 0.104  0.101  0.0788 0.113 \n3 3         Board Up (P… 0.0395 0.0482 0.0658 0.0943 0.114  0.0899 0.110  0.123 \n4 4         Broken Side… 0.0337 0.155  0.148  0.0872 0.105  0.0964 0.0696 0.0735\n5 5         Building Ma… 0.0708 0.0919 0.103  0.0739 0.0842 0.0829 0.0725 0.0919\n6 6         Building Wi… 0.0842 0.0697 0.0636 0.0577 0.105  0.0883 0.0924 0.0815\n# ℹ 16 more variables: Sep &lt;dbl&gt;, Oct &lt;dbl&gt;, Nov &lt;dbl&gt;, Dec &lt;dbl&gt;,\n#   .fittedPC1 &lt;dbl&gt;, .fittedPC2 &lt;dbl&gt;, .fittedPC3 &lt;dbl&gt;, .fittedPC4 &lt;dbl&gt;,\n#   .fittedPC5 &lt;dbl&gt;, .fittedPC6 &lt;dbl&gt;, .fittedPC7 &lt;dbl&gt;, .fittedPC8 &lt;dbl&gt;,\n#   .fittedPC9 &lt;dbl&gt;, .fittedPC10 &lt;dbl&gt;, .fittedPC11 &lt;dbl&gt;, .fittedPC12 &lt;dbl&gt;\n\n\nPlot how the PCA object explains the variance in the data\n\npc %&gt;% \n  tidy(\"pcs\") %&gt;%\n  select(-std.dev) %&gt;% \n  gather(measure, value, -PC) %&gt;% \n    ggplot(aes(PC, value)) +\n    geom_line() +\n    geom_point() +\n    facet_wrap(~measure) +\n    labs(title = \"Variance explained by each principal component\",\n         x = \"Principal Component\",\n         y = NULL) +\n    scale_x_continuous(breaks = 1:12)\n\n\n\n\n\n\n\n\nThe first two principal components explain most of the variance\nFor an in-depth plot we need to create the PCA object a different way\n\ndf_months %&gt;% \n  nest() %&gt;% \n  mutate(pca = map(data, ~ prcomp(.x %&gt;% select(-request_type), \n                                  center = TRUE, scale = TRUE)),\n         pca_aug = map2(pca, data, ~augment(.x, data = .y))) -&gt; df_months_pca2\n\nPlot the PCA data\n\ndf_months_pca2 %&gt;%\n  mutate(\n    pca_graph = map2(\n      .x = pca,\n      .y = data,\n      ~ autoplot(.x, loadings = TRUE, loadings.label = TRUE,\n                 loadings.label.repel = TRUE,\n                 data = .y) +\n        theme_bw() +\n        labs(x = \"Principal Component 1\",\n             y = \"Principal Component 2\",\n             title = \"First two principal components of PCA on 311 dataset\")\n    )\n  ) %&gt;%\n  pull(pca_graph)\n\n[[1]]\n\n\n\n\n\n\n\n\n\nThis shows that summer and winter explain a significant part of the variance\nPlot the data to show the outliers\n\nau %&gt;% \n  mutate(outlier = case_when(abs(.fittedPC1) &gt; 2 & abs(.fittedPC2) &gt; 1.5 ~ TRUE),\n         pothole = case_when(request_type == \"Potholes\" ~ \"Potholes\",\n                             request_type != \"Potholes\" ~ \"Other\")) -&gt; au\n\nau %&gt;% \nggplot(aes(.fittedPC1, .fittedPC2)) +\n  geom_point() +\n  geom_label_repel(data = au %&gt;% filter(outlier),\n             aes(label = request_type)) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nau %&gt;% \nggplot(aes(.fittedPC1, .fittedPC2)) +\n  geom_point(aes(color = pothole)) +\n  geom_label_repel(data = au %&gt;% filter(request_type == \"Potholes\"),\n             aes(label = request_type)) +\n  theme_bw() +\n  scale_color_manual(NULL, values = c(\"black\", \"red\"))"
  },
  {
    "objectID": "posts/exploring-311-data-with-pca/index.html#setup",
    "href": "posts/exploring-311-data-with-pca/index.html#setup",
    "title": "Exploring 311 Data With PCA",
    "section": "",
    "text": "install.packages(c(\"tidyverse\", \"lubridate\", \"broom\", \"ggfortify\", \"ggrepel\", \"janitor\"))\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(broom)\nlibrary(ggfortify)\nlibrary(ggrepel)\nlibrary(janitor)\n\noptions(scipen = 999, digits = 4)\nset.seed(1234)\n\ntheme_set(theme_bw())"
  },
  {
    "objectID": "posts/exploring-311-data-with-pca/index.html#load-the-data",
    "href": "posts/exploring-311-data-with-pca/index.html#load-the-data",
    "title": "Exploring 311 Data With PCA",
    "section": "",
    "text": "read_csv(\"https://raw.githubusercontent.com/conorotompkins/pittsburgh_311/master/data/pittsburgh_311.csv\", progress = FALSE) %&gt;% \n  clean_names() %&gt;% \n  mutate(date = ymd(str_sub(created_on, 1, 10)),\n         month = month(date, label = TRUE)) %&gt;% \n  filter(date &lt; \"2018-07-19\") -&gt; df"
  },
  {
    "objectID": "posts/exploring-311-data-with-pca/index.html#prep-the-data",
    "href": "posts/exploring-311-data-with-pca/index.html#prep-the-data",
    "title": "Exploring 311 Data With PCA",
    "section": "",
    "text": "Create a dataframe of the top request types\n\n(df %&gt;% \n  count(request_type, sort = TRUE) %&gt;% \n  filter(n &gt; 400)-&gt; df_top_requests)\n\n# A tibble: 84 × 2\n   request_type                             n\n   &lt;chr&gt;                                &lt;int&gt;\n 1 Potholes                             25202\n 2 Weeds/Debris                         16503\n 3 Building Maintenance                 10469\n 4 Snow/Ice removal                      7006\n 5 Refuse Violations                     6515\n 6 Abandoned Vehicle (parked on street)  5877\n 7 Missed Pick Up                        4689\n 8 Replace/Repair a Sign                 4445\n 9 Building Without a Permit             4404\n10 Litter                                4198\n# ℹ 74 more rows\n\n\nCount the number of requests per month by request type, filter for the top request types, and fill in gaps in the data\n\n(df %&gt;%\n  semi_join(df_top_requests) %&gt;% \n  group_by(request_type, month) %&gt;% \n  summarize(n = n()) %&gt;% \n  ungroup() %&gt;%\n  complete(request_type, month) %&gt;% \n  replace_na(replace = list(n = 0)) -&gt; df_months)\n\n# A tibble: 1,008 × 3\n   request_type                         month     n\n   &lt;chr&gt;                                &lt;ord&gt; &lt;int&gt;\n 1 Abandoned Vehicle (parked on street) Jan     523\n 2 Abandoned Vehicle (parked on street) Feb     427\n 3 Abandoned Vehicle (parked on street) Mar     452\n 4 Abandoned Vehicle (parked on street) Apr     417\n 5 Abandoned Vehicle (parked on street) May     488\n 6 Abandoned Vehicle (parked on street) Jun     466\n 7 Abandoned Vehicle (parked on street) Jul     457\n 8 Abandoned Vehicle (parked on street) Aug     596\n 9 Abandoned Vehicle (parked on street) Sep     525\n10 Abandoned Vehicle (parked on street) Oct     571\n# ℹ 998 more rows\n\n\nCalculate the percentage of a request type for each month\n\n(df_months %&gt;% \n  group_by(request_type) %&gt;% \n  mutate(request_type_total = sum(n),\n         month_percentage = n / request_type_total) -&gt; df_months)\n\n# A tibble: 1,008 × 5\n# Groups:   request_type [84]\n   request_type                  month     n request_type_total month_percentage\n   &lt;chr&gt;                         &lt;ord&gt; &lt;int&gt;              &lt;int&gt;            &lt;dbl&gt;\n 1 Abandoned Vehicle (parked on… Jan     523               5877           0.0890\n 2 Abandoned Vehicle (parked on… Feb     427               5877           0.0727\n 3 Abandoned Vehicle (parked on… Mar     452               5877           0.0769\n 4 Abandoned Vehicle (parked on… Apr     417               5877           0.0710\n 5 Abandoned Vehicle (parked on… May     488               5877           0.0830\n 6 Abandoned Vehicle (parked on… Jun     466               5877           0.0793\n 7 Abandoned Vehicle (parked on… Jul     457               5877           0.0778\n 8 Abandoned Vehicle (parked on… Aug     596               5877           0.101 \n 9 Abandoned Vehicle (parked on… Sep     525               5877           0.0893\n10 Abandoned Vehicle (parked on… Oct     571               5877           0.0972\n# ℹ 998 more rows\n\n\nCheck for bad data\n\ndf_months %&gt;% \n  filter(is.na(month_percentage) | is.nan(month_percentage))\n\n# A tibble: 0 × 5\n# Groups:   request_type [0]\n# ℹ 5 variables: request_type &lt;chr&gt;, month &lt;ord&gt;, n &lt;int&gt;,\n#   request_type_total &lt;int&gt;, month_percentage &lt;dbl&gt;\n\n\nSpread the data to turn the months into the columns\n\n(df_months %&gt;% \n  select(request_type, month, month_percentage) %&gt;% \n  spread(month, month_percentage) %&gt;% \n  ungroup() -&gt; df_months)\n\n# A tibble: 84 × 13\n   request_type     Jan     Feb    Mar    Apr    May    Jun    Jul    Aug    Sep\n   &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 Abandoned V… 0.0890  0.0727  0.0769 0.0710 0.0830 0.0793 0.0778 0.101  0.0893\n 2 Barking Dog  0.0563  0.0608  0.0608 0.0631 0.104  0.101  0.0788 0.113  0.124 \n 3 Board Up (P… 0.0395  0.0482  0.0658 0.0943 0.114  0.0899 0.110  0.123  0.0855\n 4 Broken Side… 0.0337  0.155   0.148  0.0872 0.105  0.0964 0.0696 0.0735 0.0528\n 5 Building Ma… 0.0708  0.0919  0.103  0.0739 0.0842 0.0829 0.0725 0.0919 0.0776\n 6 Building Wi… 0.0842  0.0697  0.0636 0.0577 0.105  0.0883 0.0924 0.0815 0.0829\n 7 Catch Basin… 0.0636  0.0377  0.0778 0.0748 0.0984 0.132  0.0825 0.127  0.105 \n 8 City Source… 0.00527 0.00246 0.0105 0.0428 0.196  0.213  0.195  0.164  0.0808\n 9 City Steps,… 0.0443  0.0180  0.0148 0.0197 0.116  0.216  0.203  0.146  0.118 \n10 City Steps,… 0.0265  0.0305  0.0713 0.0509 0.128  0.120  0.136  0.128  0.108 \n# ℹ 74 more rows\n# ℹ 3 more variables: Oct &lt;dbl&gt;, Nov &lt;dbl&gt;, Dec &lt;dbl&gt;\n\n\nCheck that they all add up to 1 across the rows\n\n(df_months %&gt;% \n  select(Jan:Dec) %&gt;% \n  mutate(row_sum = rowSums(.)) %&gt;% \n  select(row_sum, everything()) -&gt; test)\n\n# A tibble: 84 × 13\n   row_sum     Jan     Feb    Mar    Apr    May    Jun    Jul    Aug    Sep\n     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1       1 0.0890  0.0727  0.0769 0.0710 0.0830 0.0793 0.0778 0.101  0.0893\n 2       1 0.0563  0.0608  0.0608 0.0631 0.104  0.101  0.0788 0.113  0.124 \n 3       1 0.0395  0.0482  0.0658 0.0943 0.114  0.0899 0.110  0.123  0.0855\n 4       1 0.0337  0.155   0.148  0.0872 0.105  0.0964 0.0696 0.0735 0.0528\n 5       1 0.0708  0.0919  0.103  0.0739 0.0842 0.0829 0.0725 0.0919 0.0776\n 6       1 0.0842  0.0697  0.0636 0.0577 0.105  0.0883 0.0924 0.0815 0.0829\n 7       1 0.0636  0.0377  0.0778 0.0748 0.0984 0.132  0.0825 0.127  0.105 \n 8       1 0.00527 0.00246 0.0105 0.0428 0.196  0.213  0.195  0.164  0.0808\n 9       1 0.0443  0.0180  0.0148 0.0197 0.116  0.216  0.203  0.146  0.118 \n10       1 0.0265  0.0305  0.0713 0.0509 0.128  0.120  0.136  0.128  0.108 \n# ℹ 74 more rows\n# ℹ 3 more variables: Oct &lt;dbl&gt;, Nov &lt;dbl&gt;, Dec &lt;dbl&gt;\n\n\n\n\n\ndf_months %&gt;% \n  ggplot(aes(Jan, Jul)) +\n  geom_point()\n\n\n\n\n\n\n\n\nRemember that each dot represents a request type, and the month shows what % of that request type occurred that month\n\ndf_months %&gt;% \n  ggplot(aes(Apr, Oct)) +\n  geom_point()\n\n\n\n\n\n\n\n\nIt is not feasible to plot all the months against each other. PCA can help by condensing the columns and increasing the variance. PCA creates eigenvectors that represents the data in a concentrated way. Eigenvectors and eigenvalues do not represent observed data. They are calculated representations of the data. We will refer to eigenvectors as “principal components”.\nIn this case, where our data is measured by months in a year, each principal component could loosely be compared to a season."
  },
  {
    "objectID": "posts/exploring-311-data-with-pca/index.html#prep-the-data-for-pca",
    "href": "posts/exploring-311-data-with-pca/index.html#prep-the-data-for-pca",
    "title": "Exploring 311 Data With PCA",
    "section": "",
    "text": "The PCA function requires an all-numeric dataframe, so drop the request types into the dataframe metadata\n\n(df_months %&gt;% \n  ungroup() %&gt;% \n  remove_rownames() %&gt;% \n  column_to_rownames(var = \"request_type\") -&gt; df_months_pca1)\n\n                                                 Jan      Feb      Mar      Apr\nAbandoned Vehicle (parked on street)        0.088991 0.072656 0.076910 0.070955\nBarking Dog                                 0.056306 0.060811 0.060811 0.063063\nBoard Up (PLI referral to DPW)              0.039474 0.048246 0.065789 0.094298\nBroken Sidewalk                             0.033665 0.154552 0.147666 0.087223\nBuilding Maintenance                        0.070780 0.091890 0.103353 0.073933\nBuilding Without a Permit                   0.084242 0.069709 0.063579 0.057675\nCatch Basin, Clogged                        0.063642 0.037714 0.077784 0.074838\nCity Source (CDBG)                          0.005267 0.002458 0.010534 0.042837\nCity Steps, Need Cleared                    0.044262 0.018033 0.014754 0.019672\nCity Steps, Need Repaired                   0.026477 0.030550 0.071283 0.050916\nCollapsed Catch Basin                       0.064220 0.053899 0.061927 0.075688\nCommercial Refuse/Dumpsters                 0.079430 0.079430 0.089613 0.077393\nCurb /Broken/Deteriorated                   0.042373 0.048729 0.072034 0.110169\nCurb/Request for Asphalt Windrow            0.038660 0.020619 0.028351 0.085052\nDead Animal                                 0.038181 0.032566 0.043234 0.076923\nDead tree (Public property)                 0.034516 0.028763 0.049856 0.066155\nDrainage/Leak                               0.141304 0.050000 0.035870 0.083696\nDrug Enforcement                            0.077085 0.049755 0.065172 0.079187\nDumping, Private Property                   0.064315 0.076763 0.120332 0.093361\nDumpster (on Street)                        0.070866 0.048819 0.042520 0.088189\nEarly Set Out                               0.072444 0.069736 0.062288 0.063643\nExcessive Noise/Disturbances                0.057377 0.047131 0.056011 0.085383\nField                                       0.016432 0.014085 0.042254 0.110329\nFire Safety System Not Working              0.093750 0.185547 0.128906 0.077474\nGraffiti, Documentation                     0.057116 0.054307 0.103933 0.102060\nGraffiti, Removal                           0.088710 0.111290 0.098387 0.046774\nHydrant                                     0.121771 0.062731 0.075646 0.053506\nIllegal Dumping                             0.065672 0.057214 0.076617 0.106965\nIllegal Parking                             0.095682 0.075074 0.081943 0.079735\nJunk Vehicles                               0.079384 0.093602 0.114929 0.068720\nLeak                                        0.171456 0.097418 0.048709 0.044812\nLeaves/Street Cleaning                      0.028967 0.031486 0.030227 0.059194\nLitter                                      0.064316 0.064555 0.085755 0.093378\nLitter Can, Public                          0.064777 0.049393 0.060729 0.069636\nMaintenance Issue                           0.026455 0.031746 0.039153 0.078307\nMayor's Office                              0.158455 0.033289 0.023968 0.027963\nMissed Blue Bag                             0.094002 0.042077 0.051925 0.068935\nMissed Pick Up                              0.076775 0.048198 0.047345 0.058861\nNeed Potable Water                          0.002398 0.914868 0.001199 0.003597\nOperating Without a License                 0.041215 0.021692 0.149675 0.114967\nOvergrowth                                  0.005058 0.007867 0.006462 0.019106\nParking Authority                           0.086022 0.075269 0.064516 0.105376\nPatrol                                      0.063164 0.047816 0.071429 0.081464\nPaving Concern/Problem                      0.054819 0.043324 0.042440 0.071618\nPaving Request                              0.052950 0.047504 0.108321 0.114675\nPermit Parking (Residential Parking Permit) 0.107062 0.075171 0.063781 0.079727\nPlayground                                  0.015038 0.024436 0.043233 0.093985\nPotholes                                    0.123324 0.052972 0.105230 0.112570\nPruning (city tree)                         0.024019 0.025372 0.044317 0.057510\nPublic Right of Way                         0.033397 0.029580 0.020992 0.057252\nQuestion                                    0.079824 0.064739 0.043997 0.024513\nReferral                                    0.099161 0.050725 0.046148 0.049962\nRefuse Violations                           0.079202 0.059708 0.065848 0.084728\nReplace/Repair a Sign                       0.067492 0.053093 0.080315 0.084814\nRequest New Sign                            0.069169 0.049768 0.059047 0.097005\nRetaining Wall Maintenance                  0.066239 0.091880 0.115385 0.096154\nRodent control                              0.040957 0.033594 0.041417 0.052462\nRoot prune                                  0.022321 0.038690 0.053571 0.098214\nSidewalk Obstruction                        0.052799 0.042621 0.044529 0.052163\nSidewalk, Lack of Snow/Ice Removal          0.767726 0.090465 0.002445 0.002445\nSinkhole                                    0.103995 0.058973 0.062143 0.066582\nSmoke detectors                             0.118421 0.064145 0.062500 0.092105\nSnow/Ice removal                            0.681273 0.135027 0.005995 0.002712\nSpeeding                                    0.063973 0.060606 0.084175 0.094276\nStreet Cleaning/Sweeping                    0.027306 0.026790 0.035033 0.102009\nStreet Light - Repair                       0.078803 0.055112 0.067830 0.044888\nStreet Obstruction/Closure                  0.126273 0.040733 0.081466 0.061100\nThank you - DPW                             0.136264 0.046154 0.032967 0.065934\nTraffic                                     0.065356 0.053666 0.061637 0.073326\nTraffic or Pedestrian Signal, Repair        0.089659 0.047221 0.069934 0.069337\nTraffic or Pedestrian Signal, Request       0.056641 0.029297 0.099609 0.087891\nTree Fallen Across Road                     0.042589 0.031516 0.051959 0.034923\nTree Fallen Across Sidewalk                 0.034125 0.028190 0.044510 0.044510\nTree Issues                                 0.038384 0.056566 0.076768 0.048485\nTree Removal                                0.042949 0.036507 0.071582 0.074445\nUnpermitted Electrical Work                 0.145055 0.012088 0.030769 0.019780\nUnpermitted HVAC Work                       0.108516 0.045330 0.064560 0.048077\nUtility Cut - Other                         0.114889 0.067995 0.052755 0.059789\nUtility Cut - PWSA                          0.202261 0.050251 0.057789 0.075377\nUtility Pole                                0.065728 0.075117 0.049296 0.075117\nVacant Building                             0.088199 0.083230 0.096066 0.048861\nWeeds/Debris                                0.029631 0.024965 0.035751 0.045083\nWires                                       0.060651 0.071006 0.060651 0.078402\nZoning Issue                                0.055000 0.065000 0.080833 0.082500\n                                                  May       Jun      Jul\nAbandoned Vehicle (parked on street)        0.0830356 0.0792922 0.077761\nBarking Dog                                 0.1036036 0.1013514 0.078829\nBoard Up (PLI referral to DPW)              0.1140351 0.0899123 0.109649\nBroken Sidewalk                             0.1048202 0.0964040 0.069625\nBuilding Maintenance                        0.0841532 0.0829115 0.072500\nBuilding Without a Permit                   0.1049046 0.0883288 0.092416\nCatch Basin, Clogged                        0.0984090 0.1319976 0.082499\nCity Source (CDBG)                          0.1955758 0.2134831 0.195225\nCity Steps, Need Cleared                    0.1163934 0.2163934 0.203279\nCity Steps, Need Repaired                   0.1283096 0.1201629 0.136456\nCollapsed Catch Basin                       0.1100917 0.0917431 0.083716\nCommercial Refuse/Dumpsters                 0.0529532 0.1038697 0.105906\nCurb /Broken/Deteriorated                   0.1525424 0.1122881 0.116525\nCurb/Request for Asphalt Windrow            0.1430412 0.2113402 0.155928\nDead Animal                                 0.0713083 0.1021898 0.139809\nDead tree (Public property)                 0.1246405 0.1447747 0.154362\nDrainage/Leak                               0.0902174 0.1043478 0.102174\nDrug Enforcement                            0.0988087 0.1023125 0.088998\nDumping, Private Property                   0.0746888 0.0622407 0.064315\nDumpster (on Street)                        0.0787402 0.1354331 0.105512\nEarly Set Out                               0.0886933 0.0873392 0.111713\nExcessive Noise/Disturbances                0.0887978 0.0758197 0.075137\nField                                       0.1854460 0.1384977 0.150235\nFire Safety System Not Working              0.1139323 0.0572917 0.047526\nGraffiti, Documentation                     0.1207865 0.1254682 0.073970\nGraffiti, Removal                           0.0338710 0.0596774 0.091935\nHydrant                                     0.0922509 0.0571956 0.064576\nIllegal Dumping                             0.1000000 0.1228856 0.113930\nIllegal Parking                             0.0765456 0.0691855 0.059863\nJunk Vehicles                               0.0864929 0.0710900 0.104265\nLeak                                        0.0491963 0.0526059 0.057964\nLeaves/Street Cleaning                      0.0629723 0.0541562 0.021411\nLitter                                      0.0826584 0.0855169 0.098380\nLitter Can, Public                          0.0923077 0.0995951 0.127126\nMaintenance Issue                           0.1417989 0.1185185 0.135450\nMayor's Office                              0.0319574 0.1824234 0.065246\nMissed Blue Bag                             0.0841540 0.1020591 0.087735\nMissed Pick Up                              0.1027938 0.1123907 0.118789\nNeed Potable Water                          0.0011990 0.0023981 0.000000\nOperating Without a License                 0.4338395 0.0542299 0.028200\nOvergrowth                                  0.1219444 0.2489463 0.234335\nParking Authority                           0.0860215 0.0838710 0.081720\nPatrol                                      0.1015348 0.0879575 0.095041\nPaving Concern/Problem                      0.1114058 0.1255526 0.085765\nPaving Request                              0.1397882 0.1458396 0.118306\nPermit Parking (Residential Parking Permit) 0.0569476 0.0706150 0.079727\nPlayground                                  0.1184211 0.1691729 0.159774\nPotholes                                    0.1346322 0.1150702 0.107095\nPruning (city tree)                         0.1234777 0.1742219 0.168133\nPublic Right of Way                         0.1316794 0.1650763 0.154580\nQuestion                                    0.0483972 0.0936518 0.122564\nReferral                                    0.0362319 0.0846682 0.129291\nRefuse Violations                           0.0784344 0.0983883 0.100844\nReplace/Repair a Sign                       0.1196850 0.1113611 0.094713\nRequest New Sign                            0.0932096 0.0927879 0.097427\nRetaining Wall Maintenance                  0.0961538 0.0982906 0.085470\nRodent control                              0.0745513 0.1099862 0.141279\nRoot prune                                  0.1190476 0.1264881 0.163690\nSidewalk Obstruction                        0.0807888 0.1075064 0.123410\nSidewalk, Lack of Snow/Ice Removal          0.0097800 0.0000000 0.002445\nSinkhole                                    0.0786303 0.1122384 0.128725\nSmoke detectors                             0.0871711 0.1348684 0.046053\nSnow/Ice removal                            0.0008564 0.0001427 0.000000\nSpeeding                                    0.0976431 0.0909091 0.104377\nStreet Cleaning/Sweeping                    0.1257084 0.1298300 0.123132\nStreet Light - Repair                       0.0498753 0.0675810 0.097257\nStreet Obstruction/Closure                  0.0549898 0.0509165 0.087576\nThank you - DPW                             0.0769231 0.0967033 0.105495\nTraffic                                     0.0887354 0.0600425 0.054729\nTraffic or Pedestrian Signal, Repair        0.0854752 0.1040048 0.086671\nTraffic or Pedestrian Signal, Request       0.1074219 0.1191406 0.085938\nTree Fallen Across Road                     0.1345826 0.2206133 0.137990\nTree Fallen Across Sidewalk                 0.1424332 0.1958457 0.126113\nTree Issues                                 0.1010101 0.1010101 0.129293\nTree Removal                                0.1295634 0.1410165 0.118826\nUnpermitted Electrical Work                 0.1054945 0.1230769 0.085714\nUnpermitted HVAC Work                       0.1689560 0.0879121 0.085165\nUtility Cut - Other                         0.0797186 0.0738570 0.082063\nUtility Cut - PWSA                          0.1005025 0.0967337 0.095477\nUtility Pole                                0.1126761 0.1244131 0.107981\nVacant Building                             0.0683230 0.0608696 0.064182\nWeeds/Debris                                0.1373690 0.1666364 0.157062\nWires                                       0.0961538 0.1434911 0.087278\nZoning Issue                                0.0958333 0.0866667 0.103333\n                                                  Aug      Sep       Oct\nAbandoned Vehicle (parked on street)        0.1014123 0.089331 0.0971584\nBarking Dog                                 0.1126126 0.123874 0.1126126\nBoard Up (PLI referral to DPW)              0.1228070 0.085526 0.0877193\nBroken Sidewalk                             0.0734507 0.052793 0.0849273\nBuilding Maintenance                        0.0918903 0.077562 0.0787086\nBuilding Without a Permit                   0.0815168 0.082879 0.1128520\nCatch Basin, Clogged                        0.1266942 0.104891 0.0931055\nCity Source (CDBG)                          0.1639747 0.080758 0.0582865\nCity Steps, Need Cleared                    0.1459016 0.118033 0.0557377\nCity Steps, Need Repaired                   0.1283096 0.107943 0.0855397\nCollapsed Catch Basin                       0.1238532 0.113532 0.0986239\nCommercial Refuse/Dumpsters                 0.1344196 0.071283 0.0712831\nCurb /Broken/Deteriorated                   0.1186441 0.084746 0.0572034\nCurb/Request for Asphalt Windrow            0.1082474 0.079897 0.0605670\nDead Animal                                 0.1235261 0.112296 0.1274565\nDead tree (Public property)                 0.1236817 0.102589 0.0882071\nDrainage/Leak                               0.1076087 0.058696 0.0782609\nDrug Enforcement                            0.1135249 0.117730 0.0946041\nDumping, Private Property                   0.1327801 0.076763 0.1016598\nDumpster (on Street)                        0.1070866 0.086614 0.0881890\nEarly Set Out                               0.1570752 0.080569 0.0663507\nExcessive Noise/Disturbances                0.0881148 0.090164 0.1038251\nField                                       0.1197183 0.098592 0.0610329\nFire Safety System Not Working              0.0449219 0.050781 0.0651042\nGraffiti, Documentation                     0.0608614 0.067416 0.1207865\nGraffiti, Removal                           0.1048387 0.125806 0.0935484\nHydrant                                     0.1254613 0.077491 0.0867159\nIllegal Dumping                             0.0651741 0.068657 0.0726368\nIllegal Parking                             0.0969087 0.103042 0.0991168\nJunk Vehicles                               0.0912322 0.072275 0.0758294\nLeak                                        0.1066732 0.080857 0.0681929\nLeaves/Street Cleaning                      0.0440806 0.012594 0.0642317\nLitter                                      0.1202954 0.095760 0.0824202\nLitter Can, Public                          0.1246964 0.110121 0.0850202\nMaintenance Issue                           0.1174603 0.113228 0.1047619\nMayor's Office                              0.1118509 0.114514 0.0892144\nMissed Blue Bag                             0.1056401 0.089526 0.0957923\nMissed Pick Up                              0.1106846 0.098315 0.0835999\nNeed Potable Water                          0.0731415 0.000000 0.0000000\nOperating Without a License                 0.0390456 0.021692 0.0455531\nOvergrowth                                  0.1944366 0.104805 0.0396179\nParking Authority                           0.1032258 0.090323 0.0731183\nPatrol                                      0.1097993 0.103306 0.1151122\nPaving Concern/Problem                      0.0813439 0.089302 0.1432361\nPaving Request                              0.0928896 0.075340 0.0490166\nPermit Parking (Residential Parking Permit) 0.1321185 0.102506 0.0956720\nPlayground                                  0.1672932 0.090226 0.0733083\nPotholes                                    0.0759860 0.050155 0.0442425\nPruning (city tree)                         0.1586604 0.082206 0.0801759\nPublic Right of Way                         0.1832061 0.094466 0.0620229\nQuestion                                    0.1646763 0.065996 0.1451917\nReferral                                    0.1525553 0.089245 0.1060259\nRefuse Violations                           0.1122026 0.085035 0.0968534\nReplace/Repair a Sign                       0.0899888 0.088189 0.0758155\nRequest New Sign                            0.1100801 0.113454 0.0889920\nRetaining Wall Maintenance                  0.0769231 0.055556 0.1089744\nRodent control                              0.1385182 0.125633 0.1214910\nRoot prune                                  0.1056548 0.096726 0.0937500\nSidewalk Obstruction                        0.1246819 0.117048 0.0966921\nSidewalk, Lack of Snow/Ice Removal          0.0000000 0.002445 0.0000000\nSinkhole                                    0.1230184 0.088142 0.0798985\nSmoke detectors                             0.0756579 0.041118 0.1118421\nSnow/Ice removal                            0.0001427 0.000000 0.0001427\nSpeeding                                    0.1144781 0.112795 0.0791246\nStreet Cleaning/Sweeping                    0.1215868 0.098403 0.0963421\nStreet Light - Repair                       0.1057357 0.105237 0.1134663\nStreet Obstruction/Closure                  0.1201629 0.120163 0.0916497\nThank you - DPW                             0.1252747 0.112088 0.0879121\nTraffic                                     0.1046759 0.162062 0.1232731\nTraffic or Pedestrian Signal, Repair        0.1165571 0.105798 0.0854752\nTraffic or Pedestrian Signal, Request       0.0742188 0.111328 0.1035156\nTree Fallen Across Road                     0.1831346 0.064736 0.0442930\nTree Fallen Across Sidewalk                 0.2121662 0.063798 0.0474777\nTree Issues                                 0.1454545 0.094949 0.0848485\nTree Removal                                0.1460272 0.085183 0.0787402\nUnpermitted Electrical Work                 0.0571429 0.065934 0.1318681\nUnpermitted HVAC Work                       0.0824176 0.075549 0.0879121\nUtility Cut - Other                         0.1160610 0.110199 0.0937866\nUtility Cut - PWSA                          0.0716080 0.062814 0.0690955\nUtility Pole                                0.1197183 0.077465 0.0563380\nVacant Building                             0.0749482 0.077847 0.0815735\nWeeds/Debris                                0.1616070 0.097922 0.0718657\nWires                                       0.1316568 0.091716 0.0532544\nZoning Issue                                0.0908333 0.079167 0.0875000\n                                                 Nov      Dec\nAbandoned Vehicle (parked on street)        0.086098 0.076400\nBarking Dog                                 0.074324 0.051802\nBoard Up (PLI referral to DPW)              0.076754 0.065789\nBroken Sidewalk                             0.064269 0.030604\nBuilding Maintenance                        0.093419 0.078900\nBuilding Without a Permit                   0.086285 0.075613\nCatch Basin, Clogged                        0.064820 0.043606\nCity Source (CDBG)                          0.025632 0.005969\nCity Steps, Need Cleared                    0.022951 0.024590\nCity Steps, Need Repaired                   0.075356 0.038697\nCollapsed Catch Basin                       0.068807 0.053899\nCommercial Refuse/Dumpsters                 0.069246 0.065173\nCurb /Broken/Deteriorated                   0.046610 0.038136\nCurb/Request for Asphalt Windrow            0.047680 0.020619\nDead Animal                                 0.083661 0.048849\nDead tree (Public property)                 0.051774 0.030681\nDrainage/Leak                               0.070652 0.077174\nDrug Enforcement                            0.067274 0.045550\nDumping, Private Property                   0.074689 0.058091\nDumpster (on Street)                        0.083465 0.064567\nEarly Set Out                               0.069059 0.071090\nExcessive Noise/Disturbances                0.092213 0.140027\nField                                       0.032864 0.030516\nFire Safety System Not Working              0.051432 0.083333\nGraffiti, Documentation                     0.072097 0.041199\nGraffiti, Removal                           0.066129 0.079032\nHydrant                                     0.090406 0.092251\nIllegal Dumping                             0.083582 0.066667\nIllegal Parking                             0.088077 0.074828\nJunk Vehicles                               0.091232 0.050948\nLeak                                        0.097418 0.124696\nLeaves/Street Cleaning                      0.430730 0.159950\nLitter                                      0.070272 0.056694\nLitter Can, Public                          0.068826 0.047773\nMaintenance Issue                           0.053968 0.039153\nMayor's Office                              0.083888 0.077230\nMissed Blue Bag                             0.091316 0.086840\nMissed Pick Up                              0.070164 0.072084\nNeed Potable Water                          0.000000 0.001199\nOperating Without a License                 0.026030 0.023861\nOvergrowth                                  0.013768 0.003653\nParking Authority                           0.068817 0.081720\nPatrol                                      0.070838 0.052538\nPaving Concern/Problem                      0.108753 0.042440\nPaving Request                              0.034493 0.020877\nPermit Parking (Residential Parking Permit) 0.077449 0.059226\nPlayground                                  0.041353 0.003759\nPotholes                                    0.053289 0.025434\nPruning (city tree)                         0.041272 0.020636\nPublic Right of Way                         0.044847 0.022901\nQuestion                                    0.084852 0.061596\nReferral                                    0.081998 0.073989\nRefuse Violations                           0.067536 0.071220\nReplace/Repair a Sign                       0.070191 0.064342\nRequest New Sign                            0.076339 0.052720\nRetaining Wall Maintenance                  0.070513 0.038462\nRodent control                              0.071330 0.048780\nRoot prune                                  0.043155 0.038690\nSidewalk Obstruction                        0.086514 0.071247\nSidewalk, Lack of Snow/Ice Removal          0.000000 0.122249\nSinkhole                                    0.058339 0.039315\nSmoke detectors                             0.088816 0.077303\nSnow/Ice removal                            0.003711 0.169997\nSpeeding                                    0.052189 0.045455\nStreet Cleaning/Sweeping                    0.082947 0.030912\nStreet Light - Repair                       0.115960 0.098254\nStreet Obstruction/Closure                  0.089613 0.075356\nThank you - DPW                             0.065934 0.048352\nTraffic                                     0.088735 0.063762\nTraffic or Pedestrian Signal, Repair        0.084877 0.054991\nTraffic or Pedestrian Signal, Request       0.068359 0.056641\nTree Fallen Across Road                     0.036627 0.017036\nTree Fallen Across Sidewalk                 0.040059 0.020772\nTree Issues                                 0.070707 0.052525\nTree Removal                                0.047960 0.027201\nUnpermitted Electrical Work                 0.101099 0.121978\nUnpermitted HVAC Work                       0.074176 0.071429\nUtility Cut - Other                         0.069168 0.079719\nUtility Cut - PWSA                          0.081658 0.036432\nUtility Pole                                0.075117 0.061033\nVacant Building                             0.132091 0.123810\nWeeds/Debris                                0.045386 0.026722\nWires                                       0.076923 0.048817\nZoning Issue                                0.092500 0.080833\n\n\nCreate the PCA object\n\n(df_months_pca1 %&gt;% \n  prcomp(scale = TRUE) -&gt; pc)\n\nStandard deviations (1, .., p=12):\n [1] 2.0313544303132338165 1.5112299607905637089 1.3677583442481686671\n [4] 1.0647449915481708160 0.9373153843502739502 0.6612690017981475155\n [7] 0.6319678449167122070 0.5732234023111666410 0.4666060722915733039\n[10] 0.4192405535100036107 0.3847717270238655840 0.0000000000000002152\n\nRotation (n x k) = (12 x 12):\n        PC1     PC2       PC3      PC4      PC5        PC6      PC7      PC8\nJan -0.3509  0.2377  0.036588 -0.40554 -0.40259 -0.0878343  0.04648  0.10077\nFeb -0.2189  0.2230 -0.226391  0.69886  0.14610  0.1005237  0.15306 -0.04311\nMar -0.0235 -0.4858 -0.323760  0.11519 -0.19761 -0.1724337 -0.62037  0.31077\nApr  0.1329 -0.4686 -0.301500 -0.04306 -0.07313 -0.5014973  0.54199 -0.09929\nMay  0.2339 -0.1448 -0.445597 -0.33914  0.15507  0.5723879 -0.10942 -0.40042\nJun  0.4049  0.1866  0.002112 -0.22765  0.21210 -0.0386388  0.13438  0.42038\nJul  0.4322  0.1697  0.095923 -0.10888  0.01800 -0.1443989  0.02032  0.13081\nAug  0.3866  0.1805  0.189907  0.17598  0.04242 -0.2554247 -0.44835 -0.13536\nSep  0.2944 -0.1580  0.365255  0.16856 -0.42595 -0.0005222  0.04916 -0.53195\nOct  0.1150 -0.4130  0.389922  0.16708 -0.17452  0.5136274  0.18989  0.43757\nNov -0.1323 -0.3291  0.333424 -0.05855  0.69855 -0.1384698 -0.02866 -0.09659\nDec -0.3720 -0.1311  0.333522 -0.25017  0.03408 -0.0524049 -0.15824 -0.15902\n          PC9     PC10      PC11   PC12\nJan -0.119474 -0.19207  0.310620 0.5699\nFeb  0.085692  0.10149 -0.130098 0.5209\nMar  0.259371  0.01955  0.035606 0.1620\nApr -0.264116  0.01396 -0.143422 0.1379\nMay -0.106727  0.02849 -0.028027 0.2717\nJun  0.373786 -0.36939 -0.413396 0.2471\nJul  0.103405  0.77442  0.256091 0.2205\nAug -0.607417 -0.20278 -0.123033 0.1983\nSep  0.454920 -0.16862  0.035475 0.1519\nOct -0.304459  0.02271 -0.003209 0.1456\nNov  0.104258 -0.14115  0.387838 0.2472\nDec  0.008971  0.35391 -0.678722 0.1743\n\n\nInspect the PCA object with tidier functions from the broom library. These functions turn the PCA object into a tidy dataframe\n\npc %&gt;% \n  tidy() %&gt;% \n  head()\n\n# A tibble: 6 × 3\n  row                                     PC   value\n  &lt;chr&gt;                                &lt;dbl&gt;   &lt;dbl&gt;\n1 Abandoned Vehicle (parked on street)     1 -0.844 \n2 Abandoned Vehicle (parked on street)     2 -0.844 \n3 Abandoned Vehicle (parked on street)     3  0.383 \n4 Abandoned Vehicle (parked on street)     4  0.311 \n5 Abandoned Vehicle (parked on street)     5 -0.206 \n6 Abandoned Vehicle (parked on street)     6  0.0620\n\n\n\npc %&gt;% \n  tidy(\"pcs\")\n\n# A tibble: 12 × 4\n      PC  std.dev percent cumulative\n   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n 1     1 2.03e+ 0  0.344       0.344\n 2     2 1.51e+ 0  0.190       0.534\n 3     3 1.37e+ 0  0.156       0.690\n 4     4 1.06e+ 0  0.0945      0.785\n 5     5 9.37e- 1  0.0732      0.858\n 6     6 6.61e- 1  0.0364      0.894\n 7     7 6.32e- 1  0.0333      0.927\n 8     8 5.73e- 1  0.0274      0.955\n 9     9 4.67e- 1  0.0181      0.973\n10    10 4.19e- 1  0.0146      0.988\n11    11 3.85e- 1  0.0123      1    \n12    12 2.15e-16  0           1    \n\n\n\npc %&gt;% \n  augment(data = df_months) -&gt; au\n\nau %&gt;% \n  head()\n\n# A tibble: 6 × 26\n  .rownames request_type    Jan    Feb    Mar    Apr    May    Jun    Jul    Aug\n  &lt;chr&gt;     &lt;chr&gt;         &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 1         Abandoned V… 0.0890 0.0727 0.0769 0.0710 0.0830 0.0793 0.0778 0.101 \n2 2         Barking Dog  0.0563 0.0608 0.0608 0.0631 0.104  0.101  0.0788 0.113 \n3 3         Board Up (P… 0.0395 0.0482 0.0658 0.0943 0.114  0.0899 0.110  0.123 \n4 4         Broken Side… 0.0337 0.155  0.148  0.0872 0.105  0.0964 0.0696 0.0735\n5 5         Building Ma… 0.0708 0.0919 0.103  0.0739 0.0842 0.0829 0.0725 0.0919\n6 6         Building Wi… 0.0842 0.0697 0.0636 0.0577 0.105  0.0883 0.0924 0.0815\n# ℹ 16 more variables: Sep &lt;dbl&gt;, Oct &lt;dbl&gt;, Nov &lt;dbl&gt;, Dec &lt;dbl&gt;,\n#   .fittedPC1 &lt;dbl&gt;, .fittedPC2 &lt;dbl&gt;, .fittedPC3 &lt;dbl&gt;, .fittedPC4 &lt;dbl&gt;,\n#   .fittedPC5 &lt;dbl&gt;, .fittedPC6 &lt;dbl&gt;, .fittedPC7 &lt;dbl&gt;, .fittedPC8 &lt;dbl&gt;,\n#   .fittedPC9 &lt;dbl&gt;, .fittedPC10 &lt;dbl&gt;, .fittedPC11 &lt;dbl&gt;, .fittedPC12 &lt;dbl&gt;\n\n\nPlot how the PCA object explains the variance in the data\n\npc %&gt;% \n  tidy(\"pcs\") %&gt;%\n  select(-std.dev) %&gt;% \n  gather(measure, value, -PC) %&gt;% \n    ggplot(aes(PC, value)) +\n    geom_line() +\n    geom_point() +\n    facet_wrap(~measure) +\n    labs(title = \"Variance explained by each principal component\",\n         x = \"Principal Component\",\n         y = NULL) +\n    scale_x_continuous(breaks = 1:12)\n\n\n\n\n\n\n\n\nThe first two principal components explain most of the variance\nFor an in-depth plot we need to create the PCA object a different way\n\ndf_months %&gt;% \n  nest() %&gt;% \n  mutate(pca = map(data, ~ prcomp(.x %&gt;% select(-request_type), \n                                  center = TRUE, scale = TRUE)),\n         pca_aug = map2(pca, data, ~augment(.x, data = .y))) -&gt; df_months_pca2\n\nPlot the PCA data\n\ndf_months_pca2 %&gt;%\n  mutate(\n    pca_graph = map2(\n      .x = pca,\n      .y = data,\n      ~ autoplot(.x, loadings = TRUE, loadings.label = TRUE,\n                 loadings.label.repel = TRUE,\n                 data = .y) +\n        theme_bw() +\n        labs(x = \"Principal Component 1\",\n             y = \"Principal Component 2\",\n             title = \"First two principal components of PCA on 311 dataset\")\n    )\n  ) %&gt;%\n  pull(pca_graph)\n\n[[1]]\n\n\n\n\n\n\n\n\n\nThis shows that summer and winter explain a significant part of the variance\nPlot the data to show the outliers\n\nau %&gt;% \n  mutate(outlier = case_when(abs(.fittedPC1) &gt; 2 & abs(.fittedPC2) &gt; 1.5 ~ TRUE),\n         pothole = case_when(request_type == \"Potholes\" ~ \"Potholes\",\n                             request_type != \"Potholes\" ~ \"Other\")) -&gt; au\n\nau %&gt;% \nggplot(aes(.fittedPC1, .fittedPC2)) +\n  geom_point() +\n  geom_label_repel(data = au %&gt;% filter(outlier),\n             aes(label = request_type)) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nau %&gt;% \nggplot(aes(.fittedPC1, .fittedPC2)) +\n  geom_point(aes(color = pothole)) +\n  geom_label_repel(data = au %&gt;% filter(request_type == \"Potholes\"),\n             aes(label = request_type)) +\n  theme_bw() +\n  scale_color_manual(NULL, values = c(\"black\", \"red\"))"
  },
  {
    "objectID": "posts/actblue-interstate-political-campaign-donations/index.html",
    "href": "posts/actblue-interstate-political-campaign-donations/index.html",
    "title": "Actblue Interstate Political Campaign Donations",
    "section": "",
    "text": "ActBlue is an online service that allows people to make donations to the political campaigns of Democractic candidates across the country. This post uses graph theory to analyze how political donations moved across states."
  },
  {
    "objectID": "posts/actblue-interstate-political-campaign-donations/index.html#setup",
    "href": "posts/actblue-interstate-political-campaign-donations/index.html#setup",
    "title": "Actblue Interstate Political Campaign Donations",
    "section": "Setup",
    "text": "Setup\nThese are the libraries and graph theme I will use:\n\nlibrary(tidyverse)\nlibrary(maps)\nlibrary(sf)\n#library(rgeos)\nlibrary(janitor)\nlibrary(ggrepel)\nlibrary(tidygraph)\nlibrary(ggraph)\n\ntheme_set(theme_graph())\n\nsf::sf_use_s2(FALSE)\n\nThis code pulls the boundary polygons for the 48 continental U.S. states and the District of Columbia.\n\n#states &lt;- st_as_sf(map(\"state\", plot = FALSE, fill = TRUE))\nstates &lt;- st_as_sf(maps::map(\"state\", fill=TRUE, plot =FALSE))\nhead(states)\n\nSimple feature collection with 6 features and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -124.3834 ymin: 30.24071 xmax: -71.78015 ymax: 42.04937\nGeodetic CRS:  +proj=longlat +ellps=clrk66 +no_defs +type=crs\n                     ID                           geom\nalabama         alabama MULTIPOLYGON (((-87.46201 3...\narizona         arizona MULTIPOLYGON (((-114.6374 3...\narkansas       arkansas MULTIPOLYGON (((-94.05103 3...\ncalifornia   california MULTIPOLYGON (((-120.006 42...\ncolorado       colorado MULTIPOLYGON (((-102.0552 4...\nconnecticut connecticut MULTIPOLYGON (((-73.49902 4...\n\n\nThis code finds the center of each state, which will act as the nodes for the network graph.\n\nstates &lt;- cbind(states, st_coordinates(st_centroid(states)))\n\nI used this website to get the abbreviations for each state.\n\nstate_abbreviations &lt;- read_csv(\"https://raw.githubusercontent.com/conorotompkins/politics/master/data/state_abbreviations.csv\") %&gt;% \n  clean_names() %&gt;% \n  mutate(state_district = tolower(state_district)) %&gt;% \n  rename(abbr = postal_code) %&gt;% \n  select(-abbreviation)\n\nstates &lt;- states %&gt;% \n  left_join(state_abbreviations, by = c(\"ID\" = \"state_district\")) %&gt;% \n  arrange(abbr)\n\nThis pulls the ActBlue data from the Center of Public Integrity GitHub repo.\n\ndf &lt;- read_csv(\"https://raw.githubusercontent.com/PublicI/actblue-analysis/master/data/actblue_states.csv\")\n\nThis joins the boundary data with the state abbreviations.\n\ndf %&gt;% \n  semi_join(states, by = c(\"contributor_state\" = \"abbr\")) %&gt;% \n  semi_join(states, by = c(\"recipient_state\" = \"abbr\")) -&gt; df\n\ndf %&gt;% \n  select(-c(1, count, sum)) %&gt;% \n  gather(state_type, state_name) %&gt;% \n  distinct() %&gt;% \n  group_by(state_type) %&gt;% \n  summarize(n = n())\n\n# A tibble: 2 × 2\n  state_type            n\n  &lt;chr&gt;             &lt;int&gt;\n1 contributor_state    49\n2 recipient_state      49\n\n\nThis code joins the boundary data with the ActBlue data and excludes donations to and from non-continental U.S. states/territories.\n\nstates %&gt;% \n  semi_join(df, by = c(\"abbr\" = \"contributor_state\")) %&gt;% \n  semi_join(df, by = c(\"abbr\" = \"recipient_state\"))  -&gt; states\n\nThis plot shows that the boundary shapes and centroids are correct.\n\nstates %&gt;% \n  ggplot() +\n  geom_sf() +\n  geom_point(aes(X, Y)) +\n  theme(panel.grid.major = element_line(colour = 'transparent'))\n\n\n\n\n\n\n\n\nThis code cleans up the ActBlue data and removes intrastate donations.\n\ndf %&gt;%\n  select(-1) %&gt;% \n  arrange(contributor_state, recipient_state) %&gt;% \n  mutate(sum = sum / 10^6,\n         sum = round(sum, digits = 2)) %&gt;% \n  na.omit() %&gt;% \n  filter(!(contributor_state == recipient_state)) -&gt; df_intermediate"
  },
  {
    "objectID": "posts/actblue-interstate-political-campaign-donations/index.html#first-attempt",
    "href": "posts/actblue-interstate-political-campaign-donations/index.html#first-attempt",
    "title": "Actblue Interstate Political Campaign Donations",
    "section": "First attempt",
    "text": "First attempt\nThis is how the data looks when graphed as a typical network graph. The nodes (states) are not positioned geographically, which makes it difficult to understand. Aggregate donations less than $1,000,000 are excluded.\n\ndf_intermediate %&gt;% \n  as_tbl_graph(directed = TRUE) %&gt;% \n  activate(edges) %&gt;% \n  filter(sum &gt;= 1) %&gt;% \n  ggraph(layout =) +\n  geom_node_label(aes(label = name), size = 1, repel = FALSE) +\n  geom_edge_fan(aes(edge_width = sum, edge_alpha = sum),\n                arrow = arrow(length = unit(4, 'mm')), \n                start_cap = circle(3, 'mm'),\n                end_cap = circle(3, 'mm'),\n                color = \"blue\") +\n  scale_edge_width_continuous(\"Donations in millions USD\", range = c(.3, 2)) +\n  scale_edge_alpha_continuous(\"Donations in millions USD\", range = c(.1, 1))"
  },
  {
    "objectID": "posts/actblue-interstate-political-campaign-donations/index.html#mapping-node-positions-to-state-geography",
    "href": "posts/actblue-interstate-political-campaign-donations/index.html#mapping-node-positions-to-state-geography",
    "title": "Actblue Interstate Political Campaign Donations",
    "section": "Mapping node positions to state geography",
    "text": "Mapping node positions to state geography\nThis code turns the data into a network object and sets the minimum threshhold at $1 million\n\ndf_intermediate %&gt;% \n  as_tbl_graph(directed = TRUE) -&gt; g\n\nthreshhold &lt;- 1\n\ng %&gt;% \n  activate(edges) %&gt;% \n  filter(sum &gt;= 1) -&gt; g\n\nThis code creates the node positions for the network graph. The centroid of each state will be used as the node for that state.\n\nnode_pos &lt;- states %&gt;%\n  select(abbr, X, Y) %&gt;%\n  rename(x = X, y = Y) %&gt;%  # node positions must be called x, y\n  st_set_geometry(NULL)\nstr(node_pos)\n\n'data.frame':   49 obs. of  3 variables:\n $ abbr: chr  \"AL\" \"AR\" \"AZ\" \"CA\" ...\n $ x   : num  -86.8 -92.4 -111.7 -119.6 -105.6 ...\n $ y   : num  32.8 34.9 34.3 37.3 39 ...\n\n\nThis code creates the node layout the graph will use and merges the network data with the layout.\n\nmanual_layout &lt;- create_layout(g, \n                     #'manual',\n                     layout = node_pos)\n\nThis is the final graph:\n\nggraph(manual_layout) +\n  geom_sf(data = states) +\n  geom_node_point(size = .5, alpha = 0) +\n  geom_edge_fan(aes(edge_width = sum, edge_alpha = sum),\n                arrow = arrow(length = unit(4, 'mm')), \n                start_cap = circle(1, 'mm'),\n                end_cap = circle(1, 'mm'),\n                color = \"blue\") +\n  scale_edge_width_continuous(\"Donations in millions USD\", range = c(.3, 2)) +\n  scale_edge_alpha_continuous(\"Donations in millions USD\", range = c(.1, 1)) +\n  labs(title = \"ActBlue Political Donations\",\n       subtitle = str_c(\"Aggregate interstate donations greater than $\", threshhold, \" million USD, 2017-01-01 to 2018-09-30\"),\n       caption = \"@conor_tompkins, data from Center for Public Integrity and 538\") +\n  theme(panel.grid.major = element_line(colour = 'transparent')) -&gt; p\n\np"
  },
  {
    "objectID": "posts/actblue-interstate-political-campaign-donations/index.html#to-and-from",
    "href": "posts/actblue-interstate-political-campaign-donations/index.html#to-and-from",
    "title": "Actblue Interstate Political Campaign Donations",
    "section": "To and From",
    "text": "To and From\nThis shows all the donations from California. Note the different scale of funds.\n\nggraph(manual_layout) +\n  geom_sf(data = states) +\n  geom_node_point(size = .5, alpha = 0) +\n  geom_edge_fan(aes(edge_width = sum, edge_alpha = sum),\n                arrow = arrow(length = unit(4, 'mm')), \n                start_cap = circle(1, 'mm'),\n                end_cap = circle(1, 'mm'),\n                color = \"blue\") +\n  scale_edge_width_continuous(\"Donations in millions USD\", range = c(.3, 2)) +\n  scale_edge_alpha_continuous(\"Donations in millions USD\", range = c(.1, 1)) +\n  labs(title = \"ActBlue Political Donations\",\n       subtitle = str_c(\"Aggregate interstate donations from \", from_state, \", 2017-01-01 to 2018-09-30\"),\n       caption = \"@conor_tompkins, data from Center for Public Integrity and 538\") +\n  theme(panel.grid.major = element_line(colour = 'transparent')) -&gt; p_ca\n\np_ca\n\n\n\n\n\n\n\n\nThis shows the donations to candidates in Texas. Note the different scale of funds.\n\n# manual_layout &lt;- create_layout(graph = g,\n#                                layout = \"manual\", node.positions = node_pos)\n\n\nggraph(manual_layout) +\n  geom_sf(data = states) +\n  geom_node_point(size = .5, alpha = 0) +\n  geom_edge_fan(aes(edge_width = sum, edge_alpha = sum),\n                arrow = arrow(length = unit(4, 'mm')), \n                start_cap = circle(1, 'mm'),\n                end_cap = circle(1, 'mm'),\n                color = \"blue\") +\n  scale_edge_width_continuous(\"Donations in millions USD\", range = c(.3, 2)) +\n  scale_edge_alpha_continuous(\"Donations in millions USD\", range = c(.1, 1)) +\n  labs(title = \"ActBlue Political Donations\",\n       subtitle = str_c(\"Aggregate interstate donations to \", to_state, \", 2017-01-01 to 2018-09-30\"),\n       caption = \"@conor_tompkins, data from Center for Public Integrity and 538\") +\n  theme(panel.grid.major = element_line(colour = 'transparent')) -&gt; p_tx\n\np_tx"
  },
  {
    "objectID": "posts/actblue-interstate-political-campaign-donations/index.html#references",
    "href": "posts/actblue-interstate-political-campaign-donations/index.html#references",
    "title": "Actblue Interstate Political Campaign Donations",
    "section": "References",
    "text": "References\n\nhttps://github.com/PublicI/actblue-analysis\nhttps://datascience.blog.wzb.eu/2018/05/31/three-ways-of-visualizing-a-graph-on-a-map/\nhttps://lookatthhedata.netlify.com/2017-11-12-mapping-your-oyster-card-journeys-in-london-with-tidygraph-and-ggraph/"
  },
  {
    "objectID": "posts/animating-growth-of-allegheny-county/index.html",
    "href": "posts/animating-growth-of-allegheny-county/index.html",
    "title": "Animating Growth of Allegheny County",
    "section": "",
    "text": "In this post I will show how to create animated graphs that illustrate the increase in buildings in Allegheny County.\nOne caveat about the data: it only includes parcels that were sold at some point. If the parcel was not sold, it is not included in this data. For example, a structure that was torn down and replaced but was not sold is not included. It is also reasonable to assume that the data quality decreases the older the records are. There may be a large amount of missing data.\nThe shapefiles for the parcels come from Pennsylvania Spatial Data Access.\nThe data about the construction dates comes from the WPRDC’s Parcels n’at dashboard. To get the relevant data, draw a box around entire county, select the “Year Built” field in the Property Assessments section, and then download the data. It will take a while to download data for the entire county.\nSet up the environment:\n\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(broom)\nlibrary(sf)\nlibrary(scales)\nlibrary(gganimate)\nlibrary(lwgeom)\n\noptions(scipen = 999, digits = 4)\n\ntheme_set(theme_bw())\n\nmy_caption &lt;- \"@conor_tompkins - data from @WPRDC\"\n\nThis reads in data about the land parcel (lot lines):\n\ndf &lt;- read_csv(\"post_data/parcel_data.csv\", progress = FALSE) %&gt;% \n  clean_names() |&gt; \n  select(parid, yearblt)\n\nThis reads in the parcel geometry\n\nfile &lt;- \"post_data/AlleghenyCounty_Parcels202409/AlleghenyCounty_Parcels202409.shp\"\nfile\n\n[1] \"post_data/AlleghenyCounty_Parcels202409/AlleghenyCounty_Parcels202409.shp\"\n\nshapefile &lt;- st_read(file)\n\nReading layer `AlleghenyCounty_Parcels202409' from data source \n  `/Users/conorotompkins/Documents/github_repos/ctompkins_quarto_blog/posts/animating-growth-of-allegheny-county/post_data/AlleghenyCounty_Parcels202409/AlleghenyCounty_Parcels202409.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 585186 features and 10 fields (with 9 geometries empty)\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 1243000 ymin: 321300 xmax: 1430000 ymax: 497900\nProjected CRS: NAD83 / Pennsylvania South (ftUS)\n\n\nNext we have to clean up the parcel geometry:\n\nvalid_check &lt;- shapefile %&gt;% \n  slice(1:nrow(shapefile)) %&gt;% \n  pull(geometry) %&gt;% \n  map(st_is_valid) %&gt;% \n  unlist()\n\nshapefile$validity_check &lt;- valid_check\n\nshapefile &lt;- shapefile %&gt;% \n  filter(validity_check == TRUE)\n\n\nshapefile &lt;- shapefile %&gt;% \n  st_make_valid() %&gt;% \n  clean_names() %&gt;% \n  mutate(pin = as.character(pin))\n\nThen, join the parcel geometry and parcel data:\n\nparcel_data &lt;- shapefile %&gt;% \n  left_join(df, by = join_by(pin == parid))\n\nThis turns the parcel geometry into (x, y) coordinates:\n\ncentroids &lt;- parcel_data %&gt;% \n  st_centroid() %&gt;% \n  st_coordinates() %&gt;% \n  as_tibble() %&gt;% \n  clean_names()\n\nWe can plot the coordinates to confirm that the locations make sense:\n\ncentroids %&gt;% \n  distinct(x, y) %&gt;% \n  ggplot(aes(x, y)) +\n  geom_point(size = .1, alpha = .1) +\n  theme_void() +\n  coord_equal()\n\n\n\n\n\n\n\n\nThis plot shows that there is one row where yearblt_asmt is zero. That doesn’t make sense, so we will exclude it later.\n\ndf %&gt;% \n  ggplot(aes(yearblt)) +\n  geom_density() +\n  geom_rug() +\n  labs(title = \"Structures in Allegheny County\",\n       x = \"Year built\",\n       y = \"Density\",\n       subtitle = my_caption)\n\n\n\n\n\n\n\n\nThis combines the parcel_data and centroid data:\n\nparcel_geometry_cleaned &lt;- bind_cols(parcel_data, centroids) %&gt;% \n  select(pin, x, y, yearblt) %&gt;%\n  mutate(yearblt = as.integer(yearblt)) %&gt;% \n  filter(!is.na(yearblt),\n         yearblt &gt; 1000) %&gt;% \n  st_set_geometry(NULL)\n\nThis plots the culmulative sum of structures built:\n\nparcel_cumulative &lt;- parcel_geometry_cleaned %&gt;% \n  select(pin, yearblt) %&gt;% \n  arrange(yearblt) %&gt;% \n  count(yearblt) %&gt;% \n  mutate(cumulative_n = cumsum(n)) %&gt;% \n  ggplot(aes(yearblt, cumulative_n)) +\n  geom_line() +\n  geom_point() +\n  scale_y_continuous(label = comma) +\n    labs(title = \"Cumulative sum of structures built in Allegheny County\",\n       x = \"Year Built\",\n       y = \"Cumulative sum\",\n       caption = my_caption) +\n  transition_reveal(yearblt)\n\nparcel_cumulative\n\n\n\n\n\n\n\n\nThis creates a graph of the structures built in Allegheny County, colored by the construction year.\n\nparcel_geometry_cleaned %&gt;% \n  ggplot(aes(x, y, color = yearblt, group = pin)) +\n  geom_point(alpha = .3, size = .1) +\n  scale_color_viridis_c(\"Year structure was built\") +\n  theme_void() +\n  theme(axis.text = element_blank(),\n        axis.title = element_blank()) +\n  labs(title = \"Allegheny County land parcels\",\n       subtitle = \"Year built: {frame_along}\",\n       caption = \"@conor_tompkins, data from @WPRDC\") +\n  transition_reveal(yearblt)"
  },
  {
    "objectID": "posts/time-series-clustering-covid-19-cases/index.html",
    "href": "posts/time-series-clustering-covid-19-cases/index.html",
    "title": "Time series clustering COVID-19 case data",
    "section": "",
    "text": "Interactive Tableau visualization of the clusters\nThe goal of this post is to group states into clusters based on the shape of the curve of a state’s cumulative sum of COVID-19 cases. This type of clustering is useful when the variance in absolute values of a time series obscures the underlying pattern in the data. Since states experienced plateaus and peaks at different times, my hope is that the clustering is able to identify those differences.\nThis loads the packages I will use in the analysis and set up the environment:\nlibrary(tidyverse)\nlibrary(tsibble)\nlibrary(dtwclust)\nlibrary(tidymodels)\nlibrary(hrbrthemes)\nlibrary(tidycensus)\nlibrary(sf)\n\noptions(scipen = 999, digits = 4)\n\ntheme_set(theme_ipsum())\n\nset.seed(1234)\nI will adjust the cases to per 100,000, which requires information from the U.S. Census. This code pulls state-level population data from the Census API via tidycensus:\ncensus_data &lt;- get_acs(geography = \"state\", variables = \"B01003_001\", geometry = FALSE, year = 2020) %&gt;% \n  select(state = NAME, population = estimate)\nThis pulls the COVID-19 data from the NYTimes GitHub page:\ncovid &lt;- read_csv(\"https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-states.csv\") %&gt;% \n  arrange(state, date) %&gt;% \n  semi_join(census_data) %&gt;% \n  filter(date &lt;= \"2020-07-18\")\nI use the tsibble package to check if there are implicit gaps in the data. For example, if there was data for 2020-06-01 and 2020-06-03, there is an implicit gap because there is not data for 2020-06-02.\ncovid %&gt;% \n  as_tsibble(index = date, key = state) %&gt;% \n  count_gaps()\n\n# A tibble: 0 × 4\n# ℹ 4 variables: state &lt;chr&gt;, .from &lt;date&gt;, .to &lt;date&gt;, .n &lt;int&gt;\nThankfully, there are not any such gaps. If there were, I would have to impute values for the missing days.\nSince states experienced onset of COVID-19 at different times, I find the day each state hit 10 cases, and calculate days_since_10th_case, which I will use instead of date.\ncovid_10th_case &lt;- covid %&gt;% \n  filter(cases &gt;= 10) %&gt;% \n  group_by(state) %&gt;% \n  slice(1) %&gt;% \n  ungroup() %&gt;% \n  select(state, date_of_10th_case = date)\n\ncovid &lt;- covid %&gt;% \n  left_join(covid_10th_case, by = c(\"state\" = \"state\")) %&gt;% \n  group_by(state) %&gt;% \n  mutate(days_since_10th_case = date - date_of_10th_case) %&gt;% \n  ungroup() %&gt;% \n  filter(days_since_10th_case &gt;= 0)\n\ncovid &lt;- covid %&gt;% \n  select(state, days_since_10th_case, cases)\nNext I calculate cases_per_capita:\ncovid &lt;- covid %&gt;% \n  left_join(census_data) %&gt;% \n  mutate(cases_per_capita = (cases / population) * 100000) %&gt;% \n  select(-population)\nNext I scale the cases so that the mean is 0 and the standard deviation is 1. Each state has its own mean and standard deviation.\ncovid &lt;- covid %&gt;% \n  group_by(state) %&gt;% \n  mutate(cases_per_capita_scaled = scale(cases_per_capita, center = TRUE, scale = TRUE)) %&gt;% \n  ungroup()\nThe result of this is that the clustering algorithm will focus on the shape of the line for each state instead of absolute values. This graph shows the difference:\ncovid %&gt;% \n  pivot_longer(cols = contains(\"cases\"), names_to = \"metric\", values_to = \"value\") %&gt;% \n  ggplot(aes(days_since_10th_case, value, group = state)) +\n  geom_hline(yintercept = 0, linetype = 2) +\n  geom_line(alpha = .1) +\n  facet_wrap(~metric, ncol = 1, scales = \"free_y\") +\n  scale_y_comma()\ntsclust requires that the input data be a series of lists, not a dataframe. unstack takes a key and value as arguments and turns the dataframe into a list of lists.\ncovid_list &lt;- covid %&gt;% \n  select(state, cases_per_capita_scaled) %&gt;% \n  unstack(cases_per_capita_scaled ~ state)\nThis loops through the clustering function 20 times and saves each output to a list. The first object groups the data into 2 clusters, the second object has 3 clusters, and it continues in that pattern.\ncluster_dtw_h &lt;- list()\n\nkclust &lt;- 20\n\nfor (i in 2:kclust){\n  cluster_dtw_h[[i]] &lt;- tsclust(covid_list, \n                                type = \"h\", \n                                k = i,\n                                distance = \"dtw\", \n                                control = hierarchical_control(method = \"complete\"), \n                                seed = 390, \n                                preproc = NULL, \n                                args = tsclust_args(dist = list(window.size = 21L)))\n  \n  print(i)\n}\n\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n[1] 11\n[1] 12\n[1] 13\n[1] 14\n[1] 15\n[1] 16\n[1] 17\n[1] 18\n[1] 19\n[1] 20\nThe object that tsclust outputs has a complex structure that makes it difficult to work with at scale. The data I need to pull out is stored in various slots. The next step is to write functions that pulls out the data and tidies it up."
  },
  {
    "objectID": "posts/time-series-clustering-covid-19-cases/index.html#mapping",
    "href": "posts/time-series-clustering-covid-19-cases/index.html#mapping",
    "title": "Time series clustering COVID-19 case data",
    "section": "Mapping",
    "text": "Mapping\nThe data is aggregated at the state level, which can easily be graphed with ggplot2 and tidycensus.\n\nmap &lt;- get_acs(geography = \"state\", variables = \"B01003_001\", geometry = TRUE, shift_geo = TRUE)\n\nmap %&gt;% \n  ggplot() +\n  geom_sf() +\n  theme_void()\n\n\n\n\n\n\n\n\nThis joins the cluster assignments to the map object and summarizes the state polygons by region. This dissolves the state boundaries and creates polygons for each cluster.\n\nmap_cluster &lt;- map %&gt;% \n  left_join(cluster_assignments %&gt;% \n             filter(kclust == best_kclust), by = c(\"NAME\" = \"state\")) %&gt;% \n  add_count(cluster_assignment) %&gt;% \n  mutate(cluster_assignment = as.character(cluster_assignment),\n         cluster_assignment = fct_reorder(cluster_assignment, desc(n))) %&gt;% \n  group_by(cluster_assignment) %&gt;% \n  summarize()\n\nstate_clustered &lt;- map %&gt;% \n  left_join(cluster_assignments %&gt;% \n              filter(kclust == best_kclust), by = c(\"NAME\" = \"state\")) %&gt;% \n  add_count(cluster_assignment) %&gt;% \n  mutate(cluster_assignment = as.character(cluster_assignment),\n         cluster_assignment = fct_reorder(cluster_assignment, desc(n)))\n\nThis code creates the map, and overlays the state boundaries on the cluster polygons.\n\nmap_cluster %&gt;% \n  ggplot() +\n  geom_sf(aes(fill = cluster_assignment, color = cluster_assignment),\n          size = 1) +\n  geom_sf_text(data = state_clustered, aes(label = cluster_assignment)) +\n  labs(fill = \"Cluster assignment\",\n       color = \"Cluster assigmment\") +\n  guides(color = FALSE) +\n  theme_void() +\n  theme(legend.position = \"bottom\",\n        legend.direction = \"horizontal\")\n\n\n\n\n\n\n\n\nCluster 3 stands out as the group of states that are currently struggling with COVID-19 the most. Interestingly, these states are consistently clustered together regardless of the value of kclust, which means that these states are very similar.\nCluster 5 represents the states that had the earliest and worst outbreaks, but have beaten back the virus for now. Cluster 6 are the neighbors of New York and New Jersey. They experienced less peaky curves later than Cluster 5. Cluster 6 is an “echo” of Cluster 5.\nThe singleton clusters for kclust of 12 are Vermont, Nebraska, and Hawaii. Nebraska had a long period with almost no new cases at the beginning, but then had a very steep increase after that. Vermont’s curve started steeply almost immediately after its 10th case, which distinguishes it from the other states. Hawaii has had two periods of very steep increases sperated by a long period with few new cases. This is very likely due to the difficulty of traveling to the state with travel lockdowns in place.\n\nSources\n\nhttps://rpubs.com/esobolewska/dtw-time-series\nhttp://www.rdatamining.com/examples/time-series-clustering-classification\nhttp://rstudio-pubs-static.s3.amazonaws.com/398402_abe1a0343a4e4e03977de8f3791e96bb.html"
  },
  {
    "objectID": "posts/ebirding-in-allegheny-county/index.html",
    "href": "posts/ebirding-in-allegheny-county/index.html",
    "title": "eBirding in Allegheny County",
    "section": "",
    "text": "In this post I will do some exploratory analysis on eBird data. I’ve picked up birdwatching as a hobby during quarantine, and eBird has a ton of cool data on bird sightings."
  },
  {
    "objectID": "posts/ebirding-in-allegheny-county/index.html#setup",
    "href": "posts/ebirding-in-allegheny-county/index.html#setup",
    "title": "eBirding in Allegheny County",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(vroom)\nlibrary(janitor)\nlibrary(rebird)\nlibrary(hrbrthemes)\nlibrary(ggrepel)\nlibrary(gganimate)\nlibrary(widyr)\nlibrary(tidygraph)\nlibrary(ggraph)\nlibrary(tidytext)\n\noptions(scipen = 999, digits = 2)\n\ntheme_set(theme_ipsum())\n\nset.seed(1234)"
  },
  {
    "objectID": "posts/ebirding-in-allegheny-county/index.html#load-and-filter-data",
    "href": "posts/ebirding-in-allegheny-county/index.html#load-and-filter-data",
    "title": "eBirding in Allegheny County",
    "section": "Load and filter data",
    "text": "Load and filter data\nI downloaded data for bird sightings in Allegheny County from the eBird data portal. This code loads the data in R and prepares it for analysis.\n\ndf &lt;- vroom(\"post_data/ebd_US-PA-003_201001_202003_relFeb-2020.zip\", delim = \"\\t\") %&gt;% \n  clean_names() %&gt;% \n  mutate_at(vars(observer_id, locality, observation_date, time_observations_started, protocol_type), str_replace_na, \"NA\") %&gt;% \n  mutate(observation_date = ymd(observation_date),\n         observation_count_old = observation_count,\n         observation_count = as.numeric(str_replace(observation_count_old, \"X\", as.character(NA))),\n         observation_event_id = str_c(observer_id, locality, observation_date, time_observations_started)) %&gt;% \n  filter(all_species_reported == 1)\n\nI will focus on the two major “types” of observation protocols. The others are related to specific birding events, and might not be representative of the overall data.\n\ndf_top_protocols &lt;- df %&gt;% \n  count(protocol_type, sort = TRUE) %&gt;% \n  slice(1:2)\n\ndf_top_protocols\n\n# A tibble: 2 × 2\n  protocol_type      n\n  &lt;chr&gt;          &lt;int&gt;\n1 Traveling     545872\n2 Stationary    293991\n\n\n\ndf &lt;- df %&gt;% \n  semi_join(df_top_protocols)\n\nThis shows that Allegheny County birders began submitting data regularly in 2016. I will focus my analysis on recent observations.\n\ndf %&gt;% \n  count(observation_date) %&gt;% \n  mutate(recent_observation = year(observation_date) &gt;= 2016,\n         observation_date = ymd(observation_date)) %&gt;% \n  ggplot(aes(observation_date, n, color = recent_observation)) +\n    geom_line() +\n    scale_y_comma() +\n    scale_color_discrete(\"Recent Observation\") +\n    labs(y = \"Number of observations\",\n         x = \"Observation date\")\n\n\n\n\n\n\n\n\n\ndf &lt;- df %&gt;% \n  filter(year(observation_date) &gt;= 2016)\n\nThere is wide variation in the number of times a species was sighted in a given observation.\n\ndf %&gt;% \n  ggplot(aes(observation_count, group = common_name)) +\n    geom_density() +\n    scale_x_log10() +\n    labs(x = \"Species count (log10)\")\n\n\n\n\n\n\n\n\nThis calculates the 99th percentile of bird count per observation, and highlights the birds that are seen in large flocks.\n\ndf %&gt;% \n  group_by(common_name) %&gt;% \n  summarize(observation_count_99 = quantile(observation_count, probs = c(.99), na.rm = TRUE)) %&gt;% \n  arrange(desc(observation_count_99))\n\n# A tibble: 306 × 2\n   common_name       observation_count_99\n   &lt;chr&gt;                            &lt;dbl&gt;\n 1 Ring-billed Gull                 3400 \n 2 crow sp.                         2000 \n 3 gull sp.                          500 \n 4 Herring Gull                      248.\n 5 Tundra Swan                       230.\n 6 European Starling                 176.\n 7 Horned Lark                       175 \n 8 Bufflehead                        170.\n 9 Canada Goose                      148 \n10 Larus sp.                         143.\n# ℹ 296 more rows\n\n\nThis shows that the high count of Ring-billed Gulls is explained by groups of birders seeing flocks of thousands of the species. This also highlights that the same bird sighting can be counted twice because of simultaneous observation.\n\ndf %&gt;% \n  filter(common_name == \"Ring-billed Gull\") %&gt;% \n  arrange(desc(observation_count)) %&gt;% \n  slice(1:10) %&gt;% \n  select(observer_id, group_identifier, common_name, observation_date, duration_minutes, observation_count, locality)\n\n# A tibble: 10 × 7\n   observer_id group_identifier common_name    observation_date duration_minutes\n   &lt;chr&gt;       &lt;chr&gt;            &lt;chr&gt;          &lt;date&gt;                      &lt;dbl&gt;\n 1 obsr160352  G2943178         Ring-billed G… 2018-02-13                     38\n 2 obsr40545   G2943178         Ring-billed G… 2018-02-13                     38\n 3 obsr40545   G2930894         Ring-billed G… 2018-02-09                     50\n 4 obsr160352  G2930894         Ring-billed G… 2018-02-09                     75\n 5 obsr160352  &lt;NA&gt;             Ring-billed G… 2016-01-29                     25\n 6 obsr160352  G1578214         Ring-billed G… 2016-01-30                     71\n 7 obsr101818  G1578214         Ring-billed G… 2016-01-30                     71\n 8 obsr40545   G2940041         Ring-billed G… 2018-02-12                     55\n 9 obsr160352  &lt;NA&gt;             Ring-billed G… 2018-02-10                     55\n10 obsr620338  G2940041         Ring-billed G… 2018-02-12                     55\n# ℹ 2 more variables: observation_count &lt;dbl&gt;, locality &lt;chr&gt;"
  },
  {
    "objectID": "posts/ebirding-in-allegheny-county/index.html#species-counts",
    "href": "posts/ebirding-in-allegheny-county/index.html#species-counts",
    "title": "eBirding in Allegheny County",
    "section": "Species counts",
    "text": "Species counts\nThis does a basic count of the major species, not accounting for simultaneous observation:\n\ndf_counts &lt;- df %&gt;% \n  group_by(common_name) %&gt;% \n  summarize(species_count = sum(observation_count, na.rm = TRUE)) %&gt;% \n  arrange(desc(species_count))\n\ndf_counts\n\n# A tibble: 306 × 2\n   common_name       species_count\n   &lt;chr&gt;                     &lt;dbl&gt;\n 1 Ring-billed Gull         253373\n 2 American Crow            189804\n 3 European Starling        180456\n 4 American Robin           180148\n 5 Canada Goose             159040\n 6 Mallard                  103066\n 7 Northern Cardinal         84604\n 8 Mourning Dove             70975\n 9 Blue Jay                  64894\n10 Song Sparrow              63571\n# ℹ 296 more rows\n\n\n\ndf_counts %&gt;% \n  mutate(common_name = fct_reorder(common_name, species_count)) %&gt;% \n  slice(1:20) %&gt;% \n  ggplot(aes(species_count, common_name)) +\n    geom_col() +\n    scale_x_comma() +\n    labs(x = \"Observations\",\n         y = NULL)"
  },
  {
    "objectID": "posts/ebirding-in-allegheny-county/index.html#species-correlations",
    "href": "posts/ebirding-in-allegheny-county/index.html#species-correlations",
    "title": "eBirding in Allegheny County",
    "section": "Species correlations",
    "text": "Species correlations\nI was interested in which birds are correlated most with two common and popular birds, the Cardinal and the Blue Jay. This calculates the pairwise correlation and plots the top 10 birds correlated with the two target species:\n\nspecies_list &lt;- c(\"Northern Cardinal\", \"Blue Jay\")\n\n\ndf_pair_corr &lt;- df %&gt;% \n  pairwise_cor(common_name, observation_event_id, diag = FALSE, upper = FALSE)\n\n\ndf_pair_corr %&gt;% \n  filter(item1 %in% species_list) %&gt;% \n  drop_na(correlation) %&gt;% \n  arrange(item1, desc(correlation)) %&gt;% \n  group_by(item1) %&gt;% \n  slice(1:10) %&gt;% \n  ungroup() %&gt;% \n  mutate(item2 = reorder_within(x = item2, by = correlation, within = item1)) %&gt;% \n  ggplot(aes(correlation, item2, fill = item1)) +\n    geom_col(alpha = .9) +\n    facet_wrap(~item1, scales = \"free_y\") +\n    scale_y_reordered() +\n    scale_fill_manual(values = c(\"blue\", \"red\")) +\n    guides(fill = FALSE) +\n    labs(x = \"Correlation\",\n         y = NULL)\n\n\n\n\n\n\n\n\nCardinals and Blue Jays share many birds in common, but there are some distinctions, and the level of correlation can vary. I use a slopegraph to show the difference in how much a bird is correlated with Blue Jays vs. Cardinals.\nThis grabs the data for the two target species and gets the top 20 correlated birds for each species:\n\ndf_slopegraph &lt;- df_pair_corr %&gt;% \n  filter(item1 %in% species_list) %&gt;% \n  drop_na(correlation) %&gt;% \n  arrange(item1, desc(correlation)) %&gt;% \n  group_by(item1) %&gt;% \n  slice(1:20) %&gt;% \n  ungroup()\n\nThis calculates the difference in correlation for each bird:\n\ndf_corr_diff &lt;- df_slopegraph %&gt;% \n  pivot_wider(names_from = item1, values_from = correlation, names_prefix = \"corr_\") %&gt;% \n  clean_names() %&gt;% \n  mutate(corr_diff = abs(corr_blue_jay - corr_northern_cardinal)) %&gt;% \n  select(item2, corr_diff)\n\ndf_corr_diff\n\n# A tibble: 31 × 2\n   item2                   corr_diff\n   &lt;chr&gt;                       &lt;dbl&gt;\n 1 Northern Cardinal        NA      \n 2 Red-bellied Woodpecker    0.0347 \n 3 Tufted Titmouse           0.0142 \n 4 Carolina Wren            NA      \n 5 Downy Woodpecker         NA      \n 6 White-breasted Nuthatch   0.00495\n 7 Song Sparrow              0.118  \n 8 Northern Flicker          0.0199 \n 9 Mourning Dove            NA      \n10 Eastern Towhee           NA      \n# ℹ 21 more rows\n\n\n\ndf_slopegraph %&gt;% \n  left_join(df_corr_diff) %&gt;% \n  ggplot(aes(item1, correlation)) +\n    geom_line(aes(group = item2, color = corr_diff), size = 2) +\n    geom_point(size = 2) +\n    geom_text_repel(data = filter(df_slopegraph, item1 == species_list[2]),\n                    aes(y = correlation, label = item2), direction = \"both\", nudge_x = -.3, segment.alpha = .2) +\n    geom_text_repel(data = filter(df_slopegraph, item1 == species_list[1]),\n                    aes(y = correlation, label = item2), direction = \"both\", nudge_x = .3, segment.alpha = .2) +\n    scale_color_viridis_c(\"Absolute difference in correlation\") +\n    labs(x = NULL,\n         y = \"Correlation\") +\n    theme(panel.grid.minor.y = element_blank(),\n          panel.grid.major.y = element_blank(),\n          axis.title.x = element_blank())"
  },
  {
    "objectID": "posts/ebirding-in-allegheny-county/index.html#network-graph",
    "href": "posts/ebirding-in-allegheny-county/index.html#network-graph",
    "title": "eBirding in Allegheny County",
    "section": "Network Graph",
    "text": "Network Graph\nNetwork graphs are fun ways to show counts or correlations between groups. Since birds often exist in distinct environments and vary by season, graph analysis should be able to visualize connections.\nThis takes the top 100 birds in terms of total count and makes a network graph object consisting of nodes and edges.\n\ngraph_object_corr &lt;- df_pair_corr %&gt;% \n  semi_join(df_counts %&gt;% slice(1:75), by = c(\"item1\" = \"common_name\")) %&gt;% \n  semi_join(df_counts %&gt;% slice(1:75), by = c(\"item2\" = \"common_name\")) %&gt;% \n  as_tbl_graph(directed = FALSE) %&gt;% \n  activate(nodes) %&gt;% \n  filter(!node_is_isolated()) %&gt;% \n  activate(edges) %&gt;% \n  filter(abs(correlation) &gt;= .05) %&gt;% \n  activate(nodes) %&gt;% \n  filter(!node_is_isolated()) %&gt;% \n  left_join(df_counts, by = c(\"name\" = \"common_name\"))\n\ngraph_object_corr\n\n# A tbl_graph: 74 nodes and 1925 edges\n#\n# An undirected simple graph with 1 component\n#\n# Node Data: 74 × 2 (active)\n   name                   species_count\n   &lt;chr&gt;                          &lt;dbl&gt;\n 1 American Crow                 189804\n 2 American Goldfinch             60297\n 3 American Robin                180148\n 4 Belted Kingfisher               2754\n 5 Black-capped Chickadee         12379\n 6 Blue Jay                       64894\n 7 Brown-headed Cowbird            8877\n 8 Bufflehead                      5758\n 9 Canada Goose                  159040\n10 Carolina Chickadee             21862\n# ℹ 64 more rows\n#\n# Edge Data: 1,925 × 3\n   from    to correlation\n  &lt;int&gt; &lt;int&gt;       &lt;dbl&gt;\n1     1     2       0.262\n2     1     3       0.259\n3     2     3       0.371\n# ℹ 1,922 more rows\n\n\nThis plots the network graph to show all the connections that fit the criteria in the code above:\n\nplot &lt;- graph_object_corr %&gt;% \n    ggraph() +\n    geom_edge_link(aes(width = correlation, alpha = correlation)) +\n    geom_node_point(aes(size = species_count, alpha = species_count)) +\n    scale_size_continuous(\"Total observations\", labels = scales::comma) +\n    scale_alpha_continuous(\"Total observations\", labels = scales::comma) +\n    scale_edge_alpha(\"Observations together\", range = c(.01, .3)) +\n    scale_edge_width(\"Observations together\", range = c(.1, 2)) +\n    theme_void()\n\nplot\n\n\n\n\n\n\n\n\nThere is a dense group of birds that are highly correlated with each other. This high correlation could be caused by a shared environment or seasonal migration patterns.\n\nHighlight species\nI also wanted to be able to see where a given species fits in the network graph. This code prepares a network graph object and filters out species that are not highly connected to the main group.\n\ngraph_object_corr &lt;- df_pair_corr %&gt;% \n  as_tbl_graph(directed = FALSE) %&gt;% \n  activate(edges) %&gt;% \n  filter(abs(correlation) &gt; .2) %&gt;% \n  activate(nodes) %&gt;% \n  mutate(centrality = centrality_authority()) %&gt;% \n  filter(centrality &gt; .01) %&gt;% \n  filter(!node_is_isolated())\n\nThis code identifies the node ID for the Northern Cardinal and makes a dataframe I will use to filter with in the next code chunk.\n\nspecies_list &lt;- c(\"Northern Cardinal\")\n\ndf_species_corr_nodes &lt;- graph_object_corr %&gt;% \n  activate(nodes) %&gt;% \n  as_tibble() %&gt;% \n  mutate(node_id = row_number()) %&gt;% \n  filter(name %in% species_list)\n\nThis code identifes which node is the Northern Cardinal, and which edges connect to the Northern Cardinal. This identifies all the species that are connected to the bird, given the criteria I set above.\n\nplot_corr &lt;- graph_object_corr %&gt;% \n  activate(nodes) %&gt;% \n  mutate(name_label = case_when(name %in% species_list ~ name,\n                                TRUE ~ as.character(NA))) %&gt;% \n  activate(edges) %&gt;% \n  left_join(df_species_corr_nodes, by = c(\"from\" = \"node_id\")) %&gt;% \n  left_join(df_species_corr_nodes, by = c(\"to\" = \"node_id\")) %&gt;% \n  mutate(name = coalesce(name.x, name.y),\n         centrality = coalesce(name.x, name.y)) %&gt;% \n  select(-matches(\".x|.y\")) %&gt;% \n  mutate(species_flag = case_when(from %in% df_species_corr_nodes$node_id | to %in% df_species_corr_nodes$node_id ~ TRUE,\n                                  TRUE ~ FALSE),\n         name = case_when(is.na(name) ~ \"Other species\",\n                          TRUE ~ name)) %&gt;%\n  ggraph() +\n    geom_edge_link(aes(alpha = species_flag, width = species_flag)) +\n    geom_node_point(aes(shape = !is.na(name_label), size = !is.na(name_label), color = name_label)) +\n    geom_node_label(aes(label = name_label, color = name_label), repel =  TRUE) +\n    scale_edge_width_manual(values = c(.3, 1)) +\n    scale_edge_alpha_discrete(range = c(0.1, .5)) +\n    scale_shape_manual(values = c(1, 19)) +\n    scale_size_manual(values = c(2, 3)) +\n    guides(edge_alpha = FALSE,\n           edge_width = FALSE,\n           size = FALSE,\n           shape = FALSE,\n           color = FALSE) +\n    theme_void()\n\nplot_corr"
  },
  {
    "objectID": "posts/graphing-seasonality-in-ebird-bird-sightings/index.html",
    "href": "posts/graphing-seasonality-in-ebird-bird-sightings/index.html",
    "title": "Graphing Seasonality in Ebird Bird Sightings",
    "section": "",
    "text": "Over the winter I became interested in birding. Sitting in your back yard doing nothing but watching birds fly around is quite relaxing. Naturally I am looking for ways to optimize and quantify this relaxing activity. eBird lets you track your bird sightings and research which birds are common or more rare in your area. Luckily, the folks at ROpenSci have the {rebird} package, which provides an easy interface to the eBird API.\nIn this post I will graph the seasonality of observation frequency of the top 10 birds in Pennsylvania. Frequency in this context is the % of eBird checklists that the bird appeared in during a given period.\nLoad up packages:\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(vroom)\nlibrary(janitor)\nlibrary(rebird)\nlibrary(hrbrthemes)\nlibrary(ggrepel)\nlibrary(gganimate)\n\ntheme_set(theme_ipsum())\n\noptions(scipen = 99, digits = 2)\n\nThe ebirdfreq takes a location and time period and returns the frequency and sample size for the birds returned in the query.\n\n\n[1] 48\n\n\n[1] 48\n\n\n\ndf_freq_raw &lt;- ebirdfreq(loctype = 'states', loc = 'US-PA', startyear = 2019,\n                         endyear = 2019, startmonth = 1, endmonth = 12)\n\n\ndf_freq_raw\n\n# A tibble: 22,272 × 3\n   com_name                     month_qt   frequency\n   &lt;chr&gt;                        &lt;chr&gt;          &lt;dbl&gt;\n 1 Black-bellied Whistling-Duck January-1          0\n 2 Black-bellied Whistling-Duck January-2          0\n 3 Black-bellied Whistling-Duck January-3          0\n 4 Black-bellied Whistling-Duck January-4          0\n 5 Black-bellied Whistling-Duck February-1         0\n 6 Black-bellied Whistling-Duck February-2         0\n 7 Black-bellied Whistling-Duck February-3         0\n 8 Black-bellied Whistling-Duck February-4         0\n 9 Black-bellied Whistling-Duck March-1            0\n10 Black-bellied Whistling-Duck March-2            0\n# ℹ 22,262 more rows\n\n\nThis does some light data munging to get the data in shape.\n\ndf_freq_clean &lt;- df_freq_raw %&gt;% \n  separate(month_qt, into = c(\"month\", \"week\")) %&gt;% \n  mutate(week = as.numeric(week),\n         month = ymd(str_c(\"2019\", month, \"01\", sep = \"-\")),\n         month = month(month, label = TRUE, abbr = TRUE),\n         state = \"PA\") %&gt;% \n  rename(common_name = com_name) %&gt;% \n  arrange(common_name, month, week)\n\ndf_freq_clean\n\n# A tibble: 22,272 × 5\n   common_name        month  week frequency state\n   &lt;chr&gt;              &lt;ord&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;\n 1 Acadian Flycatcher Jan       1         0 PA   \n 2 Acadian Flycatcher Jan       2         0 PA   \n 3 Acadian Flycatcher Jan       3         0 PA   \n 4 Acadian Flycatcher Jan       4         0 PA   \n 5 Acadian Flycatcher Feb       1         0 PA   \n 6 Acadian Flycatcher Feb       2         0 PA   \n 7 Acadian Flycatcher Feb       3         0 PA   \n 8 Acadian Flycatcher Feb       4         0 PA   \n 9 Acadian Flycatcher Mar       1         0 PA   \n10 Acadian Flycatcher Mar       2         0 PA   \n# ℹ 22,262 more rows\n\n\nThis takes the month-week time series and summarizes to the month level:\n\ndf_month &lt;- df_freq_clean %&gt;% \n  group_by(common_name, month) %&gt;% \n  summarize(frequency_mean = mean(frequency) %&gt;% round(2)) %&gt;%\n  ungroup()\n\ndf_month\n\n# A tibble: 5,568 × 3\n   common_name        month frequency_mean\n   &lt;chr&gt;              &lt;ord&gt;          &lt;dbl&gt;\n 1 Acadian Flycatcher Jan             0   \n 2 Acadian Flycatcher Feb             0   \n 3 Acadian Flycatcher Mar             0   \n 4 Acadian Flycatcher Apr             0   \n 5 Acadian Flycatcher May             0.04\n 6 Acadian Flycatcher Jun             0.06\n 7 Acadian Flycatcher Jul             0.05\n 8 Acadian Flycatcher Aug             0.02\n 9 Acadian Flycatcher Sep             0   \n10 Acadian Flycatcher Oct             0   \n# ℹ 5,558 more rows\n\n\nHere I find the top 10 birds in terms of average monthly observation frequency:\n\ndf_top_birds &lt;- df_freq_clean %&gt;% \n  group_by(common_name) %&gt;% \n  summarize(frequency_mean = mean(frequency) %&gt;% round(2)) %&gt;% \n  ungroup() %&gt;% \n  arrange(desc(frequency_mean)) %&gt;% \n  select(common_name) %&gt;% \n  slice(1:10)\n\ndf_top_birds\n\n# A tibble: 10 × 1\n   common_name           \n   &lt;chr&gt;                 \n 1 Northern Cardinal     \n 2 Blue Jay              \n 3 Mourning Dove         \n 4 American Robin        \n 5 Song Sparrow          \n 6 American Crow         \n 7 Red-bellied Woodpecker\n 8 American Goldfinch    \n 9 Carolina Wren         \n10 Downy Woodpecker      \n\n\nThis basic line graph shows some of the pattern of seasonality, but fails to show the cyclical nature of the data.\n\ndf_month %&gt;% \n  semi_join(df_top_birds) %&gt;% \n  ggplot(aes(month, frequency_mean, group = common_name)) +\n    geom_line() +\n    scale_y_percent() +\n    labs(title = \"Bird observation frequency\",\n         subtitle = \"Top 10 birds in PA, 2019\",\n         x = NULL,\n         y = \"Mean frequency\",\n         caption = \"Data from ebird.org. @conorotompkins\")\n\n\n\n\n\n\n\n\nI use coord_polar to change the coordinate system to match the cyclical flow of the months:\n\ndf_month %&gt;% \n  semi_join(df_top_birds) %&gt;% \n  ggplot(aes(month, frequency_mean, group = common_name)) +\n    geom_polygon(color = \"black\", fill = NA, size = .5) +\n    coord_polar() +\n    scale_y_percent() +\n    labs(title = \"Bird observation frequency\",\n         subtitle = \"Top 10 birds in PA, 2019\",\n         x = NULL,\n         y = \"Mean frequency\",\n         caption = \"Data from ebird.org. @conorotompkins\")\n\n\ngganimate lets me focus on one species at a time while showing all the data.\n\nplot_animated &lt;- df_month %&gt;% \n  semi_join(df_top_birds) %&gt;% \n  mutate(common_name = fct_inorder(common_name)) %&gt;% \n  ggplot(aes(month, frequency_mean)) +\n  geom_polygon(data = df_month %&gt;% rename(name = common_name),\n               aes(group = name),\n               color = \"grey\", fill = NA, size = .5) +\n  geom_polygon(aes(group = common_name),\n               color = \"blue\", fill = NA, size = 1.2) +\n  coord_polar() +\n  #facet_wrap(~common_name) +\n  scale_y_percent() +\n   labs(subtitle = \"Most frequently observed birds in PA (2019)\",\n        x = NULL,\n        y = \"Frequency of observation\",\n        caption = \"Data from ebird.org. @conorotompkins\") +\n  theme(plot.margin = margin(2, 2, 2, 2),\n        plot.title = element_text(color = \"blue\"))\n\nplot_animated +\n  transition_manual(common_name) +\n  ggtitle(\"{current_frame}\")"
  },
  {
    "objectID": "posts/covid_19_vaccine_forecasts/index.html",
    "href": "posts/covid_19_vaccine_forecasts/index.html",
    "title": "COVID-19 vaccine forecast and pastcasts",
    "section": "",
    "text": "Like many other Americans, I have been following the COVID-19 vaccination campaign closely. I have found the graphs and data on the NYTimes and Bloomberg sites very useful for tracking trends.\nThis graph on the NYTimes site is particularly good, I think.\nYou can follow the trend of past vaccinations, and the forecast is useful.\nI wanted to know how it would look if multiple “historical” forecasts started along the trend line, in addition to the “current pace” line. This is possible with some nested tibbles and ggplot2. I walk through some of the data quality issues with the dataset at the end.\nSet up the environment:\n\nlibrary(tidyverse)\nlibrary(tidycensus)\nlibrary(sf)\nlibrary(broom)\nlibrary(lubridate)\nlibrary(janitor)\nlibrary(hrbrthemes)\nlibrary(ggrepel)\nlibrary(tune)\nlibrary(slider)\nlibrary(broom)\n\ntheme_set(theme_ipsum())\n\noptions(scipen = 999, digits = 4)\n\nThis sets up the total date range I will consider for the analysis:\n\n#set date range to examine.\ndate_seq &lt;- seq.Date(from = ymd(\"2020-12-01\"), to = ymd(\"2022-08-01\"), by = \"day\")\n\nI use the COVID-19 vaccine distribution data from Johns Hopkins University.\n\n#download data from JHU\nvacc_data_raw &lt;- read_csv(\"https://raw.githubusercontent.com/govex/COVID-19/master/data_tables/vaccine_data/us_data/time_series/vaccine_data_us_timeline.csv\") %&gt;% \n  clean_names() %&gt;% \n  #filter to only keep vaccine type All, which will show the cumulative sum of doses for all vaccine types\n  filter(vaccine_type == \"All\") %&gt;% \n  #select state, date, vaccine type, and stage one doses\n  select(province_state, date, stage_one_doses) %&gt;% \n  #for each state, pad the time series to include all the dates in date_seq\n  complete(date = date_seq, province_state) %&gt;% \n  #sort by state and date\n  arrange(province_state, date)\n\n\nvacc_data_raw\n\n\nThis replaces NA values of stage_one_doses with 0 if it occurred before the current date.\n\nvacc_data &lt;- vacc_data_raw %&gt;% \n  mutate(stage_one_doses = case_when(date &lt; ymd(\"2021-04-22\") & is.na(stage_one_doses) ~ 0,\n                                     !is.na(stage_one_doses) ~ stage_one_doses,\n                                     TRUE ~ NA_real_)) %&gt;% \n  arrange(province_state, date)\n\nThis shows the cumulative sum of first doses by province_state. This reveals some data quality issues that I discuss later in the post.\n\nvacc_data %&gt;% \n  filter(date &lt;= ymd(\"2021-04-22\")) %&gt;% \n  ggplot(aes(date, stage_one_doses, group = province_state)) +\n  geom_line(alpha = .5, size = .3) +\n  scale_y_comma()\n\n\nThis calculates the total sum of first doses by date.\n\nvacc_data &lt;- vacc_data %&gt;% \n  group_by(date) %&gt;% \n  summarize(stage_one_doses = sum(stage_one_doses, na.rm = F)) %&gt;%\n  ungroup()\n\nThe inconsistent data reporting issues bubble up to the national level. I will take a 7-day trailing average to smooth that out.\n\nvacc_data %&gt;% \n  filter(date &lt;= ymd(\"2021-04-22\")) %&gt;% \n  ggplot(aes(date, stage_one_doses)) +\n  geom_line()\n\n\nThis calculates the number of new doses given out by day. If the difference between day 1 and day 0 is negative, I replace it with 0.\n\nvacc_data &lt;- vacc_data %&gt;% \n  mutate(stage_one_doses_new = stage_one_doses - lag(stage_one_doses, n = 1),\n         stage_one_doses_new = case_when(stage_one_doses_new &lt; 0 ~ 0,\n                                         TRUE ~ stage_one_doses_new))\n\nThis calculates the 7-day trailing average of new first doses distributed.\n\nvacc_data_rolling &lt;- vacc_data %&gt;% \n  mutate(stage_one_doses_new_rolling = slide_index_dbl(.i = date,\n                                                       .x = stage_one_doses_new,\n                                                       .f = mean,\n                                                       .before = 6,\n                                                       .complete = FALSE))\n\nThen I recalculate the cumulative sum of first doses using the trailing average instead of the raw data. This smooths out the data collection issues.\n\nvacc_forecast &lt;- vacc_data_rolling %&gt;% \n  fill(stage_one_doses_new_rolling, .direction = \"down\") %&gt;%\n  mutate(future_flag = date &gt;= ymd(\"2021-04-22\")) %&gt;%\n  mutate(stage_one_doses_new_rolling_forecast = cumsum(coalesce(stage_one_doses_new_rolling, 0)))\n\nvacc_forecast %&gt;% \n  filter(future_flag == F) %&gt;% \n  ggplot(aes(date, stage_one_doses_new_rolling_forecast)) +\n  geom_line() +\n  scale_y_comma()\n\n\nThis calculates the percent of the population with a first dose vaccination.\n\nvacc_forecast &lt;- vacc_forecast %&gt;% \n  mutate(total_pop = 332410303) %&gt;% \n  mutate(vacc_pct = stage_one_doses_new_rolling_forecast / total_pop)\n\n#at the current rate we should hit 90% around July 20th\nvacc_forecast %&gt;% \n  filter(vacc_pct &gt; .9) %&gt;% \n  slice(1)\n\n\nThis is a basic replication of the NYTimes graph.\n\nvacc_forecast %&gt;% \n  filter(date &lt;= ymd(\"2021-04-22\") + 120) %&gt;% \n  ggplot(aes(x = date)) +\n  geom_line(aes(y = vacc_pct, color = future_flag)) +\n  geom_hline(yintercept = .9, lty = 2) +\n  scale_y_percent(limits = c(0, 1), breaks = c(0, .25, .5, .75, .9, 1)) +\n  labs(y = \"Pct with 1 vaccination\")\n\n\nNow to add the “historical” projections. I create a list of dates to filter the data with.\n\nmonth_filters &lt;- c(seq(from = ymd(\"2021-02-01\"), to = ymd(\"2021-04-01\"), by = \"month\"), ymd(\"2021-04-22\"))\n\nmonth_filters\n\nI use map to create 4 dataframes, each only containing data up until the given filter date.\n\nvaccine_forecast_data &lt;- \n  month_filters %&gt;% \n  set_names() %&gt;% \n  map(~filter(vacc_data_rolling, date &lt;= .x)) %&gt;% \n  enframe(name = \"last_date\", value = \"historical_data\") %&gt;% \n  mutate(last_date = ymd(last_date),\n         current_week = last_date == max(last_date))\n\nvaccine_forecast_data\n\n\n\nvaccine_forecast_data %&gt;% \n  unnest(historical_data) %&gt;% \n  filter(date &lt;= ymd(\"2021-04-22\") + 120) %&gt;% \n  ggplot(aes(x = date)) +\n  geom_line(aes(y = stage_one_doses)) +\n  facet_wrap(~last_date) +\n  scale_y_comma()\n\n\nThis unnests the tibbles and fills out the future data for each last_date\n\nvaccine_forecast_data &lt;- \n  vaccine_forecast_data %&gt;% \n  unnest(historical_data) %&gt;% \n  group_by(last_date) %&gt;% \n  #for each filter date table, create rows for the rest of the date range\n  complete(date = date_seq) %&gt;% \n  fill(stage_one_doses_new_rolling, current_week, .direction = \"down\") %&gt;% \n  #create a flag for whether a row is observed or predicted\n  mutate(prediction_flag = date &gt;= last_date,\n  #create a flag for whether a row is after the current date\n         future_flag = date &gt; ymd(\"2021-04-22\")) %&gt;%\n  #for each filter date, roll the 7 day moving average of vaccination rate forward\n  mutate(stage_one_doses_new_rolling_forecast = cumsum(coalesce(stage_one_doses_new_rolling, 0))) %&gt;% \n  #source of population data: https://www.census.gov/popclock/\n  mutate(total_pop = 330175936) %&gt;% \n  #calculate vaccination %\n  mutate(vacc_pct = stage_one_doses_new_rolling_forecast / total_pop,\n         vacc_pct = round(vacc_pct, 3)) %&gt;% \n  filter(vacc_pct &lt;= 1.1) %&gt;% \n  ungroup()\n\nvaccine_forecast_data\n\n\nThis calculates when the vaccination rate hits 100% for each historical projection.\n\nvaccine_forecast_data &lt;- \n  vaccine_forecast_data %&gt;% \n  mutate(total_vacc_flag = vacc_pct &gt;= 1) %&gt;% \n  group_by(last_date) %&gt;% \n  mutate(total_vacc_date = case_when(cumsum(total_vacc_flag) &gt;= 1 ~ date,\n                                     TRUE ~ NA_Date_)) %&gt;% \n  filter(cumsum(!is.na(total_vacc_date)) &lt;= 1) %&gt;% \n  ungroup()\n\nvaccine_forecast_data &lt;- vaccine_forecast_data %&gt;% \n  mutate(current_week_fct = case_when(current_week == F ~ \"Historical projection\",\n                                      current_week == T ~ \"Current rate\"))\n\nThen I create some secondary tables to annotate the final chart.\n\n#secondary tables for labeling\ncurrent_vacc_percent &lt;- vaccine_forecast_data %&gt;% \n  filter(current_week == T, date == ymd(\"2021-04-22\")) %&gt;% \n  select(last_date, date, current_week, vacc_pct)\n\ncurrent_vacc_percent_label &lt;- current_vacc_percent %&gt;% \n  mutate(text_label = str_c(\"Current\", scales::percent(vacc_pct), sep = \": \"))\n\n\nvaccine_forecast_graph &lt;- vaccine_forecast_data %&gt;% \n  ggplot(aes(x = date, y = vacc_pct, group = last_date)) +\n  #90% line\n  geom_hline(yintercept = .9, lty = 2) +\n  annotate(x = ymd(\"2021-01-25\"), y = .915,\n           label = \"Herd Immunity Threshhold\", geom = \"text\", size = 6) +\n  #past cumulative line\n  geom_line(data = filter(vaccine_forecast_data, current_week == T, date &lt;= ymd(\"2021-04-22\")),\n            color = \"black\", lty = 1, size = .7) +\n  #future cumulative lines\n  geom_line(data = filter(vaccine_forecast_data, prediction_flag == T),\n            aes(color = as.factor(last_date)),\n            size = 1.3)  +\n  # horizontal line showing current vaccination rate\n  geom_hline(data = current_vacc_percent,\n             aes(yintercept = vacc_pct),\n             size = .1) +\n  #add labels for date of 100% vaccination for the first and last filter dates\n  geom_label(data = filter(vaccine_forecast_data,\n                           last_date == min(last_date) | last_date == max(last_date)),\n             aes(label = total_vacc_date,\n                 color = as.factor(last_date)),\n             show.legend = FALSE,\n             fill = \"grey\",\n             position = position_nudge(y = .05),\n             size = 6) +\n  # label for horizontal line showing current vaccination rate\n  geom_text(data = current_vacc_percent_label,\n            aes(label = text_label),\n            position = position_nudge(x = -122, y = .02),\n            size = 6) +\n  scale_x_date(limits = c(ymd(\"2020-12-01\"), ymd(\"2022-08-01\"))) +\n  scale_y_percent(limits = c(0, 1.1), breaks = c(0, .25, .5, .75, .9, 1)) +\n  scale_alpha_manual(values = c(1, .5)) +\n  scale_color_viridis_d(labels = c(\"February 1\", \"March 1\", \"April 1\", \n                                   str_c(month(ymd(\"2021-04-22\"), label = T, abbr = F), mday(ymd(\"2021-04-22\")), sep = \" \"))) +\n  guides(color = guide_legend(override.aes = list(fill = NA))) +\n  labs(title = \"Historic and Current U.S. Vaccination Forecasts\",\n       x = NULL,\n       y = \"Single Dose Vaccination %\",\n       color = \"Projection start date\") +\n  theme_ipsum(plot_title_size = 25,\n              subtitle_size = 23,\n              axis_text_size = 23,\n              axis_title_size = 25) +\n  theme(panel.grid.minor = element_blank(),\n        legend.title = element_text(size = 20),\n        legend.text = element_text(size = 18))\n\nvaccine_forecast_graph\n\n\nBased on the difference in 7-day first-dose vaccinations, the U.S. shortened the timeline for 100% first-dose vaccination by 323 days.\nThis is a naive calculation, since the vaccination projections are based on static 7-day vaccination distribution averages. I would expect the 7-day distribution average to vary based on historical data. In addition, as the national first-dose vaccination percentage approaches 100%, the 7-day average of distribution will likely decrease because of two factors: portions of the remaining unvaccinated population probably have lower access to vaccines (i.e. homebound adults), or are hesitant to be vaccinated.\n\nvaccine_forecast_data %&gt;% \n  select(last_date, date, vacc_pct) %&gt;% \n  filter(last_date == min(last_date) | last_date == max(last_date)) %&gt;% \n  filter(vacc_pct &gt;= 1) %&gt;% \n  group_by(last_date) %&gt;% \n  slice(1) %&gt;% \n  ungroup() %&gt;% \n  select(last_date, date) %&gt;% \n  pivot_wider(names_from = last_date, values_from = date) %&gt;% \n  mutate(difference = `2021-02-01` - `2021-04-22`)\n\n\n\nData quality issues\n\nCumulative decreases\nDue to data collection issues, there are cases where the cumulative sum of vaccinations for a given state actually decreases. I think a lot of this is because of interruptions due to the nation-wide winter storm in February 2021. Other instances could be attributed to changes in methodology for tracking vaccine distribution.\n\nvacc_data_raw %&gt;% \n  group_by(province_state) %&gt;% \n  mutate(less_than_prev = stage_one_doses &lt; lag(stage_one_doses, 1)) %&gt;% \n  ungroup() %&gt;% \n  filter(less_than_prev == T) %&gt;% \n  count(date, less_than_prev) %&gt;% \n  ggplot(aes(date, n)) +\n  geom_point() +\n  labs(x = NULL,\n       y = \"Bad data observations\")\n\n\nThis shows that most of the inconsistent data comes from federal agencies, US territories, and Freely Associated States\n\nvacc_data_raw %&gt;% \n  group_by(province_state) %&gt;% \n  mutate(less_than_prev = stage_one_doses &lt; lag(stage_one_doses, 1)) %&gt;% \n  ungroup() %&gt;% \n  filter(less_than_prev == T) %&gt;% \n  count(province_state, less_than_prev, sort = T) \n\n\nThis is an example from Alabama:\n\nvacc_data_raw %&gt;% \n  mutate(less_than_prev = stage_one_doses &lt; lag(stage_one_doses, 1)) %&gt;% \n  filter(date &gt;= \"2021-02-10\", date &lt; \"2021-02-13\",\n         province_state == \"Alabama\") %&gt;% \n  arrange(province_state, date)\n\n\n\n\nPennsylvania\nFrom April 17-18th there was a big jump in stage one vaccine distribution in Pennsylvania. It is unclear what caused this jump. My guess is a data collection issue.\n\nvacc_data_raw %&gt;% \n  filter(province_state == \"Pennsylvania\") %&gt;% \n  drop_na(stage_one_doses) %&gt;% \n  ggplot(aes(date, stage_one_doses)) +\n  geom_line() +\n  scale_y_comma()"
  },
  {
    "objectID": "posts/allegheny-county-overdose-data/index.html",
    "href": "posts/allegheny-county-overdose-data/index.html",
    "title": "Allegheny County Overdose Data",
    "section": "",
    "text": "Opiate addiction and overdose have become a major problem in American society. The life expectancy in the U.S. has decreased for the past 2 years, and deaths from drug overdoses rose 21% from 2016 to 2017.\nCommunities have been flooded with prescription painkillers with deadly effects.\nPennsylvania and the Pittsburgh area have also suffered from drug overdoses. 38 out of every 100,000 Pennsylvanians died from a drug overdose in 2017\nThe Western Pennsylvania Regional Data Center has published data on accidental fatal overdoses in Allegheny County. The data spans from 2008 to present day, with a few months of delay on the most recent data due to the time required to collect and process the data.\nIn this post, I will use R to perform an exploratory data analysis on this data. I hope to find interesting trends in the data, and identify areas for further study. As always, the code and data used in this post are available here and on my GitHub repository for this post.\nThe first step is to load the libraries we will use, and to set the theme for all the plots.\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(scales)\nlibrary(viridis)\nlibrary(ggrepel)\nlibrary(knitr)\nlibrary(kableExtra)\n\n#use a customized version of theme_bw()\ntheme_set(theme_bw(base_size = 18))\nThen, load the data that the WPRDC published. I’ve already done some preprocessing of the data to get it into shape. You can find that script here.\ndf &lt;- read_csv(\"https://raw.githubusercontent.com/conorotompkins/allegheny_overdoses/master/data/overdose_df.csv\")\nThis code changes the way R treats some of the data, which makes it more useful for plotting. It also filters out data that was entered after September 1, 2017. This is because the data after that date is not complete.\ndf %&gt;% \n  mutate(year = as.factor(year),\n         incident_zip = as.character(incident_zip),\n         month = month(date, label = TRUE)) %&gt;% \n    filter(date &lt;= \"2017-09-01\") -&gt; df\nLet’s look at what the data looks like. Each row represents an individual overdose.\ndf %&gt;% \n  select(-c(year, yday, month, mday, wday, starts_with(\"combined\"))) %&gt;% \n  head() %&gt;% \n  kable(\"html\") %&gt;% \n  kable_styling()\n\n\n\n\nid\ndate\ndeath_time\nmanner_of_death\nage\nsex\nrace\ncase_dispo\nincident_zip\ndecedent_zip\nod_factors\n\n\n\n\n1\n2008-01-03\n06:20:00\nACCIDENT\n44\nMale\nBlack\nMO\n15212\n15212\nAlcohol, Codeine, Morphine\n\n\n2\n2008-01-03\n10:19:00\nACCIDENT\n42\nMale\nWhite\nMO\nNA\n15206\nMephobarbital, Morphine, Phenobarbital\n\n\n3\n2008-01-04\n09:43:00\nACCIDENT\n58\nMale\nWhite\nMO\n15132\n15132\nAlcohol, Methadone, Nordiazepam\n\n\n4\n2008-01-04\n23:15:00\nACCIDENT\n39\nMale\nWhite\nMO\n15204\n15204\nAlprazolam, Methadone, Nordiazepam\n\n\n5\n2008-01-05\n07:45:00\nACCIDENT\n42\nMale\nWhite\nMO\n15239\n15239\nAlcohol, Morphine\n\n\n6\n2008-01-05\n10:50:00\nAccidents\n34\nMale\nWhite\nMO\n15235\n15235\nDiphenhydramine, Morphine, Paroxetine, Trazodone\nThis chart shows the cumulative sum of all fatal overdoses in the timeframe of the dataset. This plot shows a visible uptick in the number of overdoses starting in 2015.\ndf %&gt;% \n  count(date) %&gt;% \n  mutate(n_cumsum = cumsum(n)) %&gt;% \n  ggplot(aes(date, n_cumsum)) +\n  geom_hline(yintercept = 0) +\n  geom_line(size = 2) +\n  scale_y_continuous(label=comma) +\n  labs(title = \"Fatal overdoses in Allegheny County\",\n       y = \"Cumulative sum of fatal overdoses\",\n       x = \"\",\n       subtitle = \"Data from the WPRDC\",\n       caption = \"Conor Tompkins @conor_tompkins\")\nWe can explore this data more granularly by looking at each year individually. The overall trend for the years 2008-2014 appear to be largely the same. As we saw in the previous graph, the number of fatal overdoses began to increase in 2015, and the situation worsened in 2016. 2017 was clearly on pace to be the worst year on record.\ndf %&gt;% \n  group_by(year, yday) %&gt;% \n  summarize(n = n()) %&gt;% \n  mutate(n_cumsum = cumsum(n)) -&gt; df_year_cumsum\n\ndf %&gt;% \n  group_by(year) %&gt;% \n  summarize(last_yday = last(yday),\n            total = n()) -&gt; df_year_tag\n\nggplot(data = df_year_cumsum, aes(x = yday, y = n_cumsum, color = year)) +\n  geom_line(size = 2) +\n  geom_label_repel(data = df_year_tag, aes(x = last_yday, y = total, label = year, color = year, group = year)) +\n  geom_point(data = df_year_tag, aes(x = last_yday, y = total, color = year, group = year), size = 3) +\n  scale_size_continuous(guide = FALSE) +\n  scale_color_discrete(guide = FALSE) +\n  labs(title = \"Fatal overdoses by year\",\n       x = \"Day of the year\",\n       y = \"Cumulative sum of fatal overdoses\",\n       subtitle = \"Data from the WPRDC\",\n       caption = \"Conor Tompkins @conor_tompkins\")\nThis plot shows a smoothed representation of the number of fatal overdoses per day for each year. 2016 and 2017 stand out here. Overdoses in 2016 escalated around June. As we saw previously, 2017 was worse than 2016.\ndf %&gt;% \n  count(year, yday) -&gt; df_year\n\ndf %&gt;% \n  count(year, yday) %&gt;% \n  group_by(year) %&gt;% \n  mutate(smooth = predict(loess(n ~ yday))) %&gt;% \n  summarize(last_yday = last(yday),\n            last_smooth = last(smooth),\n            tag = unique(year)) -&gt; df_year_label\n\nggplot(data = df_year, aes(x = yday, y = n, color = year)) +\n  geom_smooth(se = FALSE) +\n  geom_label_repel(data = df_year_label, aes(x = last_yday, y = last_smooth, label = year)) +\n  geom_point(data = df_year_label, aes(x = last_yday, y = last_smooth), size = 3) +\n  scale_color_discrete(guide = FALSE) +\n  labs(title = \"Fatal overdoses by year\",\n       x = \"Day of the year\",\n       y = \"Number of overdoses\",\n       subtitle = \"Data from the WPRDC\",\n       caption = \"Conor Tompkins @conor_tompkins\")\nThis heatmap shows the number of fatal overdoses by day of the month, month, and year. This plot shows spikes in deaths, especially in 2016 and 2017. Some media report that this phenomenon could be explained by fentanyl, a particularly potent synthetic opiate. For example, a heroin user could get a supply of heroin laced with fentanyl, and underestimate the strength of the dose.\n#create df of full calendar\ndf_dates &lt;- tibble(date = seq(first(df$date), last(df$date), by = \"day\"))\n\n#create tile df\ndf %&gt;% \n  mutate(n = 1) %&gt;% \n  right_join(df_dates) %&gt;% \n  mutate(year = year(date),\n         month = month(date, label = TRUE),\n         mday = mday(date)) %&gt;% \n  group_by(year, month, mday) %&gt;% \n  summarize(n = sum(n)) %&gt;% \n  replace_na(list(n = 0)) -&gt; df_tile\n\ndf_tile %&gt;% \n  ggplot(aes(mday, month, fill = n)) +\n  geom_tile() +\n  facet_wrap(~year, ncol = 2) +\n  scale_fill_viridis(\"Number of fatal overdoses\") +\n  coord_equal() +\n  scale_x_continuous(expand = c(0,0)) +\n  scale_y_discrete(expand = c(0,0),\n                   limits = rev(levels(df$month))) +\n  labs(title = \"Fatal overdoses in Allegheny County\",\n       y = NULL,\n       x = \"Day of the month\",\n       subtitle = \"Data from the WPRDC\",\n       caption = \"Conor Tompkins @conor_tompkins\") +\n  theme(panel.grid = element_blank())\nThe WPRDC dataset identifies the zip code that the overdose occurred in, which allows a broad geographic analysis of which communities are affected. The Mt. Oliver/Knoxville/Carrick area (15210) is most affected, followed by the North Side/Brighton Heights/Spring Hill (15212) and McKees Rocks/Robinson (15136) areas.\n#fatal overdoses by zip code\ntop_zips &lt;- df %&gt;% \n  count(incident_zip, sort = TRUE) %&gt;% \n  top_n(5) %&gt;% \n  select(incident_zip) %&gt;% \n  unlist()\n\ndf %&gt;% \n  count(incident_zip, sort = TRUE) %&gt;% \n  top_n(5) %&gt;% \n  rename(\"fatal_overdoses\" = n,\n         \"incident_zip_code\" = incident_zip) %&gt;% \n  kable(\"html\") %&gt;% \n  kable_styling()\n\n\n\n\nincident_zip_code\nfatal_overdoses\n\n\n\n\n15210\n177\n\n\n15212\n160\n\n\n15136\n106\n\n\n15216\n103\n\n\n15132\n90\nA cumulative view of the data by zip code show that 15210 and 15212 are in a different category compared to the other areas of Allegheny County.\nzips_df &lt;- df %&gt;% \n  arrange(incident_zip, date) %&gt;% \n  count(incident_zip, date) %&gt;% \n  group_by(incident_zip) %&gt;% \n  mutate(n_cumsum = cumsum(n))\n\ntop_zips_df &lt;- zips_df %&gt;% \n  filter(incident_zip %in% top_zips)\n\ntop_zips_df_labels &lt;- df %&gt;% \n  filter(incident_zip %in% top_zips) %&gt;% \n  group_by(incident_zip) %&gt;% \n  summarize(last_date = last(date),\n            tag = unique(incident_zip),\n            total = n())\n\nggplot(zips_df, aes(x = date, y = n_cumsum, group = incident_zip)) +\n  geom_line(size = 1, alpha = .1) +\n  geom_line(data = top_zips_df, aes(x = date, y = n_cumsum, color = incident_zip), size = 2) +\n  geom_label_repel(data = top_zips_df_labels, aes(x = last_date, y = total, label = tag, color = incident_zip)) +\n  geom_point(data = top_zips_df_labels, aes(x = last_date, y = total, color = incident_zip), size = 3) +\n  scale_alpha_manual(values = c(.1, 1), guide = FALSE) +\n  scale_color_discrete(guide = FALSE) +\n  scale_y_continuous(expand = c(.01, .1)) +\n  labs(title = \"Fatal overdoses per zip code\",\n       x = NULL,\n       y = \"Cumulative sum of fatal overdoses\",\n       subtitle = \"Data from the WPRDC\",\n       caption = \"Conor Tompkins @conor_tompkins\")\nA cumulative view of the number of overdoses by factor (drug) shows a striking trend. This table shows the top 10 drugs in terms of number of overdoses. It is important to note that most of these are not exclusive. An overdose victim can have multiple drugs in their system at the time of autopsy.\ndf_factor_total &lt;- tibble(factor = c(df$combined_od1,\n                 df$combined_od2,\n                 df$combined_od3,\n                 df$combined_od4,\n                 df$combined_od5,\n                 df$combined_od6,\n                 df$combined_od7))\n\ndf_factor_total &lt;- df_factor_total %&gt;% \n  filter(!is.na(factor)) %&gt;% \n  count(factor, sort = TRUE)\n\ndf_factor_total %&gt;% \n  rename(\"number_of_overdoses\" = n) %&gt;% \n  top_n(10) %&gt;% \n  kable(\"html\") %&gt;% \n  kable_styling()\n\n\n\n\nfactor\nnumber_of_overdoses\n\n\n\n\nHeroin\n1449\n\n\nCocaine\n1071\n\n\nFentanyl\n1017\n\n\nAlcohol\n807\n\n\nAlprazolam\n497\n\n\nOxycodone\n375\n\n\nMorphine\n304\n\n\nMethadone\n282\n\n\nHydrocodone\n196\n\n\nDiazepam\n190\nCocaine was the most common drug in the system of overdose victims up until 2012, but it was passed by heroin in 2013. The number of overdoses involving fentanyl exploded in 2015.\ndf %&gt;% \n  mutate(od_heroin = str_detect(od_factors, \"Heroin\"),\n         od_cocaine = str_detect(od_factors, \"Cocaine\"),\n         od_fentanyl = str_detect(od_factors, \"Fentanyl\"),\n         od_alcohol = str_detect(od_factors, \"Alcohol\"),\n         od_alprazolam = str_detect(od_factors, \"Alprazolam\"),\n         od_oxycodone = str_detect(od_factors, \"Oxycodone\"),\n         od_morphine = str_detect(od_factors, \"Morphine\"),\n         od_methadone = str_detect(od_factors, \"Methadone\"),\n         od_hydrocodone = str_detect(od_factors, \"Hydrocodone\"),\n         od_diazepam = str_detect(od_factors, \"Diazepam\")) -&gt; df_factors\n\n\ndf_factors %&gt;% \n  gather(od_factor, od_flag, starts_with(\"od_\")) %&gt;% \n  #gather(od_factor, od_flag, c(od_heroin, od_cocaine, od_fentanyl, od_alcohol)) %&gt;% \n  filter(od_flag == TRUE) %&gt;% \n  mutate(od_factor = str_to_title(str_replace(od_factor, \"od_\", \"\"))) -&gt; df_factors_long\n\n#create od_factor df\ndf_factors_long %&gt;% \n  group_by(od_factor, date) %&gt;% \n  summarize(n = n()) %&gt;% \n  group_by(od_factor) %&gt;% \n  mutate(od_cumsum = cumsum(n)) -&gt; df_factors_cumsum\n\n#create label df for od_factors\ndf_factors_long %&gt;% \n  group_by(od_factor) %&gt;% \n  summarize(last_date = last(date),\n            total = n()) -&gt; df_factors_cumsum_label\n\n#plot cumulative od_factor\nggplot(data = df_factors_cumsum, aes(x = date, y = od_cumsum, color = od_factor)) +\n  geom_line(size = 1.5) +\n  geom_label_repel(data = df_factors_cumsum_label, aes(x = last_date, y = total, label = od_factor, group = od_factor)) +\n  geom_point(data = df_factors_cumsum_label, aes(x = last_date, y = total, group = od_factor, color = od_factor), size = 3) +\n  scale_color_discrete(guide = FALSE) +\n  scale_y_continuous(expand = c(.01, .1), label = comma) +\n  labs(title = \"Fatal overdoses by drug factor\",\n       y = \"Cumulative sum of fatal overdoses\",\n       x = \"\",\n       subtitle = \"Data from the WPRDC\",\n       caption = \"Conor Tompkins @conor_tompkins \\n Drug overdoses not exclusive\")\nA view of the same data for the year 2017 show that fentanyl has by far been the most common drug linked to fatal overdoses.\ndf_factors_long %&gt;% \n  filter(date &gt;= \"2017-01-01\") %&gt;% \n  group_by(od_factor, date) %&gt;% \n  summarize(n = n()) %&gt;% \n  group_by(od_factor) %&gt;% \n  mutate(od_cumsum = cumsum(n)) -&gt; df_factors_cumsum_2017\n\ndf_factors_long %&gt;% \n  filter(date &gt;= \"2017-01-01\") %&gt;% \n  group_by(od_factor) %&gt;% \n  summarize(last_date = last(date),\n            total = n()) -&gt; df_factors_cumsum_label_2017\n\ndf_factors_cumsum_2017 %&gt;% \n  ggplot(aes(date, od_cumsum, color = od_factor)) +\n  geom_line(size = 2) +\n  geom_label_repel(data = df_factors_cumsum_label_2017, aes(x = last_date, y = total, label = od_factor, group = od_factor)) +\n  geom_point(data = df_factors_cumsum_label_2017, aes(x = last_date, y = total, group = od_factor, color = od_factor), size = 3) +\n  scale_color_discrete(guide = FALSE) +\n  scale_y_continuous(expand = c(.01, .1)) +\n  labs(title = \"Fatal overdoses by drug factor (2017)\",\n       y = \"Cumulative sum of fatal overdoses\",\n       x = \"\",\n       subtitle = \"Data from the WPRDC\",\n       caption = \"Conor Tompkins @conor_tompkins\")\nHeroin and fentanyl have proven to be a particularly deadly combination. We can explore the combination of these drugs. This plot shows the cumulative number of fatal overdoses involving heroin.\ndf_factors %&gt;% \n  select(date, od_heroin) %&gt;% \n  filter(od_heroin) %&gt;% \n  count(date) %&gt;% \n  mutate(n_cumsum = cumsum(n),\n         tag = \"Heroin\") %&gt;% \n  ggplot(aes(x = date, y = n_cumsum)) +\n  geom_line(size = 2) +\n  scale_y_continuous(label = comma) +\n  labs(title = \"Fatal overdoses involving heroin\",\n       x = \"\",\n       y = \"Cumulative sum of fatal overdoses\",\n       subtitle = \"Data from the WPRDC\",\n       caption = \"Conor Tompkins @conor_tompkins\")\nThis plot shows the % of fatal heroin overdoses that also involve fentanyl. Again, we see that fentanyl arrived in Allegheny County in 2014, and quickly became closely linked to heroin. In late 2017, over 75% of heroin overdoses involved fentanyl.\ndf_factors %&gt;% \n  select(date, od_heroin, od_fentanyl) %&gt;% \n  filter(od_heroin) %&gt;% \n  group_by(date) %&gt;% \n  summarize(percent_fentanyl = mean(od_fentanyl)) -&gt; df_factors_heroin_fent \n\ndf_factors_heroin_fent %&gt;% \n  ggplot(aes(date, percent_fentanyl)) +\n  geom_point(alpha = .1) +\n  geom_smooth() +\n  scale_y_continuous(labels = scales::percent) +\n  labs(title = \"Fatal heroin overdoses involving fentanyl\",\n       x = \"\",\n       y = \"% involving fentantyl\",\n       subtitle = \"Data from the WPRDC\",\n       caption = \"Conor Tompkins @conor_tompkins\")"
  },
  {
    "objectID": "posts/allegheny-county-overdose-data/index.html#areas-for-further-research",
    "href": "posts/allegheny-county-overdose-data/index.html#areas-for-further-research",
    "title": "Allegheny County Overdose Data",
    "section": "Areas for further research",
    "text": "Areas for further research\nGeographic analysis using the zip code data could expose more information about how communities are impacted by drug overdoses.\nDemographic analysis could be used to analyze how drug overdoses affect different parts of the population.\nThe combination of heroin and fentanyl can be explored further. It would also be worth exploring whether there is any link between fentanyl and cocaine. More generally, the correlation between the various drugs could be explored using network analysis."
  },
  {
    "objectID": "posts/mapping-healthy-ride-data/index.html",
    "href": "posts/mapping-healthy-ride-data/index.html",
    "title": "Mapping Healthy Ride Data",
    "section": "",
    "text": "This post is about mapping the Healthy Ride dataset in R.\nThis is my third post about the Healthy Ride bike service in Pittsburgh. You can find the first post and second post on my blog.\nFirst, load the R packages we will be using:\n\nlibrary(tidyverse)\nlibrary(ggmap)\nlibrary(lubridate)\nlibrary(viridis)\nlibrary(stringr)\nlibrary(gghighlight)\nlibrary(knitr)\nlibrary(kableExtra)\n\nThen load the data from the WPRDC (hosted on my GitHub page):\n\ndata &lt;- read_csv(\"https://raw.githubusercontent.com/conorotompkins/healthy_ride/master/data/data.csv\")\n\nAgain, we need to format the data and the column names to make them more useful for analysis. Since this is a repeat of the script from the last post, I will just do it all in one go:\n\ncolnames(data) &lt;- tolower(colnames(data))\ncolnames(data) &lt;- gsub(\" \", \"_\", colnames(data))\n\ndata_long &lt;- data %&gt;% \n  rename(start_date_time = starttime,\n         stop_date_time = stoptime) %&gt;% \n  gather(date_time_type, date_time, c(start_date_time, stop_date_time)) %&gt;% \n  select(date_time_type, date_time, everything()) %&gt;% \n  mutate(date_time_2 = date_time) %&gt;% \n  separate(date_time, \" \", into = c(\"date\", \"time\")) %&gt;% \n  mutate(id = row_number(),\n         date = mdy(date),\n         year = year(date),\n         month = month(date, label = TRUE),\n         week = week(date),\n         time = hm(time),\n         hour = hour(time),\n         wday = wday(date, label = TRUE),\n         is_weekday = ifelse(wday %in% c(\"Mon\", \"Tues\", \"Wed\", \"Thurs\", \"Fri\"), \"weekday\", \"weekend\"),\n         yday = yday(date),\n         mday = mday(date)) %&gt;% \n  mutate(trip_duration = (tripduration / 60) / 60) %&gt;% \n  gather(station_id_type, station_id, c(from_station_id, to_station_id)) %&gt;% \n  gather(station_name_type, station_name, c(from_station_name, to_station_name)) %&gt;% \n  select(date_time_type, \n         is_weekday, \n         date, \n         year,\n         month,\n         hour,\n         wday,\n         yday,\n         mday,\n         date_time_2, \n         station_id_type, \n         station_id, \n         station_name_type,\n         station_name,\n         everything(),\n         -time)\n\ndata_long[1:10, 1:5] %&gt;% \n  kable(\"html\") %&gt;% \n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\n\n\n\n\n\ndate_time_type\nis_weekday\ndate\nyear\nmonth\n\n\n\n\nstart_date_time\nweekend\n2015-05-31\n2015\nMay\n\n\nstart_date_time\nweekend\n2015-05-31\n2015\nMay\n\n\nstart_date_time\nweekend\n2015-05-31\n2015\nMay\n\n\nstart_date_time\nweekend\n2015-05-31\n2015\nMay\n\n\nstart_date_time\nweekend\n2015-05-31\n2015\nMay\n\n\nstart_date_time\nweekend\n2015-05-31\n2015\nMay\n\n\nstart_date_time\nweekend\n2015-05-31\n2015\nMay\n\n\nstart_date_time\nweekend\n2015-05-31\n2015\nMay\n\n\nstart_date_time\nweekend\n2015-05-31\n2015\nMay\n\n\nstart_date_time\nweekend\n2015-05-31\n2015\nMay\n\n\n\n\n\n\n\n\nImportantly, we will be excluding trips where the rider started and ended their trip at the same station. The data lacks the granularity to analyze rider location beyond the points where they began and ended their trip.\n\ndata_long &lt;- data_long %&gt;% \n  spread(station_name_type, station_name) %&gt;% \n  filter(from_station_name != to_station_name) %&gt;% \n  gather(station_name_type, station_name, c(from_station_name, to_station_name))\n\ndata_long[1:10, 1:5] %&gt;% \n  kable(\"html\") %&gt;% \n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\n\n\n\n\n\ndate_time_type\nis_weekday\ndate\nyear\nmonth\n\n\n\n\nstart_date_time\nweekend\n2015-05-31\n2015\nMay\n\n\nstart_date_time\nweekend\n2015-05-31\n2015\nMay\n\n\nstart_date_time\nweekend\n2015-05-31\n2015\nMay\n\n\nstart_date_time\nweekend\n2015-05-31\n2015\nMay\n\n\nstart_date_time\nweekend\n2015-05-31\n2015\nMay\n\n\nstart_date_time\nweekend\n2015-05-31\n2015\nMay\n\n\nstart_date_time\nweekend\n2015-05-31\n2015\nMay\n\n\nstart_date_time\nweekend\n2015-05-31\n2015\nMay\n\n\nstart_date_time\nweekend\n2015-05-31\n2015\nMay\n\n\nstart_date_time\nweekend\n2015-05-31\n2015\nMay\n\n\n\n\n\n\n\n\nWe also need to load the CSV with the longitude and latitude for the Healthy Ride stations\n\ndata_station_locations &lt;- read_csv(\"https://raw.githubusercontent.com/conorotompkins/healthy_ride/master/data/stations/station_locations.csv\")\n\n\ndf_station_totals &lt;- data_long %&gt;% \n  group_by(station_name) %&gt;% \n  summarize(number_of_trips = n()) %&gt;% \n  arrange(desc(number_of_trips), station_name) %&gt;% \n  left_join(data_station_locations) %&gt;% \n  select(station_name, number_of_trips, longitude, latitude)\n\ndf_station_totals %&gt;% \n  head() %&gt;% \n  kable(\"html\") %&gt;% \n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\n\n\n\n\n\nstation_name\nnumber_of_trips\nlongitude\nlatitude\n\n\n\n\nForbes Ave & Market Square\n51924\n-80.00308\n40.44088\n\n\n21st St & Penn Ave\n47700\n-79.98354\n40.45212\n\n\n21st St & Penn Ave\n47700\n-79.98322\n40.45174\n\n\nLiberty Ave & Stanwix St\n45692\n-80.00468\n40.44133\n\n\n10th St & Penn Ave (David L. Lawrence Convention Center)\n42868\n-79.99580\n40.44467\n\n\nS 27th St & Sidney St. (Southside Works)\n37356\n-79.96611\n40.42790\n\n\n\n\n\n\n\n\nWhere are the Healthy Ride Stations?\n\npgh_map &lt;- get_map(c(lon = -79.973859, lat = 40.447095), zoom = 13)\npgh_map &lt;- ggmap(pgh_map)\n\npgh_map +\n  geom_point(data = df_station_totals, aes(longitude, latitude, size = number_of_trips),\n             alpha = .75) +\n  scale_size_continuous(\"Number of trips\", range = c(.1, 5)) +\n  theme_minimal() +\n  theme(axis.text = element_blank(),\n        axis.title = element_blank())\n\n\n\n\n\n\n\n\nNext, join the two dataframes:\n\ndf_long &lt;- data_long %&gt;% \n  select(station_name, station_name_type) %&gt;% \n  group_by(station_name, station_name_type) %&gt;% \n  summarize(number_of_trips = n()) %&gt;% \n  arrange(desc(number_of_trips)) %&gt;% \n  left_join(data_station_locations) %&gt;% \n  ungroup()\n\ndf_long %&gt;% \n  head() %&gt;% \n  kable(\"html\") %&gt;% \n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\n\n\n\n\n\nstation_name\nstation_name_type\nnumber_of_trips\nstation_number\nnumber_of_racks\nlatitude\nlongitude\n\n\n\n\nForbes Ave & Market Square\nto_station_name\n29040\n1001\n19\n40.44088\n-80.00308\n\n\n21st St & Penn Ave\nto_station_name\n25748\n1017\n19\n40.45212\n-79.98354\n\n\n21st St & Penn Ave\nto_station_name\n25748\n1017\n18\n40.45174\n-79.98322\n\n\nLiberty Ave & Stanwix St\nto_station_name\n25380\n1000\n16\n40.44133\n-80.00468\n\n\nForbes Ave & Market Square\nfrom_station_name\n22884\n1001\n19\n40.44088\n-80.00308\n\n\n10th St & Penn Ave (David L. Lawrence Convention Center)\nto_station_name\n22040\n1010\n15\n40.44467\n-79.99580\n\n\n\n\n\n\n\n\nDo some stations function more as starting points or ending points for trips?\n\npgh_map +\n  geom_point(data = df_long, aes(longitude, latitude, size = number_of_trips, color = station_name_type),\n             alpha = .75) +\n  scale_size_continuous(\"Number of trips\",range = c(.1, 5)) +\n  scale_color_discrete(\"Station type\") +\n  facet_wrap(~station_name_type) +\n  theme_minimal() +\n  theme(axis.text = element_blank(),\n        axis.title = element_blank())\n\n\n\n\n\n\n\n\nNo differences are discernible in this view.\nA scatter plot shows the differences more effectively:\n\ndf_from_to &lt;- df_long %&gt;%\n  spread(station_name_type, number_of_trips) %&gt;% \n  rename(from_trips = from_station_name,\n         to_trips = to_station_name) %&gt;% \n  select(station_name, from_trips, to_trips) %&gt;% \n  mutate(differential = abs(from_trips - to_trips))\n\ndf_from_to %&gt;% \n  ggplot(aes(from_trips, to_trips)) +\n  geom_point(size = 1) +\n  gghighlight(label_key = station_name, \n              differential &gt; 4000) +\n  scale_x_continuous(limits = c(0, 30000)) +\n  scale_y_continuous(limits = c(0, 30000)) +\n  coord_equal() +\n  geom_abline() +\n  labs(x = \"From trips\",\n       y = \"To trips\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nWhat are the top 10 stations in terms of absolute difference between departures and arrivals?\n\ndf_from_to %&gt;% \n  ungroup() %&gt;% \n  arrange(desc(differential)) %&gt;% \n  top_n(10, differential) %&gt;% \n  kable(\"html\") %&gt;% \n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\n\n\n\n\n\nstation_name\nfrom_trips\nto_trips\ndifferential\n\n\n\n\nForbes Ave & Market Square\n22884\n29040\n6156\n\n\nS 27th St & Sidney St. (Southside Works)\n15804\n21552\n5748\n\n\nLiberty Ave & Stanwix St\n20312\n25380\n5068\n\n\n42nd St & Butler St\n11152\n15580\n4428\n\n\n42nd St & Butler St\n11152\n15580\n4428\n\n\n42nd & Penn Ave.\n9184\n4760\n4424\n\n\nTaylor St & Liberty Ave\n8948\n5072\n3876\n\n\n21st St & Penn Ave\n21952\n25748\n3796\n\n\n21st St & Penn Ave\n21952\n25748\n3796\n\n\nPenn Ave & N Fairmount St\n6604\n2816\n3788\n\n\n\n\n\n\n\n\nLet’s map the connections between stations by drawing lines between the stations.\nFirst, widen the data:\n\ndf_wide &lt;- data_long %&gt;%\n  spread(station_name_type, station_name) %&gt;% \n  select(from_station_name, to_station_name) %&gt;% \n  left_join(data_station_locations, by = c(\"from_station_name\" = \"station_name\")) %&gt;%\n  rename(from_latitude = latitude,\n         from_longitude = longitude) %&gt;% \n  left_join(data_station_locations, by = c(\"to_station_name\" = \"station_name\")) %&gt;% \n  rename(to_latitude = latitude,\n         to_longitude = longitude) %&gt;% \n  group_by(from_station_name, to_station_name, from_longitude, from_latitude, to_longitude, to_latitude) %&gt;% \n  summarise(number_of_trips = n()) %&gt;% \n  arrange(desc(number_of_trips)) %&gt;% \n  mutate(from_station_type = ifelse(from_station_name == to_station_name,\n                               \"Same station\", \"Different station\"))\n\ndf_wide %&gt;% \n  head() %&gt;% \n  kable(\"html\") %&gt;% \n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\n\n\n\n\n\nfrom_station_name\nto_station_name\nfrom_longitude\nfrom_latitude\nto_longitude\nto_latitude\nnumber_of_trips\nfrom_station_type\n\n\n\n\n10th St & Penn Ave (David L. Lawrence Convention Center)\n21st St & Penn Ave\n-79.99580\n40.44467\n-79.98354\n40.45212\n4432\nDifferent station\n\n\n10th St & Penn Ave (David L. Lawrence Convention Center)\n21st St & Penn Ave\n-79.99580\n40.44467\n-79.98322\n40.45174\n4432\nDifferent station\n\n\nBoulevard of the Allies & Parkview Ave\nFifth Ave & S Bouquet St\n-79.95188\n40.43434\n-79.95760\n40.44232\n3888\nDifferent station\n\n\n21st St & Penn Ave\n10th St & Penn Ave (David L. Lawrence Convention Center)\n-79.98354\n40.45212\n-79.99580\n40.44467\n3640\nDifferent station\n\n\n21st St & Penn Ave\n10th St & Penn Ave (David L. Lawrence Convention Center)\n-79.98322\n40.45174\n-79.99580\n40.44467\n3640\nDifferent station\n\n\nFifth Ave & S Bouquet St\nBoulevard of the Allies & Parkview Ave\n-79.95760\n40.44232\n-79.95188\n40.43434\n3560\nDifferent station\n\n\n\n\n\n\n\n\nThen, layer the data over the map:\n\npgh_map +\n  geom_segment(data = df_wide, aes(x = from_longitude, xend = to_longitude, \n                                   y = from_latitude, yend = to_latitude, \n                                   alpha = number_of_trips)) +\n  geom_point(data = df_wide, aes(from_longitude, from_latitude), shape = 21, size = 3, fill = \"white\") +\n  geom_point(data = df_wide, aes(to_longitude, to_latitude), shape = 21, size = 3, fill = \"white\") +\n  scale_alpha_continuous(\"Number of trips\", range = c(.0001, 1)) +\n  theme_minimal() +\n  theme(axis.text = element_blank(),\n        axis.title = element_blank())\n\n\n\n\n\n\n\n\nWe can also facet by the from_station_name variable to see where trips originating from certain stations end at. This plot shows the top 6 stations in terms of trips that began from that station:\n\ntop_from_stations &lt;- df_wide %&gt;% \n  group_by(from_station_name) %&gt;% \n  summarize(number_of_trips = sum(number_of_trips)) %&gt;% \n  arrange(desc(number_of_trips)) %&gt;% \n  top_n(6) %&gt;% \n  select(from_station_name) %&gt;% \n  unlist()\n\ndf_wide_specific_station &lt;- df_wide %&gt;% \n  filter(from_station_name %in% top_from_stations)\n\npgh_map +\n  geom_segment(data = df_wide_specific_station, aes(x = from_longitude, xend = to_longitude, \n                                   y = from_latitude, yend = to_latitude, \n                                   alpha = number_of_trips), arrow = arrow(length = unit(0.03, \"npc\"))) +\n  geom_point(data = df_wide_specific_station, aes(from_longitude, from_latitude), \n             shape = 1, size = 2) +\n  geom_point(data = df_wide_specific_station, aes(to_longitude, to_latitude), \n             shape = 1, size = 2) +\n  scale_alpha_continuous(\"Number of trips\", range = c(.1, 1)) +\n  facet_wrap(~from_station_name,\n             nrow = 2) +\n  theme_minimal() +\n  theme(axis.text = element_blank(),\n        axis.title = element_blank())\n\n\n\n\n\n\n\n\nWe can use the same method to examine the top stations in terms of absolute difference between departing and arriving rides:\n\ntop_diff_stations &lt;- df_from_to %&gt;% \n  arrange(desc(differential)) %&gt;% \n  distinct() %&gt;% \n  top_n(10) %&gt;% \n  select(station_name) %&gt;% \n  unlist()\n  \ndf_wide_diff_station &lt;- df_wide %&gt;% \n  filter(from_station_name %in% top_diff_stations)\n\npgh_map +\n  geom_point(data = df_wide_diff_station, aes(from_longitude, from_latitude), \n             shape = 1, size = 2) +\n  geom_point(data = df_wide_diff_station, aes(to_longitude, to_latitude), \n             shape = 1, size = 2) +\n  geom_segment(data = df_wide_diff_station, aes(x = from_longitude, xend = to_longitude, \n                                   y = from_latitude, yend = to_latitude, \n                                   alpha = number_of_trips),\n               arrow = arrow(length = unit(0.03, \"npc\"))) +\n  scale_alpha_continuous(\"Number of trips\", range = c(.1, 1)) +\n  facet_wrap(~from_station_name,\n             nrow = 2) +\n  theme_minimal() +\n  theme(axis.text = element_blank(),\n        axis.title = element_blank())\n\n\n\n\n\n\n\n\nThere does not appear to be a stark difference in the way the network behaves on weekdays vs. weekends:\n\ndf_wide_day &lt;- data_long %&gt;% \n  spread(station_name_type, station_name) %&gt;% \n  select(from_station_name, to_station_name, is_weekday) %&gt;% \n  left_join(data_station_locations, by = c(\"from_station_name\" = \"station_name\")) %&gt;%\n  rename(from_latitude = latitude,\n         from_longitude = longitude) %&gt;% \n  left_join(data_station_locations, by = c(\"to_station_name\" = \"station_name\")) %&gt;% \n  rename(to_latitude = latitude,\n         to_longitude = longitude) %&gt;% \n  group_by(is_weekday, from_station_name, to_station_name, from_longitude, from_latitude, to_longitude, to_latitude) %&gt;% \n  summarise(number_of_trips = n()) %&gt;% \n  arrange(desc(number_of_trips))\n\npgh_map +\n  geom_segment(data = df_wide_day, aes(x = from_longitude, xend = to_longitude, \n                                   y = from_latitude, yend = to_latitude, \n                                   alpha = number_of_trips)) +\n  geom_point(data = df_wide_day, aes(from_longitude, from_latitude), shape = 21, size = 3, fill = \"white\") +\n  geom_point(data = df_wide_day, aes(to_longitude, to_latitude), shape = 21, size = 3, fill = \"white\") +\n  scale_alpha_continuous(\"Number of trips\", range = c(.05, .3)) +\n  facet_wrap(~is_weekday) +\n  theme_minimal() +\n  theme(axis.text = element_blank(),\n        axis.title = element_blank())\n\n\n\n\n\n\n\n\nThere are clear differences in the number of rides across different times of day, but the geographic pattern of departures and arrivals does not appear to change:\n\ndf_wide_tod &lt;- data_long %&gt;% \n  spread(station_name_type, station_name) %&gt;% \n  select(from_station_name, to_station_name, hour) %&gt;% \n  mutate(time_of_day = cut(hour, breaks = c(-Inf, 3, 6, 9, 12, 15, 18, 21, Inf), \n                           labels = c(\"0-3\", \"3-6\", \"6-9\", \"9-12\", \"12-15\", \"15-18\", \"18-21\", \"21-24\"), \n                           ordered_result = TRUE)) %&gt;% \n  left_join(data_station_locations, by = c(\"from_station_name\" = \"station_name\")) %&gt;%\n  rename(from_latitude = latitude,\n         from_longitude = longitude) %&gt;% \n  left_join(data_station_locations, by = c(\"to_station_name\" = \"station_name\")) %&gt;% \n  rename(to_latitude = latitude,\n         to_longitude = longitude) %&gt;% \n  group_by(time_of_day, from_station_name, to_station_name, \n           from_longitude, from_latitude, \n           to_longitude, to_latitude) %&gt;% \n  summarise(number_of_trips = n()) %&gt;% \n  arrange(desc(number_of_trips))\n\npgh_map +\n  geom_segment(data = df_wide_tod, aes(x = from_longitude, xend = to_longitude, \n                                   y = from_latitude, yend = to_latitude, \n                                   alpha = number_of_trips)) +\n  geom_point(data = df_wide_tod, aes(from_longitude, from_latitude), shape = 21, size = 2, fill = \"white\") +\n  geom_point(data = df_wide_tod, aes(to_longitude, to_latitude), shape = 21, size = 2, fill = \"white\") +\n  scale_alpha_continuous(\"Number of trips\", range = c(.05, .3)) +\n  facet_wrap(~time_of_day) +\n  labs(title = \"Time of day\") +\n  theme_minimal() +\n  theme(axis.text = element_blank(),\n        axis.title = element_blank())"
  },
  {
    "objectID": "posts/clustering-allegheny-county-census-tracts-with-pca-and-kmeans/index.html",
    "href": "posts/clustering-allegheny-county-census-tracts-with-pca-and-kmeans/index.html",
    "title": "Clustering Allegheny County Census Tracts With PCA and k-means",
    "section": "",
    "text": "In this post I will use the census API discussed in the last post to cluster the Allegheny County census tracts using PCA and k-means."
  },
  {
    "objectID": "posts/clustering-allegheny-county-census-tracts-with-pca-and-kmeans/index.html#setup",
    "href": "posts/clustering-allegheny-county-census-tracts-with-pca-and-kmeans/index.html#setup",
    "title": "Clustering Allegheny County Census Tracts With PCA and k-means",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(tidycensus)\nlibrary(tigris)\nlibrary(sf)\nlibrary(broom)\nlibrary(ggfortify)\nlibrary(viridis)\nlibrary(janitor)\nlibrary(scales)\nlibrary(ggthemes)\n\noptions(tigris_use_cache = TRUE)\n\ntheme_set(theme_minimal())\n\n\ncensus_vars &lt;- load_variables(2010, \"sf1\", cache = TRUE)\n\nCensus tracts are small geographic areas analogous to local neighborhoods. This is a map of all the tracts in Allegheny County, for reference:"
  },
  {
    "objectID": "posts/clustering-allegheny-county-census-tracts-with-pca-and-kmeans/index.html#download-data",
    "href": "posts/clustering-allegheny-county-census-tracts-with-pca-and-kmeans/index.html#download-data",
    "title": "Clustering Allegheny County Census Tracts With PCA and k-means",
    "section": "Download data",
    "text": "Download data\nThis code downloads data about the ethnicities resident in the tracts and calculates them as a % of the tract population.\n\nvars_demo &lt;- c(white = \"P005003\", \n          black = \"P005004\", \n          asian = \"P005006\", \n          hispanic = \"P004003\")\n#age vars men and women\n#P0120003:P0120049\n\nget_decennial(geography = \"tract\", \n              variables = vars_demo,\n              state = \"PA\", \n              county = \"Allegheny\", \n              year = 2010,\n              geometry = FALSE,\n              summary_var = \"P001001\") %&gt;% \n  arrange(GEOID) %&gt;% \n  mutate(value = value / summary_value) %&gt;% \n  select(-summary_value) %&gt;% \n  spread(variable, value) %&gt;% \n  rename_at(vars(\"white\", \"black\", \"asian\", \"hispanic\"), funs(str_c(\"pct_\", .))) -&gt; allegheny_demographics\n\nallegheny_demographics &lt;- replace(allegheny_demographics, is.na(allegheny_demographics), 0)\n\nThis code downloads information about the housing stock in each tract, specifically what % of housing units are owned outright, owned with a loan, or rented.\n\nvars_housing &lt;- c(units_owned_loan = \"H011002\",\n          units_owned_entire = \"H011003\",\n          units_rented = \"H011004\")\n\nget_decennial(geography = \"tract\", \n              variables = vars_housing,\n              state = \"PA\", \n              county = \"Allegheny\", \n              year = 2010,\n              geometry = FALSE,\n              summary_var = \"H011001\") %&gt;% \n  arrange(GEOID) %&gt;% \n  mutate(value = value / summary_value) %&gt;% \n  select(-summary_value) %&gt;% \n  spread(variable, value) %&gt;% \n  rename_at(vars(\"units_owned_entire\", \"units_owned_loan\", \"units_rented\"), funs(str_c(\"pct_\", .))) -&gt; allegheny_housing\n\nallegheny_housing &lt;- replace(allegheny_housing, is.na(allegheny_housing), 0)\n\nThis code requests the total population of each tract.\n\n#originally I used age-sex variables, but they were not useful\nvars_age_total &lt;- census_vars %&gt;% \n  filter(name == \"P012001\")\n\nget_decennial(geography = \"tract\", \n              variables = vars_age_total$name,\n              state = \"PA\", \n              county = \"Allegheny\", \n              year = 2010,\n              geometry = FALSE,\n              summary_var = \"P012001\") %&gt;% \n  rename(var_id = variable) %&gt;% \n  mutate(value = value / summary_value) %&gt;% \n  spread(var_id, value) -&gt; allegheny_age_sex\n\ncolnames(allegheny_age_sex) &lt;- c(\"GEOID\", \"NAME\", \"summary_value\", vars_age_total$label)\n\nallegheny_age_sex %&gt;% \n  clean_names() %&gt;% \n  rename(GEOID = geoid,\n         NAME = name,\n         total_population = summary_value) -&gt; allegheny_age_sex\n\nallegheny_age_sex &lt;- replace(allegheny_age_sex, is.na(allegheny_age_sex), 0)\n\nallegheny_age_sex %&gt;% \n  select(GEOID, NAME, total_population) -&gt; allegheny_age_sex\n\nThis code requests the geometry of each tract that I will use to map them later.\n\nget_decennial(geography = \"tract\", \n              variables = vars_housing,\n              state = \"PA\", \n              county = \"Allegheny\",\n              year = 2010,\n              geometry = TRUE) %&gt;% \n  select(-c(variable, value)) %&gt;% \n  distinct(GEOID) -&gt; allegheny_geo\n\nThis joins the 4 dataframes together.\n\nallegheny_geo %&gt;% \n  left_join(allegheny_housing) %&gt;% \n  left_join(allegheny_demographics) %&gt;% \n  left_join(allegheny_age_sex) %&gt;% \n  mutate(id = str_c(GEOID, NAME, sep = \" | \")) -&gt; allegheny"
  },
  {
    "objectID": "posts/clustering-allegheny-county-census-tracts-with-pca-and-kmeans/index.html#exploratory-graph",
    "href": "posts/clustering-allegheny-county-census-tracts-with-pca-and-kmeans/index.html#exploratory-graph",
    "title": "Clustering Allegheny County Census Tracts With PCA and k-means",
    "section": "Exploratory graph",
    "text": "Exploratory graph\nThis graph compares the percent of white residents to the remaining variables in the data. pct_white is on the x axis of each of the smaller charts. Note that each chart’s Y axis has its own scale. It is already obvious that pct_white and pct_black are negatively correlated with each other.\n\nallegheny %&gt;%\n  #st_set_geometry(NULL) %&gt;% \n  st_drop_geometry() %&gt;%\n  select(contains(\"pct\")) %&gt;% \n  gather(variable, value, -pct_white) %&gt;% \n  ggplot(aes(pct_white, value)) +\n  geom_point(alpha = .5) +\n  geom_smooth() +\n  facet_wrap(~variable, scales = \"free\", nrow = 3, strip.position=\"left\") +\n  scale_x_continuous(label = percent) +\n  scale_y_continuous(label = percent) +\n  labs(x = NULL) +\n  theme_bw() +\n  theme(strip.placement = \"outside\")\n\n\n\n\n\n\n\n\nThis code plots the total population against the other variables:\n\nallegheny %&gt;%\n  st_drop_geometry() %&gt;%\n  select(contains(\"pct\"), total_population) %&gt;% \n  gather(variable, value, -total_population) %&gt;% \n  ggplot(aes(total_population, value)) +\n  geom_point(alpha = .5) +\n  geom_smooth() +\n  facet_wrap(~variable, scales = \"free\", nrow = 3, strip.position=\"left\") +\n  scale_x_continuous(label = comma) +\n  scale_y_continuous(label = percent) +\n  theme_bw() +\n  theme(strip.placement = \"outside\")"
  },
  {
    "objectID": "posts/clustering-allegheny-county-census-tracts-with-pca-and-kmeans/index.html#prepare-for-pca",
    "href": "posts/clustering-allegheny-county-census-tracts-with-pca-and-kmeans/index.html#prepare-for-pca",
    "title": "Clustering Allegheny County Census Tracts With PCA and k-means",
    "section": "Prepare for PCA",
    "text": "Prepare for PCA\nThis code prepares the data for PCA:\n\nallegheny %&gt;%\n  select(-c(id, GEOID, NAME)) %&gt;% \n  st_drop_geometry() %&gt;% \n  remove_rownames() -&gt; allegheny_pca\n\nallegheny_pca %&gt;% \n  prcomp(scale = TRUE) -&gt; pc\n\n\npc %&gt;% \n  tidy(\"pcs\")\n\n# A tibble: 8 × 4\n     PC std.dev percent cumulative\n  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n1     1  1.94   0.471        0.471\n2     2  1.28   0.205        0.676\n3     3  0.913  0.104        0.780\n4     4  0.789  0.0778       0.858\n5     5  0.769  0.0739       0.932\n6     6  0.658  0.0540       0.986\n7     7  0.323  0.0130       0.999\n8     8  0.0961 0.00115      1    \n\n\n\npc %&gt;% \n  augment(data = allegheny_pca) %&gt;% \n  as_tibble() %&gt;% \n  mutate(GEOID = allegheny %&gt;% pull(GEOID)) %&gt;% \n  select(.rownames, GEOID, everything()) -&gt; df_au\n\n\ndf_au %&gt;% \n  head()\n\n# A tibble: 6 × 18\n  .rownames GEOID   pct_units_owned_entire pct_units_owned_loan pct_units_rented\n  &lt;chr&gt;     &lt;chr&gt;                    &lt;dbl&gt;                &lt;dbl&gt;            &lt;dbl&gt;\n1 1         420034…                  0.213                0.750           0.0366\n2 2         420034…                  0.181                0.629           0.190 \n3 3         420034…                  0.245                0.685           0.0692\n4 4         420034…                  0.336                0.501           0.164 \n5 5         420034…                  0.147                0.418           0.435 \n6 6         420034…                  0.168                0.432           0.400 \n# ℹ 13 more variables: pct_asian &lt;dbl&gt;, pct_black &lt;dbl&gt;, pct_hispanic &lt;dbl&gt;,\n#   pct_white &lt;dbl&gt;, total_population &lt;dbl&gt;, .fittedPC1 &lt;dbl&gt;,\n#   .fittedPC2 &lt;dbl&gt;, .fittedPC3 &lt;dbl&gt;, .fittedPC4 &lt;dbl&gt;, .fittedPC5 &lt;dbl&gt;,\n#   .fittedPC6 &lt;dbl&gt;, .fittedPC7 &lt;dbl&gt;, .fittedPC8 &lt;dbl&gt;\n\n\nThis shows how the PCs explain the variance in the data. As explained earlier, the first few PCs explain most of the variance in the data.\n\npc %&gt;% \n  tidy(\"pcs\") %&gt;%\n  select(-std.dev) %&gt;% \n  gather(measure, value, -PC) %&gt;% \n  mutate(measure = case_when(measure == \"percent\" ~ \"Percent\",\n                             measure == \"cumulative\" ~ \"Cumulative\")) %&gt;% \n    ggplot(aes(PC, value)) +\n    geom_line() +\n    geom_point() +\n    facet_wrap(~measure) +\n    labs(title = \"Variance explained by each principal component\",\n         x = \"Principal Component\",\n         y = NULL) +\n    scale_x_continuous(breaks = 1:8) +\n    scale_y_continuous(label = percent) +\n  theme_bw()\n\n\n\n\n\n\n\n\nThis shows how the PCA function rearranged the data to maximize the variance in the first few PCs. PC1 is largely defined by the percent of a tract that is white or black, the percent of housing units that are owned, and the total population of the tract. The “pct_white” and “pct_black” arrows point in opposite directions, which reflects Pittsburgh’s status as a segregated city.\nPC2 explains less of the variance, and is influenced by the percent of a tract that is hispanic, asian, or black.\n\nallegheny %&gt;% \n  select(-c(id, GEOID)) %&gt;% \n  st_drop_geometry() %&gt;%\n  nest() %&gt;% \n  mutate(pca = map(data, ~ prcomp(.x %&gt;% select(-NAME), \n                                  center = TRUE, scale = TRUE)),\n         pca_aug = map2(pca, data, ~augment(.x, data = .y))) -&gt; allegheny_pca2\n\nallegheny_pca2 %&gt;% \nmutate(\n    pca_graph = map2(\n      .x = pca,\n      .y = data,\n      ~ autoplot(.x, loadings = TRUE, loadings.label = TRUE,\n                 loadings.label.repel = TRUE,\n                 data = .y) +\n        theme_bw() +\n        labs(x = \"Principal Component 1\",\n             y = \"Principal Component 2\",\n             title = \"First two principal components of PCA on Allegheny County Census data\")\n    )\n  ) %&gt;%\n  pull(pca_graph)\n\n[[1]]\n\n\n\n\n\n\n\n\n\nThis code maps the first two PCs to the tracts.\n\ndf_au %&gt;% \n  select(-.rownames) %&gt;% \n  gather(variable, value, -c(GEOID)) -&gt; df_au_long\n\nallegheny_geo %&gt;% \n  left_join(df_au) %&gt;% \n  gather(pc, pc_value, contains(\".fitted\")) %&gt;% \n  mutate(pc = str_replace(pc, \".fitted\", \"\")) -&gt; allegheny_pca_map\n\nleft_join(allegheny_map, allegheny_pca_map) %&gt;% \n  filter(pc %in% c(\"PC1\", \"PC2\")) %&gt;% \n  ggplot(aes(fill = pc_value, color = pc_value)) + \n  geom_sf() + \n  facet_wrap(~pc) +\n  coord_sf(crs = 26911) + \n  scale_fill_viridis(\"Principal component value\", option = \"magma\") + \n  scale_color_viridis(\"Principal component value\", option = \"magma\") +\n  labs(title = \"Allegheny County\",\n       subtitle = \"American Community Survey\") +\n  theme(axis.text = element_blank())"
  },
  {
    "objectID": "posts/clustering-allegheny-county-census-tracts-with-pca-and-kmeans/index.html#clustering-with-k-means",
    "href": "posts/clustering-allegheny-county-census-tracts-with-pca-and-kmeans/index.html#clustering-with-k-means",
    "title": "Clustering Allegheny County Census Tracts With PCA and k-means",
    "section": "Clustering with k-means",
    "text": "Clustering with k-means\nNext I will use k-means to cluster the PC data.\n\ndf_au_long %&gt;% \n  filter(str_detect(variable, \"PC\")) %&gt;% \n  spread(variable, value) -&gt; allegheny_kmeans\n\nThis code clusters the data using 1 to 9 clusters.\n\nkclusts &lt;- tibble(k = 1:9) %&gt;%\n  mutate(\n    kclust = map(k, ~kmeans(allegheny_kmeans, .x)),\n    tidied = map(kclust, tidy),\n    glanced = map(kclust, glance),\n    augmented = map(kclust, augment, allegheny_kmeans)\n  )\n\n\nclusters &lt;- kclusts %&gt;%\n  unnest(tidied)\n\nassignments &lt;- kclusts %&gt;% \n  unnest(augmented)\n\nclusterings &lt;- kclusts %&gt;%\n  unnest(glanced, .drop = TRUE)\n\nBased on this “elbow chart”, the optimum number of clusters is most likely 2.\n\nggplot(clusterings, aes(k, tot.withinss)) +\n  geom_line() +\n  geom_point() +\n  geom_vline(xintercept = 2, linetype = 2) +\n  scale_x_continuous(breaks = 1:9) +\n  labs(x = \"Number of clusters\",\n       y = \"Between-cluster sum of squares\")\n\n\n\n\n\n\n\n\nWe can visualize how the data would look if it were assigned to a different number of clusters. Clearly the clustering algorithm experiences diminishing returns after 2 or 3 clusters.\n\nggplot(assignments, aes(.fittedPC1, .fittedPC2)) +\n  geom_point(aes(color = .cluster), alpha = .7) + \n  facet_wrap(~ str_c(k, \" cluster(s)\")) +\n  scale_color_discrete(\"Cluster\") +\n  labs(x = \"PC1\",\n       y = \"PC2\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nThis code divides the data into 2 clusters and maps the clusters onto the tract map.\n\ndf_au_long %&gt;% \n  filter(str_detect(variable, \"PC\")) %&gt;% \n  spread(variable, value) -&gt; allegheny_kmeans\n\nkclust &lt;- kmeans(allegheny_kmeans, centers = 2)\n\nkclust %&gt;% \n  augment(df_au_long %&gt;% \n            filter(str_detect(variable, \".fitted\")) %&gt;% \n            spread(variable, value)) -&gt; allegheny_kmeans\n\nget_decennial(geography = \"tract\", \n              variables = vars_housing,\n              state = \"PA\", \n              county = \"Allegheny\", \n              year = 2010,                           geometry = TRUE) %&gt;% \n  select(-c(variable, value)) %&gt;% \n  distinct(GEOID) -&gt; allegheny_geo\n\nallegheny_geo %&gt;% \n  left_join(allegheny_kmeans) -&gt; allegheny\n\n\nleft_join(allegheny_map, allegheny) %&gt;% \n  ggplot(aes(fill = .cluster, color = .cluster)) +\n  geom_sf(color = \"grey\", size = .1) +\n  scale_fill_viridis(\"Cluster\", discrete = TRUE, direction = -1) + \n  scale_color_viridis(\"Cluster\", discrete = TRUE, direction = -1) +\n  theme(axis.text = element_blank())\n\n\n\n\n\n\n\n\nThe second cluster largely follows the city limits, but excludes areas such as Mount Washington, Squirrel Hill, and Shadyside. It also includes a few areas outside of the city like Duquesne and McKeesport."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog posts",
    "section": "",
    "text": "What region is Pittsburgh in?\n\n\nOr, unsupervised demography with spatially constrained clustering\n\n\n\n\n\n\n\n\nConor Tompkins\n\n\n\n\n\n\n\n\n\n\n\n\nC&S Presentation: Time Series Forecasting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 11, 2024\n\n\nConor Tompkins\n\n\n\n\n\n\n\n\n\n\n\n\nForecasting potholes with exogenous variables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 31, 2023\n\n\nConor Tomkpins\n\n\n\n\n\n\n\n\n\n\n\n\nForecasting Pittsburgh Potholes with {fable}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2023\n\n\nConor Tompkins\n\n\n\n\n\n\n\n\n\n\n\n\nSuburbanization of Allegheny County\n\n\n\n\n\n\nAllegheny County\n\n\n\n\n\n\n\n\n\nApr 18, 2022\n\n\nConor Tompkins\n\n\n\n\n\n\n\n\n\n\n\n\nMaking a Venn diagram in Shiny\n\n\n\n\n\n\nShiny\n\n\n\n\n\n\n\n\n\nMar 12, 2022\n\n\nConor Tompkins\n\n\n\n\n\n\n\n\n\n\n\n\nEffect of Geographic Resolution on ebirdst Abundance\n\n\n\n\n\n\nR\n\n\neBird\n\n\n\n\n\n\n\n\n\nNov 23, 2021\n\n\nConor Tompkins\n\n\n\n\n\n\n\n\n\n\n\n\nCOVID-19 vaccine forecast and pastcasts\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 22, 2021\n\n\nConor Tompkins\n\n\n\n\n\n\n\n\n\n\n\n\nPittsburgh Riverhounds under Coach Lilley\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 5, 2021\n\n\nConor Tompkins\n\n\n\n\n\n\n\n\n\n\n\n\nDriving Alone vs. Public Transportation in Pittsburgh\n\n\nMapping commuter modes with bivariate discretized bins\n\n\n\n\n\n\n\n\n\n\n\nMar 29, 2021\n\n\nConor Tompkins\n\n\n\n\n\n\n\n\n\n\n\n\nHouse Price Estimator Dashboard\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJan 28, 2021\n\n\nConor Tompkins\n\n\n\n\n\n\n\n\n\n\n\n\nBike rental access in Pittsburgh\n\n\n\n\n\n\nR\n\n\nPittsburgh\n\n\nHealthy Ride\n\n\nWPRDC\n\n\n\n\n\n\n\n\n\nDec 4, 2020\n\n\nConor Tompkins\n\n\n\n\n\n\n\n\n\n\n\n\nShifting political winds\n\n\nOr, drawing arrows on maps\n\n\n\nR\n\n\nPolitics\n\n\n\n\n\n\n\n\n\nNov 13, 2020\n\n\nConor Tompkins\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing major commuter routes in Allegheny County\n\n\n\n\n\n\nR\n\n\nAllegheny County\n\n\nCommuters\n\n\nCensus\n\n\nMapbox\n\n\n\n\n\n\n\n\n\nOct 25, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nGraphing volatile home values in U.S. metro areas\n\n\n\n\n\n\nHousing\n\n\nZillow\n\n\n\n\n\n\n\n\n\nOct 21, 2020\n\n\nConor Tompkins\n\n\n\n\n\n\n\n\n\n\n\n\nEffect of COVID-19 on Pittsburgh parking transactions\n\n\n\n\n\n\nPittsburgh\n\n\nParking\n\n\nCOVID-19\n\n\n\n\n\n\n\n\n\nOct 5, 2020\n\n\nConor Tompkins\n\n\n\n\n\n\n\n\n\n\n\n\nMapping BosWash commuter patterns with Flowmap.blue\n\n\n\n\n\n\nCensus\n\n\nCommuters\n\n\n\n\n\n\n\n\n\nSep 25, 2020\n\n\nConor Tompkins\n\n\n\n\n\n\n\n\n\n\n\n\nPittsburgh City Boundary Model Leaflet Map\n\n\n\n\n\n\nR\n\n\nPittsburgh\n\n\nAllegheny County\n\n\n\n\n\n\n\n\n\nAug 23, 2020\n\n\nConor Tompkins\n\n\n\n\n\n\n\n\n\n\n\n\nModeling the Pittsburgh City Boundary\n\n\n\n\n\n\n\n\n\n\n\nAug 14, 2020\n\n\nConor Tompkins\n\n\n\n\n\n\n\n\n\n\n\n\nComparing Healthy Ride Usage Pre And “Post” COVID-19\n\n\n\n\n\n\n\n\n\n\n\nAug 14, 2020\n\n\nConor Tompkins\n\n\n\n\n\n\n\n\n\n\n\n\nTime series clustering COVID-19 case data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJul 20, 2020\n\n\nConor Tompkins\n\n\n\n\n\n\n\n\n\n\n\n\nWorking with Paycheck Protection Program data in R\n\n\n\n\n\n\nR\n\n\nPolitics\n\n\n\n\n\n\n\n\n\nJul 8, 2020\n\n\nConor Tompkins\n\n\n\n\n\n\n\n\n\n\n\n\nGraphing Allegheny County COVID-19 data\n\n\n\n\n\n\nR\n\n\nCOVID-19\n\n\n\n\n\n\n\n\n\nJul 7, 2020\n\n\nConor Tompkins\n\n\n\n\n\n\n\n\n\n\n\n\n(re)Modeling the Office\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nJun 21, 2020\n\n\nConor Tompkins\n\n\n\n\n\n\n\n\n\n\n\n\nCleaning Creatively Formatted US Census Bureau Migration Data\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nMay 25, 2020\n\n\nConor Tompkins\n\n\n\n\n\n\n\n\n\n\n\n\nClustering Bird Species with Seasonality\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nMay 3, 2020\n\n\nConor Tompkins\n\n\n\n\n\n\n\n\n\n\n\n\nCumulative eBird Sightings in Allegheny County\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nApr 29, 2020\n\n\nConor Tompkins\n\n\n\n\n\n\n\n\n\n\n\n\neBirding in Allegheny County\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nApr 18, 2020\n\n\nConor Tompkins\n\n\n\n\n\n\n\n\n\n\n\n\nRoughly Calculating Allegheny County Transit Efficiency\n\n\n\n\n\n\nR\n\n\nWPRDC\n\n\nAllegheny County\n\n\nTransit\n\n\n\n\n\n\n\n\n\nApr 4, 2020\n\n\nConor Tompkins\n\n\n\n\n\n\n\n\n\n\n\n\nGraphing Seasonality in Ebird Bird Sightings\n\n\n\n\n\n\nR\n\n\neBird\n\n\n\n\n\n\n\n\n\nMar 31, 2020\n\n\nConor Tompkins\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing Transit Connections Between Pittsburgh Census Tracts\n\n\n\n\n\n\nR\n\n\nPittsburgh\n\n\nAllegheny County\n\n\nTransit\n\n\nWPRDC\n\n\n\n\n\n\n\n\n\nMar 1, 2020\n\n\nConor Tompkins\n\n\n\n\n\n\n\n\n\n\n\n\nHow Many Pittsburghers Cross the River to Get to Work\n\n\n\n\n\n\nR\n\n\nPittsburgh\n\n\nAllegheny County\n\n\nCensus\n\n\nWPRDC\n\n\n\n\n\n\n\n\n\nFeb 9, 2020\n\n\nConor Tompkins\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing Commuter Patterns in Allegheny County\n\n\n\n\n\n\nR\n\n\nPittsburgh\n\n\nAllegheny County\n\n\nCommuters\n\n\n\n\n\n\n\n\n\nNov 11, 2019\n\n\nConor Tompkins\n\n\n\n\n\n\n\n\n\n\n\n\nForecasting Healthy Ride Ridership With Prophet\n\n\n\n\n\n\nR\n\n\nPittsburgh\n\n\nHealthy Ride\n\n\n\n\n\n\n\n\n\nAug 3, 2019\n\n\nConor Tompkins\n\n\n\n\n\n\n\n\n\n\n\n\nResidential Zoning in Pittsburgh\n\n\n\n\n\n\nR\n\n\nPittsburgh\n\n\n\n\n\n\n\n\n\nJun 22, 2019\n\n\nConor Tompkins\n\n\n\n\n\n\n\n\n\n\n\n\nMap Census Data With R\n\n\n\n\n\n\nR\n\n\nPotholes\n\n\nCensus\n\n\n\n\n\n\n\n\n\nMay 28, 2019\n\n\nConor Tompkins\n\n\n\n\n\n\n\n\n\n\n\n\nPremier League 538 SPI Ratings\n\n\n\n\n\n\nR\n\n\nPittsburgh\n\n\n538\n\n\nSoccer\n\n\n\n\n\n\n\n\n\nApr 7, 2019\n\n\nConor Tompkins\n\n\n\n\n\n\n\n\n\n\n\n\nAnimating Growth of Allegheny County\n\n\n\n\n\n\nR\n\n\nAllegheny County\n\n\nWPRDC\n\n\nHousing\n\n\n\n\n\n\n\n\n\nMar 31, 2019\n\n\nConor Tompkins\n\n\n\n\n\n\n\n\n\n\n\n\nModeling Pittsburgh House Sales Linear\n\n\n\n\n\n\nR\n\n\nHousing\n\n\nAllegheny County\n\n\n\n\n\n\n\n\n\nJan 13, 2019\n\n\nConor Tompkins\n\n\n\n\n\n\n\n\n\n\n\n\nActblue Interstate Political Campaign Donations\n\n\n\n\n\n\nR\n\n\nPolitics\n\n\n\n\n\n\n\n\n\nNov 11, 2018\n\n\nConor Tompkins\n\n\n\n\n\n\n\n\n\n\n\n\nNetworking USL Club Similarity With Euclidean Distance\n\n\n\n\n\n\nR\n\n\nSoccer\n\n\n538\n\n\n\n\n\n\n\n\n\nSep 14, 2018\n\n\nConor Tompkins\n\n\n\n\n\n\n\n\n\n\n\n\nClustering Allegheny County Census Tracts With PCA and k-means\n\n\n\n\n\n\nR\n\n\nCensus\n\n\nAllegheny County\n\n\n\n\n\n\n\n\n\nSep 8, 2018\n\n\nConor Tompkins\n\n\n\n\n\n\n\n\n\n\n\n\nUSL in the 538 Global Club Soccer Rankings\n\n\n\n\n\n\nR\n\n\nUSL\n\n\nRiverhounds\n\n\n\n\n\n\n\n\n\nAug 14, 2018\n\n\nConor Tompkins\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Allegheny County With Census Data\n\n\n\n\n\n\nR\n\n\nCensus\n\n\nAllegheny County\n\n\n\n\n\n\n\n\n\nAug 6, 2018\n\n\nConor Tompkins\n\n\n\n\n\n\n\n\n\n\n\n\nExploring 311 Data With PCA\n\n\n\n\n\n\nCensus\n\n\nPittsburgh\n\n\nWPRDC\n\n\n\n\n\n\n\n\n\nJul 19, 2018\n\n\nConor Tompkins\n\n\n\n\n\n\n\n\n\n\n\n\nCar Crashes in Allegheny County\n\n\n\n\n\n\nWPRDC\n\n\nAllegheny County\n\n\n\n\n\n\n\n\n\nJun 27, 2018\n\n\nConor Tompkins\n\n\n\n\n\n\n\n\n\n\n\n\nR 311 Pothole Workshop Code for Pittsburgh\n\n\n\n\n\n\nWPRDC\n\n\nPittsburgh\n\n\nPotholes\n\n\nCode for Pittsburgh\n\n\n\n\n\n\n\n\n\nApr 18, 2018\n\n\nConor Tompkins\n\n\n\n\n\n\n\n\n\n\n\n\nComparing US Senators Casey and Toomey\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 12, 2018\n\n\nConor Tompkins\n\n\n\n\n\n\n\n\n\n\n\n\nMayor Bill Peduto Tweets\n\n\n\n\n\n\nTwitter\n\n\nPittsburgh\n\n\n\n\n\n\n\n\n\nMar 8, 2018\n\n\nConor Tompkins\n\n\n\n\n\n\n\n\n\n\n\n\nAllegheny County Overdose Data\n\n\n\n\n\n\nR\n\n\nAllegheny County\n\n\nOverdoses\n\n\n\n\n\n\n\n\n\nFeb 16, 2018\n\n\nConor Tompkins\n\n\n\n\n\n\n\n\n\n\n\n\nMapping Healthy Ride Data\n\n\n\n\n\n\nPittsburgh\n\n\nHealthy Ride\n\n\n\n\n\n\n\n\n\nNov 12, 2017\n\n\nConor Tompkins\n\n\n\n\n\n\nNo matching items"
  }
]