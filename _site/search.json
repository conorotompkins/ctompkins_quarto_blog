[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a data scientist in the Pittsburgh area with an interest in data visualization, statistical programming, birdwatching, and civic data."
  },
  {
    "objectID": "posts/model-results-leaflet-map/index.html",
    "href": "posts/model-results-leaflet-map/index.html",
    "title": "Pittsburgh City Boundary Model Leaflet Map",
    "section": "",
    "text": "Last Friday I posted a classification model that attempted to identify which census tracts in Allegheny County fall within the border of the City of Pittsburgh. I used a variety of data from the 2010 US Census via {tidycensus}, and I intentionally did not train the model with any geographic data. I am more interested in which census tracts are more “city-like”, regardless of their distance from the geographic center of the city.\nAs with most models, I think you can learn the most when you investigate the cases where the model failed. I made an interactive Leaflet map to help me to interrogate the model’s results. For each tract it includes the average city classification % and the model’s correct classification %, as well as the census data that was used to train the model. Make sure to click the “View Fullscreen” button in the top left corner to see more of the map."
  },
  {
    "objectID": "posts/shiny-venn-diagram/index.html",
    "href": "posts/shiny-venn-diagram/index.html",
    "title": "Making a Venn diagram in Shiny",
    "section": "",
    "text": "Introduction\nThis blog post is about making Venn diagrams work in Shiny, and the issues I ran into with shiny::nearPoints(). I show how this impacted my initial approach, and discuss the underlying issue.\nTLDR; shiny::nearPoints() doesn’t work with dataframes containing list-columns the way I expected\n\n\nBackground\nI have been working on a Shiny app that I will use to plan birdwatching trips. It uses the {ebirdst} package to pull abundance data for hundreds of species of birds in 27x27km tiles in North America. A major feature of the app will be the ability to compare how similar two areas (tiles) are. This compares the abundance for a species in a given tile in a given month. I wanted to include a Venn diagram that shows which species are exclusive to each tile. The user can click on the Venn diagram to see the species associated with each segment of the Venn diagram.\nThis involves making a venn diagram in ggplot2 and extracting the segment that the user clicks on with nearPoints(). This was more challenging than I had anticipated.\n\n\nVenn diagram data\nnearPoints() requires:\n\ndf: a data frame with x and y coordinates it can interpret\ncoordinfo: the user click coordinates as captured from the ui\n\nI use the ggVennDiagram package to make the venn diagram plot. This package uses ggplot2, but does a lot of pre-processing of the data beforehand. This made it difficult to get access to the df for nearPoints().\nThis is an example of a ggVennDiagram plot. It takes a list object, turns that into a dataframe, and then uses sf to draw the circles.\n\nlibrary(tidyverse)\nlibrary(ggVennDiagram)\n\ngenes &lt;- paste(\"gene\",1:100,sep=\"\")\nset.seed(20210419)\nx &lt;- list(A=sample(genes,30),\n          B=sample(genes,50))\n\nggVennDiagram(x)\n\n\n\n\nLooking under the hood of ggVennDiagram() shows the pre-processing steps:\n\nvenn &lt;- Venn(x)\ndata &lt;- process_data(venn)\n\nVenn() creates an object with slots representing the two sets A and B\n\nVenn(x)\n\nAn object of class \"Venn\"\nSlot \"sets\":\n$A\n [1] \"gene27\" \"gene76\" \"gene57\" \"gene33\" \"gene78\" \"gene39\" \"gene63\" \"gene41\"\n [9] \"gene66\" \"gene17\" \"gene16\" \"gene69\" \"gene75\" \"gene9\"  \"gene68\" \"gene3\" \n[17] \"gene34\" \"gene54\" \"gene19\" \"gene83\" \"gene2\"  \"gene40\" \"gene87\" \"gene60\"\n[25] \"gene61\" \"gene24\" \"gene44\" \"gene93\" \"gene53\" \"gene7\" \n\n$B\n [1] \"gene84\"  \"gene36\"  \"gene37\"  \"gene47\"  \"gene91\"  \"gene46\"  \"gene92\" \n [8] \"gene33\"  \"gene67\"  \"gene73\"  \"gene25\"  \"gene5\"   \"gene63\"  \"gene2\"  \n[15] \"gene83\"  \"gene56\"  \"gene77\"  \"gene10\"  \"gene12\"  \"gene95\"  \"gene76\" \n[22] \"gene53\"  \"gene99\"  \"gene19\"  \"gene31\"  \"gene86\"  \"gene80\"  \"gene65\" \n[29] \"gene48\"  \"gene100\" \"gene89\"  \"gene58\"  \"gene35\"  \"gene30\"  \"gene21\" \n[36] \"gene44\"  \"gene72\"  \"gene18\"  \"gene45\"  \"gene42\"  \"gene1\"   \"gene27\" \n[43] \"gene90\"  \"gene14\"  \"gene43\"  \"gene26\"  \"gene96\"  \"gene17\"  \"gene16\" \n[50] \"gene29\" \n\n\nSlot \"names\":\n[1] \"A\" \"B\"\n\n\nprocess_data() turns those slots into dataframes with sf columns representing the segment polygons.\n\nvenn &lt;- Venn(x)\nprocess_data(venn)\n\nAn object of class \"VennPlotData\"\nSlot \"setEdge\":\nSimple feature collection with 2 features and 5 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 125 ymin: 250 xmax: 875 ymax: 750\nCRS:           NA\n# A tibble: 2 × 6\n  id                                        geometry component item  count name \n  &lt;chr&gt;                                 &lt;LINESTRING&gt; &lt;chr&gt;     &lt;nam&gt; &lt;int&gt; &lt;chr&gt;\n1 1     (500 716, 493.065 720.007, 485.954 723.777,… setEdge   &lt;chr&gt;    30 A    \n2 2     (500 284, 506.935 279.998, 514.046 276.243,… setEdge   &lt;chr&gt;    50 B    \n\nSlot \"setLabel\":\nSimple feature collection with 2 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 250 ymin: 780 xmax: 750 ymax: 780\nCRS:           NA\n# A tibble: 2 × 4\n  id     geometry component name \n  &lt;chr&gt;   &lt;POINT&gt; &lt;chr&gt;     &lt;chr&gt;\n1 1     (250 780) setLabel  A    \n2 2     (750 780) setLabel  B    \n\nSlot \"region\":\nSimple feature collection with 3 features and 5 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 125 ymin: 250 xmax: 875 ymax: 750\nCRS:           NA\n# A tibble: 3 × 6\n  id                                        geometry component item  count name \n  &lt;chr&gt;                                    &lt;POLYGON&gt; &lt;chr&gt;     &lt;lis&gt; &lt;int&gt; &lt;chr&gt;\n1 1     ((500 716, 492.317 711.329, 484.878 706.459… region    &lt;chr&gt;    19 A    \n2 2     ((500 284, 507.683 288.649, 515.122 293.497… region    &lt;chr&gt;    39 B    \n3 12    ((507.683 711.328, 515.122 706.458, 522.317… region    &lt;chr&gt;    11 A..B \n\n\nThe region slot is most important for my purposes. It contains the sf polygons for the segments and the distinct counts exclusive to each segment.\n\nprocess_data(venn) %&gt;% \n  .@region\n\nSimple feature collection with 3 features and 5 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 125 ymin: 250 xmax: 875 ymax: 750\nCRS:           NA\n# A tibble: 3 × 6\n  id                                        geometry component item  count name \n  &lt;chr&gt;                                    &lt;POLYGON&gt; &lt;chr&gt;     &lt;lis&gt; &lt;int&gt; &lt;chr&gt;\n1 1     ((500 716, 492.317 711.329, 484.878 706.459… region    &lt;chr&gt;    19 A    \n2 2     ((500 284, 507.683 288.649, 515.122 293.497… region    &lt;chr&gt;    39 B    \n3 12    ((507.683 711.328, 515.122 706.458, 522.317… region    &lt;chr&gt;    11 A..B \n\nprocess_data(venn) %&gt;% \n  .@region %&gt;% \n  ggplot(aes(fill = name)) +\n  geom_sf()\n\n\n\n\nI thought using nearPoints() would be pretty easy once I intercepted the region object from the preprocessing steps. I was wrong.\n\n\nShiny app error\nThis basic Shiny app will reproduce the error that nearPoints() generates:\n\nlibrary(shiny)\nlibrary(tidyverse)\nlibrary(ggVennDiagram)\nlibrary(sf)\n\n#ui\nui &lt;- fluidPage(\n  \n  titlePanel(\"Shiny Venn Diagram\"),\n  \n  mainPanel(\n    plotOutput(\"venn_diagram\", click = \"plot_click\"),\n    tableOutput(\"venn_table\")\n  )\n)\n\ngenes &lt;- paste(\"gene\",1:1000,sep=\"\")\nset.seed(20210419)\nx &lt;- list(A=sample(genes,300),\n          B=sample(genes,525))\n\nvenn &lt;- Venn(x)\nvenn_data &lt;- process_data(venn)@region %&gt;% \n  mutate(centroid = st_point_on_surface(geometry),\n         x = map_dbl(centroid, 1),\n         y = map_dbl(centroid, 2)) %&gt;% \n  select(x, y, name, geometry)\n\n#server\nserver &lt;- function(input, output){\n  \n  output$venn_diagram &lt;- renderPlot({\n    \n    venn_data %&gt;% \n      ggplot(aes(x, y, fill = name, label = name)) +\n      geom_sf() +\n      geom_label()\n    \n  })\n  \n  output$venn_table &lt;- renderTable({\n    \n    req(input$plot_click)\n    \n    nearPoints(venn_data, #this is the issue\n               input$plot_click,\n               threshold = 100)\n    \n  })\n  \n}\n\nThis is the error:\n\nWarning: Error in &lt;-: number of items to replace is not a multiple of replacement length\n104: print.xtable\n98: transform\n97: func\n95: f\n94: Reduce\n85: do\n84: hybrid_chain\n83: renderFunc\n82: output$venn_table\n1: shiny::runApp\n\n\n\nThe fix\nWrapping the venn_data object in st_drop_geometry() drops the sf list-column and turns it back into a regular dataframe.\n\nlibrary(shiny)\nlibrary(tidyverse)\nlibrary(ggVennDiagram)\nlibrary(sf)\n\n#ui\nui &lt;- fluidPage(\n  \n  titlePanel(\"Shiny Venn Diagram\"),\n  \n  mainPanel(\n    plotOutput(\"venn_diagram\", click = \"plot_click\"),\n    tableOutput(\"venn_table\")\n  )\n)\n\ngenes &lt;- paste(\"gene\",1:1000,sep=\"\")\nset.seed(20210419)\nx &lt;- list(A=sample(genes,300),\n          B=sample(genes,525))\n\nvenn &lt;- Venn(x)\nvenn_data &lt;- process_data(venn)@region %&gt;% \n  mutate(centroid = st_point_on_surface(geometry),\n         x = map_dbl(centroid, 1),\n         y = map_dbl(centroid, 2)) %&gt;% \n  select(x, y, name, geometry)\n\n#server\nserver &lt;- function(input, output){\n  \n  output$venn_diagram &lt;- renderPlot({\n    \n    venn_data %&gt;% \n      ggplot(aes(x, y, fill = name, label = name)) +\n      geom_sf() +\n      geom_label()\n    \n  })\n  \n  output$venn_table &lt;- renderTable({\n    \n    req(input$plot_click)\n    \n    nearPoints(st_drop_geometry(venn_data), #the fix\n               input$plot_click,\n               threshold = 100)\n    \n  })\n  \n}\n\n\n\nWorking Shiny App\nThis is a working example of a Venn diagram in Shiny. input$plot_click captures the coordinates of the click and nearPoints() returns a dataframe of the information about the segment the user clicked on. The ID of the segment is in the name column."
  },
  {
    "objectID": "posts/time-series-clustering-covid-19-cases/index.html",
    "href": "posts/time-series-clustering-covid-19-cases/index.html",
    "title": "Time series clustering COVID-19 case data",
    "section": "",
    "text": "Interactive Tableau visualization of the clusters\nThe goal of this post is to group states into clusters based on the shape of the curve of a state’s cumulative sum of COVID-19 cases. This type of clustering is useful when the variance in absolute values of a time series obscures the underlying pattern in the data. Since states experienced plateaus and peaks at different times, my hope is that the clustering is able to identify those differences.\nThis loads the packages I will use in the analysis and set up the environment:\nlibrary(tidyverse)\nlibrary(tsibble)\nlibrary(dtwclust)\nlibrary(tidymodels)\nlibrary(hrbrthemes)\nlibrary(tidycensus)\nlibrary(sf)\n\noptions(scipen = 999, digits = 4)\n\ntheme_set(theme_ipsum())\n\nset.seed(1234)\nI will adjust the cases to per 100,000, which requires information from the U.S. Census. This code pulls state-level population data from the Census API via tidycensus:\ncensus_data &lt;- get_acs(geography = \"state\", variables = \"B01003_001\", geometry = FALSE, year = 2020) %&gt;% \n  select(state = NAME, population = estimate)\nThis pulls the COVID-19 data from the NYTimes GitHub page:\ncovid &lt;- read_csv(\"https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-states.csv\") %&gt;% \n  arrange(state, date) %&gt;% \n  semi_join(census_data) %&gt;% \n  filter(date &lt;= \"2020-07-18\")\nI use the tsibble package to check if there are implicit gaps in the data. For example, if there was data for 2020-06-01 and 2020-06-03, there is an implicit gap because there is not data for 2020-06-02.\ncovid %&gt;% \n  as_tsibble(index = date, key = state) %&gt;% \n  count_gaps()\n\n# A tibble: 0 × 4\n# ℹ 4 variables: state &lt;chr&gt;, .from &lt;date&gt;, .to &lt;date&gt;, .n &lt;int&gt;\nThankfully, there are not any such gaps. If there were, I would have to impute values for the missing days.\nSince states experienced onset of COVID-19 at different times, I find the day each state hit 10 cases, and calculate days_since_10th_case, which I will use instead of date.\ncovid_10th_case &lt;- covid %&gt;% \n  filter(cases &gt;= 10) %&gt;% \n  group_by(state) %&gt;% \n  slice(1) %&gt;% \n  ungroup() %&gt;% \n  select(state, date_of_10th_case = date)\n\ncovid &lt;- covid %&gt;% \n  left_join(covid_10th_case, by = c(\"state\" = \"state\")) %&gt;% \n  group_by(state) %&gt;% \n  mutate(days_since_10th_case = date - date_of_10th_case) %&gt;% \n  ungroup() %&gt;% \n  filter(days_since_10th_case &gt;= 0)\n\ncovid &lt;- covid %&gt;% \n  select(state, days_since_10th_case, cases)\nNext I calculate cases_per_capita:\ncovid &lt;- covid %&gt;% \n  left_join(census_data) %&gt;% \n  mutate(cases_per_capita = (cases / population) * 100000) %&gt;% \n  select(-population)\nNext I scale the cases so that the mean is 0 and the standard deviation is 1. Each state has its own mean and standard deviation.\ncovid &lt;- covid %&gt;% \n  group_by(state) %&gt;% \n  mutate(cases_per_capita_scaled = scale(cases_per_capita, center = TRUE, scale = TRUE)) %&gt;% \n  ungroup()\nThe result of this is that the clustering algorithm will focus on the shape of the line for each state instead of absolute values. This graph shows the difference:\ncovid %&gt;% \n  pivot_longer(cols = contains(\"cases\"), names_to = \"metric\", values_to = \"value\") %&gt;% \n  ggplot(aes(days_since_10th_case, value, group = state)) +\n  geom_hline(yintercept = 0, linetype = 2) +\n  geom_line(alpha = .1) +\n  facet_wrap(~metric, ncol = 1, scales = \"free_y\") +\n  scale_y_comma()\ntsclust requires that the input data be a series of lists, not a dataframe. unstack takes a key and value as arguments and turns the dataframe into a list of lists.\ncovid_list &lt;- covid %&gt;% \n  select(state, cases_per_capita_scaled) %&gt;% \n  unstack(cases_per_capita_scaled ~ state)\nThis loops through the clustering function 20 times and saves each output to a list. The first object groups the data into 2 clusters, the second object has 3 clusters, and it continues in that pattern.\ncluster_dtw_h &lt;- list()\n\nkclust &lt;- 20\n\nfor (i in 2:kclust){\n  cluster_dtw_h[[i]] &lt;- tsclust(covid_list, \n                                type = \"h\", \n                                k = i,\n                                distance = \"dtw\", \n                                control = hierarchical_control(method = \"complete\"), \n                                seed = 390, \n                                preproc = NULL, \n                                args = tsclust_args(dist = list(window.size = 21L)))\n  \n  print(i)\n}\n\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n[1] 11\n[1] 12\n[1] 13\n[1] 14\n[1] 15\n[1] 16\n[1] 17\n[1] 18\n[1] 19\n[1] 20\nThe object that tsclust outputs has a complex structure that makes it difficult to work with at scale. The data I need to pull out is stored in various slots. The next step is to write functions that pulls out the data and tidies it up."
  },
  {
    "objectID": "posts/time-series-clustering-covid-19-cases/index.html#mapping",
    "href": "posts/time-series-clustering-covid-19-cases/index.html#mapping",
    "title": "Time series clustering COVID-19 case data",
    "section": "Mapping",
    "text": "Mapping\nThe data is aggregated at the state level, which can easily be graphed with ggplot2 and tidycensus.\n\nmap &lt;- get_acs(geography = \"state\", variables = \"B01003_001\", geometry = TRUE, shift_geo = TRUE)\n\nmap %&gt;% \n  ggplot() +\n  geom_sf() +\n  theme_void()\n\n\n\n\nThis joins the cluster assignments to the map object and summarizes the state polygons by region. This dissolves the state boundaries and creates polygons for each cluster.\n\nmap_cluster &lt;- map %&gt;% \n  left_join(cluster_assignments %&gt;% \n             filter(kclust == best_kclust), by = c(\"NAME\" = \"state\")) %&gt;% \n  add_count(cluster_assignment) %&gt;% \n  mutate(cluster_assignment = as.character(cluster_assignment),\n         cluster_assignment = fct_reorder(cluster_assignment, desc(n))) %&gt;% \n  group_by(cluster_assignment) %&gt;% \n  summarize()\n\nstate_clustered &lt;- map %&gt;% \n  left_join(cluster_assignments %&gt;% \n              filter(kclust == best_kclust), by = c(\"NAME\" = \"state\")) %&gt;% \n  add_count(cluster_assignment) %&gt;% \n  mutate(cluster_assignment = as.character(cluster_assignment),\n         cluster_assignment = fct_reorder(cluster_assignment, desc(n)))\n\nThis code creates the map, and overlays the state boundaries on the cluster polygons.\n\nmap_cluster %&gt;% \n  ggplot() +\n  geom_sf(aes(fill = cluster_assignment, color = cluster_assignment),\n          size = 1) +\n  geom_sf_text(data = state_clustered, aes(label = cluster_assignment)) +\n  labs(fill = \"Cluster assignment\",\n       color = \"Cluster assigmment\") +\n  guides(color = FALSE) +\n  theme_void() +\n  theme(legend.position = \"bottom\",\n        legend.direction = \"horizontal\")\n\n\n\n\nCluster 3 stands out as the group of states that are currently struggling with COVID-19 the most. Interestingly, these states are consistently clustered together regardless of the value of kclust, which means that these states are very similar.\nCluster 5 represents the states that had the earliest and worst outbreaks, but have beaten back the virus for now. Cluster 6 are the neighbors of New York and New Jersey. They experienced less peaky curves later than Cluster 5. Cluster 6 is an “echo” of Cluster 5.\nThe singleton clusters for kclust of 12 are Vermont, Nebraska, and Hawaii. Nebraska had a long period with almost no new cases at the beginning, but then had a very steep increase after that. Vermont’s curve started steeply almost immediately after its 10th case, which distinguishes it from the other states. Hawaii has had two periods of very steep increases sperated by a long period with few new cases. This is very likely due to the difficulty of traveling to the state with travel lockdowns in place.\n\nSources\n\nhttps://rpubs.com/esobolewska/dtw-time-series\nhttp://www.rdatamining.com/examples/time-series-clustering-classification\nhttp://rstudio-pubs-static.s3.amazonaws.com/398402_abe1a0343a4e4e03977de8f3791e96bb.html"
  },
  {
    "objectID": "posts/compare-pre-post-covid/index.html",
    "href": "posts/compare-pre-post-covid/index.html",
    "title": "Comparing Healthy Ride Usage Pre And “Post” COVID-19",
    "section": "",
    "text": "Lawrence Andrews asked me on Twitter if there had been a change in Health Ride usage after COVID-19.\n\n\nWould be interested to see this @healthyridepgh data to compare pre-covid (2019) and during (2020)\n\n— Lawrence Andrews (@lawrenceandrews) August 13, 2020\n\n\nThe {tidyverts} universe of packages from Rob Hyndman provides a lot of tools that let you interrogate time series data. I will use some of these tools to decompose the Healthy Ride time series and see if there was a change.\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(janitor)\nlibrary(tsibble)\nlibrary(feasts)\nlibrary(hrbrthemes)\n\noptions(scipen = 999, digits = 4)\n\ntheme_set(theme_ipsum(base_size = 20, \n                      strip_text_size = 18, \n                      axis_title_size = 18))\n\nI had already combined the usage data from the WPRDC with list.files and map_df(read_csv), so I can just read in the combined CSV file:\n\ndata &lt;- read_csv(\"post_data/combined_ride_data.csv\")\n\nSummarizing the number of rides per day shows that the data is very seasonal. The red line is on March 6, which is the date of the first known positive COVID-19 case in the state.\n\ndata %&gt;% \n  count(date, name = \"number_of_rides\", sort = TRUE) %&gt;% \n  filter(!is.na(date)) %&gt;% \n  ggplot(aes(date, number_of_rides)) +\n  geom_point(alpha = .5, size = .5) +\n  geom_vline(xintercept = ymd(\"2020-03-06\"), color = \"red\")\n\n\n\n\nI use the {tsibble} package to make a time series tibble and fill in a few gaps in the data. Then I create 3 different models to decompose the time series. I will compare these 3 models to see which strips away the seasonality the best.\n\ndcmp &lt;- data %&gt;%\n  mutate(time = date) %&gt;% \n  count(time, name = \"number_of_rides\") %&gt;%\n  as_tsibble(index = time) %&gt;%\n  tsibble::fill_gaps(number_of_rides = 0) %&gt;% \n  model(STL(number_of_rides),\n        STL(number_of_rides ~ season(window = Inf)),\n        STL(number_of_rides ~ trend(window=7) + season(window='periodic'),\n            robust = TRUE))\n\ncomponents(dcmp) %&gt;% \n  glimpse()\n\nRows: 5,850\nColumns: 8\nKey: .model [3]\n: number_of_rides = trend + season_year + season_week + remainder\n$ .model          &lt;chr&gt; \"STL(number_of_rides)\", \"STL(number_of_rides)\", \"STL(n…\n$ time            &lt;date&gt; 2015-05-31, 2015-06-01, 2015-06-02, 2015-06-03, 2015-…\n$ number_of_rides &lt;dbl&gt; 480, 126, 139, 131, 213, 274, 380, 424, 124, 255, 267,…\n$ trend           &lt;dbl&gt; 249.2, 249.1, 249.1, 249.0, 248.9, 248.9, 248.8, 248.8…\n$ season_week     &lt;dbl&gt; 120.53, -87.43, -67.74, -17.65, -50.30, 16.87, 87.83, …\n$ season_year     &lt;dbl&gt; 132.54, 101.70, 103.87, 105.32, 131.12, 84.99, 20.29, …\n$ remainder       &lt;dbl&gt; -22.2488, -137.3844, -146.1889, -205.6720, -116.7592, …\n$ season_adjust   &lt;dbl&gt; 226.93, 111.73, 102.87, 43.33, 132.18, 172.13, 271.89,…\n\n\nThis code pivots the data long and plots the true number of rides per day and the estimate of the underlying trend per model. The “season_adjust” panel shows the number of rides adjusted for seasonal effects, the “trend” panel shows the underlying trend, and the “remainder” panel shows how much the seasonal adjustment missed by.\n\ncomponents(dcmp) %&gt;% \n  pivot_longer(cols = number_of_rides:season_adjust) %&gt;% \n  mutate(name = factor(name, levels = c(\"number_of_rides\", \"season_adjust\",\n                                        \"trend\", \"seasonal\",\n                                        \"season_year\", \"season_week\",\n                                        \"random\", \"remainder\"))) %&gt;% \n  filter(!is.na(value)) %&gt;% \n  filter(name == \"trend\" | name == \"season_adjust\" | name == \"number_of_rides\" | name == \"remainder\") %&gt;% \n  ggplot(aes(time, value, color = .model)) +\n  geom_point(alpha = .6, size = .6) +\n  annotate(geom = \"rect\", \n           xmin = ymd(\"2020-03-06\"), xmax = ymd(\"2020-12-31\"),\n           ymin = -Inf, ymax = Inf, \n           fill = \"red\", alpha = .1) +\n  facet_grid(name ~ .model, scales = \"free_y\", labeller = label_wrap_gen()) +\n  guides(color = FALSE)\n\n\n\n\nI am not a time series expert, but it appears that the most basic STL model STL(number_of_rides) does the best job because that model’s “trend” panel shows the least seasonality.\n\ncomponents(dcmp) %&gt;% \n  pivot_longer(cols = number_of_rides:season_adjust) %&gt;% \n  mutate(name = factor(name, levels = c(\"number_of_rides\", \"season_adjust\",\n                                        \"trend\", \"seasonal\",\n                                        \"season_year\", \"season_week\",\n                                        \"random\", \"remainder\"))) %&gt;% \n  filter(!is.na(value)) %&gt;% \n  filter(name == \"trend\" | name == \"season_adjust\" | name == \"number_of_rides\" | name == \"remainder\") %&gt;% \n  filter(.model == \"STL(number_of_rides)\") %&gt;% \n  ggplot(aes(time, value, color = .model)) +\n  geom_point(alpha = .6, size = .6, color = \"#619CFF\") +\n  annotate(geom = \"rect\", \n           xmin = ymd(\"2016-03-06\"), xmax = ymd(\"2016-03-30\"),\n           ymin = -Inf, ymax = Inf, \n           fill = \"black\", alpha = .3) +\n  annotate(geom = \"rect\", \n           xmin = ymd(\"2017-03-06\"), xmax = ymd(\"2017-03-30\"),\n           ymin = -Inf, ymax = Inf, \n           fill = \"black\", alpha = .3) +\n  annotate(geom = \"rect\", \n           xmin = ymd(\"2018-03-06\"), xmax = ymd(\"2018-03-30\"),\n           ymin = -Inf, ymax = Inf, \n           fill = \"black\", alpha = .3) +\n  annotate(geom = \"rect\", \n           xmin = ymd(\"2019-03-06\"), xmax = ymd(\"2019-03-30\"),\n           ymin = -Inf, ymax = Inf, \n           fill = \"black\", alpha = .3) +\n  annotate(geom = \"rect\", \n           xmin = ymd(\"2020-03-06\"), xmax = ymd(\"2020-03-06\") + 60,\n           ymin = -Inf, ymax = Inf, \n           fill = \"red\", alpha = .3) +\n  facet_grid(name ~ .model, scales = \"free_y\", labeller = label_wrap_gen()) +\n  guides(color = FALSE)\n\n\n\n\nFocusing on that model, it appears that the trend dropped in mid-March, but rebounded to normal levels quickly. I highlighted the data from previous Marches to see if there was a recurring dip in March."
  },
  {
    "objectID": "posts/volatile-zillow-house-value/index.html",
    "href": "posts/volatile-zillow-house-value/index.html",
    "title": "Graphing volatile home values in U.S. metro areas",
    "section": "",
    "text": "Zillow publishes a variety of cool data that I haven’t explored much yet. The first dataset that caught my eye was the Zillow Home Value Index (ZHVI). Zillow describes it as the “smoothed, seasonally adjusted measure of the typical home value and market changes across a given region and housing type”. In this post I will make a quick gganimate plot of the ZHVI of various metro areas in the U.S.\nThe code for this post was re-ran in 2024. The data was filtered to match the date of the original post.\nThe first thing I noticed about the data is that it is aggressively wide. There is a column for each year-month in the dataset. 293 columns is a lot to work with.\n\n#download housing data from https://www.zillow.com/research/data/\nzillow_house_value &lt;- read_csv(\"post_data/Metro_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv\")\n\ndim(zillow_house_value)\n\n[1] 895 293\n\n\nTo make the data more tidy, I use a regex to identify the columns that have a date in the name and pivot those longer. Now each row represents the ZHVI for a given region area on a given year-month.\n\nzillow_house_value &lt;- zillow_house_value %&gt;% \n  pivot_longer(cols = matches(\"\\\\d{4}-\\\\d{2}-\\\\d{2}\"),\n               names_to = \"date\", values_to = \"zhvi\") %&gt;% \n  clean_names() %&gt;% \n  mutate(date = ymd(date),\n         region_name = str_squish(region_name))\n\nglimpse(zillow_house_value)\n\nRows: 257,760\nColumns: 7\n$ region_id   &lt;dbl&gt; 102001, 102001, 102001, 102001, 102001, 102001, 102001, 10…\n$ size_rank   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ region_name &lt;chr&gt; \"United States\", \"United States\", \"United States\", \"United…\n$ region_type &lt;chr&gt; \"country\", \"country\", \"country\", \"country\", \"country\", \"co…\n$ state_name  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ date        &lt;date&gt; 2000-01-31, 2000-02-29, 2000-03-31, 2000-04-30, 2000-05-3…\n$ zhvi        &lt;dbl&gt; 118707, 118916, 119175, 119730, 120370, 121055, 121781, 12…\n\n\nOnce the data is tidy, it is easy to plot with ggplot2. In this graph, each line represents one metro area.\n\nzillow_house_value %&gt;% \n  ggplot(aes(date, zhvi, group = region_name)) +\n  geom_line(alpha = .1, size = .5)\n\n\n\n\nWhat struck me is that while most metro areas in the dataset start with ZHVI &lt; $300,000, many increase to 3x that, with many wild swings along the way due to housing bubbles, economic crashes, and housing scarcity. I will rank the metro areas by volatility (standard deviation of ZHVI) and use ggplot2 and gganimate to highlight the most volatile metro areas.\n\n#find most volatile regions\ndf_top_regions &lt;- zillow_house_value %&gt;% \n  group_by(region_name) %&gt;% \n  summarize(sd = sd(zhvi)) %&gt;% \n  ungroup() %&gt;% \n  arrange(desc(sd)) %&gt;% \n  slice(1:25) %&gt;% \n  mutate(region_name_rank = str_c(\"#\", row_number(), \" \", region_name, sep = \"\"))\n  \nregion_name_highlight_fct &lt;- df_top_regions %&gt;% \n  pull(region_name)\n\nregion_name_rank_fct &lt;- df_top_regions %&gt;% \n  pull(region_name_rank)\n\n\n#create highlight df\ndf_highlights &lt;- zillow_house_value %&gt;% \n  inner_join(df_top_regions) %&gt;% \n  mutate(region_name_highlight = region_name,\n         region_name_highlight = factor(region_name_highlight, levels = region_name_highlight_fct),\n         region_name_rank = factor(region_name_rank, levels = region_name_rank_fct))\n\nhousing_animation &lt;- zillow_house_value %&gt;% \n  ggplot() +\n  geom_line(aes(date, zhvi, group = region_name), alpha = .1, size = .5) +\n  geom_line(data = df_highlights,\n            aes(date, zhvi),\n            color = \"red\", size = 1.5) +\n  scale_y_continuous(labels = scales::dollar_format()) +\n  transition_manual(region_name_rank) +\n  labs(title = \"Top 25 most volatile housing markets 1996-2020\",\n       subtitle = \"Region: {current_frame}\",\n       x = NULL,\n       y = \"Zillow Housing Value Index\") +\n  theme(plot.subtitle = element_text(size = 15),\n        axis.title.y = element_text(size = 15))\n\nhousing_animation &lt;- animate(housing_animation, duration = 10, fps = 40)\n\nhousing_animation"
  },
  {
    "objectID": "posts/classifying-pittsburgh-city-boundary/index.html",
    "href": "posts/classifying-pittsburgh-city-boundary/index.html",
    "title": "Modeling the Pittsburgh City Boundary",
    "section": "",
    "text": "My goal is to create a classification model that can distinguish between census tracts that are inside the city or outside the City of Pittsburgh. The border is interrupted by rivers, has an enclave, and is very irregular in general, which made this an interesting intellectual exercise.\n\n\nWhile Pittsburgh was founded in 1758, the city’s borders have changed many times due to annexation of surrounding municipalities. This map shows that what we call the North Side was previously Allegheny City, and was annexed into the city in 1907.\n\nMt. Oliver is a geographic enclave that is completely surrounded by the City of Pittsburgh, but is a separate municipality. The borough has resisted multiple annexation attempts.\n\n\n\n\nThis code loads the packages I need, configures some options, and sets the seed.\n\n#set up environment\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(janitor)\nlibrary(tidycensus)\nlibrary(sf)\nlibrary(hrbrthemes)\nlibrary(GGally)\n\ntheme_set(theme_ipsum(base_size = 15))\n\noptions(scipen = 999, digits = 4, tigris_use_cache = TRUE)\n\nset.seed(1234)\n\nI created a small Shiny app that let me select which tracts are inside the city borders. I will go over that in a future post. This loads the tracts from the Census API and pulls the results from the Shiny app.\n\n#load data about census tracts\ntracts &lt;- get_decennial(year = 2010, state = \"PA\", county = \"Allegheny County\", \n                        variables = \"P001001\",\n                        geography = \"tract\", geometry = TRUE)\n\ncity_tracts &lt;- read_csv(\"post_data/selected_tracts.csv\", col_types = cols(\"c\", \"l\")) %&gt;% \n  filter(selected == TRUE)\n\nglimpse(city_tracts)\n\nRows: 137\nColumns: 2\n$ GEOID    &lt;chr&gt; \"42003563000\", \"42003562800\", \"42003563100\", \"42003281500\", \"…\n$ selected &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, T…\n\n\nThis reads in the boundary shapefile and graphs it to show which tracts are in the city.\n\npgh_official_boundary &lt;- st_read(\"post_data/Pittsburgh_City_Boundary-shp\") %&gt;% \n  mutate(geography = \"City boundary\") %&gt;% \n  st_transform(crs = \"NAD83\") %&gt;% \n  st_cast(\"POLYGON\") %&gt;% \n  filter(FID != 7)\n\nReading layer `City_Boundary' from data source \n  `/Users/conorotompkins/Documents/github_repos/ctompkins_quarto_blog/posts/classifying-pittsburgh-city-boundary/post_data/Pittsburgh_City_Boundary-shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 8 features and 8 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 1316000 ymin: 381900 xmax: 1380000 ymax: 433400\nProjected CRS: NAD83 / Pennsylvania South (ftUS)\n\ntracts %&gt;% \n  left_join(city_tracts) %&gt;% \n  mutate(type = case_when(selected == TRUE ~ \"city\",\n                          is.na(selected) ~ \"non_city\")) %&gt;% \n  ggplot() +\n  geom_sf(aes(fill = type), size = .1) +\n  geom_sf(data = pgh_official_boundary, color = \"white\", linetype = 2, alpha = 0) +\n  theme_void()\n\n\n\n\nThis reads in the data I will consider for the model. All the data is from the 2010 Census.\n\nall_data &lt;- read_csv(\"post_data/combined_census_data_tract.csv\", col_types = cols(.default = \"c\")) %&gt;%\n  left_join(city_tracts) %&gt;% \n  mutate(type = case_when(selected == TRUE ~ \"city\",\n                          is.na(selected) ~ \"non_city\")) %&gt;% \n  mutate(across(pct_units_owned_loan:housed_population_density_pop_per_square_km, as.numeric)) %&gt;% \n  select(GEOID, type, everything()) %&gt;%\n  select(-c(selected, total_population_housed))\n\nglimpse(all_data)\n\nRows: 402\nColumns: 13\n$ GEOID                                       &lt;chr&gt; \"42003010300\", \"4200302010…\n$ type                                        &lt;chr&gt; \"city\", \"city\", \"city\", \"c…\n$ pct_units_owned_loan                        &lt;dbl&gt; 0.137755, 0.119779, 0.0762…\n$ pct_units_owned_entire                      &lt;dbl&gt; 0.18367, 0.06619, 0.02110,…\n$ pct_units_rented                            &lt;dbl&gt; 0.6786, 0.8140, 0.9026, 0.…\n$ total_population                            &lt;dbl&gt; 6600, 3629, 616, 2256, 260…\n$ pct_white                                   &lt;dbl&gt; 0.658333, 0.745660, 0.7564…\n$ pct_black                                   &lt;dbl&gt; 0.31167, 0.15982, 0.15584,…\n$ pct_asian                                   &lt;dbl&gt; 0.0110606, 0.0471204, 0.06…\n$ pct_hispanic                                &lt;dbl&gt; 0.024394, 0.032791, 0.0162…\n$ workers                                     &lt;dbl&gt; 0, 963, 408, 1056, 537, 47…\n$ jobs                                        &lt;dbl&gt; 0, 79639, 8301, 4633, 1962…\n$ housed_population_density_pop_per_square_km &lt;dbl&gt; 682.7, 1511.3, 386.7, 3205…\n\n\nThis plot compares all of the variables against each other. I used this to identify covariance and determine which variables should be excluded.\n\n#eda\npairwise_plot &lt;- all_data %&gt;% \n  select(-c(GEOID)) %&gt;% \n  ggpairs(aes(color = type)) +\n  theme(axis.text = element_text(size = 8))\n\n After reviewing this graph and considering things like covariance and zero-variance, I will use these variables in the model:\n\nHousing\n\nPercent of housing units owned outright\nPercent of housing units owned with a mortgage\nPercent of housing units rented\n\nDemographics\n\nPercent of people in the tract that are\n\nWhite\nBlack\n\n\nPopulation density\nEconomic data\n\nNumber of workers that live in the tract\nNumber of jobs in the tract\n\n\nNote that I am intentionally excluding any geographic data about the tracts. I am more interested in how “city-like” a given tract is than how close it is to the geographic center of the city.\nThis finalizes the data I will use to build the model.\n\ncensus_combined &lt;- all_data %&gt;% \n  select(GEOID, type, \n         pct_units_owned_loan, pct_units_owned_entire, pct_units_rented,\n         housed_population_density_pop_per_square_km,\n         pct_white, pct_black,\n         workers, jobs)\n\nglimpse(census_combined)\n\nRows: 402\nColumns: 10\n$ GEOID                                       &lt;chr&gt; \"42003010300\", \"4200302010…\n$ type                                        &lt;chr&gt; \"city\", \"city\", \"city\", \"c…\n$ pct_units_owned_loan                        &lt;dbl&gt; 0.137755, 0.119779, 0.0762…\n$ pct_units_owned_entire                      &lt;dbl&gt; 0.18367, 0.06619, 0.02110,…\n$ pct_units_rented                            &lt;dbl&gt; 0.6786, 0.8140, 0.9026, 0.…\n$ housed_population_density_pop_per_square_km &lt;dbl&gt; 682.7, 1511.3, 386.7, 3205…\n$ pct_white                                   &lt;dbl&gt; 0.658333, 0.745660, 0.7564…\n$ pct_black                                   &lt;dbl&gt; 0.31167, 0.15982, 0.15584,…\n$ workers                                     &lt;dbl&gt; 0, 963, 408, 1056, 537, 47…\n$ jobs                                        &lt;dbl&gt; 0, 79639, 8301, 4633, 1962…\n\n\n34% of the tracts are in the city, which is a slightly unbalanced dataset.\n\ncensus_combined %&gt;% \n  tabyl(type)\n\n     type   n percent\n     city 137  0.3408\n non_city 265  0.6592\n\n\nSince the total amount of data available is small, I will bootstrap the data to try to achieve more stable results from the model. Bootstrapping resamples the data with replacement, which creates multiple replicates of the original dataset with some variation due to sampling. I created a meme of my dog Quincy to illustrate the effect: \nI stratify by type so that each bootstrap has ~34% city tracts. This generates 50 sets of data for the model to work with.\n\ntract_boot &lt;- bootstraps(census_combined, strata = type, times = 50)\n\ntract_boot\n\n# Bootstrap sampling using stratification \n# A tibble: 50 × 2\n   splits            id         \n   &lt;list&gt;            &lt;chr&gt;      \n 1 &lt;split [402/152]&gt; Bootstrap01\n 2 &lt;split [402/145]&gt; Bootstrap02\n 3 &lt;split [402/147]&gt; Bootstrap03\n 4 &lt;split [402/150]&gt; Bootstrap04\n 5 &lt;split [402/144]&gt; Bootstrap05\n 6 &lt;split [402/148]&gt; Bootstrap06\n 7 &lt;split [402/152]&gt; Bootstrap07\n 8 &lt;split [402/149]&gt; Bootstrap08\n 9 &lt;split [402/143]&gt; Bootstrap09\n10 &lt;split [402/156]&gt; Bootstrap10\n# ℹ 40 more rows\n\n\nThis code chunk prepares the data to be modeled. I define the formula and scale all the numeric variables to have a mean of 0 and standard deviation of 1.\n\n#recipe\nmodel_recipe &lt;- recipe(type ~ ., data = census_combined) %&gt;% \n  update_role(GEOID, new_role = \"id\") %&gt;% \n  step_normalize(all_predictors())\n\nmodel_recipe_prep &lt;- model_recipe %&gt;%\n  prep(strings_as_factors = FALSE)\n\nmodel_recipe %&gt;% \n  summary()\n\n# A tibble: 10 × 4\n   variable                                    type      role      source  \n   &lt;chr&gt;                                       &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n 1 GEOID                                       &lt;chr [3]&gt; id        original\n 2 pct_units_owned_loan                        &lt;chr [2]&gt; predictor original\n 3 pct_units_owned_entire                      &lt;chr [2]&gt; predictor original\n 4 pct_units_rented                            &lt;chr [2]&gt; predictor original\n 5 housed_population_density_pop_per_square_km &lt;chr [2]&gt; predictor original\n 6 pct_white                                   &lt;chr [2]&gt; predictor original\n 7 pct_black                                   &lt;chr [2]&gt; predictor original\n 8 workers                                     &lt;chr [2]&gt; predictor original\n 9 jobs                                        &lt;chr [2]&gt; predictor original\n10 type                                        &lt;chr [3]&gt; outcome   original\n\n\nThis creates the model specifications for the two types of models I will use.\n\n#logistic regression\nlm_model &lt;- logistic_reg(mode = \"classification\") %&gt;% \n  set_engine(\"glm\")\n\n#random forest\nranger_model &lt;- rand_forest(trees = 1000, mode = \"classification\") %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\")\n\nThis sets up a workflow object to fit a logistic regression model against the bootstrap resamples I created earlier.\n\n#logistic regression\nlm_workflow &lt;- workflow() %&gt;% \n  add_recipe(model_recipe) %&gt;% \n  add_model(lm_model)\n\nlm_res &lt;- lm_workflow %&gt;% \n  fit_resamples(resamples = tract_boot) %&gt;% \n  mutate(model = \"lm\")\n\nlm_res %&gt;% \n  collect_metrics()\n\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary     0.783    49 0.00383 Preprocessor1_Model1\n2 roc_auc  binary     0.856    49 0.00394 Preprocessor1_Model1\n\n\nThe logistic regression gets ~76% accuracy, which is pretty good, but I want to know if a random forest could do better. This creates a workflow to fit a random forest model, and saves the predictions so I can use them later.\n\n#rf\nrf_workflow &lt;- workflow() %&gt;% \n  add_recipe(model_recipe_prep) %&gt;% \n  add_model(ranger_model)\n\nrf_res &lt;- rf_workflow %&gt;% \n  fit_resamples(resamples = tract_boot,\n                control = control_resamples(save_pred = TRUE)) %&gt;% \n  mutate(model = \"rf\")\n\nrf_res %&gt;% \n  collect_metrics()\n\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary     0.816    50 0.00360 Preprocessor1_Model1\n2 roc_auc  binary     0.894    50 0.00289 Preprocessor1_Model1\n\n\nIf you compare the results of the two models, the random forest model does much better than the logistic regression model.\n\ncombined_res &lt;- bind_rows(rf_res, lm_res)\n\ncombined_res %&gt;% \n  unnest(.metrics) %&gt;% \n  ggplot(aes(.estimate, color = model, fill = model)) +\n  geom_density(alpha = .5) +\n  facet_wrap(~.metric)\n\n\n\n\nThis graph shows that the random forest model’s false negative and false positive rates are about the same.\n\nrf_res %&gt;% \n  collect_predictions() %&gt;% \n  count(type, .pred_class) %&gt;% \n  ggplot(aes(type, .pred_class, fill = n)) +\n  geom_tile() +\n  labs(x = \"Truth\",\n       y = \"Prediction\",\n       fill = \"Number of observations\") +\n  scale_fill_continuous(label = scales::comma) +\n  coord_equal() +\n  theme(panel.grid.major = element_blank())\n\n\n\n\nThis fits the random forest model against the entire dataset to extract the variable importance metrics.\n\n#variable importance\nvar_imp &lt;- rf_workflow %&gt;% \n  fit(juice(model_recipe_prep)) %&gt;% \n  pull_workflow_fit() %&gt;% \n  vip::vi()\n\nvar_imp %&gt;%\n  mutate(Variable = fct_reorder(Variable, Importance)) %&gt;% \n  ggplot(aes(Importance, Variable)) +\n  geom_point()\n\n\n\n\nThe top 3 variables are the percent of a tract’s population that is White, the percent of housing units that are owned with a loan, and the population density. This matches my subjective model of city vs. non-city characteristics. Areas that are low density with majority White demographics where people own their homes are typically outside of the city. This dynamic is probably connected to the history of segregation and redlining in majority African American communities in Pittsburgh.\nSince the random forest model was fit against multiple bootstraps, I have multiple predictions per tract. I stratified the bootstraps by type, so the city and non_city tracts were sampled about the same number of times.\n\n#extract probabilities from bootstrap resamples\nfull_predictions &lt;- rf_res %&gt;% \n  collect_predictions() %&gt;% \n  mutate(correct = type == .pred_class) %&gt;%\n  left_join(census_combined %&gt;%\n              mutate(.row = row_number()))\n\nfull_predictions %&gt;% \n  count(type, GEOID) %&gt;% \n  ggplot(aes(n, fill = type, color = type)) +\n  geom_density(alpha = .3) +\n  labs(x = \"Number of observations of a tract\")\n\n\n\n\nI am not solely interested in the top-line accuracy of the model. Since the city border is a geographic phenomenon, there may be interesting patterns in the geographic distribution of the model’s predictions that can be shown on a map.\nTo map the data, I calculate the following metrics per tract:\n\nthe percent of predictions that were correct\naverage city classification %\naverage non-city classification %\nthe number of times the tract was sampled\n\n\nfull_predictions_pct &lt;- full_predictions %&gt;% \n  group_by(GEOID) %&gt;% \n  summarize(pct_correct = mean(correct),\n            mean_city = mean(.pred_city),\n            mean_non_city = mean(.pred_non_city),\n            n = n())\n\nglimpse(full_predictions_pct)\n\nRows: 402\nColumns: 5\n$ GEOID         &lt;chr&gt; \"42003010300\", \"42003020100\", \"42003020300\", \"4200303050…\n$ pct_correct   &lt;dbl&gt; 1.00000, 1.00000, 1.00000, 1.00000, 1.00000, 1.00000, 1.…\n$ mean_city     &lt;dbl&gt; 0.7656, 0.7912, 0.8174, 0.7668, 0.7078, 0.8055, 0.9276, …\n$ mean_non_city &lt;dbl&gt; 0.23436, 0.20878, 0.18263, 0.23318, 0.29219, 0.19454, 0.…\n$ n             &lt;int&gt; 23, 13, 15, 21, 19, 17, 16, 19, 11, 20, 22, 21, 18, 22, …\n\n\nThis shows the % of correct predictions per tract. The model was very successful with the outlying tracts in the county, but struggled in Mt. Washington/Beechview/Brookline, the Hazelwood/Greenfield area, and Forest Hills towards Monroeville.\n\ntracts %&gt;% \n  left_join(full_predictions_pct) %&gt;% \n  ggplot() +\n  geom_sf(aes(fill = pct_correct), lwd = 0) +\n  geom_sf(data = pgh_official_boundary, alpha = 0, color = \"black\", lwd = 2) +\n  geom_sf(data = pgh_official_boundary, alpha = 0, color = \"yellow\", lwd = .3) +\n  scale_fill_viridis_c(labels = scales::percent) +\n  labs(fill = \"% predictions correct\") +\n  theme_void()\n\n\n\n\nThis shows the average % that the model classified a tract as being in the city. The model was very confident that Oakland, Shadyside, and Squirrel Hill are in the city. The model also thought that many communities to the east and communities along the Monongahela River are in the city, specifically McKeesport and West Mifflin.\n\ntracts %&gt;% \n  left_join(full_predictions_pct) %&gt;% \n  ggplot() +\n  geom_sf(aes(fill = mean_city), lwd = 0) +\n  geom_sf(data = pgh_official_boundary, alpha = 0, color = \"black\", lwd = 2) +\n  geom_sf(data = pgh_official_boundary, alpha = 0, color = \"yellow\", lwd = .3) +\n  scale_fill_viridis_c(labels = scales::percent) +\n  labs(fill = \"Average city classification %\") +\n  theme_void()\n\n\n\n\nTo review, I think it would be difficult to create a model that almost perfectly captures the current city border, which is a result of political decisions, court cases, and other non-deterministic phenomena. In addition, white flight and the relative expansion of the suburbs during the collapse of the steel industry reshaped the Pittsburgh metro area. City borders are defined by people and politicians, not a clustering algorithm based on Census data (although that would be interesting). My experience is that many people that don’t technically live within the border consider themselves to be Pittsburghers. So what is a border, anyways?"
  },
  {
    "objectID": "posts/classifying-pittsburgh-city-boundary/index.html#introduction",
    "href": "posts/classifying-pittsburgh-city-boundary/index.html#introduction",
    "title": "Modeling the Pittsburgh City Boundary",
    "section": "",
    "text": "My goal is to create a classification model that can distinguish between census tracts that are inside the city or outside the City of Pittsburgh. The border is interrupted by rivers, has an enclave, and is very irregular in general, which made this an interesting intellectual exercise.\n\n\nWhile Pittsburgh was founded in 1758, the city’s borders have changed many times due to annexation of surrounding municipalities. This map shows that what we call the North Side was previously Allegheny City, and was annexed into the city in 1907.\n\nMt. Oliver is a geographic enclave that is completely surrounded by the City of Pittsburgh, but is a separate municipality. The borough has resisted multiple annexation attempts.\n\n\n\n\nThis code loads the packages I need, configures some options, and sets the seed.\n\n#set up environment\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(janitor)\nlibrary(tidycensus)\nlibrary(sf)\nlibrary(hrbrthemes)\nlibrary(GGally)\n\ntheme_set(theme_ipsum(base_size = 15))\n\noptions(scipen = 999, digits = 4, tigris_use_cache = TRUE)\n\nset.seed(1234)\n\nI created a small Shiny app that let me select which tracts are inside the city borders. I will go over that in a future post. This loads the tracts from the Census API and pulls the results from the Shiny app.\n\n#load data about census tracts\ntracts &lt;- get_decennial(year = 2010, state = \"PA\", county = \"Allegheny County\", \n                        variables = \"P001001\",\n                        geography = \"tract\", geometry = TRUE)\n\ncity_tracts &lt;- read_csv(\"post_data/selected_tracts.csv\", col_types = cols(\"c\", \"l\")) %&gt;% \n  filter(selected == TRUE)\n\nglimpse(city_tracts)\n\nRows: 137\nColumns: 2\n$ GEOID    &lt;chr&gt; \"42003563000\", \"42003562800\", \"42003563100\", \"42003281500\", \"…\n$ selected &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, T…\n\n\nThis reads in the boundary shapefile and graphs it to show which tracts are in the city.\n\npgh_official_boundary &lt;- st_read(\"post_data/Pittsburgh_City_Boundary-shp\") %&gt;% \n  mutate(geography = \"City boundary\") %&gt;% \n  st_transform(crs = \"NAD83\") %&gt;% \n  st_cast(\"POLYGON\") %&gt;% \n  filter(FID != 7)\n\nReading layer `City_Boundary' from data source \n  `/Users/conorotompkins/Documents/github_repos/ctompkins_quarto_blog/posts/classifying-pittsburgh-city-boundary/post_data/Pittsburgh_City_Boundary-shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 8 features and 8 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 1316000 ymin: 381900 xmax: 1380000 ymax: 433400\nProjected CRS: NAD83 / Pennsylvania South (ftUS)\n\ntracts %&gt;% \n  left_join(city_tracts) %&gt;% \n  mutate(type = case_when(selected == TRUE ~ \"city\",\n                          is.na(selected) ~ \"non_city\")) %&gt;% \n  ggplot() +\n  geom_sf(aes(fill = type), size = .1) +\n  geom_sf(data = pgh_official_boundary, color = \"white\", linetype = 2, alpha = 0) +\n  theme_void()\n\n\n\n\nThis reads in the data I will consider for the model. All the data is from the 2010 Census.\n\nall_data &lt;- read_csv(\"post_data/combined_census_data_tract.csv\", col_types = cols(.default = \"c\")) %&gt;%\n  left_join(city_tracts) %&gt;% \n  mutate(type = case_when(selected == TRUE ~ \"city\",\n                          is.na(selected) ~ \"non_city\")) %&gt;% \n  mutate(across(pct_units_owned_loan:housed_population_density_pop_per_square_km, as.numeric)) %&gt;% \n  select(GEOID, type, everything()) %&gt;%\n  select(-c(selected, total_population_housed))\n\nglimpse(all_data)\n\nRows: 402\nColumns: 13\n$ GEOID                                       &lt;chr&gt; \"42003010300\", \"4200302010…\n$ type                                        &lt;chr&gt; \"city\", \"city\", \"city\", \"c…\n$ pct_units_owned_loan                        &lt;dbl&gt; 0.137755, 0.119779, 0.0762…\n$ pct_units_owned_entire                      &lt;dbl&gt; 0.18367, 0.06619, 0.02110,…\n$ pct_units_rented                            &lt;dbl&gt; 0.6786, 0.8140, 0.9026, 0.…\n$ total_population                            &lt;dbl&gt; 6600, 3629, 616, 2256, 260…\n$ pct_white                                   &lt;dbl&gt; 0.658333, 0.745660, 0.7564…\n$ pct_black                                   &lt;dbl&gt; 0.31167, 0.15982, 0.15584,…\n$ pct_asian                                   &lt;dbl&gt; 0.0110606, 0.0471204, 0.06…\n$ pct_hispanic                                &lt;dbl&gt; 0.024394, 0.032791, 0.0162…\n$ workers                                     &lt;dbl&gt; 0, 963, 408, 1056, 537, 47…\n$ jobs                                        &lt;dbl&gt; 0, 79639, 8301, 4633, 1962…\n$ housed_population_density_pop_per_square_km &lt;dbl&gt; 682.7, 1511.3, 386.7, 3205…\n\n\nThis plot compares all of the variables against each other. I used this to identify covariance and determine which variables should be excluded.\n\n#eda\npairwise_plot &lt;- all_data %&gt;% \n  select(-c(GEOID)) %&gt;% \n  ggpairs(aes(color = type)) +\n  theme(axis.text = element_text(size = 8))\n\n After reviewing this graph and considering things like covariance and zero-variance, I will use these variables in the model:\n\nHousing\n\nPercent of housing units owned outright\nPercent of housing units owned with a mortgage\nPercent of housing units rented\n\nDemographics\n\nPercent of people in the tract that are\n\nWhite\nBlack\n\n\nPopulation density\nEconomic data\n\nNumber of workers that live in the tract\nNumber of jobs in the tract\n\n\nNote that I am intentionally excluding any geographic data about the tracts. I am more interested in how “city-like” a given tract is than how close it is to the geographic center of the city.\nThis finalizes the data I will use to build the model.\n\ncensus_combined &lt;- all_data %&gt;% \n  select(GEOID, type, \n         pct_units_owned_loan, pct_units_owned_entire, pct_units_rented,\n         housed_population_density_pop_per_square_km,\n         pct_white, pct_black,\n         workers, jobs)\n\nglimpse(census_combined)\n\nRows: 402\nColumns: 10\n$ GEOID                                       &lt;chr&gt; \"42003010300\", \"4200302010…\n$ type                                        &lt;chr&gt; \"city\", \"city\", \"city\", \"c…\n$ pct_units_owned_loan                        &lt;dbl&gt; 0.137755, 0.119779, 0.0762…\n$ pct_units_owned_entire                      &lt;dbl&gt; 0.18367, 0.06619, 0.02110,…\n$ pct_units_rented                            &lt;dbl&gt; 0.6786, 0.8140, 0.9026, 0.…\n$ housed_population_density_pop_per_square_km &lt;dbl&gt; 682.7, 1511.3, 386.7, 3205…\n$ pct_white                                   &lt;dbl&gt; 0.658333, 0.745660, 0.7564…\n$ pct_black                                   &lt;dbl&gt; 0.31167, 0.15982, 0.15584,…\n$ workers                                     &lt;dbl&gt; 0, 963, 408, 1056, 537, 47…\n$ jobs                                        &lt;dbl&gt; 0, 79639, 8301, 4633, 1962…\n\n\n34% of the tracts are in the city, which is a slightly unbalanced dataset.\n\ncensus_combined %&gt;% \n  tabyl(type)\n\n     type   n percent\n     city 137  0.3408\n non_city 265  0.6592\n\n\nSince the total amount of data available is small, I will bootstrap the data to try to achieve more stable results from the model. Bootstrapping resamples the data with replacement, which creates multiple replicates of the original dataset with some variation due to sampling. I created a meme of my dog Quincy to illustrate the effect: \nI stratify by type so that each bootstrap has ~34% city tracts. This generates 50 sets of data for the model to work with.\n\ntract_boot &lt;- bootstraps(census_combined, strata = type, times = 50)\n\ntract_boot\n\n# Bootstrap sampling using stratification \n# A tibble: 50 × 2\n   splits            id         \n   &lt;list&gt;            &lt;chr&gt;      \n 1 &lt;split [402/152]&gt; Bootstrap01\n 2 &lt;split [402/145]&gt; Bootstrap02\n 3 &lt;split [402/147]&gt; Bootstrap03\n 4 &lt;split [402/150]&gt; Bootstrap04\n 5 &lt;split [402/144]&gt; Bootstrap05\n 6 &lt;split [402/148]&gt; Bootstrap06\n 7 &lt;split [402/152]&gt; Bootstrap07\n 8 &lt;split [402/149]&gt; Bootstrap08\n 9 &lt;split [402/143]&gt; Bootstrap09\n10 &lt;split [402/156]&gt; Bootstrap10\n# ℹ 40 more rows\n\n\nThis code chunk prepares the data to be modeled. I define the formula and scale all the numeric variables to have a mean of 0 and standard deviation of 1.\n\n#recipe\nmodel_recipe &lt;- recipe(type ~ ., data = census_combined) %&gt;% \n  update_role(GEOID, new_role = \"id\") %&gt;% \n  step_normalize(all_predictors())\n\nmodel_recipe_prep &lt;- model_recipe %&gt;%\n  prep(strings_as_factors = FALSE)\n\nmodel_recipe %&gt;% \n  summary()\n\n# A tibble: 10 × 4\n   variable                                    type      role      source  \n   &lt;chr&gt;                                       &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n 1 GEOID                                       &lt;chr [3]&gt; id        original\n 2 pct_units_owned_loan                        &lt;chr [2]&gt; predictor original\n 3 pct_units_owned_entire                      &lt;chr [2]&gt; predictor original\n 4 pct_units_rented                            &lt;chr [2]&gt; predictor original\n 5 housed_population_density_pop_per_square_km &lt;chr [2]&gt; predictor original\n 6 pct_white                                   &lt;chr [2]&gt; predictor original\n 7 pct_black                                   &lt;chr [2]&gt; predictor original\n 8 workers                                     &lt;chr [2]&gt; predictor original\n 9 jobs                                        &lt;chr [2]&gt; predictor original\n10 type                                        &lt;chr [3]&gt; outcome   original\n\n\nThis creates the model specifications for the two types of models I will use.\n\n#logistic regression\nlm_model &lt;- logistic_reg(mode = \"classification\") %&gt;% \n  set_engine(\"glm\")\n\n#random forest\nranger_model &lt;- rand_forest(trees = 1000, mode = \"classification\") %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\")\n\nThis sets up a workflow object to fit a logistic regression model against the bootstrap resamples I created earlier.\n\n#logistic regression\nlm_workflow &lt;- workflow() %&gt;% \n  add_recipe(model_recipe) %&gt;% \n  add_model(lm_model)\n\nlm_res &lt;- lm_workflow %&gt;% \n  fit_resamples(resamples = tract_boot) %&gt;% \n  mutate(model = \"lm\")\n\nlm_res %&gt;% \n  collect_metrics()\n\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary     0.783    49 0.00383 Preprocessor1_Model1\n2 roc_auc  binary     0.856    49 0.00394 Preprocessor1_Model1\n\n\nThe logistic regression gets ~76% accuracy, which is pretty good, but I want to know if a random forest could do better. This creates a workflow to fit a random forest model, and saves the predictions so I can use them later.\n\n#rf\nrf_workflow &lt;- workflow() %&gt;% \n  add_recipe(model_recipe_prep) %&gt;% \n  add_model(ranger_model)\n\nrf_res &lt;- rf_workflow %&gt;% \n  fit_resamples(resamples = tract_boot,\n                control = control_resamples(save_pred = TRUE)) %&gt;% \n  mutate(model = \"rf\")\n\nrf_res %&gt;% \n  collect_metrics()\n\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary     0.816    50 0.00360 Preprocessor1_Model1\n2 roc_auc  binary     0.894    50 0.00289 Preprocessor1_Model1\n\n\nIf you compare the results of the two models, the random forest model does much better than the logistic regression model.\n\ncombined_res &lt;- bind_rows(rf_res, lm_res)\n\ncombined_res %&gt;% \n  unnest(.metrics) %&gt;% \n  ggplot(aes(.estimate, color = model, fill = model)) +\n  geom_density(alpha = .5) +\n  facet_wrap(~.metric)\n\n\n\n\nThis graph shows that the random forest model’s false negative and false positive rates are about the same.\n\nrf_res %&gt;% \n  collect_predictions() %&gt;% \n  count(type, .pred_class) %&gt;% \n  ggplot(aes(type, .pred_class, fill = n)) +\n  geom_tile() +\n  labs(x = \"Truth\",\n       y = \"Prediction\",\n       fill = \"Number of observations\") +\n  scale_fill_continuous(label = scales::comma) +\n  coord_equal() +\n  theme(panel.grid.major = element_blank())\n\n\n\n\nThis fits the random forest model against the entire dataset to extract the variable importance metrics.\n\n#variable importance\nvar_imp &lt;- rf_workflow %&gt;% \n  fit(juice(model_recipe_prep)) %&gt;% \n  pull_workflow_fit() %&gt;% \n  vip::vi()\n\nvar_imp %&gt;%\n  mutate(Variable = fct_reorder(Variable, Importance)) %&gt;% \n  ggplot(aes(Importance, Variable)) +\n  geom_point()\n\n\n\n\nThe top 3 variables are the percent of a tract’s population that is White, the percent of housing units that are owned with a loan, and the population density. This matches my subjective model of city vs. non-city characteristics. Areas that are low density with majority White demographics where people own their homes are typically outside of the city. This dynamic is probably connected to the history of segregation and redlining in majority African American communities in Pittsburgh.\nSince the random forest model was fit against multiple bootstraps, I have multiple predictions per tract. I stratified the bootstraps by type, so the city and non_city tracts were sampled about the same number of times.\n\n#extract probabilities from bootstrap resamples\nfull_predictions &lt;- rf_res %&gt;% \n  collect_predictions() %&gt;% \n  mutate(correct = type == .pred_class) %&gt;%\n  left_join(census_combined %&gt;%\n              mutate(.row = row_number()))\n\nfull_predictions %&gt;% \n  count(type, GEOID) %&gt;% \n  ggplot(aes(n, fill = type, color = type)) +\n  geom_density(alpha = .3) +\n  labs(x = \"Number of observations of a tract\")\n\n\n\n\nI am not solely interested in the top-line accuracy of the model. Since the city border is a geographic phenomenon, there may be interesting patterns in the geographic distribution of the model’s predictions that can be shown on a map.\nTo map the data, I calculate the following metrics per tract:\n\nthe percent of predictions that were correct\naverage city classification %\naverage non-city classification %\nthe number of times the tract was sampled\n\n\nfull_predictions_pct &lt;- full_predictions %&gt;% \n  group_by(GEOID) %&gt;% \n  summarize(pct_correct = mean(correct),\n            mean_city = mean(.pred_city),\n            mean_non_city = mean(.pred_non_city),\n            n = n())\n\nglimpse(full_predictions_pct)\n\nRows: 402\nColumns: 5\n$ GEOID         &lt;chr&gt; \"42003010300\", \"42003020100\", \"42003020300\", \"4200303050…\n$ pct_correct   &lt;dbl&gt; 1.00000, 1.00000, 1.00000, 1.00000, 1.00000, 1.00000, 1.…\n$ mean_city     &lt;dbl&gt; 0.7656, 0.7912, 0.8174, 0.7668, 0.7078, 0.8055, 0.9276, …\n$ mean_non_city &lt;dbl&gt; 0.23436, 0.20878, 0.18263, 0.23318, 0.29219, 0.19454, 0.…\n$ n             &lt;int&gt; 23, 13, 15, 21, 19, 17, 16, 19, 11, 20, 22, 21, 18, 22, …\n\n\nThis shows the % of correct predictions per tract. The model was very successful with the outlying tracts in the county, but struggled in Mt. Washington/Beechview/Brookline, the Hazelwood/Greenfield area, and Forest Hills towards Monroeville.\n\ntracts %&gt;% \n  left_join(full_predictions_pct) %&gt;% \n  ggplot() +\n  geom_sf(aes(fill = pct_correct), lwd = 0) +\n  geom_sf(data = pgh_official_boundary, alpha = 0, color = \"black\", lwd = 2) +\n  geom_sf(data = pgh_official_boundary, alpha = 0, color = \"yellow\", lwd = .3) +\n  scale_fill_viridis_c(labels = scales::percent) +\n  labs(fill = \"% predictions correct\") +\n  theme_void()\n\n\n\n\nThis shows the average % that the model classified a tract as being in the city. The model was very confident that Oakland, Shadyside, and Squirrel Hill are in the city. The model also thought that many communities to the east and communities along the Monongahela River are in the city, specifically McKeesport and West Mifflin.\n\ntracts %&gt;% \n  left_join(full_predictions_pct) %&gt;% \n  ggplot() +\n  geom_sf(aes(fill = mean_city), lwd = 0) +\n  geom_sf(data = pgh_official_boundary, alpha = 0, color = \"black\", lwd = 2) +\n  geom_sf(data = pgh_official_boundary, alpha = 0, color = \"yellow\", lwd = .3) +\n  scale_fill_viridis_c(labels = scales::percent) +\n  labs(fill = \"Average city classification %\") +\n  theme_void()\n\n\n\n\nTo review, I think it would be difficult to create a model that almost perfectly captures the current city border, which is a result of political decisions, court cases, and other non-deterministic phenomena. In addition, white flight and the relative expansion of the suburbs during the collapse of the steel industry reshaped the Pittsburgh metro area. City borders are defined by people and politicians, not a clustering algorithm based on Census data (although that would be interesting). My experience is that many people that don’t technically live within the border consider themselves to be Pittsburghers. So what is a border, anyways?"
  },
  {
    "objectID": "posts/classifying-pittsburgh-city-boundary/index.html#references",
    "href": "posts/classifying-pittsburgh-city-boundary/index.html#references",
    "title": "Modeling the Pittsburgh City Boundary",
    "section": "References",
    "text": "References\n\nhttps://juliasilge.com/blog/multinomial-volcano-eruptions/\nhttp://www.rebeccabarter.com/blog/2020-03-25_machine_learning/#split-into-traintest\nhttps://www.brodrigues.co/blog/2018-11-25-tidy_cv/\nhttps://agailloty.rbind.io/en/post/tidymodels/\nhttps://alison.rbind.io/post/2020-02-27-better-tidymodels/\nhttps://hansjoerg.me/2020/02/09/tidymodels-for-machine-learning/\nhttps://towardsdatascience.com/modelling-with-tidymodels-and-parsnip-bae2c01c131c\nhttps://www.benjaminsorensen.me/post/modeling-with-parsnip-and-tidymodels/\nhttps://rviews.rstudio.com/2019/06/19/a-gentle-intro-to-tidymodels/\nhttps://en.wikipedia.org/wiki/Allegheny,_Pennsylvania"
  },
  {
    "objectID": "posts/house_price_estimator/index.html",
    "href": "posts/house_price_estimator/index.html",
    "title": "House Price Estimator Dashboard",
    "section": "",
    "text": "Click here to view the full dashboard"
  },
  {
    "objectID": "posts/house_price_estimator/index.html#training-set-metrics-75-of-total-observations",
    "href": "posts/house_price_estimator/index.html#training-set-metrics-75-of-total-observations",
    "title": "House Price Estimator Dashboard",
    "section": "Training set metrics (75% of total observations)",
    "text": "Training set metrics (75% of total observations)\nI used 10-fold cross-validation to assess model performance against the training set:\n\ntrain_metrics %&gt;% \n  select(model_name, id, .metric, .estimate) %&gt;% \n  pivot_wider(names_from = .metric, values_from = .estimate) %&gt;% \n  ggplot(aes(rmse, rsq, color = model_name)) +\n  geom_point() +\n  scale_x_continuous(label = dollar) +\n  labs(x = \"Root Mean Squared Error\",\n       y = \"R^2\")"
  },
  {
    "objectID": "posts/house_price_estimator/index.html#test-set-metrics-25-of-total-observations",
    "href": "posts/house_price_estimator/index.html#test-set-metrics-25-of-total-observations",
    "title": "House Price Estimator Dashboard",
    "section": "Test set metrics (25% of total observations)",
    "text": "Test set metrics (25% of total observations)\n\ntest_metrics %&gt;% \n  select(.metric, .estimate)\n\n# A tibble: 3 × 2\n  .metric   .estimate\n  &lt;chr&gt;         &lt;dbl&gt;\n1 rmse     251406.   \n2 rsq           0.607\n3 mape    3724488.   \n\n\n\nmodel_results %&gt;% \n  ggplot(aes(.resid)) +\n  geom_density() +\n  geom_vline(xintercept = 0, lty = 2) +\n  scale_x_continuous(label = label_dollar())\n\n\n\n\n\nmodel_results %&gt;% \n  ggplot(aes(sale_price_adj, .pred_dollar)) +\n  geom_density_2d_filled(contour_var = \"count\") +\n  scale_x_log10(label = label_dollar()) +\n  scale_y_log10(label = label_dollar()) +\n  guides(fill = guide_coloursteps()) +\n  labs(x = \"Inflation-adjusted sale price log10 scale\",\n       y = \"Prediction\",\n       fill = \"Sales\")\n\n\n\n\nThe model becomes less effective as the actual sale price increases.\n\nmodel_results %&gt;% \n  ggplot(aes(sale_price_adj, .resid)) +\n  geom_point(alpha = .01) +\n  scale_x_log10(label = dollar) +\n  scale_y_continuous(label = dollar) +\n  labs(x = \"Inflation-adjusted sale price log10 scale\",\n       y = \"Residual\")\n\n\n\n\n\ngeo_ids &lt;- st_read(\"post_data/unified_geo_ids/unified_geo_ids.shp\",\n                   quiet = T)\n\ngeo_id_median_resid &lt;- model_results %&gt;% \n  group_by(geo_id) %&gt;% \n  summarize(median_resid = median(.resid))\n\npal &lt;- colorNumeric(\n  palette = \"viridis\",\n  domain = geo_id_median_resid$median_resid)\n\ngeo_ids %&gt;% \n  left_join(geo_id_median_resid) %&gt;% \n  leaflet() %&gt;% \n  addProviderTiles(providers$OpenStreetMap.Mapnik,\n                   options = providerTileOptions(noWrap = TRUE,\n                                                 minZoom = 9\n                                                 #maxZoom = 8\n                   )) %&gt;%\n  addPolygons(popup = ~ str_c(geo_id, \" \", \"median residual: \", round(median_resid, 2), sep = \"\"),\n              fillColor = ~pal(median_resid),\n              fillOpacity = .7,\n              color = \"black\",\n              weight = 3) %&gt;% \n  addLegend(\"bottomright\", pal = pal, values = ~median_resid,\n            title = \"Median of residual\",\n            opacity = 1)\n\nJoining with `by = join_by(geo_id)`\n\n\n\n\n\n\n\nmodel_results %&gt;% \n  add_count(geo_id) %&gt;% \n  mutate(geo_id = fct_reorder(geo_id, .resid, .fun = median)) %&gt;% \n  ggplot(aes(.resid, geo_id, fill = n)) +\n  geom_boxplot(color = \"grey\",\n               outlier.alpha = 0) +\n  geom_vline(xintercept = 0, lty = 2, color = \"red\") +\n  scale_fill_viridis_c() +\n  coord_cartesian(xlim = c(-10^5, 10^5)) +\n  labs(fill = \"Sales\")\n\n\n\n\n\nmodel_results %&gt;% \n  add_count(style_desc) %&gt;% \n  mutate(style_desc = fct_reorder(style_desc, .resid, .fun = median)) %&gt;% \n  ggplot(aes(.resid, style_desc, fill = n)) +\n  geom_boxplot(color = \"grey\",\n               outlier.alpha = 0) +\n  geom_vline(xintercept = 0, lty = 2, color = \"red\") +\n  coord_cartesian(xlim = c(-10.5^5, 10.5^5)) +\n  scale_x_continuous(labels = label_dollar()) +\n  scale_fill_viridis_c() +\n  labs(fill = \"Sales\",\n       x = \"Residual\",\n       y = \"House style\")\n\n\n\n\n\nmodel_results %&gt;% \n  add_count(grade_desc) %&gt;% \n  mutate(grade_desc = fct_reorder(grade_desc, .resid, .fun = median)) %&gt;% \n  ggplot(aes(.resid, grade_desc, fill = n)) +\n  geom_boxplot(color = \"grey\",\n               outlier.alpha = 0) +\n  scale_fill_viridis_c() +\n  scale_x_continuous(labels = label_dollar()) +\n  coord_cartesian(xlim = c(-10^5, 10.5^6)) +\n  labs(x = \"Residual\",\n       y = \"Grade\",\n       fill = \"Sales\")\n\n\n\n\n\nmodel_results %&gt;% \n  add_count(condition_desc) %&gt;% \n  mutate(condition_desc = fct_explicit_na(condition_desc),\n         condition_desc = fct_reorder(condition_desc, .resid, .fun = median)) %&gt;% \n  ggplot(aes(.resid, condition_desc, fill = n)) +\n  geom_boxplot(color = \"grey\",\n               outlier.alpha = 0) +\n  geom_vline(xintercept = 0, lty = 2, color = \"red\") +\n  scale_fill_viridis_c() +\n  scale_x_continuous(labels = label_dollar()) +\n  coord_cartesian(xlim = c(-10^5, 10.5^5)) +\n  labs(x = \"Residual\",\n       y = \"Condition\",\n       fill = \"Sales\")\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `condition_desc = fct_explicit_na(condition_desc)`.\nCaused by warning:\n! `fct_explicit_na()` was deprecated in forcats 1.0.0.\nℹ Please use `fct_na_value_to_level()` instead.\n\n\n\n\n\n\nmodel_results %&gt;% \n  ggplot(aes(finished_living_area, .resid)) +\n  geom_point(alpha = .1) +\n  scale_x_log10() +\n  scale_y_continuous(label = dollar) +\n  labs(x = \"Finished Living Area sq. ft. log10 scale\",\n       y = \"Residual\")\n\n\n\n\n\nmodel_results %&gt;% \n  ggplot(aes(lot_area, .resid)) +\n  geom_point(alpha = .1) +\n  scale_x_log10(labels = label_comma()) +\n  scale_y_continuous(labels = label_dollar()) +\n  labs(x = \"Lot Area sq. ft. log10 scale\",\n       y = \"Residual\")\n\nWarning: Transformation introduced infinite values in continuous x-axis\n\n\n\n\n\n\nmodel_results %&gt;% \n  group_by(house_age_at_sale) %&gt;% \n  rmse(truth = sale_price_adj, estimate = .pred_dollar) %&gt;% \n  ggplot(aes(house_age_at_sale, .estimate)) +\n  geom_point(alpha = .5) +\n  scale_y_continuous(labels = label_dollar()) +\n  labs(x = \"House age at sale\",\n       y = \"RMSE\")\n\n\n\n\nThe model is best at predicting the sale price of houses built in the 1940s to 1980s. This is when most of the houses in the county were built.\n\nmodel_results %&gt;% \n  group_by(year_built) %&gt;% \n  rmse(truth = sale_price_adj, estimate = .pred_dollar) %&gt;% \n  ggplot(aes(year_built, .estimate)) +\n  geom_point(alpha = .5) +\n  scale_y_continuous(labels = label_dollar()) +\n  labs(x = \"Year Built\",\n       y = \"RMSE\")\n\n\n\n\n\nmodel_results %&gt;% \n  add_count(bedrooms) %&gt;% \n  ggplot(aes(.resid, bedrooms, group = bedrooms, fill = n)) +\n  geom_boxplot(color = \"grey\",\n               outlier.alpha = 0) +\n  geom_vline(xintercept = 0, lty = 2, color = \"red\") +\n  scale_y_continuous(breaks = c(0:15)) +\n  scale_fill_viridis_c() +\n  scale_x_continuous(labels = label_dollar()) +\n  coord_cartesian(xlim = c(-10^5, 10^5)) +\n  labs(x = \"Residual\",\n       y = \"Bedrooms\",\n       fill = \"Sales\")\n\nWarning: Removed 2 rows containing missing values (`stat_boxplot()`).\n\n\n\n\n\n\nmodel_results %&gt;% \n  add_count(full_baths) %&gt;% \n  ggplot(aes(.resid, full_baths, group = full_baths, fill = n)) +\n  geom_boxplot(color = \"grey\",\n               outlier.alpha = 0) +\n  geom_vline(xintercept = 0, lty = 2, color = \"red\") +\n  scale_y_continuous(breaks = c(0:12)) +\n  scale_fill_viridis_c() +\n  scale_x_continuous(label = dollar) +\n  coord_cartesian(xlim = c(-10^5, 750000)) +\n  labs(x = \"Residual\",\n       y = \"Full bathrooms\",\n       fill = \"Sales\")\n\nWarning: Removed 14 rows containing missing values (`stat_boxplot()`).\n\n\n\n\n\n\nmodel_results %&gt;% \n  add_count(half_baths) %&gt;% \n  ggplot(aes(.resid, half_baths, group = half_baths, fill = n)) +\n  geom_boxplot(color = \"grey\",\n               outlier.alpha = 0) +\n  geom_vline(xintercept = 0, lty = 2, color = \"red\") +\n  scale_y_continuous(breaks = c(0:8)) +\n  scale_x_continuous(labels = label_dollar()) +\n  scale_fill_viridis_c() +\n  coord_cartesian(xlim = c(-10^5, 10^5)) +\n  labs(x = \"Residual\",\n       y = \"Half bathrooms\",\n       fill = \"Sales\")\n\nWarning: Removed 785 rows containing missing values (`stat_boxplot()`).\n\n\n\n\n\n\nmodel_results %&gt;% \n  group_by(sale_year) %&gt;% \n  rmse(truth = sale_price_adj, estimate = .pred_dollar) %&gt;% \n  ggplot(aes(sale_year, .estimate)) +\n  geom_line() +\n  scale_y_continuous(label = dollar) +\n  labs(x = \"Sale year\",\n       y = \"RMSE\")"
  },
  {
    "objectID": "posts/ac_driving_commuter_routes/index.html",
    "href": "posts/ac_driving_commuter_routes/index.html",
    "title": "Analyzing major commuter routes in Allegheny County",
    "section": "",
    "text": "In this post I will use the Mapbox API to calculate metrics for major commuter routes in Allegheny County. The API will provide the distance and duration of the trip, as well as turn-by-turn directions. The route duration should be considered a “minimum duration” because it does not consider traffic. Then I will estimate the duration of the trips with a linear model and compare that to the actual duration from the Mapbox API. I will use the difference between the actual and estimated duration to identify neighborhoods that experience longer or shorter commutes than expected.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(mapboxapi)\nlibrary(tidycensus)\nlibrary(janitor)\nlibrary(lehdr)\nlibrary(tigris)\nlibrary(sf)\nlibrary(hrbrthemes)\n\noptions(tigris_use_cache = TRUE,\n        scipen = 999,\n        digits = 4)\n\ntheme_set(theme_ipsum())\n\nsf_use_s2(FALSE)"
  },
  {
    "objectID": "posts/ac_driving_commuter_routes/index.html#intro",
    "href": "posts/ac_driving_commuter_routes/index.html#intro",
    "title": "Analyzing major commuter routes in Allegheny County",
    "section": "",
    "text": "In this post I will use the Mapbox API to calculate metrics for major commuter routes in Allegheny County. The API will provide the distance and duration of the trip, as well as turn-by-turn directions. The route duration should be considered a “minimum duration” because it does not consider traffic. Then I will estimate the duration of the trips with a linear model and compare that to the actual duration from the Mapbox API. I will use the difference between the actual and estimated duration to identify neighborhoods that experience longer or shorter commutes than expected.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(mapboxapi)\nlibrary(tidycensus)\nlibrary(janitor)\nlibrary(lehdr)\nlibrary(tigris)\nlibrary(sf)\nlibrary(hrbrthemes)\n\noptions(tigris_use_cache = TRUE,\n        scipen = 999,\n        digits = 4)\n\ntheme_set(theme_ipsum())\n\nsf_use_s2(FALSE)"
  },
  {
    "objectID": "posts/ac_driving_commuter_routes/index.html#gather-data",
    "href": "posts/ac_driving_commuter_routes/index.html#gather-data",
    "title": "Analyzing major commuter routes in Allegheny County",
    "section": "Gather data",
    "text": "Gather data\nThe first step is to download the census tract shapefiles for the county:\n\n#get tracts\nallegheny_county_tracts &lt;- tracts(state = \"PA\", county = \"Allegheny\", cb = TRUE) %&gt;% \n  select(GEOID)\n\nst_erase &lt;- function(x, y) {\n  st_difference(x, st_union(y))\n}\n\nac_water &lt;- area_water(\"PA\", \"Allegheny\", class = \"sf\")\n\nallegheny_county_tracts &lt;- st_erase(allegheny_county_tracts, ac_water)\n\nThen I download the “Origin-Destination” LODES file from the Census for Pennsylvania in 2017 and subset it to commuters within Allegheny County.\n\n#load od tract-level data\nlodes_od_ac_main &lt;- grab_lodes(state = \"pa\", year = 2017, \n                               lodes_type = \"od\", job_type = \"JT00\", \n                               segment = \"S000\", state_part = \"main\", \n                               agg_geo = \"tract\", use_cache = TRUE) %&gt;%\n  select(state, w_tract, h_tract, S000, year) %&gt;% \n  rename(commuters = S000) %&gt;% \n  mutate(intra_tract_commute = h_tract == w_tract) %&gt;% \n  semi_join(allegheny_county_tracts, by = c(\"w_tract\" = \"GEOID\")) %&gt;% \n  semi_join(allegheny_county_tracts, by = c(\"h_tract\" = \"GEOID\"))\n\nThis analysis only considers routes where the commuter changed census tracts. 96% of commuters in Allegheny County change census tracts.\n\nlodes_od_ac_main %&gt;% \n  group_by(intra_tract_commute) %&gt;% \n  summarize(commuters = sum(commuters)) %&gt;% \n  ungroup() %&gt;% \n  mutate(pct = commuters / sum(commuters))\n\n# A tibble: 2 × 3\n  intra_tract_commute commuters    pct\n  &lt;lgl&gt;                   &lt;dbl&gt;  &lt;dbl&gt;\n1 FALSE                  465637 0.963 \n2 TRUE                    18127 0.0375"
  },
  {
    "objectID": "posts/ac_driving_commuter_routes/index.html#get-directions",
    "href": "posts/ac_driving_commuter_routes/index.html#get-directions",
    "title": "Analyzing major commuter routes in Allegheny County",
    "section": "Get directions",
    "text": "Get directions\nThis is the code that identifies the center of each tract, geocodes those centroids to get an address, and gets the turn-by-turn directions and route data for each pair of home and work addresses. I will focus on the top 20% of these routes (in terms of cumulative percent of commuters) because the Mapbox API is not designed for the size of query I would need to get directions for all combinations of census tracts.\nNote that I manually replaced the geocoded address for the Wexford and Swissvale areas because the geocoder returned results outside of the county, probably because the center of those tracts intersect with highways.\n\n#filter out rows where commuter doesn't change tracts\ncombined_tract_sf &lt;- lodes_od_ac_main %&gt;%\n  arrange(desc(commuters)) %&gt;% \n  filter(w_tract != h_tract)\n\n#calculate cumulative pct of commuters, keep only top 20%\ncombined_tract_sf_small &lt;- combined_tract_sf %&gt;% \n  select(h_tract, w_tract, commuters) %&gt;% \n  arrange(desc(commuters)) %&gt;% \n  mutate(id = row_number(),\n         pct_commuters = commuters / sum(commuters),\n         cumulative_pct_commuters = cumsum(pct_commuters)) %&gt;%\n  filter(cumulative_pct_commuters &lt; .2) %&gt;%\n  select(h_tract, w_tract, commuters)\n\n#add census centroid geometry\ncombined_tract_sf_small &lt;- combined_tract_sf_small %&gt;% \n  left_join(st_centroid(allegheny_county_tracts), by = c(\"h_tract\" = \"GEOID\")) %&gt;% \n  rename(h_tract_geo = geometry) %&gt;% \n  left_join(st_centroid(allegheny_county_tracts), by = c(\"w_tract\" = \"GEOID\")) %&gt;% \n  rename(w_tract_geo = geometry) %&gt;% \n  select(h_tract, h_tract_geo, w_tract, w_tract_geo, commuters)\n\ncombined_tract_sf_small |&gt; \n  st_sf() |&gt; \n  ggplot() +\n  geom_sf(data = allegheny_county_tracts) +\n  geom_sf()\n\n#get addresses for tract centroids\ntract_od_directions &lt;- combined_tract_sf_small %&gt;%\n  mutate(home_address = map_chr(h_tract_geo, mb_reverse_geocode),\n         work_address = map_chr(w_tract_geo, mb_reverse_geocode))\n\n#replace bad address with good address\nwexford_good_address &lt;- \"3321 Wexford Rd, Gibsonia, PA 15044\"\nswissvale_good_address &lt;- \"1118 S Braddock Ave, Swissvale, PA 15218\"\n\ntract_od_directions &lt;- tract_od_directions %&gt;% \n  #fix wexford address\n  mutate(home_address = case_when(h_tract == \"42003409000\" ~ wexford_good_address,\n                                  h_tract != \"42003409000\" ~ home_address),\n         work_address = case_when(w_tract == \"42003409000\" ~ wexford_good_address,\n                                  w_tract != \"42003409000\" ~ work_address)) |&gt; \n  #fix swissvale address\n  mutate(home_address = case_when(h_tract == \"42003515401\" ~ swissvale_good_address,\n                                  TRUE ~ home_address))\n\n#define error-safe mb_directions function\nmb_directions_possibly &lt;- possibly(mb_directions, otherwise = NA)\n\n#geocode addresses, get directions\ntract_od_directions &lt;- tract_od_directions %&gt;% \n  mutate(home_address_location_geocoded = map(home_address, mb_geocode),\n         work_address_location_geocoded = map(work_address, mb_geocode)) %&gt;% \n  mutate(directions = map2(home_address, work_address, ~ mb_directions_possibly(origin = .x,\n                                                                       destination = .y,\n                                                                       steps = TRUE,\n                                                                       profile = \"driving\"))) %&gt;% \n  select(h_tract, h_tract_geo, home_address, home_address_location_geocoded,\n         w_tract, w_tract_geo, work_address, work_address_location_geocoded,\n         directions, commuters)\n\nThe core of the above code is combining map2 and mb_directions_possibly. This maps the mb_directions_possibly function against two inputs (the home address and work address).\nThe result is a dataframe with a row per turn-by-turn direction for each commuter route.\nThis summarizes the data so there is one row per commuter route and creates summarized route data.\n\n#summarize direction data\ntract_od_stats &lt;- tract_od_directions %&gt;% \n  group_by(h_tract, home_address, w_tract, work_address) %&gt;%\n  summarize(duration = sum(duration),\n            distance = sum(distance),\n            steps = n(),\n            commuters = unique(commuters)) %&gt;% \n  ungroup()\n\nAs expected, route duration and distance are highly correlated. The median duration of a trip is 16.7 minutes.\n\n#graph od stats\ntract_od_stats %&gt;% \n  ggplot(aes(distance, duration, size = commuters)) +\n  geom_point(alpha = .3) +\n  geom_abline(linetype = 2, color = \"red\") +\n  coord_equal() +\n  theme_ipsum() +\n  labs(title = \"Commutes between census tracts\",\n       subtitle = \"Allegheny County, PA\",\n       x = \"Distance in KM\",\n       y = \"Duration in minutes\",\n       size = \"Commuters\")\n\n\n\n\n\nmedian_duration &lt;- tract_od_stats %&gt;% \n  uncount(weights = commuters) %&gt;% \n  summarize(median_duration = median(duration)) %&gt;% \n  pull(median_duration)\n\ntract_od_stats %&gt;% \n  uncount(weights = commuters) %&gt;% \n  ggplot(aes(duration)) +\n  geom_density(fill = \"grey\") +\n  geom_vline(xintercept = median_duration, lty = 2, color = \"red\") +\n  annotate(\"text\", x = 21, y = .05, label = \"median\", color = \"red\") +\n  theme_ipsum() +\n  labs(title = \"Trip duration\",\n       x = \"Duration in minutes\",\n       y = \"Density of observations\")\n\n\n\n\nThis map shows the main roads that commuter routes use I-376, I-279, and Route 28 are major arteries, as expected.\n\n#map routes\ntract_od_stats %&gt;% \n  ggplot() +\n  geom_sf(data = allegheny_county_tracts, linewidth = .1, fill = \"black\") +\n  geom_sf(aes(alpha = commuters, linewidth = commuters), color = \"#ffcc01\", alpha = .1) +\n  guides(linewidth = guide_legend(override.aes= list(alpha = 1))) +\n  scale_linewidth_continuous(range = c(.1, 5)) +\n  theme_void() +\n  labs(title = \"Commuter routes between Allegheny County census tracts\",\n       subtitle = \"Driving routes\",\n       linewidth = \"Commuters\")\n\n\n\n\nA high-resolution image of this map is available here. An animation of the routes is here.\nPeople that live closer to downtown Pittsburgh have shorter commutes, on average.\n\nallegheny_county_tracts %&gt;% \n  st_drop_geometry() %&gt;% \n  left_join(tract_od_stats %&gt;% \n              st_drop_geometry() |&gt; \n              select(h_tract, w_tract, duration) %&gt;% \n              pivot_longer(contains(\"tract\")) %&gt;% \n              group_by(name, value) %&gt;% \n              summarize(avg_duration = mean(duration)) %&gt;% \n              ungroup(),\n            by = c(\"GEOID\" = \"value\")) %&gt;% \n  complete(GEOID, name) %&gt;% \n  filter(!is.na(name)) %&gt;% \n  left_join(allegheny_county_tracts, by = \"GEOID\") %&gt;%\n  mutate(name = case_when(name == \"h_tract\" ~ \"Origin tract\",\n                          name == \"w_tract\" ~ \"Destination tract\"),\n         name = as.factor(name) %&gt;% fct_rev()) %&gt;% \n  st_sf() %&gt;% \n  ggplot() +\n  geom_sf(aes(fill = avg_duration), size = .1) +\n  facet_wrap(~name, ncol = 1) +\n  scale_fill_viridis_c(na.value = \"grey90\") +\n  labs(title = \"Average commute duration\",\n       fill = \"Minutes\") +\n  theme_void()"
  },
  {
    "objectID": "posts/ac_driving_commuter_routes/index.html#model",
    "href": "posts/ac_driving_commuter_routes/index.html#model",
    "title": "Analyzing major commuter routes in Allegheny County",
    "section": "Model",
    "text": "Model\nThe next step is to create a model that estimates the duration of a given commute. I will use the number of steps in the turn-by-turn directions and the distance as predictors. Additionally, I will calculate which rivers a commute route crosses and use those as logical variables in the model.\nThis collects the geometry for the main rivers in the county.\n\nmain_rivers &lt;- ac_water %&gt;% \n  group_by(FULLNAME) %&gt;% \n  summarize(AWATER = sum(AWATER)) %&gt;% \n  arrange(desc(AWATER)) %&gt;% \n  slice(1:4)\n\nThis code calculates whether a given commuter route crosses a river.\n\ntract_od_stats_rivers &lt;- tract_od_stats %&gt;% \n  mutate(intersects_ohio = st_intersects(., main_rivers %&gt;% \n                                           filter(FULLNAME == \"Ohio Riv\")) %&gt;% as.logical(),\n         intersects_allegheny = st_intersects(., main_rivers %&gt;% \n                                                filter(FULLNAME == \"Allegheny Riv\")) %&gt;% as.logical(),\n         intersects_monongahela = st_intersects(., main_rivers %&gt;% \n                                                  filter(FULLNAME == \"Monongahela Riv\")) %&gt;% as.logical(),\n         intersects_youghiogheny = st_intersects(., main_rivers %&gt;% \n                                                   filter(FULLNAME == \"Youghiogheny Riv\")) %&gt;% as.logical()) %&gt;% \n  replace_na(list(intersects_ohio = FALSE,\n                  intersects_allegheny = FALSE,\n                  intersects_monongahela = FALSE,\n                  intersects_youghiogheny = FALSE)) %&gt;% \n  st_drop_geometry()\n\nglimpse(tract_od_stats_rivers)\n\nRows: 780\nColumns: 12\n$ h_tract                 &lt;chr&gt; \"42003020100\", \"42003020300\", \"42003030500\", \"…\n$ home_address            &lt;chr&gt; \"445 Wood Street, Pittsburgh, Pennsylvania 152…\n$ w_tract                 &lt;chr&gt; \"42003982200\", \"42003020100\", \"42003020100\", \"…\n$ work_address            &lt;chr&gt; \"4215 Fifth Avenue, Pittsburgh, Pennsylvania 1…\n$ duration                &lt;dbl&gt; 14.087, 9.225, 7.460, 10.951, 2.510, 10.517, 1…\n$ distance                &lt;dbl&gt; 5.6640, 2.6518, 1.5922, 3.4968, 0.7169, 4.2762…\n$ steps                   &lt;int&gt; 8, 7, 7, 5, 6, 6, 9, 7, 10, 11, 14, 10, 10, 8,…\n$ commuters               &lt;dbl&gt; 58, 106, 287, 68, 81, 129, 87, 121, 116, 59, 1…\n$ intersects_ohio         &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS…\n$ intersects_allegheny    &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS…\n$ intersects_monongahela  &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS…\n$ intersects_youghiogheny &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS…\n\ntract_od_stats_rivers &lt;- tract_od_stats_rivers %&gt;% \n  mutate(od_id = str_c(\"h_tract: \", h_tract, \", \", \"w_tract: \", w_tract, sep = \"\"))\n\nFirst I set the seed and split the data into training and testing sets.\n\nset.seed(1234)\n\n#split data\nsplits &lt;- initial_split(tract_od_stats_rivers, prop = .75)\n\ntraining_data &lt;- training(splits)\ntesting_data &lt;- testing(splits)\n\nThen I use {tidymodels} to define a linear model, cross-validate it, and extract the coefficients.\n\n#recipe\nmodel_recipe &lt;- recipe(duration ~ ., \n                       data = training_data) %&gt;% \n  update_role(od_id, new_role = \"id\") %&gt;%\n  step_rm(h_tract, home_address, w_tract, work_address, commuters) %&gt;% \n  step_normalize(distance, steps) %&gt;% \n  step_zv(all_predictors())\n\nmodel_recipe %&gt;% \n  prep() %&gt;% \n  summary()\n\n# A tibble: 8 × 4\n  variable                type      role      source  \n  &lt;chr&gt;                   &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n1 distance                &lt;chr [2]&gt; predictor original\n2 steps                   &lt;chr [2]&gt; predictor original\n3 intersects_ohio         &lt;chr [1]&gt; predictor original\n4 intersects_allegheny    &lt;chr [1]&gt; predictor original\n5 intersects_monongahela  &lt;chr [1]&gt; predictor original\n6 intersects_youghiogheny &lt;chr [1]&gt; predictor original\n7 od_id                   &lt;chr [3]&gt; id        original\n8 duration                &lt;chr [2]&gt; outcome   original\n\nmodel_recipe_prep &lt;- model_recipe %&gt;% \n  prep()\n\n\n#apply cv to training data\ntraining_vfold &lt;- vfold_cv(training_data, v = 10, repeats = 2)\n\n\n#model specification\nlm_model &lt;- linear_reg(mode = \"regression\") %&gt;% \n  set_engine(\"lm\")\n\n#linear regression workflow\nlm_workflow &lt;- workflow() %&gt;% \n  add_recipe(model_recipe) %&gt;% \n  add_model(lm_model)\n\n#fit against training resamples\nkeep_pred &lt;- control_resamples(save_pred = TRUE)\n\nlm_training_fit &lt;- lm_workflow %&gt;% \n  fit_resamples(training_vfold, control = keep_pred) %&gt;% \n  mutate(model = \"lm\")\n\n#get results from training cv\nlm_training_fit %&gt;% \n  collect_metrics()\n\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   2.88     20 0.0590  Preprocessor1_Model1\n2 rsq     standard   0.870    20 0.00571 Preprocessor1_Model1\n\n\nThe model averaged an R-squared of .82 on the training data, which is pretty good.\nThe predictions from the training set fit the actual duration pretty well.\n\n#graph predictions from assessment sets\nlm_training_fit %&gt;% \n  collect_predictions() %&gt;% \n  ggplot(aes(duration, .pred)) +\n  geom_point(alpha = .3) +\n  geom_abline(linetype = 2, color = \"red\") +\n  coord_equal() +\n  labs(x = \"Actual duration\",\n       y = \"Predicted duration\")\n\n\n\n\nNext I fit the model against the test data to extract the coefficients. Holding the other variables constant, distance is by far the most influential variable in the model. For every kilometer increase in distance, the duration of the commute can be expected to increase by around 5 minutes. Crossing the Monongahela will add around 2 minutes to a commute, while crossing the Allegheny and Ohio actually decrease commute times. This is probably related to the bridge that the commuter uses.\n\n#variable importance\nlm_workflow %&gt;% \n  fit(testing_data) %&gt;% \n  pull_workflow_fit() %&gt;% \n  tidy() %&gt;% \n  filter(term != \"(Intercept)\") %&gt;% \n  mutate(term = fct_reorder(term, estimate)) %&gt;% \n  ggplot(aes(estimate, term)) +\n  geom_col(fill = \"grey\", color = \"black\")\n\n\n\n\nThis fits the model to the full dataset and plots the predicted duration against the actual duration. The fit is tighter than just plotting distance vs. duration.\n\n#final model\ntract_od_pred &lt;- lm_workflow %&gt;% \n  fit(testing_data) %&gt;% \n  predict(tract_od_stats_rivers) %&gt;% \n  bind_cols(tract_od_stats_rivers) %&gt;% \n  select(h_tract, w_tract, distance, steps, duration, .pred, commuters)\n\ntract_od_pred %&gt;% \n  ggplot(aes(duration, .pred, size = commuters)) +\n  geom_point(alpha = .3) +\n  geom_abline(lty = 2, color = \"red\") +\n  coord_equal() +\n  labs(x = \"Duration in minutes\",\n       y = \"Predicted duration\",\n       size = \"Number of commuters\")\n\n\n\n\nThis calculates how far off the model’s estimation of duration was for each census tract in the dataset (origin and destination). Commuters originating from neighborhoods between State Route 51 and the Monongahela River experience longer than expected commutes.\n\nallegheny_county_tracts %&gt;% \n  st_drop_geometry() %&gt;% \n  left_join(tract_od_pred %&gt;% \n              mutate(.resid = duration - .pred) %&gt;% \n              select(h_tract, w_tract, .resid) %&gt;% \n              pivot_longer(contains(\"tract\")) %&gt;% \n              group_by(name, value) %&gt;% \n              summarize(avg_resid = mean(.resid)) %&gt;% \n              ungroup(),\n            by = c(\"GEOID\" = \"value\")) %&gt;% \n  complete(GEOID, name) %&gt;% \n  filter(!is.na(name)) %&gt;% \n  left_join(allegheny_county_tracts) %&gt;%\n  mutate(name = case_when(name == \"h_tract\" ~ \"Origin tract\",\n                          name == \"w_tract\" ~ \"Destination tract\"),\n         name = as.factor(name) %&gt;% fct_rev()) %&gt;% \n  st_sf() %&gt;% \n  ggplot() +\n  geom_sf(aes(fill = avg_resid), size = .1) +\n  facet_wrap(~name, ncol = 1) +\n  scale_fill_viridis_c(na.value = \"grey90\") +\n  labs(title = \"Commute duration above/below expected\",\n       fill = \"Minutes\") +\n  theme_void()"
  },
  {
    "objectID": "posts/healthy_ride_access_pittsburgh/index.html",
    "href": "posts/healthy_ride_access_pittsburgh/index.html",
    "title": "Bike rental access in Pittsburgh",
    "section": "",
    "text": "This is an interactive Leaflet map of Healthy Ride access in Pittsburgh. It counts how many Healthy Ride stations are within a 10 minute bike ride of a given location. This gives an estimation of how accessible the Healthy Ride service is in a given neighborhood (lighter green = more accessible). As you zoom in, individual bike stations will appear. Click the “full screen” button on the left to maximize your view.\nThere are some obvious cases (like the Wabash Tunnel) where the API doesn’t know that a bicyclist shouldn’t go in there, but overall it is accurate.\nThis map was built with the Mapbox API and was inspired by the Penn MUSA Masterclass 2020 talk that Kyle Walker gave."
  },
  {
    "objectID": "posts/healthy_ride_access_pittsburgh/index.html#build-one-isochrone",
    "href": "posts/healthy_ride_access_pittsburgh/index.html#build-one-isochrone",
    "title": "Bike rental access in Pittsburgh",
    "section": "Build one isochrone",
    "text": "Build one isochrone\nThis takes the first station in the dataframe and uses the Mapbox API to make a test isochrone that shows how far a bicyclist can go in a given period of time (5, 10, 15 minutes).\n\ntest_isochrone_data &lt;- stations %&gt;% \n  slice(1) %&gt;% \n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326) %&gt;% \n  mb_isochrone(profile = \"cycling\", time = c(5, 10, 15))\n\ntest_isochrone_map &lt;- mapbox_map %&gt;% \n  mapdeck() %&gt;% \n  add_polygon(data = test_isochrone_data,\n              fill_colour = \"time\",\n              fill_opacity = 0.5,\n              legend = TRUE) %&gt;% \n  add_scatterplot(data = stations %&gt;% \n                          slice(1) %&gt;% \n                          st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326),\n                  radius = 100,\n                  fill_colour = \"#ffffff\") %&gt;% \n  mapdeck_view(location = c(pgh_coords[1], pgh_coords[2]), zoom = 11)\n\ntest_isochrone_map\n\n\n\n\n\nThe same graph can be made in ggplot2:\n\ntest_isochrone_data %&gt;% \n  ggplot() +\n  geom_sf(aes(fill = time)) +\n  scale_fill_viridis_c() +\n  theme_void()\n\n\n\n\nThis calculates the isochrones for all the stations and transforms them into a projected coordinate system:\n\nstation_isochrone &lt;- stations %&gt;%\n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326) %&gt;%\n  mb_isochrone(profile = \"cycling\", time = c(10)) %&gt;%\n  st_transform(3857)\n\nThis shows the overlap between all the isochrones. Interesting to look at, but not very informative.\n\nstation_isochrone %&gt;% \n  ggplot() +\n  geom_sf(fill = \"black\", lwd = 0, alpha = .05) +\n  theme_void()"
  },
  {
    "objectID": "posts/healthy_ride_access_pittsburgh/index.html#build-raster",
    "href": "posts/healthy_ride_access_pittsburgh/index.html#build-raster",
    "title": "Bike rental access in Pittsburgh",
    "section": "Build Raster",
    "text": "Build Raster\nThis builds a raster object that calculates how many isochrones overlap on a given area:\n\n#raster\npolygons_proj &lt;- station_isochrone %&gt;%\n  mutate(test_id = 1) %&gt;% \n  filter(time == 10) %&gt;% \n  st_transform(3857)\n\ntemplate &lt;- raster(polygons_proj, resolution = 25)\n\nraster_surface &lt;- fasterize(polygons_proj, template, field = \"test_id\", fun = \"sum\")\n\nraster_values &lt;- tibble(values = values(raster_surface)) %&gt;% \n  filter(!is.na(values)) %&gt;% \n  distinct(values) %&gt;% \n  pull(values)\n\nplot(raster_surface)"
  },
  {
    "objectID": "posts/healthy_ride_access_pittsburgh/index.html#build-interactive-map",
    "href": "posts/healthy_ride_access_pittsburgh/index.html#build-interactive-map",
    "title": "Bike rental access in Pittsburgh",
    "section": "Build interactive Map",
    "text": "Build interactive Map\nThis builds out the interactive map using leaflet and Mapbox libraries:\n\ncustom_pal &lt;- colorNumeric(\"viridis\", \n                           #0:max_bike_stations, \n                           raster_values,\n                           na.color = \"transparent\")\n\npopup_labels &lt;- sprintf(\"%s \n                        &lt;br&gt;Number of bike racks: %s\",\n                        stations$station_name, stations$number_of_racks) %&gt;% \n  map(htmltools::HTML)\n\nhealth_ride_icon &lt;- makeIcon(\n  iconUrl = \"https://healthyridepgh.com/wp-content/uploads/sites/3/2019/05/NEXTBIKE-LOGO-01.png\",\n  #iconUrl = \"https://healthyridepgh.com/wp-content/uploads/sites/3/2016/09/Healthy-Ride-Logo.Stacked-01.png\",\n  iconWidth = 50, iconHeight = 50,\n  iconAnchorX = 0, iconAnchorY = 0\n)\n\nstation_heatmap &lt;- mapbox_map %&gt;%\n  addPolygons(data = city_boundary,\n              opacity = 1,\n              color = \"black\",\n              fillColor = \"#ffffff\",\n              group = \"City boundary\") %&gt;% \n  addRasterImage(raster_surface, colors = custom_pal, opacity = .75,\n                 group = \"Raster\") %&gt;% \n  addLegend(pal = custom_pal, \n            values = raster_values,\n            title = \"Number of stations&lt;br&gt;within 10-minute bike ride\") %&gt;% \n  addMarkers(data = stations, lng = ~longitude, lat = ~latitude,\n             popup = popup_labels,\n             icon = health_ride_icon,\n             clusterOptions = markerClusterOptions(),\n             group = \"Stations\") %&gt;% \n  addLayersControl(overlayGroups = c(\"City boundary\", \"Raster\", \"Stations\"),\n                   options = layersControlOptions(collapsed = FALSE)) %&gt;% \n  addFullscreenControl() %&gt;% \n  setView(lng = pgh_coords[1], lat = pgh_coords[2], zoom = 12)\n\nframeWidget(station_heatmap, options=frameOptions(allowfullscreen = TRUE))"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/mapping-boswash-commuter-patterns/index.html",
    "href": "posts/mapping-boswash-commuter-patterns/index.html",
    "title": "Mapping BosWash commuter patterns with Flowmap.blue",
    "section": "",
    "text": "This map shows the commuter patterns in the Northeast Megalopolis/Acela Corridor/BosWash metro area. I pulled the data from the Census Longitudinal Employer-Household Dynamics (LODES) system via the {lehdr} package. The map was created through the Flowmap.blue tool, which makes interactive maps of movement between areas. Flowmap.blue also exposes a bunch of cool features, like animating and clustering connections, among others.\nYou can view a full version of the map here."
  },
  {
    "objectID": "posts/mapping-boswash-commuter-patterns/index.html#code",
    "href": "posts/mapping-boswash-commuter-patterns/index.html#code",
    "title": "Mapping BosWash commuter patterns with Flowmap.blue",
    "section": "Code",
    "text": "Code\nThis code is what I used to query the LODES data and aggregate it. First, load the required libraries.\n\nlibrary(tidyverse)\nlibrary(tidycensus)\nlibrary(janitor)\nlibrary(lehdr)\nlibrary(tigris)\nlibrary(sf)\nlibrary(ggraph)\nlibrary(tidygraph)\nlibrary(googlesheets4)\nlibrary(googledrive)\n\noptions(tigris_use_cache = TRUE,\n        scipen = 999,\n        digits = 4)\n\nThis code gets the main and aux LODES data for each state that I name in the states object. I then combine the data into lodes_combined and check that there are no duplicate origin-destination pairs. Be warned that these files are large (100-500MB each), and can take a bit to read into R, depending on your machine.\n\n#get lodes\nstates &lt;- c(\"pa\", \"wv\", \"va\", \"dc\", \"de\",\n            \"md\", \"ny\", \"ri\", \"ct\", \"ma\", \"vt\", \"nh\", \"me\")\n\nlodes_od_main &lt;- grab_lodes(state = states, year = 2017, \n                            lodes_type = \"od\", job_type = \"JT00\", \n                            segment = \"S000\", state_part = \"main\", \n                            agg_geo = \"county\",\n                            use_cache = TRUE) %&gt;% \n  select(state, w_county, h_county, S000, year) %&gt;% \n  rename(commuters = S000)\n\nlodes_od_aux &lt;- grab_lodes(state = states, year = 2017, \n                           lodes_type = \"od\", job_type = \"JT00\", \n                           segment = \"S000\", state_part = \"aux\", \n                           agg_geo = \"county\",\n                           use_cache = TRUE) %&gt;% \n  select(state, w_county, h_county, S000, year) %&gt;% \n  rename(commuters = S000)\n\nlodes_combined &lt;- bind_rows(lodes_od_main, lodes_od_aux)\n\nThis code pulls the geometry for the states from the TIGER shapefile API:\n\ncounties_combined &lt;- tigris::counties(state = c(\"PA\", \"NY\", \"NJ\", \"MD\", \n                                                \"WV\", \"DE\", \"VA\", \n                                                \"DC\", \"MA\", \"CT\", \"VT\", \n                                                \"RI\", \"NH\", \"ME\"), \n                                      cb = TRUE) %&gt;% \n  arrange(STATEFP) %&gt;% \n  left_join(fips_codes %&gt;% distinct(state_code, state_name), by = c(\"STATEFP\" = \"state_code\"))\n\ncounties_combined %&gt;% \n  ggplot() +\n  geom_sf(aes(fill = state_name))\n\n\n\n\nThe next step is to calculate the centroid of each county that will be used in the final map.\n\nnode_pos &lt;- counties_combined %&gt;% \n  mutate(centroid = map(geometry, st_centroid),\n         x = map_dbl(centroid, 1),\n         y = map_dbl(centroid, 2)) %&gt;% \n  select(GEOID, NAME, x, y) %&gt;% \n  arrange(GEOID) %&gt;% \n  st_drop_geometry() %&gt;% \n  as_tibble() %&gt;% \n  select(-NAME) %&gt;%\n  rename(lon = x,\n         lat = y) %&gt;% \n  mutate(id = row_number()) %&gt;% \n  select(id, GEOID, lat, lon)\n\nThen I add the county and state name to the node positions so the name is intelligible.\n\nnode_pos &lt;- node_pos %&gt;% \n  left_join(st_drop_geometry(counties_combined), by = c(\"GEOID\" = \"GEOID\")) %&gt;% \n  mutate(county_name = str_c(NAME, \"County\", sep = \" \"),\n         name = str_c(county_name, state_name, sep = \", \"))\n\nnode_pos &lt;- node_pos %&gt;% \n  select(id, name, lat, lon, GEOID)\n\nnode_pos\n\n\n\nRows: 433\nColumns: 5\n$ id    &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 1…\n$ name  &lt;chr&gt; \"Fairfield County, Connecticut\", \"Hartford County, Connecticut\",…\n$ lat   &lt;dbl&gt; 41.27, 41.81, 41.79, 41.46, 41.41, 41.49, 41.86, 41.83, 39.09, 3…\n$ lon   &lt;dbl&gt; -73.39, -72.73, -73.25, -72.54, -72.93, -72.10, -72.34, -71.99, …\n$ GEOID &lt;chr&gt; \"09001\", \"09003\", \"09005\", \"09007\", \"09009\", \"09011\", \"09013\", \"…\n\n\nThis processes the LODES origin-destination data and creates the node-edge network graph object that will be fed into the Flowmap.blue service.\n\nnetwork_graph &lt;- lodes_combined %&gt;%\n  semi_join(counties_combined, by = c(\"w_county\" = \"GEOID\")) %&gt;%\n  semi_join(counties_combined, by = c(\"h_county\" = \"GEOID\")) %&gt;%\n  select(h_county, w_county, commuters) %&gt;% \n  as_tbl_graph(directed = TRUE) %&gt;% \n  activate(edges) %&gt;% \n  filter(commuters &gt;= 500,\n         #!edge_is_loop()\n  ) %&gt;%\n  activate(nodes) %&gt;%\n  arrange(name)\n\n\nnodes &lt;- network_graph %&gt;%\n  activate(nodes) %&gt;%\n  as_tibble()\n\nglimpse(nodes)\n\n\n\nRows: 433\nColumns: 1\n$ name &lt;chr&gt; \"09001\", \"09003\", \"09005\", \"09007\", \"09009\", \"09011\", \"09013\", \"0…\n\n\n\nedges &lt;- network_graph %&gt;% \n  activate(edges) %&gt;% \n  as_tibble() %&gt;% \n  rename(origin = from,\n         dest = to,\n         count = commuters) %&gt;% \n  arrange(desc(count))\n\nglimpse(edges)\n\n\n\nRows: 3,309\nColumns: 3\n$ origin &lt;dbl&gt; 128, 161, 121, 149, 61, 121, 138, 210, 112, 2, 127, 125, 138, 1…\n$ dest   &lt;dbl&gt; 128, 161, 128, 149, 61, 121, 128, 210, 112, 2, 127, 125, 138, 1…\n$ count  &lt;dbl&gt; 558308, 483764, 475073, 468885, 450975, 398535, 384941, 371075,…\n\n\nFinally, this code checks that the node position data matches up with the nodes from the network object. If these checks fail, the origin-destination pairs will be mapped to the wrong geographic coordinates.\n\n#check that nodes match up\nall(node_pos$GEOID == nodes$name)\n\n[1] TRUE\n\nidentical(node_pos$GEOID, nodes$name)\n\n[1] TRUE\n\nlength(node_pos$GEOID) == length(nodes$name)\n\n[1] TRUE\n\n\nThis code creates the metadata that Flowmap.blue requires and loads the data into Google Sheets.\n\nmy_properties &lt;- c(\n  \"title\"=\"BosWash regional US commuter flow\",\n  \"description\"=\"Miniumum 500 commuters per origin-destination pair\",\n  \"source.name\"=\"2017 US Census LODES\",\n  \"source.url\"=\"https://lehd.ces.census.gov/data/\",\n  \"createdBy.name\"=\"Conor Tompkins\",\n  \"createdBy.url\"=\"https://ctompkins.netlify.app/\",\n  \"mapbox.mapStyle\"=NA,\n  \"flows.sheets\" = \"flows\",\n  \"colors.scheme\"=\"interpolateViridis\",\n  \"colors.darkMode\"=\"yes\",\n  \"animate.flows\"=\"no\",\n  \"clustering\"=\"yes\"\n)\n\nproperties &lt;- tibble(property=names(my_properties)) %&gt;%\n  mutate(value=my_properties[property])\n\ngs4_auth()\n\ngoogledrive::drive_auth()\n\ndrive_trash(\"lodes_flowmapblue\")\n\nss &lt;- gs4_create(\"lodes_flowmapblue\", sheets = list(properties = properties,\n                                                    locations = node_pos,\n                                                    flows = edges))\n\nThe final step is to allow the Google Sheet to be read by anyone with the link, and copy the Sheet’s link to Flowmap.blue"
  },
  {
    "objectID": "posts/mapping-boswash-commuter-patterns/index.html#references",
    "href": "posts/mapping-boswash-commuter-patterns/index.html#references",
    "title": "Mapping BosWash commuter patterns with Flowmap.blue",
    "section": "References",
    "text": "References\n\nhttps://doodles.mountainmath.ca/blog/2020/01/06/flow-maps/\nhttps://jamgreen.github.io/lehdr/articles/getting_started.html"
  },
  {
    "objectID": "posts/riverhounds_lilley/index.html",
    "href": "posts/riverhounds_lilley/index.html",
    "title": "Pittsburgh Riverhounds under Coach Lilley",
    "section": "",
    "text": "I have been a season-ticket holder with the Pittsburgh Riverhounds for a couple seasons now. The stadium has a great fan experience, and the team has gotten a lot better over the past few years. A major part of that is the head coach, Bob Lilley. I will use some data from American Soccer Analysis to show how the Riverhounds have improved. Their website has an explainer on expected goals and other metrics they calculate.\nLoad libraries and configure settings:\n\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(hrbrthemes)\nlibrary(ggrepel)\n\ntheme_set(theme_ipsum(base_size = 18))\n\n#source https://app.americansocceranalysis.com/#!/\n\nI pulled a CSV of team-level goal metrics for the last 4 USL seasons from the ASA website. This shows the available data:\n\nusl &lt;- read_csv(\"post_data/american_soccer_analysis_uslc_xgoals_teams_2023-10-15.csv\") %&gt;% \n  clean_names() %&gt;% \n  select(-x1) %&gt;% \n  mutate(coach = case_when(team == \"PIT\" & season &gt;= 2018 ~ \"Lilley\",\n                           team == \"PIT\" & season &lt; 2018 ~ \"Brandt\",\n                           TRUE ~ NA_character_)) |&gt; \n  filter(season &lt; 2021)\n\nglimpse(usl)\n\nRows: 134\nColumns: 15\n$ team    &lt;chr&gt; \"PHX\", \"CIN\", \"RNO\", \"LOU\", \"HFD\", \"PIT\", \"SLC\", \"SA\", \"TBR\", …\n$ season  &lt;dbl&gt; 2019, 2018, 2020, 2020, 2020, 2020, 2017, 2020, 2020, 2020, 20…\n$ games   &lt;dbl&gt; 34, 34, 16, 16, 16, 16, 32, 16, 16, 16, 34, 15, 16, 34, 34, 34…\n$ sht_f   &lt;dbl&gt; 16.79, 12.68, 16.06, 15.06, 10.31, 10.81, 11.88, 14.81, 12.63,…\n$ sht_a   &lt;dbl&gt; 13.32, 15.15, 14.94, 9.56, 12.19, 7.81, 11.41, 12.44, 9.38, 13…\n$ gf      &lt;dbl&gt; 2.53, 2.06, 2.69, 1.75, 1.88, 2.38, 1.84, 1.88, 1.56, 2.75, 1.…\n$ ga      &lt;dbl&gt; 1.00, 0.97, 1.31, 0.75, 1.44, 0.63, 0.91, 0.75, 0.63, 1.19, 0.…\n$ gd      &lt;dbl&gt; 1.53, 1.09, 1.38, 1.00, 0.44, 1.75, 0.94, 1.13, 0.94, 1.56, 0.…\n$ x_gf    &lt;dbl&gt; 2.08, 1.43, 2.25, 1.48, 1.32, 1.69, 1.46, 1.58, 1.63, 2.39, 1.…\n$ x_ga    &lt;dbl&gt; 1.37, 1.26, 1.53, 1.01, 1.35, 0.94, 1.34, 1.17, 0.84, 1.30, 0.…\n$ x_gd    &lt;dbl&gt; 0.71, 0.17, 0.72, 0.47, -0.03, 0.75, 0.12, 0.42, 0.79, 1.10, 0…\n$ gd_x_gd &lt;dbl&gt; 0.82, 0.92, 0.65, 0.53, 0.46, 1.00, 0.81, 0.71, 0.15, 0.47, 0.…\n$ pts     &lt;dbl&gt; 2.29, 2.26, 2.25, 2.19, 2.19, 2.13, 2.09, 2.06, 2.06, 2.00, 2.…\n$ x_pts   &lt;dbl&gt; 1.80, 1.49, 1.83, 1.70, 1.39, 1.86, 1.44, 1.64, 1.85, 1.98, 1.…\n$ coach   &lt;chr&gt; NA, NA, NA, NA, NA, \"Lilley\", NA, NA, NA, NA, \"Lilley\", NA, NA…\n\n\nThe Riverhound’s statistics show clear improvement in 2018 when Lilley took over from Brandt. The team immediately began scoring more than they allowed. The team’s expected goals for and against also improved, which shows that the improvement wasn’t a matter of luck.\n\ngoal_data &lt;- usl %&gt;% \n  filter(team == \"PIT\") %&gt;% \n  select(team, season, gf, x_gf, ga, x_ga) %&gt;% \n  pivot_longer(cols = c(gf, x_gf, ga, x_ga), names_to = \"g_type\", values_to = \"g_value\") %&gt;%\n  mutate(goal_type = case_when(str_detect(g_type, \"gf$\") ~ \"For\",\n                               TRUE ~ \"Against\")) %&gt;% \n  mutate(metric_type = case_when(str_detect(g_type, \"^x_\") ~ \"Expected\",\n                                 TRUE ~ \"Actual\"))\n\ngoal_data %&gt;% \n  ggplot(aes(season, g_value, color = goal_type, lty = metric_type)) +\n  geom_line(size = 1.5) +\n  geom_point(data = filter(goal_data, metric_type == \"Actual\"), size = 2) +\n  labs(title = \"Pittsburgh Riverhounds\",\n       subtitle = \"Expected and Actual Goals per game\",\n       x = \"Season\",\n       y = \"Goals\",\n       color = \"Goal Type\",\n       lty = \"Metric Type\")\n\n\n\n\nThis shows that in terms of expected goal difference, the Riverhounds became one of the top teams in the USL once Lilley took over.\n\nusl %&gt;% \n  ggplot(aes(season, x_gd, group = team)) +\n  geom_hline(yintercept = 0, size = 1, lty = 2) +\n  geom_line(color = \"black\", alpha = .2) +\n  geom_line(data = filter(usl, team == \"PIT\"), \n            color = \"gold\", size = 2) +\n  geom_point(data = filter(usl, team == \"PIT\"),\n             aes(fill = coach),\n             shape = 21, size = 4) +\n  scale_fill_manual(values = c(\"grey\", \"gold\")) +\n  #coord_fixed(ratio = .5) +\n  labs(title = \"xG difference per game\",\n       x = \"Season\",\n       y = \"xG Difference\",\n       fill = \"Riverhounds Coach\",\n       caption = \"Grey lines show other USL teams\")\n\n\n\n\nLilley’s Riverhounds are consistently better than league average in terms of expected goals.\n\nusl %&gt;% \n  ggplot(aes(x_gd)) +\n  #geom_histogram(binwidth = .2) +\n  geom_vline(data = filter(usl, team == \"PIT\"), aes(xintercept = x_gd), size = 3) +\n  geom_vline(data = filter(usl, team == \"PIT\"), aes(xintercept = x_gd, color = coach),\n             size = 2.5, key_glyph = \"rect\") +\n  geom_density(aes(y = ..count.. * .2), fill = \"white\", alpha = 1) +\n  geom_vline(xintercept = 0, lty = 2) +\n  geom_hline(yintercept = 0) +\n  scale_color_manual(values = c(\"grey\", \"gold\")) +\n  scale_x_continuous(expand = c(0,0)) +\n  scale_y_continuous(expand = c(0,0)) +\n  coord_cartesian(ylim = c(0, 25)) +\n  #coord_fixed(ratio = .1) +\n  labs(title = \"xG Difference Per Game\",\n       subtitle = \"Distribution of all USL teams 2017-2020\",\n       x = \"xG\",\n       y = \"Number of teams\",\n       color = \"Riverhounds Coach\") +\n  theme(legend.key = element_rect(color = \"black\"))\n\n\n\n\nWhile the 2020 Riverhounds were a very good team, they were not quite as good as their plain goals for/against would show. This graph shows that they were fortunate to do as well as they did (which, again, was very well).\n\nusl %&gt;% \n  mutate(logo = case_when(team == \"PIT\" ~ \"post_data/pit_logo.png\",\n                          TRUE ~ NA_character_)) %&gt;% \n  ggplot(aes(x_gd, gd)) +\n  geom_abline(lty = 2) +\n  geom_point(alpha = .3) +\n  ggimage::geom_image(aes(image = logo)) +\n  geom_label_repel(data = filter(usl, team == \"PIT\"),\n                   aes(label = season, fill = coach),\n                   force = 5,\n                   key_glyph = \"rect\") +\n  annotate(\"text\", label = \"Under-performing\",\n           x = .75, y = -1.5) +\n  annotate(\"text\", label = \"Over-performing\",\n           x = -1, y = 1.5) +\n  tune::coord_obs_pred() +\n  scale_fill_manual(values = c(\"grey\", \"gold\")) +\n  labs(title = \"Goal and xG difference per game\",\n       x = \"xG Difference\",\n       y = \"Goal Difference\",\n       fill = \"Riverhounds Coach\") +\n  theme(legend.key = element_rect(color = \"black\"))\n\n\n\n\nThis shows that the 2020 Riverhounds were probably one of the most fortunate teams in the league, in addition to being very good.\n\nusl %&gt;% \n  ggplot(aes(season, gd_x_gd, group = team)) +\n  geom_hline(yintercept = 0, lty = 2) +\n  geom_line(color = \"black\", alpha = .2) +\n  geom_line(data = filter(usl, team == \"PIT\"),\n            color = \"gold\", size = 2) +\n  geom_point(data = filter(usl, team == \"PIT\"),\n             aes(fill = coach, group = team),\n             shape = 21, size = 4, color = \"black\") +\n  scale_fill_manual(values = c(\"grey\", \"gold\")) +\n  coord_cartesian(ylim = c(-1.5, 1.5)) +\n  #coord_fixed(ratio = .5) +\n  labs(title = \"Goal difference - xG difference\",\n       subtitle = \"Per game\",\n       x = \"Season\",\n       y = substitute(paste(\"\" %&lt;-% \"\", \"Under-performing\", \"  |  \", \"Over-performing\", \"\" %-&gt;% \"\")),\n       fill = \"Riverhounds Coach\",\n       caption = \"Grey lines show other USL teams\")\n\n\n\n\nIn FiveThirtyEights’ Global Soccer Power Index, the Riverhounds will begin the 2021 season ranked around #460 out of 639 teams."
  },
  {
    "objectID": "posts/effect-of-geographic-resolution-on-ebirdst-abundance/index.html",
    "href": "posts/effect-of-geographic-resolution-on-ebirdst-abundance/index.html",
    "title": "Effect of Geographic Resolution on ebirdst Abundance",
    "section": "",
    "text": "While exploring some of the citizen science bird observation data available through ebirdst, I was confused by how to understand the calculation of ebirdst’s abundance metric.\nThe ebirdst documentation (?ebirdst::load_raster) defines abundance as:\nI had seen some weird results when trying to manually calculate abundance as occurrence * count. My initial attempt had aggregated the results by month.\nThe underlying problem is that abundance and count are the results of models, and are subject to model error. I also believe that the data outputted from load_raster lacks the necessary significant digits to accurately recreate abundance. Lowering the resolution or aggregating the data will exacerbate this issue.\nThis code loads my convenience function to retrieve a metric for a species at a given geographic resolution. This gets occurrence, count, and abundance for the Northern Cardinal at high (3 km), medium (9 km), and low resolutions (27 km). The function also crops the underlying raster data to Pennsylvania.\nlibrary(here)\nlibrary(hrbrthemes)\nlibrary(patchwork)\n\nsource(\"https://raw.githubusercontent.com/conorotompkins/ebird_shiny_app/main/scripts/functions/get_species_metric.R\")\n\ntheme_set(theme_ipsum())\n\nspecies_table &lt;- crossing(location = \"Pennsylvania\",\n                          species = c(\"Northern Cardinal\"),\n                          metric = c(\"occurrence\", \"count\", \"abundance\"),\n                          resolution = c(\"hr\", \"mr\", \"lr\"))\nspecies_table\n\n# A tibble: 9 × 4\n  location     species           metric     resolution\n  &lt;chr&gt;        &lt;chr&gt;             &lt;chr&gt;      &lt;chr&gt;     \n1 Pennsylvania Northern Cardinal abundance  hr        \n2 Pennsylvania Northern Cardinal abundance  lr        \n3 Pennsylvania Northern Cardinal abundance  mr        \n4 Pennsylvania Northern Cardinal count      hr        \n5 Pennsylvania Northern Cardinal count      lr        \n6 Pennsylvania Northern Cardinal count      mr        \n7 Pennsylvania Northern Cardinal occurrence hr        \n8 Pennsylvania Northern Cardinal occurrence lr        \n9 Pennsylvania Northern Cardinal occurrence mr\nspecies_metrics &lt;- species_table %&gt;% \n  mutate(data = pmap(list(location, species, metric, resolution), ~get_species_metric(..1, ..2, ..2, ..3, ..4))) %&gt;% \n  mutate(resolution = fct_relevel(resolution, c(\"hr\", \"mr\", \"lr\"))) %&gt;% \n  arrange(species, metric, resolution) |&gt; \n  unnest(data) %&gt;% \n  unnest(data)\nspecies_metrics\n\n# A tibble: 1,243,320 × 13\n   location species metric resolution family_common_name common_name metric_desc\n   &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;              &lt;chr&gt;       &lt;chr&gt;      \n 1 Pennsyl… Northe… abund… hr         Northern Cardinal  Northern C… abundance  \n 2 Pennsyl… Northe… abund… hr         Northern Cardinal  Northern C… abundance  \n 3 Pennsyl… Northe… abund… hr         Northern Cardinal  Northern C… abundance  \n 4 Pennsyl… Northe… abund… hr         Northern Cardinal  Northern C… abundance  \n 5 Pennsyl… Northe… abund… hr         Northern Cardinal  Northern C… abundance  \n 6 Pennsyl… Northe… abund… hr         Northern Cardinal  Northern C… abundance  \n 7 Pennsyl… Northe… abund… hr         Northern Cardinal  Northern C… abundance  \n 8 Pennsyl… Northe… abund… hr         Northern Cardinal  Northern C… abundance  \n 9 Pennsyl… Northe… abund… hr         Northern Cardinal  Northern C… abundance  \n10 Pennsyl… Northe… abund… hr         Northern Cardinal  Northern C… abundance  \n# ℹ 1,243,310 more rows\n# ℹ 6 more variables: date &lt;date&gt;, value &lt;dbl&gt;, month &lt;chr&gt;, region &lt;chr&gt;,\n#   x &lt;dbl&gt;, y &lt;dbl&gt;\nThis unnests the data and recalculates abundance (abundance_test) and the difference between actual abundance and abundance_test.\nspecies_table_unnested &lt;- species_metrics %&gt;%\n  select(species, resolution, date, month, x, y, metric_desc, value) %&gt;% \n  pivot_wider(id_cols = c(species, resolution, date, month, x, y),\n              names_from = metric_desc,\n              values_from = value) %&gt;% \n  select(species, resolution, date, month, x, y, count, occurrence, abundance) %&gt;% \n  mutate(abundance_test = count * occurrence,\n         diff = abundance - abundance_test)\nGrouping by month to get to the county level changes the grain of the data so much that abundance_test undershoots abundance by 20%. This occurs at all resolutions.\nspecies_metrics %&gt;%\n  select(species, resolution, date, month, x, y, metric_desc, value) %&gt;% \n  pivot_wider(id_cols = c(species, resolution, date, month, x, y),\n              names_from = metric_desc,\n              values_from = value) %&gt;% \n  select(species, resolution, date, month, x, y, count, occurrence, abundance) %&gt;% \n  group_by(species, month, resolution) %&gt;% \n  summarize(occurrence = mean(occurrence, na.rm = T),\n            count = mean(count, na.rm = T),\n            abundance = mean(abundance, na.rm = T)) %&gt;% \n  ungroup() %&gt;% \n  mutate(abundance_test = count * occurrence,\n         diff = abundance - abundance_test) %&gt;% \n  ggplot(aes(abundance, abundance_test)) +\n  geom_abline() +\n  geom_point() +\n  facet_wrap(~resolution) +\n  tune::coord_obs_pred()\n\n`summarise()` has grouped output by 'species', 'month'. You can override using\nthe `.groups` argument.\nTotally un-aggregated, abundance_test closely resembles abundance, but degrades as resolution decreases.\nspecies_table_unnested %&gt;% \n  select(abundance, abundance_test, resolution) %&gt;% \n  drop_na() %&gt;% \n  ggplot(aes(abundance, abundance_test)) +\n  geom_density_2d_filled(contour_var = \"ndensity\") +\n  geom_abline(color = \"white\") +\n  facet_wrap(~resolution) +\n  tune::coord_obs_pred() +\n  coord_cartesian(xlim = c(0, 4),\n                  ylim = c(0, 4)) +\n  guides(fill = guide_colorsteps())\n\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\nAt lower resolutions, the difference is positively skewed, which means that abundance is higher than abundance_test.\nspecies_table_unnested %&gt;% \n  drop_na(diff) %&gt;% \n  ggplot(aes(diff)) +\n  geom_histogram() +\n  facet_wrap(~resolution, scale = \"free_y\", ncol = 1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\nAt the highest resolution, diff is heteroskedastic. At lower resolutions, there are patterns to the error.\nspecies_table_unnested %&gt;% \n  drop_na(occurrence, diff) %&gt;% \n  ggplot(aes(occurrence, diff)) +\n  geom_density_2d_filled(contour_var = \"ndensity\") +\n  facet_wrap(~resolution) + \n  scale_x_percent() +\n  guides(fill = guide_colorsteps())\nThis was a useful exercise for me to understand how the geographic resolution and other aggregation of the data can affect estimated metrics, specifically in the citizen science context."
  },
  {
    "objectID": "posts/effect-of-geographic-resolution-on-ebirdst-abundance/index.html#update",
    "href": "posts/effect-of-geographic-resolution-on-ebirdst-abundance/index.html#update",
    "title": "Effect of Geographic Resolution on ebirdst Abundance",
    "section": "Update",
    "text": "Update\nI made an issue on the ebirdst Github page and talked to one of the maintainers about their definitions of count and abundance. I now have a much stronger understanding of these variables.\nThe following code reproduces the graph I attached to the issue:\n\nlibrary(hrbrthemes)\ntheme_set(theme_ipsum())\n\nnorcar_table &lt;- crossing(location = \"Pennsylvania\",\n                         species = c(\"Northern Cardinal\"),\n                         metric = c(\"occurrence\", \"count\", \"abundance\"),\n                         resolution = c(\"hr\"))\n\n\nnorcar_metrics &lt;- norcar_table %&gt;% \n  mutate(data = pmap(list(location, species, metric, resolution), ~get_species_metric(..1, ..2, ..2, ..3, ..4))) %&gt;% \n  mutate(resolution = fct_relevel(resolution, c(\"hr\", \"mr\", \"lr\"))) %&gt;% \n  arrange(species, metric, resolution) %&gt;%\n  unnest(data) |&gt; \n  unnest(data)\n\n\nnorcar_metrics_wide &lt;- norcar_metrics %&gt;% \n  select(species, date, x, y, metric_desc, value) %&gt;% \n  pivot_wider(names_from = metric_desc,\n              values_from = value)\n\nplot_1 &lt;- norcar_metrics_wide %&gt;% \n  drop_na(occurrence, count) %&gt;% \n  ggplot(aes(occurrence, count)) +\n  geom_density_2d_filled(contour_var = \"ndensity\") +\n  scale_x_percent() +\n  guides(fill = \"none\") +\n  theme_bw()\n\nplot_2 &lt;- norcar_metrics_wide %&gt;% \n  drop_na() %&gt;% \n  ggplot(aes(occurrence, abundance)) +\n  geom_density_2d_filled(contour_var = \"ndensity\") +\n  scale_x_percent() +\n  guides(fill = \"none\") +\n  theme_bw()\n\nplot_3 &lt;- norcar_metrics_wide %&gt;%\n  drop_na() %&gt;% \n  ggplot(aes(count, abundance)) +\n  geom_density_2d_filled(contour_var = \"ndensity\") +\n  geom_abline() +\n  guides(fill = \"none\") +\n  theme_bw()\n\nlayout &lt;- \"\nAACC\nBBCC\n\"\n\nplot_1 + plot_2 + plot_3 + \n  plot_layout(guides = 'collect', design = layout) +\n  plot_annotation(title = \"Northern Cardinal in Pennsylvania\")"
  },
  {
    "objectID": "posts/effect-of-geographic-resolution-on-ebirdst-abundance/index.html#citations",
    "href": "posts/effect-of-geographic-resolution-on-ebirdst-abundance/index.html#citations",
    "title": "Effect of Geographic Resolution on ebirdst Abundance",
    "section": "Citations",
    "text": "Citations\nFink, D., T. Auer, A. Johnston, M. Strimas-Mackey, O. Robinson, S. Ligocki, W. Hochachka, C. Wood, I. Davies, M. Iliff, L. Seitz. 2020. eBird Status and Trends, Data Version: 2019; Released: 2020 Cornell Lab of Ornithology, Ithaca, New York. https://doi.org/10.2173/ebirdst.2019"
  },
  {
    "objectID": "posts/bivariate_transit_map/index.html",
    "href": "posts/bivariate_transit_map/index.html",
    "title": "Driving Alone vs. Public Transportation in Pittsburgh",
    "section": "",
    "text": "Intro\nThe clash between public transportation and single passenger vehicles is a heated topic of discussion nationally and in the Pittsburgh area. Public transit ridership has been heavily reduced by COVID-19 in many countries. These two commuting modes compete for the same riders, and investment dollars, and space. Car drivers are frustrated when a bus stops during rush hour to pick up passengers, while bus passengers are frustrated sitting in traffic caused by single passenger vehicles because transit doesn’t have right-of-way.\nFrom my point of view, Pittsburgh’s geography lends itself to a focus on public transit, at the expense of the single passenger vehicle. Most of the jobs in the county are in a single census tract Downtown, which is reflected in the spoke (and no wheel) design of the transit system. Downtown is surrounded by rivers and mountains, which drastically narrows the geography suited to infrastructure. You pretty much have to use a tunnel or bridge to commute Downtown, unless you are coming from directly east. It would make sense to give public transit priority access to those tunnels and bridges, since their throughput is many times higher than roads designated for single passenger vehicles.\n\n\n\n\n\nThe historical priority towards single passenger vehicles is reflected in the Census statistics about commuting modes in the area. Most people in the area commute to work by themselves in cars. In the Wexford-area census tract, 78% (5,141) of commuters drive to work alone (and sit in traffic on the parkway together). Public transit use is limited to areas where the government invested in transit, but even there transit is not typically the majority mode.\nIn this post I will use {tidycensus} to pull data about how many people commute by driving alone or taking public transit Allegheny County. I chose these two modes because they are the two most popular modes in the county, and are the most different in terms of style. I then graph the data with {ggplot2} and {biscale}. I hack the {biscale} legend a bit to get it to show the % of commuters, which may be of interest to other R users.\n\n\nCode and graphs\nLoad libraries and set up the environment:\n\nlibrary(tidyverse)\nlibrary(tidycensus)\nlibrary(sf)\nlibrary(tigris)\nlibrary(janitor)\nlibrary(biscale)\nlibrary(patchwork)\nlibrary(hrbrthemes)\nlibrary(kableExtra)\n\noptions(scipen = 999, digits = 4)\n\ntheme_set(theme_ipsum(base_size = 25))\n\nThese are the variables about commuter mode that the Census has for the 2019 American Community Survey (ACS):\n\nacs1_vars &lt;- load_variables(2019, 'acs1') %&gt;% \n  mutate(across(c(label, concept), str_to_lower))\n\nacs1_vars %&gt;%\n  filter(str_detect(name, \"^B08301_\")) %&gt;% \n  kbl() %&gt;% \n  scroll_box(height = \"400px\") %&gt;% \n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"),\n                position = \"left\")\n\n\n\n\n\n\nname\nlabel\nconcept\n\n\n\n\nB08301_001\nestimate!!total:\nmeans of transportation to work\n\n\nB08301_002\nestimate!!total:!!car, truck, or van:\nmeans of transportation to work\n\n\nB08301_003\nestimate!!total:!!car, truck, or van:!!drove alone\nmeans of transportation to work\n\n\nB08301_004\nestimate!!total:!!car, truck, or van:!!carpooled:\nmeans of transportation to work\n\n\nB08301_005\nestimate!!total:!!car, truck, or van:!!carpooled:!!in 2-person carpool\nmeans of transportation to work\n\n\nB08301_006\nestimate!!total:!!car, truck, or van:!!carpooled:!!in 3-person carpool\nmeans of transportation to work\n\n\nB08301_007\nestimate!!total:!!car, truck, or van:!!carpooled:!!in 4-person carpool\nmeans of transportation to work\n\n\nB08301_008\nestimate!!total:!!car, truck, or van:!!carpooled:!!in 5- or 6-person carpool\nmeans of transportation to work\n\n\nB08301_009\nestimate!!total:!!car, truck, or van:!!carpooled:!!in 7-or-more-person carpool\nmeans of transportation to work\n\n\nB08301_010\nestimate!!total:!!public transportation (excluding taxicab):\nmeans of transportation to work\n\n\nB08301_011\nestimate!!total:!!public transportation (excluding taxicab):!!bus\nmeans of transportation to work\n\n\nB08301_012\nestimate!!total:!!public transportation (excluding taxicab):!!subway or elevated rail\nmeans of transportation to work\n\n\nB08301_013\nestimate!!total:!!public transportation (excluding taxicab):!!long-distance train or commuter rail\nmeans of transportation to work\n\n\nB08301_014\nestimate!!total:!!public transportation (excluding taxicab):!!light rail, streetcar or trolley (carro público in puerto rico)\nmeans of transportation to work\n\n\nB08301_015\nestimate!!total:!!public transportation (excluding taxicab):!!ferryboat\nmeans of transportation to work\n\n\nB08301_016\nestimate!!total:!!taxicab\nmeans of transportation to work\n\n\nB08301_017\nestimate!!total:!!motorcycle\nmeans of transportation to work\n\n\nB08301_018\nestimate!!total:!!bicycle\nmeans of transportation to work\n\n\nB08301_019\nestimate!!total:!!walked\nmeans of transportation to work\n\n\nB08301_020\nestimate!!total:!!other means\nmeans of transportation to work\n\n\nB08301_021\nestimate!!total:!!worked from home\nmeans of transportation to work\n\n\n\n\n\n\n\nDriving alone in a single-passenger vehicle is by far the dominant commuting mode in the county.\n\nall_transit_vars &lt;- c(\"B08301_003\", \n                      \"B08301_004\", \n                      \"B08301_010\", \n                      \"B08301_016\", \n                      \"B08301_017\", \n                      \"B08301_018\", \n                      \"B08301_019\", \n                      \"B08301_020\",\n                      \"B08301_021\")\n\nall_transit_modes &lt;- get_acs(geography = \"county\", \n                             variables = acs1_vars %&gt;%\n                               filter(name %in% all_transit_vars) %&gt;% \n                               pull(name, label),\n                             summary_var = \"B08301_001\",\n                             year = 2019, state = \"PA\", county = \"Allegheny\",\n                             geometry = F)\n\nall_transit_modes %&gt;% \n  mutate(variable = str_remove(variable, \"^estimate!!total:\"),\n         variable = str_remove(variable, \"\\\\(excluding taxicab\\\\)\"),\n         variable = str_remove_all(variable, \"\\\\!\"),\n         variable = str_remove(variable, \":$\"),\n         variable = str_replace(variable, \":\", \" : \"),\n         variable = str_trim(variable),\n         variable = str_to_title(variable)) %&gt;% \n  group_by(variable) %&gt;% \n  summarize(estimate = sum(estimate),) %&gt;% \n  mutate(variable = fct_reorder(variable, estimate),\n         pct = estimate / sum(estimate)) %&gt;% \n  ggplot(aes(estimate, variable)) +\n  geom_col() +\n  geom_text(aes(x = estimate + 26000, label = scales::percent(pct, 1)),\n            size = 4) +\n  labs(title = \"Allegheny County Commuter Modes\",\n       subtitle = \"2019 American Community Survey\",\n       x = \"Commuters\",\n       y = NULL) +\n  scale_x_comma(limits = c(0, 500000),\n                labels = c(\"0\", \"1k\", \"2k\", \"3k\", \"4k\", \"5k\")) +\n  theme_ipsum(axis_text_size = 15)\n\n\n\n\nI will use these two variables to directly compare the use of single-passenger vehicles and public transit in the county.\n\nvars &lt;- c(\"Drove alone\" = \"B08301_003\",\n          \"Public transportation\" = \"B08301_010\")\n\nacs1_vars %&gt;%\n  filter(name %in% vars) %&gt;% \n  pull(label)\n\n[1] \"estimate!!total:!!car, truck, or van:!!drove alone\"          \n[2] \"estimate!!total:!!public transportation (excluding taxicab):\"\n\n\nThis downloads the commuter mode data and subtracts the rivers from the census tract polygons so it looks nice on a map:\n\ntract_transit %&gt;% \n  glimpse()\n\nRows: 804\nColumns: 5\n$ GEOID       &lt;chr&gt; \"42003408002\", \"42003408002\", \"42003210700\", \"42003210700\"…\n$ variable    &lt;chr&gt; \"Drove alone\", \"Public transportation\", \"Drove alone\", \"Pu…\n$ estimate    &lt;dbl&gt; 2815, 8, 589, 189, 566, 146, 1224, 260, 466, 100, 1063, 21…\n$ summary_est &lt;dbl&gt; 3165, 3165, 1231, 1231, 1110, 1110, 1992, 1992, 702, 702, …\n$ geometry    &lt;POLYGON [°]&gt; POLYGON ((-79.99 40.61, -79..., POLYGON ((-79.99 4…\n\n\nAs discussed earlier, public transit is not the majority commuting mode in most areas:\n\ntract_transit %&gt;% \n  st_drop_geometry() %&gt;% \n  group_by(GEOID) %&gt;%\n  mutate(pct_tract_commuters = estimate / sum(estimate),\n         combined_commuters = sum(estimate)) %&gt;%\n  ungroup() %&gt;%\n  mutate(GEOID = fct_reorder(GEOID, summary_est)) %&gt;%\n  arrange(desc(GEOID), desc(summary_est)) %&gt;% \n  mutate(is_downtown_label = case_when(GEOID == \"42003020100\" & variable == \"Drove alone\" ~ \"Downtown*\",\n                                       TRUE ~ NA_character_)) %&gt;% \n  slice(1:60) %&gt;% \n  ggplot(aes(estimate, GEOID, fill = variable)) +\n  geom_col(color = \"black\") +\n  geom_text(aes(x = estimate + 3000, label = is_downtown_label)) +\n  labs(title = \"Top 30 census tracts\",\n       subtitle = \"Total commuter population from all modes\",\n       x = \"Commuters\",\n       y = \"Census tracts\",\n       fill = \"Commute mode\") +\n  scale_x_comma() +\n  theme_ipsum(base_size = 15) +\n  theme(axis.text.y = element_blank(),\n        panel.grid.major = element_blank())\n\n\n\n\n*Most commuters that live in Downtown walk to work.\nThis shows that in absolute numbers, driving alone swamps public transit across the county.\n\nscatter_graph &lt;- tract_transit %&gt;% \n  st_drop_geometry() %&gt;% \n  select(GEOID, variable, estimate) %&gt;% \n  pivot_wider(names_from = variable, values_from = estimate) %&gt;% \n  clean_names() %&gt;% \n  ggplot(aes(drove_alone, public_transportation)) +\n  geom_point(alpha = .7, size = 1) +\n  labs(title = \"Commuter modes in Allegheny County\",\n       x = \"Driving Alone\",\n       y = \"Using Public Transportation\") +\n  scale_x_comma() +\n  scale_y_comma() +\n  tune::coord_obs_pred() +\n  theme_ipsum(base_size = 15)\n\nscatter_graph\n\n\n\n\nI made the X and Y axes symmetric to emphasize the difference in scale between the two variables.\nThis uses the bi_class function to divide the data into discrete bins based on how many people drive alone vs. use public transit. This turns two continuous variables into one categorical variable. I had to play around with the style argument to find an option that worked for the unbalanced data.\n\ntransit_bivariate_geo &lt;- tract_transit %&gt;% \n  st_drop_geometry() %&gt;% \n  drop_na(estimate) %&gt;% \n  select(GEOID, variable, estimate) %&gt;% \n  pivot_wider(names_from = variable, values_from = estimate) %&gt;% \n  clean_names() %&gt;% \n  replace_na(list(drove_alone = 0, public_transportation = 0)) %&gt;% \n  bi_class(x = drove_alone, \n           y = public_transportation, \n           style = \"fisher\", \n           dim = 3) %&gt;% \n  left_join(tracts, \n            by = c(\"geoid\" = \"GEOID\")) %&gt;% \n  st_sf()\n\nglimpse(transit_bivariate_geo)\n\nRows: 402\nColumns: 9\n$ geoid                 &lt;chr&gt; \"42003408002\", \"42003210700\", \"42003220600\", \"42…\n$ drove_alone           &lt;dbl&gt; 2815, 589, 566, 1224, 466, 1063, 887, 826, 551, …\n$ public_transportation &lt;dbl&gt; 8, 189, 146, 260, 100, 215, 262, 342, 670, 61, 3…\n$ bi_class              &lt;chr&gt; \"3-1\", \"1-2\", \"1-1\", \"2-2\", \"1-1\", \"2-2\", \"1-2\",…\n$ NAME                  &lt;chr&gt; \"Census Tract 4080.02, Allegheny County, Pennsyl…\n$ variable              &lt;chr&gt; \"B08301_001\", \"B08301_001\", \"B08301_001\", \"B0830…\n$ estimate              &lt;dbl&gt; 3165, 1231, 1110, 1992, 702, 1487, 1317, 1712, 1…\n$ moe                   &lt;dbl&gt; 231, 199, 116, 189, 80, 233, 163, 185, 257, 104,…\n$ geometry              &lt;POLYGON [°]&gt; POLYGON ((-79.99 40.61, -79..., POLYGON …\n\n\n\ntable(transit_bivariate_geo$bi_class) %&gt;% \n  enframe(name = \"bi_class\", value = \"count_tracts\") %&gt;% \n  kbl()\n\n\n\n\nbi_class\ncount_tracts\n\n\n\n\n1-1\n124\n\n\n1-2\n72\n\n\n1-3\n9\n\n\n2-1\n84\n\n\n2-2\n66\n\n\n2-3\n9\n\n\n3-1\n34\n\n\n3-2\n4\n\n\n\n\n\n\n\nThis graph overlays the discrete biscale bins on the previous data to show how the function discretized the data.\n\ntransit_bivariate_geo %&gt;% \n  ggplot(aes(drove_alone, public_transportation, color = bi_class)) +\n  geom_point(alpha = .75, size = 1) +\n  scale_x_comma() +\n  labs(x = \"Drove Alone\",\n       y = \"Used Public Transit\") +\n  guides(color = FALSE) +\n  theme_ipsum(base_size = 15)\n\n\n\n\nNote that the X and Y axes are independent in this graph.\nThis creates the biscale legend I will put next to the map.\n\nbi_var_legend &lt;- bi_legend(pal = \"DkBlue\",\n                           dim = 3,\n                           xlab = \" More drove alone\",\n                           ylab = \"More used public transit\",\n                           size = 26) +\n  theme(plot.background = element_rect(fill = alpha(\"white\", 0)),\n        panel.background = element_rect(fill = alpha(\"white\", 0)))\n\nbi_var_legend\n\n\n\n\nI would like to show the % of commuters that each bin represents, so I extract the color palette from the ggplot2 object and make my own legend with geom_tile.\n\nbuilt_legend &lt;- ggplot_build(bi_var_legend)\n\nlegend_palette &lt;- built_legend$data[[1]] %&gt;%\n  mutate(bi_class = str_c(x, y, sep = \"-\")) %&gt;% \n  select(fill, bi_class)\n\nlegend_palette %&gt;% \n  kbl()\n\n\n\n\nfill\nbi_class\n\n\n\n\n#e8e8e8\n1-1\n\n\n#ace4e4\n2-1\n\n\n#5ac8c8\n3-1\n\n\n#dfb0d6\n1-2\n\n\n#a5add3\n2-2\n\n\n#5698b9\n3-2\n\n\n#be64ac\n1-3\n\n\n#8c62aa\n2-3\n\n\n#3b4994\n3-3\n\n\n\n\n\n\n\n\ntransit_bivariate &lt;- transit_bivariate_geo %&gt;% \n  st_drop_geometry() %&gt;% \n  select(geoid, bi_class, drove_alone, public_transportation) %&gt;% \n  separate(bi_class, \n           into = c(\"drove_alone_bi\", \"public_transportation_bi\"), \n           sep = \"-\",\n           remove = FALSE) %&gt;% \n  complete(drove_alone_bi, public_transportation_bi, fill = list(drove_alone = 0, public_transportation = 0)) %&gt;% \n  mutate(bi_class = str_c(drove_alone_bi, public_transportation_bi, sep = \"-\"),\n         total = drove_alone + public_transportation,\n         pct_commuters = total / sum(total)) %&gt;%\n  group_by(bi_class, drove_alone_bi, public_transportation_bi) %&gt;% \n  summarize(count_tract = n(),\n            pct_commuters = sum(pct_commuters)) %&gt;% \n  ungroup()\n\nglimpse(transit_bivariate)\n\nRows: 9\nColumns: 5\n$ bi_class                 &lt;chr&gt; \"1-1\", \"1-2\", \"1-3\", \"2-1\", \"2-2\", \"2-3\", \"3-…\n$ drove_alone_bi           &lt;chr&gt; \"1\", \"1\", \"1\", \"2\", \"2\", \"2\", \"3\", \"3\", \"3\"\n$ public_transportation_bi &lt;chr&gt; \"1\", \"2\", \"3\", \"1\", \"2\", \"3\", \"1\", \"2\", \"3\"\n$ count_tract              &lt;int&gt; 124, 72, 9, 84, 66, 9, 34, 4, 1\n$ pct_commuters            &lt;dbl&gt; 0.13661, 0.11754, 0.02073, 0.25273, 0.22252, …\n\n\n\nlegend_palette &lt;- transit_bivariate %&gt;% \n  distinct(bi_class) %&gt;% \n  left_join(legend_palette, by = \"bi_class\")\n\nlegend_palette %&gt;% \n  kbl()\n\n\n\n\nbi_class\nfill\n\n\n\n\n1-1\n#e8e8e8\n\n\n1-2\n#dfb0d6\n\n\n1-3\n#be64ac\n\n\n2-1\n#ace4e4\n\n\n2-2\n#a5add3\n\n\n2-3\n#8c62aa\n\n\n3-1\n#5ac8c8\n\n\n3-2\n#5698b9\n\n\n3-3\n#3b4994\n\n\n\n\n\n\n\nNote that scale_fill_manual uses the palette I extracted from the ggplot2 object.\n\nbi_var_legend_new &lt;- transit_bivariate %&gt;% \n  mutate(pct_commuters = scales::percent(pct_commuters, accuracy = 1)) %&gt;% \n  ggplot(aes(x = drove_alone_bi, y = public_transportation_bi, fill = bi_class)) +\n  geom_tile() +\n  geom_label(fill = \"white\", alpha = .75, size = 12, label = \"    \") +\n  geom_text(aes(label = pct_commuters), alpha = 1, size = 7) +\n  coord_fixed(ratio = 1) +\n  labs(x = substitute(paste(\"More drove alone\", \"\" %-&gt;% \"\")),\n       y = substitute(paste(\"More used public transit\", \"\" %-&gt;% \"\"))) +\n  guides(fill = FALSE) +\n  scale_fill_manual(values = pull(legend_palette, fill)) +\n  theme_ipsum(plot_title_size = 30,\n              axis_title_size = 30) +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        panel.background = element_blank(),\n        axis.text.x = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks = element_blank())\n\nbi_var_legend_new +\n  labs(title = 'Percent of \"drive alone\" + \"public transit\" commuters')\n\n\n\n\nThis creates the map of commuter mode by census tract, filled by the discretized biscale bin.\n\ntransit_bi_var_plot &lt;- transit_bivariate_geo %&gt;% \n  ggplot(aes(fill = bi_class)) +\n  geom_sf(show.legend = FALSE, lwd = 0) +\n  geom_sf(data = rivers, fill = \"black\", color = \"black\") +\n  bi_scale_fill(pal = \"DkBlue\", dim = 3) +\n  bi_theme() +\n  theme_ipsum(base_size = 15) +\n  theme(axis.text.x = element_blank(),\n        axis.text.y = element_blank(),\n        panel.background = element_blank(),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank())\n\ntransit_bi_var_plot\n\n\n\n\nNow that I have my legend and map, I use patchwork to stitch them together.\n\ndesign = c(area(t = 2, l = 4, b = 20, r = 20),\n           area(t = 1, l = 1, b = 6, r = 6))\n\nplot(design)\n\n\n\n\n\ncombined_bi_var_plot &lt;- transit_bi_var_plot + bi_var_legend_new +\n  plot_layout(design = design) +\n  plot_annotation(title = \"Allegheny County Commuter Patterns\",\n                  subtitle = \"Legend: % of commuters that drove alone or use public transit\",\n                  caption = \"2019 American Community Survey\",\n                  theme = theme(panel.background = element_rect(fill = \"black\"),\n                                plot.title = element_text(size = 30),\n                                plot.subtitle = element_text(size = 25),\n                                plot.caption = element_text(size = 25)))\n\n\n\n\n\n\n\nLinks:\n\nhttps://www.pghcitypaper.com/pittsburgh/low-income-pittsburghers-are-becoming-increasingly-reliant-on-public-transit-bikes-walking-and-alternative-transportation/Content?oid=19059768\nhttps://www.pghcitypaper.com/pittsburgh/new-commutes-analyzing-the-changing-ways-pittsburghers-get-to-work/Content?oid=6405396\nhttps://www.pghcitypaper.com/pittsburgh/pittsburgh-is-the-7th-least-car-dependent-metro-in-america-study-says/Content?oid=16755873\nhttps://www.nytimes.com/2020/07/09/opinion/sunday/ban-cars-manhattan-cities.html\nhttps://www.nytimes.com/2021/03/25/climate/buses-trains-ridership-climate-change.html\nhttps://nacto.org/publication/transit-street-design-guide/introduction/why/designing-move-people/\nhttps://rweekly.org/"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/shifting_political_winds/index.html",
    "href": "posts/shifting_political_winds/index.html",
    "title": "Shifting political winds",
    "section": "",
    "text": "The purpose of this post is to recreate the “Shift from 2016” arrow map that the New York Times used to show which counties became more Democratic or Republican-leaning from 2016 to 2020. This is a screenshot of the NYTimes figure:\nI will use county-level Presidential election data from the MIT Election Data + Science Lab to recreate the chart. Since 2020 results are not final yet, I will focus on data from 2000-2016. I ran into multiple issues with the dataset, which I explain in the Code section below. The most signifcant issue was with the data from Alaska, which I excluded from the charts below because of problems with the data."
  },
  {
    "objectID": "posts/shifting_political_winds/index.html#recreating-the-nytimes-figure",
    "href": "posts/shifting_political_winds/index.html#recreating-the-nytimes-figure",
    "title": "Shifting political winds",
    "section": "Recreating the NYTimes Figure",
    "text": "Recreating the NYTimes Figure\nMy approach is to use {ggplot2} and {sf} to map the data and draw arrows at angles to display shifts in the Democratic margin.\nThis is the dataframe I use to make the final map. It contains the year, state, county, FIPS code, county and state geometries, and election results per county.\n\nglimpse(shift_map)\n\nRows: 12,610\nColumns: 22\n$ year                          &lt;dbl&gt; 2004, 2008, 2012, 2016, 2004, 2008, 2012…\n$ state                         &lt;chr&gt; \"ALABAMA\", \"ALABAMA\", \"ALABAMA\", \"ALABAM…\n$ county_name                   &lt;chr&gt; \"AUTAUGA\", \"AUTAUGA\", \"AUTAUGA\", \"AUTAUG…\n$ county_fips                   &lt;chr&gt; \"01001\", \"01001\", \"01001\", \"01001\", \"010…\n$ candidatevotes_sum_democrat   &lt;dbl&gt; 4758, 6093, 6363, 5936, 15599, 19386, 18…\n$ candidatevotes_sum_republican &lt;dbl&gt; 15196, 17403, 17379, 18172, 52971, 61271…\n$ pct_vote_democrat             &lt;dbl&gt; 0.23845, 0.25932, 0.26801, 0.24623, 0.22…\n$ pct_vote_republican           &lt;dbl&gt; 0.7616, 0.7407, 0.7320, 0.7538, 0.7725, …\n$ dem_margin_pct                &lt;dbl&gt; -0.52310, -0.48136, -0.46399, -0.50755, …\n$ dem_margin_votes              &lt;dbl&gt; -10438, -11310, -11016, -12236, -37372, …\n$ shift_pct                     &lt;dbl&gt; -0.106746, 0.041745, 0.017371, -0.043561…\n$ shift_votes                   &lt;dbl&gt; -3387, -872, 294, -1220, -10497, -4513, …\n$ shift_pct_scaled              &lt;dbl&gt; 71.83, 91.08, 87.92, 80.02, 78.51, 89.01…\n$ shift_votes_scaled            &lt;dbl&gt; 16602, 11700, 10573, 12378, 30460, 18796…\n$ shift_pct_binary              &lt;chr&gt; \"Republican\", \"Democratic\", \"Democratic\"…\n$ shift_votes_binned            &lt;lgl&gt; FALSE, TRUE, TRUE, TRUE, FALSE, FALSE, F…\n$ geometry                      &lt;POLYGON [m]&gt; POLYGON ((1269841 -1303980,..., …\n$ center                        &lt;list&gt; &lt;POINT (1253837 -1285138)&gt;, &lt;POINT (125…\n$ lng0                          &lt;dbl&gt; 1253837, 1253837, 1253837, 1253837, 1177…\n$ lat0                          &lt;dbl&gt; -1285138, -1285138, -1285138, -1285138, …\n$ lng1                          &lt;dbl&gt; 1259015, 1253616, 1254221, 1255982, 1183…\n$ lat1                          &lt;dbl&gt; -1269365, -1273441, -1274572, -1272948, …\n\n\n\nshift_map_filtered &lt;- shift_map %&gt;% \n  filter(state != \"ALASKA\") %&gt;%\n  filter(year == 2016) %&gt;% \n  mutate(shift_pct_binary = case_when(sign(shift_pct) == 1 ~ \"Democratic\",\n                                      sign(shift_pct) == -1 ~ \"Republican\"),\n         shift_pct_binary = as.factor(shift_pct_binary)) %&gt;% \n  mutate(shift_votes_binned = abs(shift_votes) &lt;= 3000)\n\nggplot() +\n  geom_sf(data = filter(state_geo, !str_detect(NAME, \"ALASKA\")),\n          linewidth = .2,\n          fill = NA) +\n  geom_point(data = filter(shift_map_filtered, abs(shift_votes) &lt;= 1500),\n             aes(x = lng0, y = lat0,\n                 color = shift_pct_binary),\n             size = .75,\n             alpha = .3) +\n  geom_segment(data = filter(shift_map_filtered, abs(shift_votes) &gt; 1500),\n               aes(x = lng0, xend = lng1,\n                   y = lat0, yend = lat1,\n                   color = shift_pct_binary,\n                   linewidth = shift_votes,\n                   alpha = shift_votes_binned),\n               linejoin = \"mitre\",\n               arrow = arrow(length = unit(0.08, \"inches\"))) +\n  scale_color_manual(values = c(\"#1375B7\", \"#C93135\"), guide = guide_legend(title.position = \"top\")) +\n  scale_linewidth_continuous(range = c(.001, 2), guide = \"none\") +\n  scale_alpha_manual(values = c(1, .3), guide = \"none\") +\n  labs(color = \"Shift in election margin\") +\n  facet_wrap(~year) +\n  theme_void(base_size = 25) +\n  theme(legend.direction = \"horizontal\",\n        legend.position = \"bottom\")\n\n\n\n\nThe starting point of the line is the centroid of the county. The length and width of the lines are scaled to the shift in terms of number of votes. The NYTimes figure treats the shift as a binary variable when it rescales to degrees of the angle. In their graph, a Democratic shift is about 45 degrees (diagonal left) and a Republican shift is about 135 degrees (diagonal right). My figure maintains the continuous nature of the shift in %. I use the range 0-180 in degrees to indicate the shift. 0 degrees (all the way left) indicates a 100% shift towards Democrats, 90 degrees (pointing upwards) indicates no change, and 180 degrees (all the way to the right) indicates a 100% shift towards Republicans.\nThe end point of the line is calculated using the sine and cosine of the margin shift in % (re-scaled to be interpreted as degrees of an angle) multiplied by the margin shift in votes (re-scaled to be interpreted as meters), which is added to the origin point.\nI lower the opacity of the lines in counties where the vote totals did not shift much. I use points instead of lines for counties where there was a very small shift in votes. This prevents overplotting in geographically dense areas with small populations.\nThis animation shows the shift in Presidential election margin from 2004-2016.\n\npolitical_winds_anim &lt;- shift_map %&gt;% \n  filter(state != \"ALASKA\") %&gt;% \n  mutate(id = str_c(state, county_name, county_fips)) %&gt;% \n  mutate(year = as.integer(year)) %&gt;% \n  mutate(shift_pct_binary = case_when(sign(shift_pct) == 1 ~ \"Democratic\",\n                                      sign(shift_pct) == -1 ~ \"Republican\"),\n         shift_pct_binary = as.factor(shift_pct_binary)) %&gt;% \n  mutate(shift_votes_binned = abs(shift_votes) &lt;= 3000) %&gt;% \n  ggplot() +\n  geom_sf(data = filter(state_geo, NAME != \"ALASKA\"),\n          linewidth = .2,\n          fill = NA) +\n  geom_segment(aes(x = lng0, xend = lng1,\n                   y = lat0, yend = lat1,\n                   color = shift_pct_binary,\n                   linewidth = shift_votes,\n                   alpha = shift_votes_binned,\n                   group = id),\n               linejoin = \"mitre\",\n               arrow = arrow(length = unit(0.09, \"inches\"))) +\n  scale_color_manual(values = c(\"#1375B7\", \"#C93135\"), guide = guide_legend(title.position = \"top\")) +\n  scale_linewidth_continuous(range = c(.001, 1.3), guide = \"none\") +\n  scale_alpha_manual(values = c(1, .3), guide = \"none\") +\n  theme_void(base_size = 25) +\n  theme(legend.direction = \"horizontal\",\n        legend.position = \"bottom\") +\n  transition_states(year) +\n  labs(title = \"Shift in Presidential election Democratic margin\",\n       subtitle = \"Year: {closest_state}\",\n       color = \"Shift in Democratic margin\")\n\npolitical_winds_anim\n\n\n\n\nIn the animation there is less overplotting, so I do not replace lines with dots for counties where there was a very small shift in votes."
  },
  {
    "objectID": "posts/shifting_political_winds/index.html#code",
    "href": "posts/shifting_political_winds/index.html#code",
    "title": "Shifting political winds",
    "section": "Code",
    "text": "Code\n\nIngest\n\n#election shift\n#script to clean data\n\n#data from https://electionlab.mit.edu/data\n\n#fips info\n#https://en.wikipedia.org/wiki/Federal_Information_Processing_Standard_state_code#FIPS_state_codes\n#https://en.wikipedia.org/wiki/List_of_United_States_FIPS_codes_by_county\n#changes https://www.census.gov/programs-surveys/geography/technical-documentation/county-changes.2010.html\n\n#read in data\ndata &lt;- read_csv(\"post_data/countypres_2000-2020.csv\",\n                 col_types = cols(\n                   year = col_double(),\n                   state = col_character(),\n                   state_po = col_character(),\n                   county_name = col_character(),\n                   county_fips = col_character(),\n                   office = col_character(),\n                   candidate = col_character(),\n                   party = col_character(),\n                   candidatevotes = col_double(),\n                   totalvotes = col_double(),\n                   version = col_double()\n                 )) %&gt;% \n  clean_names() |&gt;\n  filter(year &lt;= 2016,\n         mode == \"TOTAL\") |&gt; \n  select(-mode)\n\nglimpse(data)\n\nRows: 50,524\nColumns: 11\n$ year           &lt;dbl&gt; 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2…\n$ state          &lt;chr&gt; \"ALABAMA\", \"ALABAMA\", \"ALABAMA\", \"ALABAMA\", \"ALABAMA\", …\n$ state_po       &lt;chr&gt; \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"…\n$ county_name    &lt;chr&gt; \"AUTAUGA\", \"AUTAUGA\", \"AUTAUGA\", \"AUTAUGA\", \"BALDWIN\", …\n$ county_fips    &lt;chr&gt; \"01001\", \"01001\", \"01001\", \"01001\", \"01003\", \"01003\", \"…\n$ office         &lt;chr&gt; \"US PRESIDENT\", \"US PRESIDENT\", \"US PRESIDENT\", \"US PRE…\n$ candidate      &lt;chr&gt; \"AL GORE\", \"GEORGE W. BUSH\", \"RALPH NADER\", \"OTHER\", \"A…\n$ party          &lt;chr&gt; \"DEMOCRAT\", \"REPUBLICAN\", \"GREEN\", \"OTHER\", \"DEMOCRAT\",…\n$ candidatevotes &lt;dbl&gt; 4942, 11993, 160, 113, 13997, 40872, 1033, 578, 5188, 5…\n$ totalvotes     &lt;dbl&gt; 17208, 17208, 17208, 17208, 56480, 56480, 56480, 56480,…\n$ version        &lt;dbl&gt; 20220315, 20220315, 20220315, 20220315, 20220315, 20220…\n\n\n\n\nClean\nThis code filters out state-wide vote tabulations and then filters only on the two-party Presidential vote.\n\ndata &lt;- data %&gt;% \n  rename(fips_raw = county_fips) %&gt;% \n  #filter out state-wide ballot collection\n  filter(!(state == \"CONNECTICUT\" & county_name == \"STATEWIDE WRITEIN\")) %&gt;% \n  filter(!(state == \"MAINE\" & county_name == \"MAINE UOCAVA\")) %&gt;% \n  filter(!(state == \"RHODE ISLAND\" & county_name == \"FEDERAL PRECINCT\"))\n\n#filter for only 2-party vote in presidential elections\ndata &lt;- data %&gt;% \n  filter(office == \"US PRESIDENT\",\n         party == \"DEMOCRAT\" | party == \"REPUBLICAN\") %&gt;% \n  arrange(state, county_name, fips_raw, year) %&gt;% \n  replace_na(list(candidatevotes = 0))\n\nMany of the FIPS codes from the source data dropped leading zeroes, which makes them unuseable for joining with Census data. This code adds the leading zeroes back.\nThese problems were fixed in a later update by MIT, so this code is not strictly necessary anymore\n\n#clean fips data\nstates_with_bad_fips &lt;- str_to_title(c(\"ALABAMA\", \"ALASKA\", \"ARIZONA\", \n                                      \"ARKANSAS\", \"CALIFORNIA\",\n                                      \"COLORADO\", \"CONNECTICUT\"))\ndata %&gt;% \n  filter(state %in% states_with_bad_fips) %&gt;% \n  mutate(county_fips = paste0(\"0\", fips_raw)) %&gt;% \n  distinct(fips_raw, county_fips)\n\n# A tibble: 0 × 2\n# ℹ 2 variables: fips_raw &lt;chr&gt;, county_fips &lt;chr&gt;\n\ndata &lt;- data %&gt;% \n  #add \"0\" to front of states where leading \"0\" was dropped\n  mutate(county_fips = case_when(state %in% states_with_bad_fips ~ paste0(\"0\", fips_raw),\n                          !(state %in% states_with_bad_fips) ~ fips_raw))\n\nI had to make a variety of decisions about how to clean up the data with regards to county geometries. The MIT data does not reflect cases where counties changed names or FIPS codes, or where counties merged. This code manually makes the changes necessary to join the data with Census geometry data. Note that I do not attempt to fix the data for Alaska, which was extremely different than the Census data. I was not confident that I could make accurate adjustments in this case, so I excluded Alaska entirely. These changes are not optimal, but I think it is close enough.\nThese problems were fixed in a later update by MIT, so this code is not strictly necessary anymore\n\n#decisions to make with wonky geometry\n#merge records for Shannnon and Oglala Lakota counties in SD\n#merge Kansas City Missouri and Jackson County Missouri\n#merge Bedford (city) fips 51515 with Bedford county 51019\n\ndata &lt;- data %&gt;% \n  #update Oglala Lakota SD fips\n  #changed in 2015 https://www.census.gov/programs-surveys/geography/technical-documentation/county-changes.2010.html\n  mutate(county_fips = case_when(state == \"SOUTH DAKOTA\" & county_name == \"OGLALA LAKOTA\" ~ \"46102\",\n                          TRUE ~ county_fips)) %&gt;% \n  #merge Kansas City Missouri with Jackson County Missouri\n  mutate(county_name = case_when(state == \"MISSOURI\" & county_name == \"KANSAS CITY\" ~ \"JACKSON\",\n                            TRUE ~ county_name),\n         county_fips = case_when(state == \"MISSOURI\" & county_name == \"JACKSON\" ~ \"29095\",\n                          TRUE ~ county_fips)) %&gt;% \n  #merge Bedford (city) fips 51515 with Bedford county 51019\n  mutate(county_fips = case_when(state == \"VIRGINIA\" & county_name == \"BEDFORD\" & county_fips == \"51515\" ~ \"51019\",\n                          TRUE ~ county_fips))\n\nThis compares the counties in the MIT data vs. what is in the Census API. Besides Alaska, this shows that my manual changes accounted for the issues I identified.\n\ncounties &lt;- get_acs(variables = \"B19013_001\",\n                      geography = \"county\",\n                      geometry = FALSE) %&gt;% \n  #mutate(census_geo_year = 2010) %&gt;% \n  select(NAME, GEOID)\n\nGetting data from the 2017-2021 5-year ACS\n\n\n\n#alaska falls out: this is expected\n#Broomfield County CO falls out for year 2000: was part of Boulder County in 2000\n#Oglala Lakota County SD falls out for year 2000: was Shannon County in 2000\n#\ndata %&gt;% \n  select(year, state, county_name, county_fips) %&gt;% \n  filter(state != \"ALASKA\") %&gt;% \n  anti_join(counties, by = c(\"county_fips\" = \"GEOID\")) %&gt;% \n  count(state, county_name)\n\n# A tibble: 1 × 3\n  state        county_name     n\n  &lt;chr&gt;        &lt;chr&gt;       &lt;int&gt;\n1 SOUTH DAKOTA SHANNON         8\n\n\nThe process of merging some counties meant that I had to summarize the election results to the level of my new “adjusted” counties. This code performs that process.\n\n#some counties have 4 records because of merging process\ndata %&gt;%\n  select(state, county_name, county_fips, year) %&gt;% \n  add_count(state, county_name, county_fips, year) %&gt;% \n  distinct(n)\n\n# A tibble: 2 × 1\n      n\n  &lt;int&gt;\n1     2\n2     4\n\n\n\n#summarize candidatevotes to account for merged counties\ndata %&gt;% \n  select(state, county_name, county_fips, year, office, party, candidate, candidatevotes) %&gt;% \n  group_by(state, county_name, county_fips, year, office, party, candidate) %&gt;% \n  summarize(candidatevotes_sum = sum(candidatevotes)) %&gt;% \n  ungroup() %&gt;% \n  add_count(state, county_name, county_fips, year) %&gt;% \n  #confirm that each county only has 2 records\n  distinct(n)\n\n`summarise()` has grouped output by 'state', 'county_name', 'county_fips',\n'year', 'office', 'party'. You can override using the `.groups` argument.\n\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1     2\n\n\n\ndata &lt;- data %&gt;% \n  select(state, county_name, county_fips, year, office, party, candidate, candidatevotes) %&gt;% \n  group_by(state, county_name, county_fips, year, office, party, candidate) %&gt;% \n  summarize(candidatevotes_sum = sum(candidatevotes)) %&gt;% \n  ungroup()\n\n`summarise()` has grouped output by 'state', 'county_name', 'county_fips',\n'year', 'office', 'party'. You can override using the `.groups` argument.\n\n\n\n\nMunge\nThis part performs the more straightfoward tasks of calculating a candidate’s % of the vote and the election-to-election shift in %.\n\npresidential_votes &lt;- data %&gt;% \n  group_by(year, state, county_name, county_fips) %&gt;% \n  mutate(pct_vote = candidatevotes_sum / sum(candidatevotes_sum)) %&gt;% \n  ungroup() %&gt;% \n  select(year, state, county_name, county_fips, party, candidatevotes_sum, pct_vote)\n\n\npresidential_votes_shift &lt;- presidential_votes %&gt;% \n  mutate(party = str_to_lower(party)) %&gt;%\n  pivot_wider(names_from = party, values_from = c(candidatevotes_sum, pct_vote)) %&gt;%\n  mutate(dem_margin_pct = pct_vote_democrat - pct_vote_republican,\n         dem_margin_votes = candidatevotes_sum_democrat - candidatevotes_sum_republican) %&gt;% \n  arrange(state, county_name, county_fips, year) %&gt;% \n  group_by(state, county_name, county_fips) %&gt;% \n  mutate(shift_pct = dem_margin_pct - lag(dem_margin_pct),\n         shift_votes = dem_margin_votes - lag(dem_margin_votes)) %&gt;% \n  filter(row_number() &gt; 1) %&gt;% \n  ungroup()\n\nFinally, this creates new variables that rescale the shift in % and votes to degrees and meters, respectively. I also create variations of shift_pct and shift_votes to use in the graph.\n\npresidential_votes_shift &lt;- presidential_votes_shift %&gt;% \n  mutate(shift_pct_scaled = rescale(shift_pct, to = c(0, 180)), #republican 0, democrat 180\n         shift_votes_scaled = rescale(abs(shift_votes), to = c(10^4, 10^6))) %&gt;% \n  mutate(shift_pct_binary = case_when(sign(shift_pct) == 1 ~ \"Democratic\",\n                                      sign(shift_pct) == -1 ~ \"Republican\"),\n         shift_pct_binary = as.factor(shift_pct_binary)) %&gt;% \n  mutate(shift_votes_binned = abs(shift_votes) &lt;= 3000)\n\n\n#create shift map object\nshift_map &lt;- presidential_votes_shift %&gt;% \n  left_join(county_geo, by = c(\"county_fips\" = \"GEOID\")) %&gt;% \n  st_sf() %&gt;% \n  rename(lng0 = center_lon_x,\n         lat0 = center_lat_y) %&gt;% \n  mutate(lng1 = lng0 + (shift_votes_scaled * cos(NISTdegTOradian(shift_pct_scaled))),\n         lat1 = lat0 + (shift_votes_scaled * sin(NISTdegTOradian(shift_pct_scaled))))\n\n\nshift_map_filtered &lt;- shift_map %&gt;% \n  filter(state != \"ALASKA\") %&gt;%\n  filter(year == 2016) %&gt;% \n  mutate(shift_pct_binary = case_when(sign(shift_pct) == 1 ~ \"Democratic\",\n                                      sign(shift_pct) == -1 ~ \"Republican\"),\n         shift_pct_binary = as.factor(shift_pct_binary))\n\nggplot() +\n  geom_sf(data = filter(state_geo, !str_detect(NAME, \"ALASKA\")),\n          linewidth = .2,\n          fill = NA) +\n  geom_point(data = filter(shift_map_filtered, abs(shift_votes) &lt;= 1500),\n             aes(x = lng0, y = lat0,\n                 color = shift_pct_binary),\n             size = .75,\n             alpha = .3) +\n  geom_segment(data = filter(shift_map_filtered, abs(shift_votes) &gt; 1500),\n               aes(x = lng0, xend = lng1,\n                   y = lat0, yend = lat1,\n                   color = shift_pct_binary,\n                   linewidth = shift_votes,\n                   alpha = shift_votes_binned),\n               linejoin = \"mitre\",\n               arrow = arrow(length = unit(0.08, \"inches\"))) +\n  scale_color_manual(values = c(\"#1375B7\", \"#C93135\"), guide = guide_legend(title.position = \"top\")) +\n  scale_linewidth_continuous(range = c(.001, 2), guide = \"none\") +\n  scale_alpha_manual(values = c(1, .3), guide = \"none\") +\n  labs(color = \"Shift in election margin\") +\n  facet_wrap(~year) +\n  theme_void(base_size = 25) +\n  theme(legend.direction = \"horizontal\",\n        legend.position = \"bottom\")\n\n\npolitical_winds_anim &lt;- shift_map %&gt;% \n  filter(state != \"Alaska\") %&gt;% \n  mutate(id = str_c(state, county_name, county_fips)) %&gt;% \n  mutate(year = as.integer(year)) %&gt;% \n  mutate(shift_votes_binned = abs(shift_votes) &lt;= 3000) %&gt;% \n  ggplot() +\n  geom_sf(data = filter(state_geo, NAME != \"Alaska\"),\n          linewidth = .2,\n          fill = NA) +\n  geom_segment(aes(x = lng0, xend = lng1,\n                   y = lat0, yend = lat1,\n                   color = shift_pct_binary,\n                   linewidth = shift_votes,\n                   alpha = shift_votes_binned,\n                   group = id),\n               linejoin = \"mitre\",\n               arrow = arrow(length = unit(0.09, \"inches\"))) +\n  scale_color_manual(values = c(\"#1375B7\", \"#C93135\"), guide = guide_legend(title.position = \"top\")) +\n  scale_size_continuous(range = c(.001, 1.3), guide = \"none\") +\n  scale_alpha_manual(values = c(1, .3), guide = \"none\") +\n  theme_void(base_size = 25) +\n  theme(legend.direction = \"horizontal\",\n        legend.position = \"bottom\") +\n  transition_states(year) +\n  labs(title = \"Shift in Presidential election Democratic margin\",\n       subtitle = \"Year: {closest_state}\",\n       color = \"Shift in Democratic margin\")\n\npolitical_winds_anim"
  },
  {
    "objectID": "posts/pittsburgh-parking-covid-change/index.html",
    "href": "posts/pittsburgh-parking-covid-change/index.html",
    "title": "Effect of COVID-19 on Pittsburgh parking transactions",
    "section": "",
    "text": "The COVID-19 pandemic’s affect on commerce and mobility habits is well documented. For example, Apple publishes Mobility Trends reports about utilization of various transportation modes.\nApple’s data shows that utilization of driving in Pittsburgh dropped significantly in late March, but has rebounded above pre-COVID-19 levels since then.\nThe WPDRC publishes parking meter transactions for 60 parking zones in Pittsburgh. In this post I will use the frequency of parking transactions over time as a proxy for commercial activity in the city. This information only represents commerce that people use vehicles to perform, so it does not include mass transit or drivers that use private parking areas or meters that are not captured in this dataset. I will be interested to see if the parking meter data matches Apple’s report about driving."
  },
  {
    "objectID": "posts/pittsburgh-parking-covid-change/index.html#top-level-analysis",
    "href": "posts/pittsburgh-parking-covid-change/index.html#top-level-analysis",
    "title": "Effect of COVID-19 on Pittsburgh parking transactions",
    "section": "Top-level analysis",
    "text": "Top-level analysis\n\nRead in data\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(vroom)\nlibrary(hrbrthemes)\nlibrary(scales)\nlibrary(plotly)\nlibrary(broom)\nlibrary(heatwaveR)\nlibrary(gt)\n\noptions(scipen = 999,\n        digits = 4)\n\ntheme_set(theme_ipsum())\n\nAs of September 28th there are ~5 million rows in the dataset. Each row consists of a 10-minute period in a given zone with the aggregated number of transactions and the amount paid.\n\ndata &lt;- vroom(\"post_data/1ad5394f-d158-46c1-9af7-90a9ef4e0ce1.csv\")\n\nglimpse(data)\n\nRows: 7,354,373\nColumns: 9\n$ `_id`               &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,…\n$ zone                &lt;chr&gt; \"421 - NorthSide\", \"403 - Uptown\", \"412 - East Lib…\n$ start               &lt;dttm&gt; 2018-01-01 00:20:00, 2018-01-01 01:10:00, 2018-01…\n$ end                 &lt;dttm&gt; 2018-01-01 00:30:00, 2018-01-01 01:20:00, 2018-01…\n$ utc_start           &lt;dttm&gt; 2018-01-01 05:20:00, 2018-01-01 06:10:00, 2018-01…\n$ meter_transactions  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ meter_payments      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ mobile_transactions &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 2, 5, 1, 1, 1, 3, 1, 1, 1,…\n$ mobile_payments     &lt;dbl&gt; 4.00, 3.00, 3.00, 4.00, 16.25, 4.00, 3.00, 1.00, 2…\n\n\nThere are 60 distinct parking zones in the dataset.\n\ndata %&gt;% \n  distinct(zone) %&gt;% \n  arrange(zone)\n\n# A tibble: 62 × 1\n   zone                       \n   &lt;chr&gt;                      \n 1 209 - Mon Wharf            \n 2 213 - Second Avenue Plaza  \n 3 301 - Sheridan Harvard Lot \n 4 302 - Sheridan Kirkwood Lot\n 5 304 - Tamello Beatty Lot   \n 6 307 - Eva Beatty Lot       \n 7 308 - Harvard Beatty Lot   \n 8 311 - Ansley Beatty Lot    \n 9 314 - Penn Circle NW Lot   \n10 321 - Beacon Bartlett Lot  \n# ℹ 52 more rows\n\n\nThis code chunk performs most of the aggregation and manipulation. It separates the start column into start_date and start_time, calculates the number of transactions per day, and creates some date columns that I use later.\n\ndf_ts &lt;- data %&gt;%\n  select(start, meter_transactions, mobile_transactions) %&gt;%\n  separate(start, into = c(\"start_date\", \"start_time\"), remove = TRUE, sep = \" \") %&gt;%\n  mutate(start_date = ymd(start_date)) %&gt;%\n  filter(start_date &lt;= \"2020-10-05\") |&gt; \n  group_by(start_date) %&gt;%\n  summarize(meter_transactions = sum(meter_transactions),\n            mobile_transactions = sum(mobile_transactions)) %&gt;%\n  ungroup() %&gt;%\n  rowwise() %&gt;%\n  mutate(total_parking_transactions = meter_transactions + mobile_transactions) %&gt;%\n  ungroup() %&gt;%\n  mutate(year = year(start_date),\n         day_of_year = yday(start_date),\n         week_of_year = week(start_date),\n         weekday = wday(start_date, label = TRUE)) %&gt;%\n  group_by(year, week_of_year) %&gt;%\n  mutate(first_date_of_week = min(start_date)) %&gt;% \n  ungroup() %&gt;% \n  select(start_date, year, week_of_year, day_of_year, weekday, everything())"
  },
  {
    "objectID": "posts/pittsburgh-parking-covid-change/index.html#overall-timeline",
    "href": "posts/pittsburgh-parking-covid-change/index.html#overall-timeline",
    "title": "Effect of COVID-19 on Pittsburgh parking transactions",
    "section": "Overall timeline",
    "text": "Overall timeline\nThis view of the daily transactions shows that parking transactions dropped off steeply in late March 2020.\n\ndf_ts %&gt;% \n  ggplot(aes(first_date_of_week, total_parking_transactions)) +\n  geom_point(alpha = .2, size = .5) +\n  labs(title = \"Daily parking transactions\",\n       subtitle = \"2014-2020\",\n       x = \"Year\",\n       y = \"Total parking transactions\") +\n  scale_y_comma() +\n  scale_x_date(date_labels = \"%Y\")\n\n\n\n\n\n2020 vs. previous years\nStarting in March, parking transactions in 2020 fell way below the historical norm. At the most extreme, weekly transactions fell below 10,000.\n\ncompare_2020_before &lt;- df_ts %&gt;% \n  select(year, week_of_year, total_parking_transactions) %&gt;% \n  group_by(year, week_of_year) %&gt;% \n  summarize(total_parking_transactions = sum(total_parking_transactions)) %&gt;% \n  group_by(week_of_year) %&gt;% \n  mutate(week_median_parking_events = median(total_parking_transactions)) %&gt;% \n  ungroup() %&gt;% \n  mutate(period = case_when(year == 2020 ~ \"2020\",\n                               year &lt; 2020 ~ \"Before times\"))\n\ncompare_2020_before %&gt;% \n  ggplot(aes(x = week_of_year, y = total_parking_transactions, color = period, group = year)) +\n  geom_hline(yintercept = 0) +\n  geom_line(data = compare_2020_before %&gt;% filter(period == \"Before times\"),\n            size = 1.5, alpha = .7) +\n  geom_line(data = compare_2020_before %&gt;% filter(period == \"2020\"),\n            size = 1.5) +\n  scale_x_continuous(breaks = seq(0, 54, by = 4)) +\n  scale_color_manual(values = c(\"red\", \"grey\")) +\n  scale_y_comma(breaks = seq(0, 200000, by = 20000)) +\n  labs(title = \"Weekly parking transactions\",\n       x = \"Week of year\",\n       y = \"Total parking events\",\n       color = \"Period\")\n\n\n\n\n\n\n2020 vs. historical average\nThis code calculates the % difference between the number of parking transactions in 2020 and the historical average for a given week.\n\ndata_historical &lt;- df_ts %&gt;% \n  filter(start_date &lt; \"2020-01-01\") %&gt;% \n  select(year, week_of_year, total_parking_transactions) %&gt;% \n  group_by(year, week_of_year) %&gt;% \n  summarize(total_parking_transactions = sum(total_parking_transactions)) %&gt;% \n  group_by(week_of_year) %&gt;% \n  summarize(median_historical_transactions = median(total_parking_transactions),\n            day_count = n()) %&gt;% \n  ungroup()\n\n\ndata_2020 &lt;- df_ts %&gt;% \n  select(start_date, first_date_of_week, week_of_year, total_parking_transactions) %&gt;% \n  filter(start_date &gt;= \"2020-01-01\") %&gt;% \n  group_by(first_date_of_week, week_of_year) %&gt;% \n  summarize(total_parking_transactions = sum(total_parking_transactions)) %&gt;% \n  ungroup()\n\n\ndf &lt;- data_2020 %&gt;% \n  left_join(data_historical)\n\n\nsmoothed_line_df &lt;- df %&gt;% \n  mutate(pct_difference = (total_parking_transactions - median_historical_transactions) / median_historical_transactions) %&gt;% \n  select(week_of_year, first_date_of_week, pct_difference) %&gt;% \n  nest(parking_data = everything()) %&gt;% \n  mutate(model = map(parking_data, ~loess(pct_difference ~ week_of_year, data = .x, span = .3)),\n         coeff = map(model, augment))\n\nsmoothed_line_df &lt;- smoothed_line_df %&gt;% \n  unnest(parking_data) %&gt;% \n  left_join(unnest(smoothed_line_df, coeff)) %&gt;% \n  select(first_date_of_week, .fitted) %&gt;% \n  mutate(sign = .fitted &gt; 0,\n         population = \"total\")\n\nThis shows that after starting 2020 slightly above average, parking transactions fell to almost -100% in early April.\n\nsmoothed_line_df %&gt;% \n  ggplot(aes(x = first_date_of_week)) +\n  heatwaveR::geom_flame(aes(y = 0, y2 = .fitted)) +\n  geom_line(aes(y = .fitted), size = 1.5) +\n  geom_hline(yintercept = 0, lty = 2) +\n  scale_y_percent() +\n  labs(title = \"2020 vs. historical average\",\n       x = \"Date\",\n       y = \"Percent difference\")\n\n\n\n\nWhile the number of transactions recovered from the depths of March and April, it has not matched the increase that Apple’s mobility report showed for driving after May. Parking transactions are still 50% below their historical average.\n\n\nWeekday vs weekend difference, 2020 vs. historical\nThe difference between the number of parking transactions on weekdays vs. weekends did not change significantly after March 2020.\n\nweekday_weekend_df &lt;- df_ts %&gt;% \n  select(start_date, week_of_year, weekday, total_parking_transactions) %&gt;% \n  mutate(period = case_when(start_date &gt;= \"2020-01-01\" ~ \"2020\",\n                            start_date &lt; \"2020-01-01\" ~ \"Before times\"),\n         is_weekend = case_when(weekday %in% c(\"Sat\", \"Sun\") ~ \"weekend\",\n                                !(weekday %in% c(\"Sat\", \"Sun\")) ~ \"weekday\")) %&gt;% \n  mutate(period = fct_relevel(period, \"Before times\"),\n         is_weekend = fct_relevel(is_weekend, \"weekday\")) %&gt;% \n  group_by(period, is_weekend) %&gt;% \n  summarize(total_parking_transactions = sum(total_parking_transactions)) %&gt;% \n  mutate(pct_of_parking_transactions = total_parking_transactions / sum(total_parking_transactions))\n\nweekday_weekend_df %&gt;% \n  ggplot(aes(x = is_weekend, y =  pct_of_parking_transactions, fill = period)) +\n  geom_col(position = position_dodge(width = 1), color = \"black\", alpha = .8) +\n  scale_y_percent() +\n  scale_fill_viridis_d() +\n  labs(title = \"Weekday vs. weekend parking transactions\",\n       x = NULL,\n       y = \"Percent of transactions\",\n       fill = \"Period\")"
  },
  {
    "objectID": "posts/pittsburgh-parking-covid-change/index.html#neighborhood-level-analysis",
    "href": "posts/pittsburgh-parking-covid-change/index.html#neighborhood-level-analysis",
    "title": "Effect of COVID-19 on Pittsburgh parking transactions",
    "section": "Neighborhood-level analysis",
    "text": "Neighborhood-level analysis\nNext I perform the same analysis at the neighborhood level to see if any areas in the city were particularly affected. I manually aggregated the parking zones up to the neighborhood level. This code reads in that data.\n\ngeocoded_parking_locations &lt;- read_csv(\"post_data/geocoded_parking_locations.csv\")\n\ngeocoded_parking_locations %&gt;%\n  arrange(zone_region, zone)\n\n# A tibble: 62 × 3\n   zone                                 n zone_region\n   &lt;chr&gt;                            &lt;dbl&gt; &lt;chr&gt;      \n 1 354 - Walter/Warrington Lot      11735 Allentown  \n 2 355 - Asteroid Warrington Lot    48240 Allentown  \n 3 417 - Allentown                  35577 Allentown  \n 4 363 - Beechview Lot              13890 Beechview  \n 5 418 - Beechview                  68416 Beechview  \n 6 334 - Taylor Street Lot          95186 Bloomfield \n 7 335 - Friendship Cedarville Lot 188572 Bloomfield \n 8 406 - Bloomfield (On-street)    220290 Bloomfield \n 9 361 - Brookline Lot               6473 Brookline  \n10 419 - Brookline                 173071 Brookline  \n# ℹ 52 more rows\n\n\nThis code does the same aggregation as before, but adds neighborhood in the group_by function.\n\ndf_ts_neighborhood &lt;- data %&gt;%\n  left_join(geocoded_parking_locations) %&gt;%\n  select(zone_region, start, meter_transactions, mobile_transactions) %&gt;%\n  separate(start, into = c(\"start_date\", \"start_time\"), remove = TRUE, sep = \" \") %&gt;%\n  mutate(start_date = ymd(start_date)) %&gt;%\n  group_by(zone_region, start_date) %&gt;%\n  summarize(meter_transactions = sum(meter_transactions),\n            mobile_transactions = sum(mobile_transactions)) %&gt;%\n  ungroup() %&gt;%\n  rowwise() %&gt;%\n  mutate(total_parking_events = meter_transactions + mobile_transactions) %&gt;%\n  ungroup() %&gt;%\n  mutate(year = year(start_date),\n         day_of_year = yday(start_date),\n         week_of_year = week(start_date),\n         weekday = wday(start_date, label = TRUE)) %&gt;%\n  group_by(year, week_of_year) %&gt;% \n  mutate(first_date_of_week = min(start_date)) %&gt;% \n  ungroup() %&gt;% \n  select(zone_region, start_date, day_of_year, week_of_year, weekday, everything())\n\nMost of the parking transactions occur in ~13 neighborhoods, so I will focus on those.\n\nzone_fct &lt;- df_ts_neighborhood %&gt;% \n  group_by(zone_region) %&gt;% \n  summarize(total_parking_events = sum(total_parking_events)) %&gt;% \n  arrange(total_parking_events) %&gt;% \n  pull(zone_region)\n\ndf_ts_neighborhood %&gt;% \n  group_by(zone_region) %&gt;% \n  summarize(total_parking_events = sum(total_parking_events)) %&gt;% \n  mutate(zone_region = factor(zone_region, levels = zone_fct)) %&gt;% \n  ggplot(aes(total_parking_events, zone_region)) +\n  geom_col() +\n  scale_x_comma() +\n  labs(x = \"Total parking transactions\",\n       y = \"Neighborhood\")\n\n\n\n\n\ntop_zone_regions &lt;- df_ts_neighborhood %&gt;% \n  group_by(zone_region) %&gt;% \n  summarize(total_parking_events = sum(total_parking_events)) %&gt;% \n  arrange(desc(total_parking_events)) %&gt;% \n  select(zone_region) %&gt;% \n  slice(1:13)\n\n\n2020 vs. historical average\nThis code calculates the weekly % difference in parking transactions between 2020 and the previous years, by neighborhood.\n\ndf_historical &lt;- df_ts_neighborhood %&gt;% \n  arrange(zone_region, start_date) %&gt;% \n  filter(start_date &lt; \"2020-01-01\") %&gt;% \n  group_by(zone_region, year, week_of_year) %&gt;% \n  summarize(total_parking_events = sum(total_parking_events)) %&gt;% \n  ungroup()\n\ndf_historical &lt;- df_historical %&gt;% \n  group_by(zone_region, week_of_year) %&gt;% \n  summarize(median_parking_events_historical = median(total_parking_events)) %&gt;% \n  ungroup()\n\ndf_2020 &lt;- df_ts_neighborhood %&gt;% \n  filter(start_date &gt;= \"2020-01-01\", start_date &lt;= \"2020-10-05\") %&gt;% \n  complete(zone_region, week_of_year, fill = list(total_parking_events = 0)) %&gt;% \n  group_by(zone_region, week_of_year, first_date_of_week) %&gt;% \n  summarize(total_parking_events = sum(total_parking_events)) %&gt;% \n  ungroup()\n\ndf_combined &lt;- df_2020 %&gt;% \n  left_join(df_historical, by = c(\"zone_region\", \"week_of_year\")) %&gt;%\n  mutate(pct_difference = (total_parking_events - median_parking_events_historical) / median_parking_events_historical)\n\nThis shows that all the neighborhoods experienced severe drops in parking transactions. Only the North Shore returned to regular levels, and even then only temporarily.\n\nline_chart &lt;- df_combined %&gt;%\n  semi_join(top_zone_regions) %&gt;% \n  rename(neighborhood = zone_region) %&gt;% \n  mutate(pct_difference = round(pct_difference, 2)) %&gt;% \n  ggplot(aes(first_date_of_week, pct_difference, group = neighborhood)) +\n  geom_hline(yintercept = 0, lty = 2, alpha = .5) +\n  geom_line(alpha = .3) +\n  scale_y_percent() +\n  labs(title = \"2020 vs. historical average in top neighborhoods\",\n       x = \"Date\",\n       y = \"Percent difference\")\n\nline_chart %&gt;% \n  ggplotly(tooltip = c(\"neighborhood\", \"pct_difference\"))\n\n\n\n\n\nThis tile chart shows a similar pattern.\n\ntile_chart &lt;- df_combined %&gt;% \n  semi_join(top_zone_regions) %&gt;% \n  mutate(zone_region = factor(zone_region, levels = zone_fct),\n         ) %&gt;% \n  mutate(pct_difference = pct_difference %&gt;% round(2),\n         pct_difference_tooltip = pct_difference %&gt;% round(2) %&gt;% percent(accuracy = 1)) %&gt;% \n  ggplot(aes(week_of_year, zone_region, fill = pct_difference)) +\n  geom_tile() +\n  scale_fill_viridis_c(labels = percent) +\n  scale_x_continuous(expand = c(0,0)) +\n  scale_y_discrete(expand = c(0,0)) +\n  labs(title = \"2020 vs. historical average in top neighborhoods\",\n       x = \"Week of year\",\n       y = NULL,\n       fill = \"Percent difference\") +\n  theme(panel.grid = element_blank(),\n        legend.position = \"bottom\")\n\nggplotly(tile_chart, tooltip = c(\"zone_region\", \"week_of_year\", \"pct_difference\")) %&gt;% \n  layout(xaxis = list(showgrid = F),\n         yaxis = list(showgrid = F))\n\n\n\n\n\nAggregating the neighborhoods into boxplots shows that the drop in transactions mirrors the overall trend.\n\ndf_combined %&gt;% \n  semi_join(top_zone_regions) %&gt;% \n  ggplot(aes(first_date_of_week, pct_difference, group = week_of_year)) +\n  geom_boxplot(outlier.alpha = .3, outlier.size = 1) +\n  geom_hline(yintercept = 0, lty = 2, alpha = .5) +\n  scale_y_percent() +\n  labs(title = \"2020 vs. historical average\",\n       subtitle = \"Top 13 neighborhoods\",\n       x = \"Date\",\n       y = \"Percent difference\")"
  },
  {
    "objectID": "posts/pittsburgh-parking-covid-change/index.html#week-to-week-difference",
    "href": "posts/pittsburgh-parking-covid-change/index.html#week-to-week-difference",
    "title": "Effect of COVID-19 on Pittsburgh parking transactions",
    "section": "2020 week-to-week difference",
    "text": "2020 week-to-week difference\nIn terms of week-to-week difference in parking transactions, the week starting March 18th was the worst, with a -84% drop from the week before.\n\nweekly_pct_difference_df &lt;- data_2020 %&gt;% \n  mutate(weekly_difference = total_parking_transactions - lag(total_parking_transactions),\n         weekly_pct_difference = weekly_difference / lag(total_parking_transactions))\n\nweekly_pct_difference_df %&gt;% \n  mutate(max_drop_flag = weekly_pct_difference == min(weekly_pct_difference, na.rm = TRUE),\n         max_drop = case_when(max_drop_flag == TRUE ~ weekly_pct_difference,\n                              max_drop_flag == FALSE ~ NA_real_)) %&gt;% \n  ggplot(aes(first_date_of_week, weekly_pct_difference)) +\n  geom_line() +\n  geom_point() +\n  geom_point(aes(y = max_drop), color = \"red\", size = 3) +\n  ggrepel::geom_label_repel(aes(y = max_drop, label = scales::percent(max_drop)),\n                            direction = \"x\") +\n  scale_y_percent() +\n  coord_cartesian(ylim = c(-1, 1)) +\n  labs(title = \"Week-to-week difference\",\n       x = \"Date\",\n       y = \"Percent difference\")"
  },
  {
    "objectID": "posts/suburbanization-of-allegheny-county/index.html",
    "href": "posts/suburbanization-of-allegheny-county/index.html",
    "title": "Suburbanization of Allegheny County",
    "section": "",
    "text": "This March, researchers at the University of Georgia and Florida State University released the HHUUD10 dataset, which contains estimates of the number of housing units for decennial census years 1940-2010 and 2019. A “housing unit” could be a studio apartment or 5 bedroom single-family home. The data uses 2010 census tracts, which allows for historical comparison of housing trends across constant geometry. The full paper explains the approach.\nThis paper and the dataset can be used for a wide variety of socioeconomic issues. I will focus on suburbanization trends in the Pittsburgh area."
  },
  {
    "objectID": "posts/suburbanization-of-allegheny-county/index.html#overall-trend",
    "href": "posts/suburbanization-of-allegheny-county/index.html#overall-trend",
    "title": "Suburbanization of Allegheny County",
    "section": "Overall trend",
    "text": "Overall trend\n\nFix date formatting\nSince the data comes in a wide format, I pivot it long and fix up the year column to make it easy to graph with.\n\nac_housing_hu &lt;- ac_housing |&gt; \n  select(GEOID10, starts_with(\"hu\")) |&gt; \n  pivot_longer(cols = starts_with(\"hu\"), names_to = \"year\", values_to = \"housing_units\")\n\nyear_lookup &lt;- ac_housing_hu |&gt; \n  st_drop_geometry() |&gt; \n  distinct(year) |&gt; \n  mutate(year_fixed = c(1940, 1950, 1960, 1970, 1980, 1990, 2000, 2010, 2019))\n\nac_housing_hu &lt;- ac_housing_hu |&gt; \n  left_join(year_lookup) |&gt; \n  select(-year) |&gt; \n  rename(year = year_fixed)\n\nglimpse(ac_housing_hu)\n\nRows: 3,618\nColumns: 4\n$ GEOID10       &lt;chr&gt; \"42003560500\", \"42003560500\", \"42003560500\", \"4200356050…\n$ geometry      &lt;POLYGON [US_survey_foot]&gt; POLYGON ((1373906 410182, 1..., POL…\n$ housing_units &lt;dbl&gt; 1349, 1509, 1515, 1441, 1424, 1433, 1381, 1349, 1487, 13…\n$ year          &lt;dbl&gt; 1940, 1950, 1960, 1970, 1980, 1990, 2000, 2010, 2019, 19…\n\n\nThe number of housing units in the county stagnated after 1960, which is expected given the collapse of the steel industry.\n\nac_housing_hu |&gt; \n  st_drop_geometry() |&gt; \n  mutate(year = as.character(year) |&gt; fct_inorder()) |&gt; \n  group_by(year) |&gt; \n  summarize(housing_units = sum(housing_units)) |&gt; \n  ungroup() |&gt; \n  ggplot(aes(year, housing_units, group = 1)) +\n  geom_line() +\n  geom_point() +\n  scale_y_comma() +\n  labs(x = \"Year\",\n       y  = \"Housing units\")\n\n\n\n\nThe decennial difference in “gross” housing units also shows that growth stagnated after 1960.\n\nac_housing_hu |&gt; \n  st_drop_geometry() |&gt; \n  mutate(year = as.character(year) |&gt; fct_inorder()) |&gt; \n  group_by(year) |&gt; \n  summarize(housing_units = sum(housing_units)) |&gt; \n  ungroup() |&gt; \n  mutate(diff = housing_units - lag(housing_units)) |&gt; \n  ggplot(aes(year, diff, group = 1)) +\n  geom_line() +\n  geom_point() +\n  scale_y_comma(prefix = \"+ \") +\n  coord_cartesian(ylim = c(0, 90000)) +\n  labs(title = \"Growth stagnated after 1960\",\n       x = \"Year\",\n       y  = \"Change in housing units\")"
  },
  {
    "objectID": "posts/suburbanization-of-allegheny-county/index.html#change-from-1940-to-2019",
    "href": "posts/suburbanization-of-allegheny-county/index.html#change-from-1940-to-2019",
    "title": "Suburbanization of Allegheny County",
    "section": "Change from 1940 to 2019",
    "text": "Change from 1940 to 2019\nThis interactive map shows the areas that gained or lost the most housing units from 1940-2019. Dense housing around industrial areas along the Allegheny and Monongahela Rivers was erased. Homestead and Braddock stand out.\n\nhu_diff &lt;- ac_housing_hu |&gt; \n  group_by(GEOID10) |&gt; \n  filter(year == min(year) | year == max(year)) |&gt; \n  ungroup() |&gt; \n  select(GEOID10, year, housing_units) |&gt; \n  as_tibble() |&gt; \n  pivot_wider(names_from = year, names_prefix = \"units_\", values_from = housing_units) |&gt; \n  mutate(diff = units_2019 - units_1940) |&gt; \n  st_as_sf()\n\npal &lt;- colorNumeric(\n  palette = \"viridis\",\n  domain = hu_diff$diff)\n\nleaflet_map &lt;- hu_diff |&gt; \n  mutate(diff_formatted = comma(diff, accuracy = 1),\n         diff_label = str_c(\"Census tract: \", GEOID10, \"&lt;br/&gt;\", \"Difference: \", diff_formatted)) |&gt; \n  st_transform(crs = 4326) |&gt; \n  leaflet() |&gt; \n  setView(lat = 40.441606, lng = -80.010957, zoom = 10) |&gt; \n  addProviderTiles(providers$Stamen.TonerLite,\n                   options = providerTileOptions(noWrap = TRUE,\n                                                 minZoom = 9),\n                   group = \"Base map\") |&gt; \n  addPolygons(popup = ~ diff_label,\n              fillColor = ~pal(diff),\n              fillOpacity = .7,\n              color = \"black\",\n              weight = 1,\n              group = \"Housing\") |&gt; \n  addLegend(\"bottomright\", pal = pal, values = ~diff,\n            title = \"Difference\",\n            opacity = 1) |&gt; \n  addLayersControl(overlayGroups = c(\"Base map\", \"Housing\"),\n                   options = layersControlOptions(collapsed = FALSE)) |&gt; \n  addFullscreenControl()\n\nleaflet_map\n\n\n\n\n#frameWidget(leaflet_map, options=frameOptions(allowfullscreen = TRUE))\n\nThe North Side and the Hill were targets of “urban renewal” in the middle of the century. Dense housing in heavily African-American communities were demolished to make way for an opera house, the 279 and 579 highways, and parking lots. The highways are directly related to the white flight exodus to the suburbs, especially in the west and north. Those highways made it easy for the new suburbanites to commute longer distances in single passenger vehicles.\nThese graphs shows that the areas with the most housing in 1940 lost thousands of units, while outlying areas gained thousands of units.\n\nslope_graph_anim &lt;- hu_diff |&gt; \n  as_tibble() |&gt; \n  select(-geometry) |&gt;\n  arrange(desc(units_1940)) |&gt; \n  pivot_longer(cols = c(units_1940, units_2019), names_to = \"year\", values_to = \"housing_units\") |&gt; \n  mutate(year = str_remove(year, \"^units_\")) |&gt; \n  mutate(order = row_number()) |&gt; \n  ggplot(aes(year, housing_units)) +\n  geom_line(aes(group = GEOID10), alpha = .1) +\n  geom_point(aes(group = str_c(year, GEOID10)), alpha = .05) +\n  scale_y_comma() +\n  transition_reveal(order) +\n  labs(title = \"Housing unit change from 1940-2019\",\n       subtitle = \"From areas with the most units in 1940 to the least\",\n       x = \"Year\",\n       y = \"Housing units\") +\n  theme(panel.grid.minor.y = element_blank(),\n        panel.grid.major.y = element_blank(),\n        panel.grid.major.x = element_blank(),\n        panel.border = element_blank(),\n        axis.title.x = element_blank())\n\nslope_graph_anim &lt;- animate(slope_graph_anim, duration = 10, fps = 40, end_pause = 60)\n\nslope_graph_anim\n\n\n\n\n\nhu_diff |&gt; \n  ggplot(aes(units_1940, units_2019)) +\n  geom_abline(lty = 2) +\n  geom_point(alpha = .2) +\n  annotate(\"text\", x = 3500, y = 3800, label = \"No change\", angle = 45) +\n  annotate(\"text\", x = 300, y = 4500, label = \"Gain\") +\n  annotate(\"text\", x = 4300, y = 100, label = \"Loss\") +\n  tune::coord_obs_pred() +\n  scale_x_comma() +\n  scale_y_comma() +\n  labs(title = \"Change in housing units\",\n       x = \"Units in 1940\",\n       y = \"Units in 2019\")\n\n\n\n\n\nMoving north and west\nThese maps show the estimates of housing units for each decennial period. Outlying areas in the north and west, directly served by the new highway system, gained thousands of housing units.\n\nac_housing_hu |&gt; \n  ggplot() +\n  geom_sf(aes(fill = housing_units), color = NA) +\n  scale_fill_viridis_c(\"Housing units\", labels = comma) +\n  facet_wrap(~year) +\n  theme_bw() +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        axis.text.x = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks = element_blank())\n\n\n\n\nGeographically larger Census tracts gained more of the % of total housing over time.\n\nac_sqmi &lt;- ac_housing |&gt; \n  select(GEOID10, starts_with(\"sqmi\")) |&gt; \n  st_drop_geometry() |&gt; \n  as_tibble() |&gt; \n  pivot_longer(starts_with(\"sqmi\"), names_to = \"year\", values_to = \"sqmi\")\n\nac_sqmi_year &lt;- ac_sqmi |&gt; \n  distinct(year) |&gt; \n  mutate(year_fixed = c(1940, 1950, 1960, 1970, 1980, 1990, 2000, 2010, 2019))\n\nac_sqmi &lt;- ac_sqmi |&gt; \n  left_join(ac_sqmi_year) |&gt; \n  select(-year) |&gt; \n  rename(year = year_fixed)\n\nac_density &lt;- ac_housing_hu |&gt; \n  select(GEOID10, year, housing_units) |&gt; \n  left_join(ac_sqmi) |&gt; \n  mutate(density = housing_units / sqmi)\n\ncurve_anim &lt;- ac_density |&gt; \n  st_drop_geometry() |&gt; \n  select(GEOID10, year, housing_units, sqmi) |&gt; \n  mutate(year = as.character(year) |&gt; fct_inorder()) |&gt; \n  arrange(year, sqmi) |&gt; \n  group_by(year) |&gt; \n  mutate(housing_units_cumsum = cumsum(housing_units),\n         pct_units = housing_units_cumsum / sum(housing_units)) |&gt; \n  ungroup() |&gt; \n  ggplot(aes(sqmi, pct_units, color = year)) +\n  geom_line() +\n  scale_y_percent() +\n  labs(title = \"Housing moves to outlying areas over time\",\n       subtitle = \"Year: {closest_state}\",\n       x = \"Square miles\",\n       y = \"Cumulative percent of units\",\n       color = \"Year\") +\n  transition_states(year) +\n  shadow_mark()\n\ncurve_anim &lt;- animate(curve_anim, duration = 10, fps = 20)\n\ncurve_anim\n\n\n\n\n\n\nHousing peaks\nThis shows the year that each census tract peaked in terms of housing units. The areas that attracted heavy industry in the late 19th/early 20th century (and built housing nearby to support it) were crushed by the collapse of that industry. The single census tract that makes up “Downtown” has clawed back some housing recently.\n\nac_housing_hu |&gt; \n  group_by(GEOID10) |&gt; \n  filter(housing_units == max(housing_units)) |&gt; \n  ungroup() |&gt; \n  rename(max_year = year) |&gt; \n  ggplot() +\n  geom_sf(aes(fill = max_year), color = NA) +\n  scale_fill_viridis_c(direction = -1) +\n  labs(title = \"Year of peak housing\",\n       fill = \"Peak\") +\n  theme(panel.grid.major = element_blank(),\n        axis.text.x = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks = element_blank(),\n        panel.border = element_blank())"
  },
  {
    "objectID": "posts/suburbanization-of-allegheny-county/index.html#housing-moves-away-from-the-center",
    "href": "posts/suburbanization-of-allegheny-county/index.html#housing-moves-away-from-the-center",
    "title": "Suburbanization of Allegheny County",
    "section": "Housing moves away from the center",
    "text": "Housing moves away from the center\nA major trend from 1940-2019 is the significant shift in housing from around the core to outlying suburbs. This code calculates the distance between each tract and the “Downtown” tract (42003020100), and plots the number of units compared to that distance.\n\ndowntown_tract &lt;- ac_housing_hu |&gt; \n  filter(GEOID10 == \"42003020100\") |&gt; \n  distinct(GEOID10, geometry) |&gt; \n  mutate(centroid = st_point_on_surface(geometry)) |&gt; \n  st_set_geometry(\"centroid\") |&gt; \n  select(-geometry)\n\ndistance_anim &lt;- ac_housing_hu |&gt; \n  select(GEOID10, year, housing_units) |&gt; \n  mutate(centroid = st_point_on_surface(geometry),\n         geoid = str_c(GEOID10, year, sep = \"_\"),\n         year = as.integer(year)) |&gt; \n  mutate(distance_to_downtown = st_distance(centroid, downtown_tract) |&gt; as.numeric() / 5280) |&gt; \n  ggplot(aes(distance_to_downtown, housing_units)) +\n  geom_point(aes(group = GEOID10), alpha = .3) +\n  geom_smooth(aes(group = year)) +\n  scale_x_continuous() +\n  scale_y_comma() +\n  transition_states(year, \n                    state_length = 10) +\n  labs(title = \"Housing has moved farther away from downtown\",\n       subtitle = \"{closest_state}\",\n       x = \"Miles from downtown\",\n       y = \"Housing units\") +\n  theme(panel.grid.minor = element_blank())\n\ndistance_anim &lt;- animate(distance_anim)\n\ndistance_anim"
  },
  {
    "objectID": "posts/suburbanization-of-allegheny-county/index.html#land-use",
    "href": "posts/suburbanization-of-allegheny-county/index.html#land-use",
    "title": "Suburbanization of Allegheny County",
    "section": "Land use",
    "text": "Land use\nThe HHUUD10 data also contains estimates for the percentage of land in a tract that is “developed” for the years 1992, 2001, and 2011. “Developed” in this context means “covered by an urban land use”.\n\nac_dev &lt;- ac_housing |&gt; \n  select(GEOID10, starts_with(\"pdev\")) |&gt; \n  pivot_longer(cols = starts_with(\"pdev\"), names_to = \"year\", values_to = \"pct_dev\") \n\ndev_years &lt;- ac_dev |&gt; \n  st_drop_geometry() |&gt; \n  distinct(year) |&gt; \n  mutate(year_fixed = c(1992, 2001, 2011))\n\nac_dev &lt;- ac_dev |&gt; \n  left_join(dev_years) |&gt; \n  select(-year) |&gt; \n  rename(year = year_fixed)\n\nac_dev |&gt; \n  ggplot() +\n  geom_sf(aes(fill = pct_dev), color = NA) +\n  facet_wrap(~year) +\n  scale_fill_viridis_c(labels = percent) +\n  labs(title = \"Percent of land that is developed\",\n       fill = NULL) +\n  theme_bw() +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        axis.text.x = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks = element_blank())\n\n\n\n\nI find it interesting that more of the South Hills is developed than the North Hills. I would have expected more development in the North Hills due to the McKnight Road area and Wexford. My guess is that the tracts in the North Hills cover more land area, which decreases the % that is developed. Conversely, the tracts in the South Hills cover less land area, and less of the South Hills is useful for development because of steep hills and creeks. This concentrates development in a smaller area."
  },
  {
    "objectID": "posts/suburbanization-of-allegheny-county/index.html#conclusion",
    "href": "posts/suburbanization-of-allegheny-county/index.html#conclusion",
    "title": "Suburbanization of Allegheny County",
    "section": "Conclusion",
    "text": "Conclusion\nOver the past 80 years, Allegheny County has lost a significant amount of housing in its core urban area. Much of this is directly related to the collapse of the steel industry and “urban renewal”. At the same time, new housing development has been pushed out to the suburbs. This is a loss in terms of housing density, which has become a major discussion point in urban planning over the past 20 years.\nHigher density areas have a lower per capita carbon footprint due to non-car commute modes and agglomeration effects. Higher density also does not expand the wildland-urban interface. This leaves more land for the natural environment, moves humans away from dangers such as wildfires, and lowers the frequency of interaction between wild animals and humans, which can transfer disease (coronavirus, ebola). It will be interesting to see whether the suburbanization trend continues after the initial shocks of COVID-19 pandemic subside."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ctompkins_quarto_blog",
    "section": "",
    "text": "Post With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nOct 8, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2023\n\n\nTristan O’Malley\n\n\n\n\n\n\n  \n\n\n\n\nSuburbanization of Allegheny County\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 18, 2022\n\n\nR package build\n\n\n\n\n\n\n  \n\n\n\n\nMaking a Venn diagram in Shiny\n\n\n\n\n\n\n\nshiny\n\n\n\n\n\n\n\n\n\n\n\nMar 12, 2022\n\n\nConor Tompkins\n\n\n\n\n\n\n  \n\n\n\n\nEffect of Geographic Resolution on ebirdst Abundance\n\n\n\n\n\n\n\nR\n\n\neBird\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2021\n\n\nConor Tompkins\n\n\n\n\n\n\n  \n\n\n\n\nPittsburgh Riverhounds under Coach Lilley\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 5, 2021\n\n\n\n\n\n\n  \n\n\n\n\nDriving Alone vs. Public Transportation in Pittsburgh\n\n\nMapping commuter modes with bivariate discretized bins\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 29, 2021\n\n\n\n\n\n\n  \n\n\n\n\nHouse Price Estimator Dashboard\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJan 28, 2021\n\n\n\n\n\n\n  \n\n\n\n\nBike rental access in Pittsburgh\n\n\n\n\n\n\n\nR\n\n\nPittsburgh\n\n\nHealthy Ride\n\n\nWPRDC\n\n\n\n\n\n\n\n\n\n\n\nDec 4, 2020\n\n\n\n\n\n\n  \n\n\n\n\nShifting political winds\n\n\nOr, drawing arrows on maps\n\n\n\n\nR\n\n\nPolitics\n\n\n\n\n\n\n\n\n\n\n\nNov 13, 2020\n\n\n\n\n\n\n  \n\n\n\n\nAnalyzing major commuter routes in Allegheny County\n\n\n\n\n\n\n\nR\n\n\nAllegheny County\n\n\nCommuters\n\n\nCensus\n\n\nMapbox\n\n\n\n\n\n\n\n\n\n\n\nOct 25, 2020\n\n\n\n\n\n\n  \n\n\n\n\nGraphing volatile home values in U.S. metro areas\n\n\n\n\n\n\n\nHousing\n\n\nZillow\n\n\n\n\n\n\n\n\n\n\n\nOct 21, 2020\n\n\n\n\n\n\n  \n\n\n\n\nEffect of COVID-19 on Pittsburgh parking transactions\n\n\n\n\n\n\n\nPittsburgh\n\n\nParking\n\n\nCOVID-19\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2020\n\n\n\n\n\n\n  \n\n\n\n\nMapping BosWash commuter patterns with Flowmap.blue\n\n\n\n\n\n\n\nCensus\n\n\nCommuter patterns\n\n\n\n\n\n\n\n\n\n\n\nSep 25, 2020\n\n\n\n\n\n\n  \n\n\n\n\nPittsburgh City Boundary Model Leaflet Map\n\n\n\n\n\n\n\nR\n\n\nPittsburgh\n\n\nAllegheny County\n\n\n\n\n\n\n\n\n\n\n\nAug 23, 2020\n\n\nConor Tompkins\n\n\n\n\n\n\n  \n\n\n\n\nModeling the Pittsburgh City Boundary\n\n\n\n\n\n\n\n\n\n\n\n\nAug 14, 2020\n\n\nConor Tompkins\n\n\n\n\n\n\n  \n\n\n\n\nComparing Healthy Ride Usage Pre And “Post” COVID-19\n\n\n\n\n\n\n\n\n\n\n\n\nAug 14, 2020\n\n\nConor Tompkins\n\n\n\n\n\n\n  \n\n\n\n\nTime series clustering COVID-19 case data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJul 20, 2020\n\n\nConor Tompkins\n\n\n\n\n\n\nNo matching items"
  }
]