[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a data scientist in the Pittsburgh area with an interest in data visualization, statistical programming, birdwatching, and civic data."
  },
  {
    "objectID": "posts/clustering-allegheny-county-census-tracts-with-pca-and-kmeans/index.html",
    "href": "posts/clustering-allegheny-county-census-tracts-with-pca-and-kmeans/index.html",
    "title": "Clustering Allegheny County Census Tracts With PCA and k-means",
    "section": "",
    "text": "In this post I will use the census API discussed in the last post to cluster the Allegheny County census tracts using PCA and k-means."
  },
  {
    "objectID": "posts/clustering-allegheny-county-census-tracts-with-pca-and-kmeans/index.html#setup",
    "href": "posts/clustering-allegheny-county-census-tracts-with-pca-and-kmeans/index.html#setup",
    "title": "Clustering Allegheny County Census Tracts With PCA and k-means",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(tidycensus)\nlibrary(tigris)\nlibrary(sf)\nlibrary(broom)\nlibrary(ggfortify)\nlibrary(viridis)\nlibrary(janitor)\nlibrary(scales)\nlibrary(ggthemes)\n\noptions(tigris_use_cache = TRUE)\n\ntheme_set(theme_minimal())\n\n\ncensus_vars &lt;- load_variables(2010, \"sf1\", cache = TRUE)\n\nCensus tracts are small geographic areas analogous to local neighborhoods. This is a map of all the tracts in Allegheny County, for reference:"
  },
  {
    "objectID": "posts/clustering-allegheny-county-census-tracts-with-pca-and-kmeans/index.html#download-data",
    "href": "posts/clustering-allegheny-county-census-tracts-with-pca-and-kmeans/index.html#download-data",
    "title": "Clustering Allegheny County Census Tracts With PCA and k-means",
    "section": "Download data",
    "text": "Download data\nThis code downloads data about the ethnicities resident in the tracts and calculates them as a % of the tract population.\n\nvars_demo &lt;- c(white = \"P005003\", \n          black = \"P005004\", \n          asian = \"P005006\", \n          hispanic = \"P004003\")\n#age vars men and women\n#P0120003:P0120049\n\nget_decennial(geography = \"tract\", \n              variables = vars_demo,\n              state = \"PA\", \n              county = \"Allegheny\", \n              year = 2010,\n              geometry = FALSE,\n              summary_var = \"P001001\") %&gt;% \n  arrange(GEOID) %&gt;% \n  mutate(value = value / summary_value) %&gt;% \n  select(-summary_value) %&gt;% \n  spread(variable, value) %&gt;% \n  rename_at(vars(\"white\", \"black\", \"asian\", \"hispanic\"), funs(str_c(\"pct_\", .))) -&gt; allegheny_demographics\n\nallegheny_demographics &lt;- replace(allegheny_demographics, is.na(allegheny_demographics), 0)\n\nThis code downloads information about the housing stock in each tract, specifically what % of housing units are owned outright, owned with a loan, or rented.\n\nvars_housing &lt;- c(units_owned_loan = \"H011002\",\n          units_owned_entire = \"H011003\",\n          units_rented = \"H011004\")\n\nget_decennial(geography = \"tract\", \n              variables = vars_housing,\n              state = \"PA\", \n              county = \"Allegheny\", \n              year = 2010,\n              geometry = FALSE,\n              summary_var = \"H011001\") %&gt;% \n  arrange(GEOID) %&gt;% \n  mutate(value = value / summary_value) %&gt;% \n  select(-summary_value) %&gt;% \n  spread(variable, value) %&gt;% \n  rename_at(vars(\"units_owned_entire\", \"units_owned_loan\", \"units_rented\"), funs(str_c(\"pct_\", .))) -&gt; allegheny_housing\n\nallegheny_housing &lt;- replace(allegheny_housing, is.na(allegheny_housing), 0)\n\nThis code requests the total population of each tract.\n\n#originally I used age-sex variables, but they were not useful\nvars_age_total &lt;- census_vars %&gt;% \n  filter(name == \"P012001\")\n\nget_decennial(geography = \"tract\", \n              variables = vars_age_total$name,\n              state = \"PA\", \n              county = \"Allegheny\", \n              year = 2010,\n              geometry = FALSE,\n              summary_var = \"P012001\") %&gt;% \n  rename(var_id = variable) %&gt;% \n  mutate(value = value / summary_value) %&gt;% \n  spread(var_id, value) -&gt; allegheny_age_sex\n\ncolnames(allegheny_age_sex) &lt;- c(\"GEOID\", \"NAME\", \"summary_value\", vars_age_total$label)\n\nallegheny_age_sex %&gt;% \n  clean_names() %&gt;% \n  rename(GEOID = geoid,\n         NAME = name,\n         total_population = summary_value) -&gt; allegheny_age_sex\n\nallegheny_age_sex &lt;- replace(allegheny_age_sex, is.na(allegheny_age_sex), 0)\n\nallegheny_age_sex %&gt;% \n  select(GEOID, NAME, total_population) -&gt; allegheny_age_sex\n\nThis code requests the geometry of each tract that I will use to map them later.\n\nget_decennial(geography = \"tract\", \n              variables = vars_housing,\n              state = \"PA\", \n              county = \"Allegheny\",\n              year = 2010,\n              geometry = TRUE) %&gt;% \n  select(-c(variable, value)) %&gt;% \n  distinct(GEOID) -&gt; allegheny_geo\n\nThis joins the 4 dataframes together.\n\nallegheny_geo %&gt;% \n  left_join(allegheny_housing) %&gt;% \n  left_join(allegheny_demographics) %&gt;% \n  left_join(allegheny_age_sex) %&gt;% \n  mutate(id = str_c(GEOID, NAME, sep = \" | \")) -&gt; allegheny"
  },
  {
    "objectID": "posts/clustering-allegheny-county-census-tracts-with-pca-and-kmeans/index.html#exploratory-graph",
    "href": "posts/clustering-allegheny-county-census-tracts-with-pca-and-kmeans/index.html#exploratory-graph",
    "title": "Clustering Allegheny County Census Tracts With PCA and k-means",
    "section": "Exploratory graph",
    "text": "Exploratory graph\nThis graph compares the percent of white residents to the remaining variables in the data. pct_white is on the x axis of each of the smaller charts. Note that each chart’s Y axis has its own scale. It is already obvious that pct_white and pct_black are negatively correlated with each other.\n\nallegheny %&gt;%\n  #st_set_geometry(NULL) %&gt;% \n  st_drop_geometry() %&gt;%\n  select(contains(\"pct\")) %&gt;% \n  gather(variable, value, -pct_white) %&gt;% \n  ggplot(aes(pct_white, value)) +\n  geom_point(alpha = .5) +\n  geom_smooth() +\n  facet_wrap(~variable, scales = \"free\", nrow = 3, strip.position=\"left\") +\n  scale_x_continuous(label = percent) +\n  scale_y_continuous(label = percent) +\n  labs(x = NULL) +\n  theme_bw() +\n  theme(strip.placement = \"outside\")\n\n\n\n\n\n\n\n\nThis code plots the total population against the other variables:\n\nallegheny %&gt;%\n  st_drop_geometry() %&gt;%\n  select(contains(\"pct\"), total_population) %&gt;% \n  gather(variable, value, -total_population) %&gt;% \n  ggplot(aes(total_population, value)) +\n  geom_point(alpha = .5) +\n  geom_smooth() +\n  facet_wrap(~variable, scales = \"free\", nrow = 3, strip.position=\"left\") +\n  scale_x_continuous(label = comma) +\n  scale_y_continuous(label = percent) +\n  theme_bw() +\n  theme(strip.placement = \"outside\")"
  },
  {
    "objectID": "posts/clustering-allegheny-county-census-tracts-with-pca-and-kmeans/index.html#prepare-for-pca",
    "href": "posts/clustering-allegheny-county-census-tracts-with-pca-and-kmeans/index.html#prepare-for-pca",
    "title": "Clustering Allegheny County Census Tracts With PCA and k-means",
    "section": "Prepare for PCA",
    "text": "Prepare for PCA\nThis code prepares the data for PCA:\n\nallegheny %&gt;%\n  select(-c(id, GEOID, NAME)) %&gt;% \n  st_drop_geometry() %&gt;% \n  remove_rownames() -&gt; allegheny_pca\n\nallegheny_pca %&gt;% \n  prcomp(scale = TRUE) -&gt; pc\n\n\npc %&gt;% \n  tidy(\"pcs\")\n\n# A tibble: 8 × 4\n     PC std.dev percent cumulative\n  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n1     1  1.94   0.471        0.471\n2     2  1.28   0.205        0.676\n3     3  0.913  0.104        0.780\n4     4  0.789  0.0778       0.858\n5     5  0.769  0.0739       0.932\n6     6  0.658  0.0540       0.986\n7     7  0.323  0.0130       0.999\n8     8  0.0961 0.00115      1    \n\n\n\npc %&gt;% \n  augment(data = allegheny_pca) %&gt;% \n  as_tibble() %&gt;% \n  mutate(GEOID = allegheny %&gt;% pull(GEOID)) %&gt;% \n  select(.rownames, GEOID, everything()) -&gt; df_au\n\n\ndf_au %&gt;% \n  head()\n\n# A tibble: 6 × 18\n  .rownames GEOID   pct_units_owned_entire pct_units_owned_loan pct_units_rented\n  &lt;chr&gt;     &lt;chr&gt;                    &lt;dbl&gt;                &lt;dbl&gt;            &lt;dbl&gt;\n1 1         420034…                  0.213                0.750           0.0366\n2 2         420034…                  0.181                0.629           0.190 \n3 3         420034…                  0.245                0.685           0.0692\n4 4         420034…                  0.336                0.501           0.164 \n5 5         420034…                  0.147                0.418           0.435 \n6 6         420034…                  0.168                0.432           0.400 \n# ℹ 13 more variables: pct_asian &lt;dbl&gt;, pct_black &lt;dbl&gt;, pct_hispanic &lt;dbl&gt;,\n#   pct_white &lt;dbl&gt;, total_population &lt;dbl&gt;, .fittedPC1 &lt;dbl&gt;,\n#   .fittedPC2 &lt;dbl&gt;, .fittedPC3 &lt;dbl&gt;, .fittedPC4 &lt;dbl&gt;, .fittedPC5 &lt;dbl&gt;,\n#   .fittedPC6 &lt;dbl&gt;, .fittedPC7 &lt;dbl&gt;, .fittedPC8 &lt;dbl&gt;\n\n\nThis shows how the PCs explain the variance in the data. As explained earlier, the first few PCs explain most of the variance in the data.\n\npc %&gt;% \n  tidy(\"pcs\") %&gt;%\n  select(-std.dev) %&gt;% \n  gather(measure, value, -PC) %&gt;% \n  mutate(measure = case_when(measure == \"percent\" ~ \"Percent\",\n                             measure == \"cumulative\" ~ \"Cumulative\")) %&gt;% \n    ggplot(aes(PC, value)) +\n    geom_line() +\n    geom_point() +\n    facet_wrap(~measure) +\n    labs(title = \"Variance explained by each principal component\",\n         x = \"Principal Component\",\n         y = NULL) +\n    scale_x_continuous(breaks = 1:8) +\n    scale_y_continuous(label = percent) +\n  theme_bw()\n\n\n\n\n\n\n\n\nThis shows how the PCA function rearranged the data to maximize the variance in the first few PCs. PC1 is largely defined by the percent of a tract that is white or black, the percent of housing units that are owned, and the total population of the tract. The “pct_white” and “pct_black” arrows point in opposite directions, which reflects Pittsburgh’s status as a segregated city.\nPC2 explains less of the variance, and is influenced by the percent of a tract that is hispanic, asian, or black.\n\nallegheny %&gt;% \n  select(-c(id, GEOID)) %&gt;% \n  st_drop_geometry() %&gt;%\n  nest() %&gt;% \n  mutate(pca = map(data, ~ prcomp(.x %&gt;% select(-NAME), \n                                  center = TRUE, scale = TRUE)),\n         pca_aug = map2(pca, data, ~augment(.x, data = .y))) -&gt; allegheny_pca2\n\nallegheny_pca2 %&gt;% \nmutate(\n    pca_graph = map2(\n      .x = pca,\n      .y = data,\n      ~ autoplot(.x, loadings = TRUE, loadings.label = TRUE,\n                 loadings.label.repel = TRUE,\n                 data = .y) +\n        theme_bw() +\n        labs(x = \"Principal Component 1\",\n             y = \"Principal Component 2\",\n             title = \"First two principal components of PCA on Allegheny County Census data\")\n    )\n  ) %&gt;%\n  pull(pca_graph)\n\n[[1]]\n\n\n\n\n\n\n\n\n\nThis code maps the first two PCs to the tracts.\n\ndf_au %&gt;% \n  select(-.rownames) %&gt;% \n  gather(variable, value, -c(GEOID)) -&gt; df_au_long\n\nallegheny_geo %&gt;% \n  left_join(df_au) %&gt;% \n  gather(pc, pc_value, contains(\".fitted\")) %&gt;% \n  mutate(pc = str_replace(pc, \".fitted\", \"\")) -&gt; allegheny_pca_map\n\nleft_join(allegheny_map, allegheny_pca_map) %&gt;% \n  filter(pc %in% c(\"PC1\", \"PC2\")) %&gt;% \n  ggplot(aes(fill = pc_value, color = pc_value)) + \n  geom_sf() + \n  facet_wrap(~pc) +\n  coord_sf(crs = 26911) + \n  scale_fill_viridis(\"Principal component value\", option = \"magma\") + \n  scale_color_viridis(\"Principal component value\", option = \"magma\") +\n  labs(title = \"Allegheny County\",\n       subtitle = \"American Community Survey\") +\n  theme(axis.text = element_blank())"
  },
  {
    "objectID": "posts/clustering-allegheny-county-census-tracts-with-pca-and-kmeans/index.html#clustering-with-k-means",
    "href": "posts/clustering-allegheny-county-census-tracts-with-pca-and-kmeans/index.html#clustering-with-k-means",
    "title": "Clustering Allegheny County Census Tracts With PCA and k-means",
    "section": "Clustering with k-means",
    "text": "Clustering with k-means\nNext I will use k-means to cluster the PC data.\n\ndf_au_long %&gt;% \n  filter(str_detect(variable, \"PC\")) %&gt;% \n  spread(variable, value) -&gt; allegheny_kmeans\n\nThis code clusters the data using 1 to 9 clusters.\n\nkclusts &lt;- tibble(k = 1:9) %&gt;%\n  mutate(\n    kclust = map(k, ~kmeans(allegheny_kmeans, .x)),\n    tidied = map(kclust, tidy),\n    glanced = map(kclust, glance),\n    augmented = map(kclust, augment, allegheny_kmeans)\n  )\n\n\nclusters &lt;- kclusts %&gt;%\n  unnest(tidied)\n\nassignments &lt;- kclusts %&gt;% \n  unnest(augmented)\n\nclusterings &lt;- kclusts %&gt;%\n  unnest(glanced, .drop = TRUE)\n\nBased on this “elbow chart”, the optimum number of clusters is most likely 2.\n\nggplot(clusterings, aes(k, tot.withinss)) +\n  geom_line() +\n  geom_point() +\n  geom_vline(xintercept = 2, linetype = 2) +\n  scale_x_continuous(breaks = 1:9) +\n  labs(x = \"Number of clusters\",\n       y = \"Between-cluster sum of squares\")\n\n\n\n\n\n\n\n\nWe can visualize how the data would look if it were assigned to a different number of clusters. Clearly the clustering algorithm experiences diminishing returns after 2 or 3 clusters.\n\nggplot(assignments, aes(.fittedPC1, .fittedPC2)) +\n  geom_point(aes(color = .cluster), alpha = .7) + \n  facet_wrap(~ str_c(k, \" cluster(s)\")) +\n  scale_color_discrete(\"Cluster\") +\n  labs(x = \"PC1\",\n       y = \"PC2\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nThis code divides the data into 2 clusters and maps the clusters onto the tract map.\n\ndf_au_long %&gt;% \n  filter(str_detect(variable, \"PC\")) %&gt;% \n  spread(variable, value) -&gt; allegheny_kmeans\n\nkclust &lt;- kmeans(allegheny_kmeans, centers = 2)\n\nkclust %&gt;% \n  augment(df_au_long %&gt;% \n            filter(str_detect(variable, \".fitted\")) %&gt;% \n            spread(variable, value)) -&gt; allegheny_kmeans\n\nget_decennial(geography = \"tract\", \n              variables = vars_housing,\n              state = \"PA\", \n              county = \"Allegheny\", \n              year = 2010,                           geometry = TRUE) %&gt;% \n  select(-c(variable, value)) %&gt;% \n  distinct(GEOID) -&gt; allegheny_geo\n\nallegheny_geo %&gt;% \n  left_join(allegheny_kmeans) -&gt; allegheny\n\n\nleft_join(allegheny_map, allegheny) %&gt;% \n  ggplot(aes(fill = .cluster, color = .cluster)) +\n  geom_sf(color = \"grey\", size = .1) +\n  scale_fill_viridis(\"Cluster\", discrete = TRUE, direction = -1) + \n  scale_color_viridis(\"Cluster\", discrete = TRUE, direction = -1) +\n  theme(axis.text = element_blank())\n\n\n\n\n\n\n\n\nThe second cluster largely follows the city limits, but excludes areas such as Mount Washington, Squirrel Hill, and Shadyside. It also includes a few areas outside of the city like Duquesne and McKeesport."
  },
  {
    "objectID": "posts/suburbanization-of-allegheny-county/index.html",
    "href": "posts/suburbanization-of-allegheny-county/index.html",
    "title": "Suburbanization of Allegheny County",
    "section": "",
    "text": "This March, researchers at the University of Georgia and Florida State University released the HHUUD10 dataset, which contains estimates of the number of housing units for decennial census years 1940-2010 and 2019. A “housing unit” could be a studio apartment or 5 bedroom single-family home. The data uses 2010 census tracts, which allows for historical comparison of housing trends across constant geometry. The full paper explains the approach.\nThis paper and the dataset can be used for a wide variety of socioeconomic issues. I will focus on suburbanization trends in the Pittsburgh area."
  },
  {
    "objectID": "posts/suburbanization-of-allegheny-county/index.html#overall-trend",
    "href": "posts/suburbanization-of-allegheny-county/index.html#overall-trend",
    "title": "Suburbanization of Allegheny County",
    "section": "Overall trend",
    "text": "Overall trend\n\nFix date formatting\nSince the data comes in a wide format, I pivot it long and fix up the year column to make it easy to graph with.\n\nac_housing_hu &lt;- ac_housing |&gt; \n  select(GEOID10, starts_with(\"hu\")) |&gt; \n  pivot_longer(cols = starts_with(\"hu\"), names_to = \"year\", values_to = \"housing_units\")\n\nyear_lookup &lt;- ac_housing_hu |&gt; \n  st_drop_geometry() |&gt; \n  distinct(year) |&gt; \n  mutate(year_fixed = c(1940, 1950, 1960, 1970, 1980, 1990, 2000, 2010, 2019))\n\nac_housing_hu &lt;- ac_housing_hu |&gt; \n  left_join(year_lookup) |&gt; \n  select(-year) |&gt; \n  rename(year = year_fixed)\n\nglimpse(ac_housing_hu)\n\nRows: 3,618\nColumns: 4\n$ GEOID10       &lt;chr&gt; \"42003560500\", \"42003560500\", \"42003560500\", \"4200356050…\n$ geometry      &lt;POLYGON [US_survey_foot]&gt; POLYGON ((1373906 410182, 1..., POL…\n$ housing_units &lt;dbl&gt; 1349, 1509, 1515, 1441, 1424, 1433, 1381, 1349, 1487, 13…\n$ year          &lt;dbl&gt; 1940, 1950, 1960, 1970, 1980, 1990, 2000, 2010, 2019, 19…\n\n\nThe number of housing units in the county stagnated after 1960, which is expected given the collapse of the steel industry.\n\nac_housing_hu |&gt; \n  st_drop_geometry() |&gt; \n  mutate(year = as.character(year) |&gt; fct_inorder()) |&gt; \n  group_by(year) |&gt; \n  summarize(housing_units = sum(housing_units)) |&gt; \n  ungroup() |&gt; \n  ggplot(aes(year, housing_units, group = 1)) +\n  geom_line() +\n  geom_point() +\n  scale_y_comma() +\n  labs(x = \"Year\",\n       y  = \"Housing units\")\n\n\n\n\n\n\n\n\nThe decennial difference in “gross” housing units also shows that growth stagnated after 1960.\n\nac_housing_hu |&gt; \n  st_drop_geometry() |&gt; \n  mutate(year = as.character(year) |&gt; fct_inorder()) |&gt; \n  group_by(year) |&gt; \n  summarize(housing_units = sum(housing_units)) |&gt; \n  ungroup() |&gt; \n  mutate(diff = housing_units - lag(housing_units)) |&gt; \n  ggplot(aes(year, diff, group = 1)) +\n  geom_line() +\n  geom_point() +\n  scale_y_comma(prefix = \"+ \") +\n  coord_cartesian(ylim = c(0, 90000)) +\n  labs(title = \"Growth stagnated after 1960\",\n       x = \"Year\",\n       y  = \"Change in housing units\")"
  },
  {
    "objectID": "posts/suburbanization-of-allegheny-county/index.html#change-from-1940-to-2019",
    "href": "posts/suburbanization-of-allegheny-county/index.html#change-from-1940-to-2019",
    "title": "Suburbanization of Allegheny County",
    "section": "Change from 1940 to 2019",
    "text": "Change from 1940 to 2019\nThis interactive map shows the areas that gained or lost the most housing units from 1940-2019. Dense housing around industrial areas along the Allegheny and Monongahela Rivers was erased. Homestead and Braddock stand out.\n\nhu_diff &lt;- ac_housing_hu |&gt; \n  group_by(GEOID10) |&gt; \n  filter(year == min(year) | year == max(year)) |&gt; \n  ungroup() |&gt; \n  select(GEOID10, year, housing_units) |&gt; \n  as_tibble() |&gt; \n  pivot_wider(names_from = year, names_prefix = \"units_\", values_from = housing_units) |&gt; \n  mutate(diff = units_2019 - units_1940) |&gt; \n  st_as_sf()\n\npal &lt;- colorNumeric(\n  palette = \"viridis\",\n  domain = hu_diff$diff)\n\nleaflet_map &lt;- hu_diff |&gt; \n  mutate(diff_formatted = comma(diff, accuracy = 1),\n         diff_label = str_c(\"Census tract: \", GEOID10, \"&lt;br/&gt;\", \"Difference: \", diff_formatted)) |&gt; \n  st_transform(crs = 4326) |&gt; \n  leaflet() |&gt; \n  setView(lat = 40.441606, lng = -80.010957, zoom = 10) |&gt; \n  addProviderTiles(providers$Stamen.TonerLite,\n                   options = providerTileOptions(noWrap = TRUE,\n                                                 minZoom = 9),\n                   group = \"Base map\") |&gt; \n  addPolygons(popup = ~ diff_label,\n              fillColor = ~pal(diff),\n              fillOpacity = .7,\n              color = \"black\",\n              weight = 1,\n              group = \"Housing\") |&gt; \n  addLegend(\"bottomright\", pal = pal, values = ~diff,\n            title = \"Difference\",\n            opacity = 1) |&gt; \n  addLayersControl(overlayGroups = c(\"Base map\", \"Housing\"),\n                   options = layersControlOptions(collapsed = FALSE)) |&gt; \n  addFullscreenControl()\n\nleaflet_map\n\n\n\n\n#frameWidget(leaflet_map, options=frameOptions(allowfullscreen = TRUE))\n\nThe North Side and the Hill were targets of “urban renewal” in the middle of the century. Dense housing in heavily African-American communities were demolished to make way for an opera house, the 279 and 579 highways, and parking lots. The highways are directly related to the white flight exodus to the suburbs, especially in the west and north. Those highways made it easy for the new suburbanites to commute longer distances in single passenger vehicles.\nThese graphs shows that the areas with the most housing in 1940 lost thousands of units, while outlying areas gained thousands of units.\n\nslope_graph_anim &lt;- hu_diff |&gt; \n  as_tibble() |&gt; \n  select(-geometry) |&gt;\n  arrange(desc(units_1940)) |&gt; \n  pivot_longer(cols = c(units_1940, units_2019), names_to = \"year\", values_to = \"housing_units\") |&gt; \n  mutate(year = str_remove(year, \"^units_\")) |&gt; \n  mutate(order = row_number()) |&gt; \n  ggplot(aes(year, housing_units)) +\n  geom_line(aes(group = GEOID10), alpha = .1) +\n  geom_point(aes(group = str_c(year, GEOID10)), alpha = .05) +\n  scale_y_comma() +\n  transition_reveal(order) +\n  labs(title = \"Housing unit change from 1940-2019\",\n       subtitle = \"From areas with the most units in 1940 to the least\",\n       x = \"Year\",\n       y = \"Housing units\") +\n  theme(panel.grid.minor.y = element_blank(),\n        panel.grid.major.y = element_blank(),\n        panel.grid.major.x = element_blank(),\n        panel.border = element_blank(),\n        axis.title.x = element_blank())\n\nslope_graph_anim &lt;- animate(slope_graph_anim, duration = 10, fps = 40, end_pause = 60)\n\nslope_graph_anim\n\n\n\n\n\n\n\n\n\nhu_diff |&gt; \n  ggplot(aes(units_1940, units_2019)) +\n  geom_abline(lty = 2) +\n  geom_point(alpha = .2) +\n  annotate(\"text\", x = 3500, y = 3800, label = \"No change\", angle = 45) +\n  annotate(\"text\", x = 300, y = 4500, label = \"Gain\") +\n  annotate(\"text\", x = 4300, y = 100, label = \"Loss\") +\n  tune::coord_obs_pred() +\n  scale_x_comma() +\n  scale_y_comma() +\n  labs(title = \"Change in housing units\",\n       x = \"Units in 1940\",\n       y = \"Units in 2019\")\n\n\n\n\n\n\n\n\n\nMoving north and west\nThese maps show the estimates of housing units for each decennial period. Outlying areas in the north and west, directly served by the new highway system, gained thousands of housing units.\n\nac_housing_hu |&gt; \n  ggplot() +\n  geom_sf(aes(fill = housing_units), color = NA) +\n  scale_fill_viridis_c(\"Housing units\", labels = comma) +\n  facet_wrap(~year) +\n  theme_bw() +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        axis.text.x = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks = element_blank())\n\n\n\n\n\n\n\n\nGeographically larger Census tracts gained more of the % of total housing over time.\n\nac_sqmi &lt;- ac_housing |&gt; \n  select(GEOID10, starts_with(\"sqmi\")) |&gt; \n  st_drop_geometry() |&gt; \n  as_tibble() |&gt; \n  pivot_longer(starts_with(\"sqmi\"), names_to = \"year\", values_to = \"sqmi\")\n\nac_sqmi_year &lt;- ac_sqmi |&gt; \n  distinct(year) |&gt; \n  mutate(year_fixed = c(1940, 1950, 1960, 1970, 1980, 1990, 2000, 2010, 2019))\n\nac_sqmi &lt;- ac_sqmi |&gt; \n  left_join(ac_sqmi_year) |&gt; \n  select(-year) |&gt; \n  rename(year = year_fixed)\n\nac_density &lt;- ac_housing_hu |&gt; \n  select(GEOID10, year, housing_units) |&gt; \n  left_join(ac_sqmi) |&gt; \n  mutate(density = housing_units / sqmi)\n\ncurve_anim &lt;- ac_density |&gt; \n  st_drop_geometry() |&gt; \n  select(GEOID10, year, housing_units, sqmi) |&gt; \n  mutate(year = as.character(year) |&gt; fct_inorder()) |&gt; \n  arrange(year, sqmi) |&gt; \n  group_by(year) |&gt; \n  mutate(housing_units_cumsum = cumsum(housing_units),\n         pct_units = housing_units_cumsum / sum(housing_units)) |&gt; \n  ungroup() |&gt; \n  ggplot(aes(sqmi, pct_units, color = year)) +\n  geom_line() +\n  scale_y_percent() +\n  labs(title = \"Housing moves to outlying areas over time\",\n       subtitle = \"Year: {closest_state}\",\n       x = \"Square miles\",\n       y = \"Cumulative percent of units\",\n       color = \"Year\") +\n  transition_states(year) +\n  shadow_mark()\n\ncurve_anim &lt;- animate(curve_anim, duration = 10, fps = 20)\n\ncurve_anim\n\n\n\n\n\n\n\n\n\n\nHousing peaks\nThis shows the year that each census tract peaked in terms of housing units. The areas that attracted heavy industry in the late 19th/early 20th century (and built housing nearby to support it) were crushed by the collapse of that industry. The single census tract that makes up “Downtown” has clawed back some housing recently.\n\nac_housing_hu |&gt; \n  group_by(GEOID10) |&gt; \n  filter(housing_units == max(housing_units)) |&gt; \n  ungroup() |&gt; \n  rename(max_year = year) |&gt; \n  ggplot() +\n  geom_sf(aes(fill = max_year), color = NA) +\n  scale_fill_viridis_c(direction = -1) +\n  labs(title = \"Year of peak housing\",\n       fill = \"Peak\") +\n  theme(panel.grid.major = element_blank(),\n        axis.text.x = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks = element_blank(),\n        panel.border = element_blank())"
  },
  {
    "objectID": "posts/suburbanization-of-allegheny-county/index.html#housing-moves-away-from-the-center",
    "href": "posts/suburbanization-of-allegheny-county/index.html#housing-moves-away-from-the-center",
    "title": "Suburbanization of Allegheny County",
    "section": "Housing moves away from the center",
    "text": "Housing moves away from the center\nA major trend from 1940-2019 is the significant shift in housing from around the core to outlying suburbs. This code calculates the distance between each tract and the “Downtown” tract (42003020100), and plots the number of units compared to that distance.\n\ndowntown_tract &lt;- ac_housing_hu |&gt; \n  filter(GEOID10 == \"42003020100\") |&gt; \n  distinct(GEOID10, geometry) |&gt; \n  mutate(centroid = st_point_on_surface(geometry)) |&gt; \n  st_set_geometry(\"centroid\") |&gt; \n  select(-geometry)\n\ndistance_anim &lt;- ac_housing_hu |&gt; \n  select(GEOID10, year, housing_units) |&gt; \n  mutate(centroid = st_point_on_surface(geometry),\n         geoid = str_c(GEOID10, year, sep = \"_\"),\n         year = as.integer(year)) |&gt; \n  mutate(distance_to_downtown = st_distance(centroid, downtown_tract) |&gt; as.numeric() / 5280) |&gt; \n  ggplot(aes(distance_to_downtown, housing_units)) +\n  geom_point(aes(group = GEOID10), alpha = .3) +\n  geom_smooth(aes(group = year)) +\n  scale_x_continuous() +\n  scale_y_comma() +\n  transition_states(year, \n                    state_length = 10) +\n  labs(title = \"Housing has moved farther away from downtown\",\n       subtitle = \"{closest_state}\",\n       x = \"Miles from downtown\",\n       y = \"Housing units\") +\n  theme(panel.grid.minor = element_blank())\n\ndistance_anim &lt;- animate(distance_anim)\n\ndistance_anim"
  },
  {
    "objectID": "posts/suburbanization-of-allegheny-county/index.html#land-use",
    "href": "posts/suburbanization-of-allegheny-county/index.html#land-use",
    "title": "Suburbanization of Allegheny County",
    "section": "Land use",
    "text": "Land use\nThe HHUUD10 data also contains estimates for the percentage of land in a tract that is “developed” for the years 1992, 2001, and 2011. “Developed” in this context means “covered by an urban land use”.\n\nac_dev &lt;- ac_housing |&gt; \n  select(GEOID10, starts_with(\"pdev\")) |&gt; \n  pivot_longer(cols = starts_with(\"pdev\"), names_to = \"year\", values_to = \"pct_dev\") \n\ndev_years &lt;- ac_dev |&gt; \n  st_drop_geometry() |&gt; \n  distinct(year) |&gt; \n  mutate(year_fixed = c(1992, 2001, 2011))\n\nac_dev &lt;- ac_dev |&gt; \n  left_join(dev_years) |&gt; \n  select(-year) |&gt; \n  rename(year = year_fixed)\n\nac_dev |&gt; \n  ggplot() +\n  geom_sf(aes(fill = pct_dev), color = NA) +\n  facet_wrap(~year) +\n  scale_fill_viridis_c(labels = percent) +\n  labs(title = \"Percent of land that is developed\",\n       fill = NULL) +\n  theme_bw() +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        axis.text.x = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks = element_blank())\n\n\n\n\n\n\n\n\nI find it interesting that more of the South Hills is developed than the North Hills. I would have expected more development in the North Hills due to the McKnight Road area and Wexford. My guess is that the tracts in the North Hills cover more land area, which decreases the % that is developed. Conversely, the tracts in the South Hills cover less land area, and less of the South Hills is useful for development because of steep hills and creeks. This concentrates development in a smaller area."
  },
  {
    "objectID": "posts/suburbanization-of-allegheny-county/index.html#conclusion",
    "href": "posts/suburbanization-of-allegheny-county/index.html#conclusion",
    "title": "Suburbanization of Allegheny County",
    "section": "Conclusion",
    "text": "Conclusion\nOver the past 80 years, Allegheny County has lost a significant amount of housing in its core urban area. Much of this is directly related to the collapse of the steel industry and “urban renewal”. At the same time, new housing development has been pushed out to the suburbs. This is a loss in terms of housing density, which has become a major discussion point in urban planning over the past 20 years.\nHigher density areas have a lower per capita carbon footprint due to non-car commute modes and agglomeration effects. Higher density also does not expand the wildland-urban interface. This leaves more land for the natural environment, moves humans away from dangers such as wildfires, and lowers the frequency of interaction between wild animals and humans, which can transfer disease (coronavirus, ebola). It will be interesting to see whether the suburbanization trend continues after the initial shocks of COVID-19 pandemic subside."
  },
  {
    "objectID": "posts/pittsburgh-parking-covid-change/index.html",
    "href": "posts/pittsburgh-parking-covid-change/index.html",
    "title": "Effect of COVID-19 on Pittsburgh parking transactions",
    "section": "",
    "text": "The COVID-19 pandemic’s affect on commerce and mobility habits is well documented. For example, Apple publishes Mobility Trends reports about utilization of various transportation modes.\nApple’s data shows that utilization of driving in Pittsburgh dropped significantly in late March, but has rebounded above pre-COVID-19 levels since then.\nThe WPDRC publishes parking meter transactions for 60 parking zones in Pittsburgh. In this post I will use the frequency of parking transactions over time as a proxy for commercial activity in the city. This information only represents commerce that people use vehicles to perform, so it does not include mass transit or drivers that use private parking areas or meters that are not captured in this dataset. I will be interested to see if the parking meter data matches Apple’s report about driving."
  },
  {
    "objectID": "posts/pittsburgh-parking-covid-change/index.html#top-level-analysis",
    "href": "posts/pittsburgh-parking-covid-change/index.html#top-level-analysis",
    "title": "Effect of COVID-19 on Pittsburgh parking transactions",
    "section": "Top-level analysis",
    "text": "Top-level analysis\n\nRead in data\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(vroom)\nlibrary(hrbrthemes)\nlibrary(scales)\nlibrary(plotly)\nlibrary(broom)\nlibrary(heatwaveR)\nlibrary(gt)\n\noptions(scipen = 999,\n        digits = 4)\n\ntheme_set(theme_ipsum())\n\nAs of September 28th there are ~5 million rows in the dataset. Each row consists of a 10-minute period in a given zone with the aggregated number of transactions and the amount paid.\n\ndata &lt;- vroom(\"post_data/1ad5394f-d158-46c1-9af7-90a9ef4e0ce1.csv\")\n\nglimpse(data)\n\nRows: 7,354,373\nColumns: 9\n$ `_id`               &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,…\n$ zone                &lt;chr&gt; \"421 - NorthSide\", \"403 - Uptown\", \"412 - East Lib…\n$ start               &lt;dttm&gt; 2018-01-01 00:20:00, 2018-01-01 01:10:00, 2018-01…\n$ end                 &lt;dttm&gt; 2018-01-01 00:30:00, 2018-01-01 01:20:00, 2018-01…\n$ utc_start           &lt;dttm&gt; 2018-01-01 05:20:00, 2018-01-01 06:10:00, 2018-01…\n$ meter_transactions  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ meter_payments      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ mobile_transactions &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 2, 5, 1, 1, 1, 3, 1, 1, 1,…\n$ mobile_payments     &lt;dbl&gt; 4.00, 3.00, 3.00, 4.00, 16.25, 4.00, 3.00, 1.00, 2…\n\n\nThere are 60 distinct parking zones in the dataset.\n\ndata %&gt;% \n  distinct(zone) %&gt;% \n  arrange(zone)\n\n# A tibble: 62 × 1\n   zone                       \n   &lt;chr&gt;                      \n 1 209 - Mon Wharf            \n 2 213 - Second Avenue Plaza  \n 3 301 - Sheridan Harvard Lot \n 4 302 - Sheridan Kirkwood Lot\n 5 304 - Tamello Beatty Lot   \n 6 307 - Eva Beatty Lot       \n 7 308 - Harvard Beatty Lot   \n 8 311 - Ansley Beatty Lot    \n 9 314 - Penn Circle NW Lot   \n10 321 - Beacon Bartlett Lot  \n# ℹ 52 more rows\n\n\nThis code chunk performs most of the aggregation and manipulation. It separates the start column into start_date and start_time, calculates the number of transactions per day, and creates some date columns that I use later.\n\ndf_ts &lt;- data %&gt;%\n  select(start, meter_transactions, mobile_transactions) %&gt;%\n  separate(start, into = c(\"start_date\", \"start_time\"), remove = TRUE, sep = \" \") %&gt;%\n  mutate(start_date = ymd(start_date)) %&gt;%\n  filter(start_date &lt;= \"2020-10-05\") |&gt; \n  group_by(start_date) %&gt;%\n  summarize(meter_transactions = sum(meter_transactions),\n            mobile_transactions = sum(mobile_transactions)) %&gt;%\n  ungroup() %&gt;%\n  rowwise() %&gt;%\n  mutate(total_parking_transactions = meter_transactions + mobile_transactions) %&gt;%\n  ungroup() %&gt;%\n  mutate(year = year(start_date),\n         day_of_year = yday(start_date),\n         week_of_year = week(start_date),\n         weekday = wday(start_date, label = TRUE)) %&gt;%\n  group_by(year, week_of_year) %&gt;%\n  mutate(first_date_of_week = min(start_date)) %&gt;% \n  ungroup() %&gt;% \n  select(start_date, year, week_of_year, day_of_year, weekday, everything())"
  },
  {
    "objectID": "posts/pittsburgh-parking-covid-change/index.html#overall-timeline",
    "href": "posts/pittsburgh-parking-covid-change/index.html#overall-timeline",
    "title": "Effect of COVID-19 on Pittsburgh parking transactions",
    "section": "Overall timeline",
    "text": "Overall timeline\nThis view of the daily transactions shows that parking transactions dropped off steeply in late March 2020.\n\ndf_ts %&gt;% \n  ggplot(aes(first_date_of_week, total_parking_transactions)) +\n  geom_point(alpha = .2, size = .5) +\n  labs(title = \"Daily parking transactions\",\n       subtitle = \"2014-2020\",\n       x = \"Year\",\n       y = \"Total parking transactions\") +\n  scale_y_comma() +\n  scale_x_date(date_labels = \"%Y\")\n\n\n\n\n\n\n\n\n\n2020 vs. previous years\nStarting in March, parking transactions in 2020 fell way below the historical norm. At the most extreme, weekly transactions fell below 10,000.\n\ncompare_2020_before &lt;- df_ts %&gt;% \n  select(year, week_of_year, total_parking_transactions) %&gt;% \n  group_by(year, week_of_year) %&gt;% \n  summarize(total_parking_transactions = sum(total_parking_transactions)) %&gt;% \n  group_by(week_of_year) %&gt;% \n  mutate(week_median_parking_events = median(total_parking_transactions)) %&gt;% \n  ungroup() %&gt;% \n  mutate(period = case_when(year == 2020 ~ \"2020\",\n                               year &lt; 2020 ~ \"Before times\"))\n\ncompare_2020_before %&gt;% \n  ggplot(aes(x = week_of_year, y = total_parking_transactions, color = period, group = year)) +\n  geom_hline(yintercept = 0) +\n  geom_line(data = compare_2020_before %&gt;% filter(period == \"Before times\"),\n            size = 1.5, alpha = .7) +\n  geom_line(data = compare_2020_before %&gt;% filter(period == \"2020\"),\n            size = 1.5) +\n  scale_x_continuous(breaks = seq(0, 54, by = 4)) +\n  scale_color_manual(values = c(\"red\", \"grey\")) +\n  scale_y_comma(breaks = seq(0, 200000, by = 20000)) +\n  labs(title = \"Weekly parking transactions\",\n       x = \"Week of year\",\n       y = \"Total parking events\",\n       color = \"Period\")\n\n\n\n\n\n\n\n\n\n\n2020 vs. historical average\nThis code calculates the % difference between the number of parking transactions in 2020 and the historical average for a given week.\n\ndata_historical &lt;- df_ts %&gt;% \n  filter(start_date &lt; \"2020-01-01\") %&gt;% \n  select(year, week_of_year, total_parking_transactions) %&gt;% \n  group_by(year, week_of_year) %&gt;% \n  summarize(total_parking_transactions = sum(total_parking_transactions)) %&gt;% \n  group_by(week_of_year) %&gt;% \n  summarize(median_historical_transactions = median(total_parking_transactions),\n            day_count = n()) %&gt;% \n  ungroup()\n\n\ndata_2020 &lt;- df_ts %&gt;% \n  select(start_date, first_date_of_week, week_of_year, total_parking_transactions) %&gt;% \n  filter(start_date &gt;= \"2020-01-01\") %&gt;% \n  group_by(first_date_of_week, week_of_year) %&gt;% \n  summarize(total_parking_transactions = sum(total_parking_transactions)) %&gt;% \n  ungroup()\n\n\ndf &lt;- data_2020 %&gt;% \n  left_join(data_historical)\n\n\nsmoothed_line_df &lt;- df %&gt;% \n  mutate(pct_difference = (total_parking_transactions - median_historical_transactions) / median_historical_transactions) %&gt;% \n  select(week_of_year, first_date_of_week, pct_difference) %&gt;% \n  nest(parking_data = everything()) %&gt;% \n  mutate(model = map(parking_data, ~loess(pct_difference ~ week_of_year, data = .x, span = .3)),\n         coeff = map(model, augment))\n\nsmoothed_line_df &lt;- smoothed_line_df %&gt;% \n  unnest(parking_data) %&gt;% \n  left_join(unnest(smoothed_line_df, coeff)) %&gt;% \n  select(first_date_of_week, .fitted) %&gt;% \n  mutate(sign = .fitted &gt; 0,\n         population = \"total\")\n\nThis shows that after starting 2020 slightly above average, parking transactions fell to almost -100% in early April.\n\nsmoothed_line_df %&gt;% \n  ggplot(aes(x = first_date_of_week)) +\n  heatwaveR::geom_flame(aes(y = 0, y2 = .fitted)) +\n  geom_line(aes(y = .fitted), size = 1.5) +\n  geom_hline(yintercept = 0, lty = 2) +\n  scale_y_percent() +\n  labs(title = \"2020 vs. historical average\",\n       x = \"Date\",\n       y = \"Percent difference\")\n\n\n\n\n\n\n\n\nWhile the number of transactions recovered from the depths of March and April, it has not matched the increase that Apple’s mobility report showed for driving after May. Parking transactions are still 50% below their historical average.\n\n\nWeekday vs weekend difference, 2020 vs. historical\nThe difference between the number of parking transactions on weekdays vs. weekends did not change significantly after March 2020.\n\nweekday_weekend_df &lt;- df_ts %&gt;% \n  select(start_date, week_of_year, weekday, total_parking_transactions) %&gt;% \n  mutate(period = case_when(start_date &gt;= \"2020-01-01\" ~ \"2020\",\n                            start_date &lt; \"2020-01-01\" ~ \"Before times\"),\n         is_weekend = case_when(weekday %in% c(\"Sat\", \"Sun\") ~ \"weekend\",\n                                !(weekday %in% c(\"Sat\", \"Sun\")) ~ \"weekday\")) %&gt;% \n  mutate(period = fct_relevel(period, \"Before times\"),\n         is_weekend = fct_relevel(is_weekend, \"weekday\")) %&gt;% \n  group_by(period, is_weekend) %&gt;% \n  summarize(total_parking_transactions = sum(total_parking_transactions)) %&gt;% \n  mutate(pct_of_parking_transactions = total_parking_transactions / sum(total_parking_transactions))\n\nweekday_weekend_df %&gt;% \n  ggplot(aes(x = is_weekend, y =  pct_of_parking_transactions, fill = period)) +\n  geom_col(position = position_dodge(width = 1), color = \"black\", alpha = .8) +\n  scale_y_percent() +\n  scale_fill_viridis_d() +\n  labs(title = \"Weekday vs. weekend parking transactions\",\n       x = NULL,\n       y = \"Percent of transactions\",\n       fill = \"Period\")"
  },
  {
    "objectID": "posts/pittsburgh-parking-covid-change/index.html#neighborhood-level-analysis",
    "href": "posts/pittsburgh-parking-covid-change/index.html#neighborhood-level-analysis",
    "title": "Effect of COVID-19 on Pittsburgh parking transactions",
    "section": "Neighborhood-level analysis",
    "text": "Neighborhood-level analysis\nNext I perform the same analysis at the neighborhood level to see if any areas in the city were particularly affected. I manually aggregated the parking zones up to the neighborhood level. This code reads in that data.\n\ngeocoded_parking_locations &lt;- read_csv(\"post_data/geocoded_parking_locations.csv\")\n\ngeocoded_parking_locations %&gt;%\n  arrange(zone_region, zone)\n\n# A tibble: 62 × 3\n   zone                                 n zone_region\n   &lt;chr&gt;                            &lt;dbl&gt; &lt;chr&gt;      \n 1 354 - Walter/Warrington Lot      11735 Allentown  \n 2 355 - Asteroid Warrington Lot    48240 Allentown  \n 3 417 - Allentown                  35577 Allentown  \n 4 363 - Beechview Lot              13890 Beechview  \n 5 418 - Beechview                  68416 Beechview  \n 6 334 - Taylor Street Lot          95186 Bloomfield \n 7 335 - Friendship Cedarville Lot 188572 Bloomfield \n 8 406 - Bloomfield (On-street)    220290 Bloomfield \n 9 361 - Brookline Lot               6473 Brookline  \n10 419 - Brookline                 173071 Brookline  \n# ℹ 52 more rows\n\n\nThis code does the same aggregation as before, but adds neighborhood in the group_by function.\n\ndf_ts_neighborhood &lt;- data %&gt;%\n  left_join(geocoded_parking_locations) %&gt;%\n  select(zone_region, start, meter_transactions, mobile_transactions) %&gt;%\n  separate(start, into = c(\"start_date\", \"start_time\"), remove = TRUE, sep = \" \") %&gt;%\n  mutate(start_date = ymd(start_date)) %&gt;%\n  group_by(zone_region, start_date) %&gt;%\n  summarize(meter_transactions = sum(meter_transactions),\n            mobile_transactions = sum(mobile_transactions)) %&gt;%\n  ungroup() %&gt;%\n  rowwise() %&gt;%\n  mutate(total_parking_events = meter_transactions + mobile_transactions) %&gt;%\n  ungroup() %&gt;%\n  mutate(year = year(start_date),\n         day_of_year = yday(start_date),\n         week_of_year = week(start_date),\n         weekday = wday(start_date, label = TRUE)) %&gt;%\n  group_by(year, week_of_year) %&gt;% \n  mutate(first_date_of_week = min(start_date)) %&gt;% \n  ungroup() %&gt;% \n  select(zone_region, start_date, day_of_year, week_of_year, weekday, everything())\n\nMost of the parking transactions occur in ~13 neighborhoods, so I will focus on those.\n\nzone_fct &lt;- df_ts_neighborhood %&gt;% \n  group_by(zone_region) %&gt;% \n  summarize(total_parking_events = sum(total_parking_events)) %&gt;% \n  arrange(total_parking_events) %&gt;% \n  pull(zone_region)\n\ndf_ts_neighborhood %&gt;% \n  group_by(zone_region) %&gt;% \n  summarize(total_parking_events = sum(total_parking_events)) %&gt;% \n  mutate(zone_region = factor(zone_region, levels = zone_fct)) %&gt;% \n  ggplot(aes(total_parking_events, zone_region)) +\n  geom_col() +\n  scale_x_comma() +\n  labs(x = \"Total parking transactions\",\n       y = \"Neighborhood\")\n\n\n\n\n\n\n\n\n\ntop_zone_regions &lt;- df_ts_neighborhood %&gt;% \n  group_by(zone_region) %&gt;% \n  summarize(total_parking_events = sum(total_parking_events)) %&gt;% \n  arrange(desc(total_parking_events)) %&gt;% \n  select(zone_region) %&gt;% \n  slice(1:13)\n\n\n2020 vs. historical average\nThis code calculates the weekly % difference in parking transactions between 2020 and the previous years, by neighborhood.\n\ndf_historical &lt;- df_ts_neighborhood %&gt;% \n  arrange(zone_region, start_date) %&gt;% \n  filter(start_date &lt; \"2020-01-01\") %&gt;% \n  group_by(zone_region, year, week_of_year) %&gt;% \n  summarize(total_parking_events = sum(total_parking_events)) %&gt;% \n  ungroup()\n\ndf_historical &lt;- df_historical %&gt;% \n  group_by(zone_region, week_of_year) %&gt;% \n  summarize(median_parking_events_historical = median(total_parking_events)) %&gt;% \n  ungroup()\n\ndf_2020 &lt;- df_ts_neighborhood %&gt;% \n  filter(start_date &gt;= \"2020-01-01\", start_date &lt;= \"2020-10-05\") %&gt;% \n  complete(zone_region, week_of_year, fill = list(total_parking_events = 0)) %&gt;% \n  group_by(zone_region, week_of_year, first_date_of_week) %&gt;% \n  summarize(total_parking_events = sum(total_parking_events)) %&gt;% \n  ungroup()\n\ndf_combined &lt;- df_2020 %&gt;% \n  left_join(df_historical, by = c(\"zone_region\", \"week_of_year\")) %&gt;%\n  mutate(pct_difference = (total_parking_events - median_parking_events_historical) / median_parking_events_historical)\n\nThis shows that all the neighborhoods experienced severe drops in parking transactions. Only the North Shore returned to regular levels, and even then only temporarily.\n\nline_chart &lt;- df_combined %&gt;%\n  semi_join(top_zone_regions) %&gt;% \n  rename(neighborhood = zone_region) %&gt;% \n  mutate(pct_difference = round(pct_difference, 2)) %&gt;% \n  ggplot(aes(first_date_of_week, pct_difference, group = neighborhood)) +\n  geom_hline(yintercept = 0, lty = 2, alpha = .5) +\n  geom_line(alpha = .3) +\n  scale_y_percent() +\n  labs(title = \"2020 vs. historical average in top neighborhoods\",\n       x = \"Date\",\n       y = \"Percent difference\")\n\nline_chart %&gt;% \n  ggplotly(tooltip = c(\"neighborhood\", \"pct_difference\"))\n\n\n\n\n\nThis tile chart shows a similar pattern.\n\ntile_chart &lt;- df_combined %&gt;% \n  semi_join(top_zone_regions) %&gt;% \n  mutate(zone_region = factor(zone_region, levels = zone_fct),\n         ) %&gt;% \n  mutate(pct_difference = pct_difference %&gt;% round(2),\n         pct_difference_tooltip = pct_difference %&gt;% round(2) %&gt;% percent(accuracy = 1)) %&gt;% \n  ggplot(aes(week_of_year, zone_region, fill = pct_difference)) +\n  geom_tile() +\n  scale_fill_viridis_c(labels = percent) +\n  scale_x_continuous(expand = c(0,0)) +\n  scale_y_discrete(expand = c(0,0)) +\n  labs(title = \"2020 vs. historical average in top neighborhoods\",\n       x = \"Week of year\",\n       y = NULL,\n       fill = \"Percent difference\") +\n  theme(panel.grid = element_blank(),\n        legend.position = \"bottom\")\n\nggplotly(tile_chart, tooltip = c(\"zone_region\", \"week_of_year\", \"pct_difference\")) %&gt;% \n  layout(xaxis = list(showgrid = F),\n         yaxis = list(showgrid = F))\n\n\n\n\n\nAggregating the neighborhoods into boxplots shows that the drop in transactions mirrors the overall trend.\n\ndf_combined %&gt;% \n  semi_join(top_zone_regions) %&gt;% \n  ggplot(aes(first_date_of_week, pct_difference, group = week_of_year)) +\n  geom_boxplot(outlier.alpha = .3, outlier.size = 1) +\n  geom_hline(yintercept = 0, lty = 2, alpha = .5) +\n  scale_y_percent() +\n  labs(title = \"2020 vs. historical average\",\n       subtitle = \"Top 13 neighborhoods\",\n       x = \"Date\",\n       y = \"Percent difference\")"
  },
  {
    "objectID": "posts/pittsburgh-parking-covid-change/index.html#week-to-week-difference",
    "href": "posts/pittsburgh-parking-covid-change/index.html#week-to-week-difference",
    "title": "Effect of COVID-19 on Pittsburgh parking transactions",
    "section": "2020 week-to-week difference",
    "text": "2020 week-to-week difference\nIn terms of week-to-week difference in parking transactions, the week starting March 18th was the worst, with a -84% drop from the week before.\n\nweekly_pct_difference_df &lt;- data_2020 %&gt;% \n  mutate(weekly_difference = total_parking_transactions - lag(total_parking_transactions),\n         weekly_pct_difference = weekly_difference / lag(total_parking_transactions))\n\nweekly_pct_difference_df %&gt;% \n  mutate(max_drop_flag = weekly_pct_difference == min(weekly_pct_difference, na.rm = TRUE),\n         max_drop = case_when(max_drop_flag == TRUE ~ weekly_pct_difference,\n                              max_drop_flag == FALSE ~ NA_real_)) %&gt;% \n  ggplot(aes(first_date_of_week, weekly_pct_difference)) +\n  geom_line() +\n  geom_point() +\n  geom_point(aes(y = max_drop), color = \"red\", size = 3) +\n  ggrepel::geom_label_repel(aes(y = max_drop, label = scales::percent(max_drop)),\n                            direction = \"x\") +\n  scale_y_percent() +\n  coord_cartesian(ylim = c(-1, 1)) +\n  labs(title = \"Week-to-week difference\",\n       x = \"Date\",\n       y = \"Percent difference\")"
  },
  {
    "objectID": "posts/cumulative-ebird-sightings-in-allegheny-county/index.html",
    "href": "posts/cumulative-ebird-sightings-in-allegheny-county/index.html",
    "title": "Cumulative eBird Sightings in Allegheny County",
    "section": "",
    "text": "This will be a quick post on cumulative bird observations in Allegheny County. Cumulative graphs show overall trends, seasonality, and quirks in how the data was recorded. They are also fun to turn into animated gifs with gganimate.\nLoad the relevant libraries:\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(tidyquant)\nlibrary(janitor)\nlibrary(hrbrthemes)\nlibrary(vroom)\nlibrary(ggrepel)\nlibrary(gganimate)\n\nset.seed(1234)\n\ntheme_set(theme_bw(base_size = 16))\n\nThis reads in data from the eBird data portal:\n\ndf &lt;- vroom(\"post_data/ebd_US-PA-003_201001_202003_relFeb-2020.zip\", delim = \"\\t\") %&gt;% \n  clean_names() %&gt;% \n  mutate_at(vars(observer_id, locality, observation_date, time_observations_started, protocol_type), str_replace_na, \"NA\") %&gt;% \n  mutate(observation_count = as.numeric(str_replace(observation_count, \"X\", as.character(NA))),\n         observation_event_id = str_c(observer_id, locality, observation_date, time_observations_started, sep = \"-\"),\n         observation_date = ymd(observation_date)) %&gt;%\n  filter(all_species_reported == 1)\n\nglimpse(df)\n\nRows: 908,622\nColumns: 48\n$ global_unique_identifier     &lt;chr&gt; \"URN:CornellLabOfOrnithology:EBIRD:OBS815…\n$ last_edited_date             &lt;dttm&gt; 2018-08-03 11:44:16, 2018-08-03 11:44:34…\n$ taxonomic_order              &lt;dbl&gt; 493, 20638, 20638, 20638, 20638, 20638, 2…\n$ category                     &lt;chr&gt; \"species\", \"species\", \"species\", \"species…\n$ common_name                  &lt;chr&gt; \"American Black Duck\", \"American Crow\", \"…\n$ scientific_name              &lt;chr&gt; \"Anas rubripes\", \"Corvus brachyrhynchos\",…\n$ subspecies_common_name       &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ subspecies_scientific_name   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ observation_count            &lt;dbl&gt; 1, 7, 3, 2, 4, 3, NA, NA, 25, 2, 7, 2, 3,…\n$ breeding_bird_atlas_code     &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ breeding_bird_atlas_category &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ age_sex                      &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ country                      &lt;chr&gt; \"United States\", \"United States\", \"United…\n$ country_code                 &lt;chr&gt; \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\",…\n$ state                        &lt;chr&gt; \"Pennsylvania\", \"Pennsylvania\", \"Pennsylv…\n$ state_code                   &lt;chr&gt; \"US-PA\", \"US-PA\", \"US-PA\", \"US-PA\", \"US-P…\n$ county                       &lt;chr&gt; \"Allegheny\", \"Allegheny\", \"Allegheny\", \"A…\n$ county_code                  &lt;chr&gt; \"US-PA-003\", \"US-PA-003\", \"US-PA-003\", \"U…\n$ iba_code                     &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ bcr_code                     &lt;dbl&gt; 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 2…\n$ usfws_code                   &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ atlas_block                  &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ locality                     &lt;chr&gt; \"Rachel Carson Riverfront Park\", \"Southsi…\n$ locality_id                  &lt;chr&gt; \"L2640334\", \"L841018\", \"L329082\", \"L69207…\n$ locality_type                &lt;chr&gt; \"H\", \"H\", \"H\", \"P\", \"P\", \"H\", \"H\", \"H\", \"…\n$ latitude                     &lt;dbl&gt; 40.53731, 40.43124, 40.54348, 40.65688, 4…\n$ longitude                    &lt;dbl&gt; -79.79531, -79.97032, -79.90623, -80.1138…\n$ observation_date             &lt;date&gt; 2010-01-14, 2010-01-31, 2010-01-23, 2010…\n$ time_observations_started    &lt;chr&gt; \"11:05:00\", \"16:45:00\", \"13:15:00\", \"14:4…\n$ observer_id                  &lt;chr&gt; \"obsr39944\", \"obsr197993\", \"obsr197993\", …\n$ sampling_event_identifier    &lt;chr&gt; \"S5760087\", \"S5839167\", \"S5798726\", \"S580…\n$ protocol_type                &lt;chr&gt; \"Traveling\", \"Traveling\", \"Area\", \"Travel…\n$ protocol_code                &lt;chr&gt; \"P22\", \"P22\", \"P23\", \"P22\", \"P21\", \"P23\",…\n$ project_code                 &lt;chr&gt; \"EBIRD\", \"EBIRD\", \"EBIRD\", \"EBIRD\", \"EBIR…\n$ duration_minutes             &lt;dbl&gt; 25, 30, 90, 90, 120, 30, 35, 30, 70, 60, …\n$ effort_distance_km           &lt;dbl&gt; 0.483, 0.483, NA, 8.047, NA, NA, NA, NA, …\n$ effort_area_ha               &lt;dbl&gt; NA, NA, 24.2811, NA, NA, 4.0469, 4.0469, …\n$ number_observers             &lt;dbl&gt; 1, 2, 2, 1, NA, 2, 2, 2, 4, 2, 1, 1, 1, 4…\n$ all_species_reported         &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ group_identifier             &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ has_media                    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ approved                     &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ reviewed                     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ reason                       &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ trip_comments                &lt;chr&gt; NA, NA, NA, NA, NA, \"Temperature 8F Winds…\n$ species_comments             &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ x47                          &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ observation_event_id         &lt;chr&gt; \"obsr39944-Rachel Carson Riverfront Park-…\n\n\nI focus on the two main ways people use the eBird app: traveling and stationary. I also filter to only observations from 2016 onwards, since that is when eBird usage became stable in the county.\n\ndf_top_protocols &lt;- df %&gt;% \n  count(protocol_type, sort = TRUE) %&gt;% \n  slice(1:2)\n\ndf &lt;- df %&gt;% \n  semi_join(df_top_protocols) %&gt;% \n  filter(year(observation_date) &gt;= 2016)\n\nThis identifies the top 10 birds in terms of total observations:\n\ndf_species_count &lt;- df %&gt;% \n  group_by(common_name) %&gt;% \n  summarize(observation_count = sum(observation_count, na.rm = TRUE)) %&gt;% \n  arrange(desc(observation_count)) %&gt;% \n  slice(1:10)\n\nThis code filters on the top 10 birds and caculates the cumulative number of sightings and the rolling 21 day average of sightings.\n\ndf_cumulative &lt;- df %&gt;% \n  semi_join(df_species_count, by = c(\"common_name\")) %&gt;% \n  group_by(common_name, observation_date) %&gt;% \n  summarize(observation_count = sum(observation_count, na.rm = TRUE)) %&gt;% \n  ungroup() %&gt;% \n  arrange(common_name, observation_date) %&gt;% \n  group_by(common_name) %&gt;% \n  mutate(observation_count_cumulative = cumsum(observation_count)) %&gt;% \n  tq_mutate(\n    # tq_mutate args\n    select     = observation_count,\n    mutate_fun = rollapply, \n    # rollapply args\n    width      = 21,\n    align      = \"right\",\n    FUN        = mean,\n    # mean args\n    na.rm      = TRUE,\n    # tq_mutate args\n    col_rename = \"mean_21\"\n  )\n\nThis plots the cumulative observations by bird and creates an animation with gganimate:\n\nplot &lt;- df_cumulative %&gt;% \n  ggplot(aes(observation_date, observation_count_cumulative, group = common_name)) +\n  geom_line(alpha = .5) +\n  geom_segment(aes(xend = last(df_cumulative$observation_date) + 240, yend = observation_count_cumulative), linetype = 2, colour = 'grey') +\n  geom_point(aes(size = mean_21)) +\n  geom_label(aes(x = last(df_cumulative$observation_date) + 210, label = common_name), size = 6) +\n  scale_y_comma() +\n  scale_size_continuous(\"21 day rolling average of observation count\", range = c(2, 10), labels = scales::comma) +\n  scale_x_date(limits = c(first(df_cumulative$observation_date), last(df_cumulative$observation_date) + 250)) +\n  labs(x = NULL,\n       y = \"Cumulative observations\",\n       title = \"eBird observations in Allegheny County\",\n       subtitle = \"Top 10 birds 2016 through January 2020\",\n       caption = \"@conor_tompkins\") +\n  coord_cartesian(clip = 'off') +\n  transition_reveal(observation_date)\n\nplot"
  },
  {
    "objectID": "posts/animating-growth-of-allegheny-county/index.html",
    "href": "posts/animating-growth-of-allegheny-county/index.html",
    "title": "Animating Growth of Allegheny County",
    "section": "",
    "text": "In this post I will show how to create animated graphs that illustrate the increase in buildings in Allegheny County.\nOne caveat about the data: it only includes parcels that were sold at some point. If the parcel was not sold, it is not included in this data. For example, a structure that was torn down and replaced but was not sold is not included. It is also reasonable to assume that the data quality decreases the older the records are. There may be a large amount of missing data.\nThe shapefiles for the parcels come from Pennsylvania Spatial Data Access.\nThe data about the construction dates comes from the WPRDC’s Parcels n’at dashboard. To get the relevant data, draw a box around entire county, select the “Year Built” field in the Property Assessments section, and then download the data. It will take a while to download data for the entire county.\nSet up the environment:\n\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(broom)\nlibrary(sf)\nlibrary(scales)\nlibrary(gganimate)\nlibrary(lwgeom)\n\noptions(scipen = 999, digits = 4)\n\ntheme_set(theme_bw())\n\nmy_caption &lt;- \"@conor_tompkins - data from @WPRDC\"\n\nThis reads in data about the land parcel (lot lines):\n\ndf &lt;- read_csv(\"post_data/parcel_data.csv\", progress = FALSE) %&gt;% \n  clean_names() |&gt; \n  select(parid, yearblt)\n\nThis reads in the parcel geometry\n\nfile &lt;- \"post_data/AlleghenyCounty_Parcels202409/AlleghenyCounty_Parcels202409.shp\"\nfile\n\n[1] \"post_data/AlleghenyCounty_Parcels202409/AlleghenyCounty_Parcels202409.shp\"\n\nshapefile &lt;- st_read(file)\n\nReading layer `AlleghenyCounty_Parcels202409' from data source \n  `/Users/conorotompkins/Documents/github_repos/ctompkins_quarto_blog/posts/animating-growth-of-allegheny-county/post_data/AlleghenyCounty_Parcels202409/AlleghenyCounty_Parcels202409.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 585186 features and 10 fields (with 9 geometries empty)\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 1243000 ymin: 321300 xmax: 1430000 ymax: 497900\nProjected CRS: NAD83 / Pennsylvania South (ftUS)\n\n\nNext we have to clean up the parcel geometry:\n\nvalid_check &lt;- shapefile %&gt;% \n  slice(1:nrow(shapefile)) %&gt;% \n  pull(geometry) %&gt;% \n  map(st_is_valid) %&gt;% \n  unlist()\n\nshapefile$validity_check &lt;- valid_check\n\nshapefile &lt;- shapefile %&gt;% \n  filter(validity_check == TRUE)\n\n\nshapefile &lt;- shapefile %&gt;% \n  st_make_valid() %&gt;% \n  clean_names() %&gt;% \n  mutate(pin = as.character(pin))\n\nThen, join the parcel geometry and parcel data:\n\nparcel_data &lt;- shapefile %&gt;% \n  left_join(df, by = join_by(pin == parid))\n\nThis turns the parcel geometry into (x, y) coordinates:\n\ncentroids &lt;- parcel_data %&gt;% \n  st_centroid() %&gt;% \n  st_coordinates() %&gt;% \n  as_tibble() %&gt;% \n  clean_names()\n\nWe can plot the coordinates to confirm that the locations make sense:\n\ncentroids %&gt;% \n  distinct(x, y) %&gt;% \n  ggplot(aes(x, y)) +\n  geom_point(size = .1, alpha = .1) +\n  theme_void() +\n  coord_equal()\n\n\n\n\n\n\n\n\nThis plot shows that there is one row where yearblt_asmt is zero. That doesn’t make sense, so we will exclude it later.\n\ndf %&gt;% \n  ggplot(aes(yearblt)) +\n  geom_density() +\n  geom_rug() +\n  labs(title = \"Structures in Allegheny County\",\n       x = \"Year built\",\n       y = \"Density\",\n       subtitle = my_caption)\n\n\n\n\n\n\n\n\nThis combines the parcel_data and centroid data:\n\nparcel_geometry_cleaned &lt;- bind_cols(parcel_data, centroids) %&gt;% \n  select(pin, x, y, yearblt) %&gt;%\n  mutate(yearblt = as.integer(yearblt)) %&gt;% \n  filter(!is.na(yearblt),\n         yearblt &gt; 1000) %&gt;% \n  st_set_geometry(NULL)\n\nThis plots the culmulative sum of structures built:\n\nparcel_cumulative &lt;- parcel_geometry_cleaned %&gt;% \n  select(pin, yearblt) %&gt;% \n  arrange(yearblt) %&gt;% \n  count(yearblt) %&gt;% \n  mutate(cumulative_n = cumsum(n)) %&gt;% \n  ggplot(aes(yearblt, cumulative_n)) +\n  geom_line() +\n  geom_point() +\n  scale_y_continuous(label = comma) +\n    labs(title = \"Cumulative sum of structures built in Allegheny County\",\n       x = \"Year Built\",\n       y = \"Cumulative sum\",\n       caption = my_caption) +\n  transition_reveal(yearblt)\n\nparcel_cumulative\n\n\n\n\n\n\n\n\nThis creates a graph of the structures built in Allegheny County, colored by the construction year.\n\nparcel_geometry_cleaned %&gt;% \n  ggplot(aes(x, y, color = yearblt, group = pin)) +\n  geom_point(alpha = .3, size = .1) +\n  scale_color_viridis_c(\"Year structure was built\") +\n  theme_void() +\n  theme(axis.text = element_blank(),\n        axis.title = element_blank()) +\n  labs(title = \"Allegheny County land parcels\",\n       subtitle = \"Year built: {frame_along}\",\n       caption = \"@conor_tompkins, data from @WPRDC\") +\n  transition_reveal(yearblt)"
  },
  {
    "objectID": "posts/actblue-interstate-political-campaign-donations/index.html",
    "href": "posts/actblue-interstate-political-campaign-donations/index.html",
    "title": "Actblue Interstate Political Campaign Donations",
    "section": "",
    "text": "ActBlue is an online service that allows people to make donations to the political campaigns of Democractic candidates across the country. This post uses graph theory to analyze how political donations moved across states."
  },
  {
    "objectID": "posts/actblue-interstate-political-campaign-donations/index.html#setup",
    "href": "posts/actblue-interstate-political-campaign-donations/index.html#setup",
    "title": "Actblue Interstate Political Campaign Donations",
    "section": "Setup",
    "text": "Setup\nThese are the libraries and graph theme I will use:\n\nlibrary(tidyverse)\nlibrary(maps)\nlibrary(sf)\n#library(rgeos)\nlibrary(janitor)\nlibrary(ggrepel)\nlibrary(tidygraph)\nlibrary(ggraph)\n\ntheme_set(theme_graph())\n\nsf::sf_use_s2(FALSE)\n\nThis code pulls the boundary polygons for the 48 continental U.S. states and the District of Columbia.\n\n#states &lt;- st_as_sf(map(\"state\", plot = FALSE, fill = TRUE))\nstates &lt;- st_as_sf(maps::map(\"state\", fill=TRUE, plot =FALSE))\nhead(states)\n\nSimple feature collection with 6 features and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -124.3834 ymin: 30.24071 xmax: -71.78015 ymax: 42.04937\nGeodetic CRS:  +proj=longlat +ellps=clrk66 +no_defs +type=crs\n                     ID                           geom\nalabama         alabama MULTIPOLYGON (((-87.46201 3...\narizona         arizona MULTIPOLYGON (((-114.6374 3...\narkansas       arkansas MULTIPOLYGON (((-94.05103 3...\ncalifornia   california MULTIPOLYGON (((-120.006 42...\ncolorado       colorado MULTIPOLYGON (((-102.0552 4...\nconnecticut connecticut MULTIPOLYGON (((-73.49902 4...\n\n\nThis code finds the center of each state, which will act as the nodes for the network graph.\n\nstates &lt;- cbind(states, st_coordinates(st_centroid(states)))\n\nI used this website to get the abbreviations for each state.\n\nstate_abbreviations &lt;- read_csv(\"https://raw.githubusercontent.com/conorotompkins/politics/master/data/state_abbreviations.csv\") %&gt;% \n  clean_names() %&gt;% \n  mutate(state_district = tolower(state_district)) %&gt;% \n  rename(abbr = postal_code) %&gt;% \n  select(-abbreviation)\n\nstates &lt;- states %&gt;% \n  left_join(state_abbreviations, by = c(\"ID\" = \"state_district\")) %&gt;% \n  arrange(abbr)\n\nThis pulls the ActBlue data from the Center of Public Integrity GitHub repo.\n\ndf &lt;- read_csv(\"https://raw.githubusercontent.com/PublicI/actblue-analysis/master/data/actblue_states.csv\")\n\nThis joins the boundary data with the state abbreviations.\n\ndf %&gt;% \n  semi_join(states, by = c(\"contributor_state\" = \"abbr\")) %&gt;% \n  semi_join(states, by = c(\"recipient_state\" = \"abbr\")) -&gt; df\n\ndf %&gt;% \n  select(-c(1, count, sum)) %&gt;% \n  gather(state_type, state_name) %&gt;% \n  distinct() %&gt;% \n  group_by(state_type) %&gt;% \n  summarize(n = n())\n\n# A tibble: 2 × 2\n  state_type            n\n  &lt;chr&gt;             &lt;int&gt;\n1 contributor_state    49\n2 recipient_state      49\n\n\nThis code joins the boundary data with the ActBlue data and excludes donations to and from non-continental U.S. states/territories.\n\nstates %&gt;% \n  semi_join(df, by = c(\"abbr\" = \"contributor_state\")) %&gt;% \n  semi_join(df, by = c(\"abbr\" = \"recipient_state\"))  -&gt; states\n\nThis plot shows that the boundary shapes and centroids are correct.\n\nstates %&gt;% \n  ggplot() +\n  geom_sf() +\n  geom_point(aes(X, Y)) +\n  theme(panel.grid.major = element_line(colour = 'transparent'))\n\n\n\n\n\n\n\n\nThis code cleans up the ActBlue data and removes intrastate donations.\n\ndf %&gt;%\n  select(-1) %&gt;% \n  arrange(contributor_state, recipient_state) %&gt;% \n  mutate(sum = sum / 10^6,\n         sum = round(sum, digits = 2)) %&gt;% \n  na.omit() %&gt;% \n  filter(!(contributor_state == recipient_state)) -&gt; df_intermediate"
  },
  {
    "objectID": "posts/actblue-interstate-political-campaign-donations/index.html#first-attempt",
    "href": "posts/actblue-interstate-political-campaign-donations/index.html#first-attempt",
    "title": "Actblue Interstate Political Campaign Donations",
    "section": "First attempt",
    "text": "First attempt\nThis is how the data looks when graphed as a typical network graph. The nodes (states) are not positioned geographically, which makes it difficult to understand. Aggregate donations less than $1,000,000 are excluded.\n\ndf_intermediate %&gt;% \n  as_tbl_graph(directed = TRUE) %&gt;% \n  activate(edges) %&gt;% \n  filter(sum &gt;= 1) %&gt;% \n  ggraph(layout =) +\n  geom_node_label(aes(label = name), size = 1, repel = FALSE) +\n  geom_edge_fan(aes(edge_width = sum, edge_alpha = sum),\n                arrow = arrow(length = unit(4, 'mm')), \n                start_cap = circle(3, 'mm'),\n                end_cap = circle(3, 'mm'),\n                color = \"blue\") +\n  scale_edge_width_continuous(\"Donations in millions USD\", range = c(.3, 2)) +\n  scale_edge_alpha_continuous(\"Donations in millions USD\", range = c(.1, 1))"
  },
  {
    "objectID": "posts/actblue-interstate-political-campaign-donations/index.html#mapping-node-positions-to-state-geography",
    "href": "posts/actblue-interstate-political-campaign-donations/index.html#mapping-node-positions-to-state-geography",
    "title": "Actblue Interstate Political Campaign Donations",
    "section": "Mapping node positions to state geography",
    "text": "Mapping node positions to state geography\nThis code turns the data into a network object and sets the minimum threshhold at $1 million\n\ndf_intermediate %&gt;% \n  as_tbl_graph(directed = TRUE) -&gt; g\n\nthreshhold &lt;- 1\n\ng %&gt;% \n  activate(edges) %&gt;% \n  filter(sum &gt;= 1) -&gt; g\n\nThis code creates the node positions for the network graph. The centroid of each state will be used as the node for that state.\n\nnode_pos &lt;- states %&gt;%\n  select(abbr, X, Y) %&gt;%\n  rename(x = X, y = Y) %&gt;%  # node positions must be called x, y\n  st_set_geometry(NULL)\nstr(node_pos)\n\n'data.frame':   49 obs. of  3 variables:\n $ abbr: chr  \"AL\" \"AR\" \"AZ\" \"CA\" ...\n $ x   : num  -86.8 -92.4 -111.7 -119.6 -105.6 ...\n $ y   : num  32.8 34.9 34.3 37.3 39 ...\n\n\nThis code creates the node layout the graph will use and merges the network data with the layout.\n\nmanual_layout &lt;- create_layout(g, \n                     #'manual',\n                     layout = node_pos)\n\nThis is the final graph:\n\nggraph(manual_layout) +\n  geom_sf(data = states) +\n  geom_node_point(size = .5, alpha = 0) +\n  geom_edge_fan(aes(edge_width = sum, edge_alpha = sum),\n                arrow = arrow(length = unit(4, 'mm')), \n                start_cap = circle(1, 'mm'),\n                end_cap = circle(1, 'mm'),\n                color = \"blue\") +\n  scale_edge_width_continuous(\"Donations in millions USD\", range = c(.3, 2)) +\n  scale_edge_alpha_continuous(\"Donations in millions USD\", range = c(.1, 1)) +\n  labs(title = \"ActBlue Political Donations\",\n       subtitle = str_c(\"Aggregate interstate donations greater than $\", threshhold, \" million USD, 2017-01-01 to 2018-09-30\"),\n       caption = \"@conor_tompkins, data from Center for Public Integrity and 538\") +\n  theme(panel.grid.major = element_line(colour = 'transparent')) -&gt; p\n\np"
  },
  {
    "objectID": "posts/actblue-interstate-political-campaign-donations/index.html#to-and-from",
    "href": "posts/actblue-interstate-political-campaign-donations/index.html#to-and-from",
    "title": "Actblue Interstate Political Campaign Donations",
    "section": "To and From",
    "text": "To and From\nThis shows all the donations from California. Note the different scale of funds.\n\nggraph(manual_layout) +\n  geom_sf(data = states) +\n  geom_node_point(size = .5, alpha = 0) +\n  geom_edge_fan(aes(edge_width = sum, edge_alpha = sum),\n                arrow = arrow(length = unit(4, 'mm')), \n                start_cap = circle(1, 'mm'),\n                end_cap = circle(1, 'mm'),\n                color = \"blue\") +\n  scale_edge_width_continuous(\"Donations in millions USD\", range = c(.3, 2)) +\n  scale_edge_alpha_continuous(\"Donations in millions USD\", range = c(.1, 1)) +\n  labs(title = \"ActBlue Political Donations\",\n       subtitle = str_c(\"Aggregate interstate donations from \", from_state, \", 2017-01-01 to 2018-09-30\"),\n       caption = \"@conor_tompkins, data from Center for Public Integrity and 538\") +\n  theme(panel.grid.major = element_line(colour = 'transparent')) -&gt; p_ca\n\np_ca\n\n\n\n\n\n\n\n\nThis shows the donations to candidates in Texas. Note the different scale of funds.\n\n# manual_layout &lt;- create_layout(graph = g,\n#                                layout = \"manual\", node.positions = node_pos)\n\n\nggraph(manual_layout) +\n  geom_sf(data = states) +\n  geom_node_point(size = .5, alpha = 0) +\n  geom_edge_fan(aes(edge_width = sum, edge_alpha = sum),\n                arrow = arrow(length = unit(4, 'mm')), \n                start_cap = circle(1, 'mm'),\n                end_cap = circle(1, 'mm'),\n                color = \"blue\") +\n  scale_edge_width_continuous(\"Donations in millions USD\", range = c(.3, 2)) +\n  scale_edge_alpha_continuous(\"Donations in millions USD\", range = c(.1, 1)) +\n  labs(title = \"ActBlue Political Donations\",\n       subtitle = str_c(\"Aggregate interstate donations to \", to_state, \", 2017-01-01 to 2018-09-30\"),\n       caption = \"@conor_tompkins, data from Center for Public Integrity and 538\") +\n  theme(panel.grid.major = element_line(colour = 'transparent')) -&gt; p_tx\n\np_tx"
  },
  {
    "objectID": "posts/actblue-interstate-political-campaign-donations/index.html#references",
    "href": "posts/actblue-interstate-political-campaign-donations/index.html#references",
    "title": "Actblue Interstate Political Campaign Donations",
    "section": "References",
    "text": "References\n\nhttps://github.com/PublicI/actblue-analysis\nhttps://datascience.blog.wzb.eu/2018/05/31/three-ways-of-visualizing-a-graph-on-a-map/\nhttps://lookatthhedata.netlify.com/2017-11-12-mapping-your-oyster-card-journeys-in-london-with-tidygraph-and-ggraph/"
  },
  {
    "objectID": "posts/exploring-311-data-with-pca/index.html",
    "href": "posts/exploring-311-data-with-pca/index.html",
    "title": "Exploring 311 Data With PCA",
    "section": "",
    "text": "Principal Component Analysis is an unsupervised method that reduces the number of dimensions in a dataset and highlights where the data varies. We will use PCA to analyze the 311 dataset from the WPRDC.\n\n\n\n\n\ninstall.packages(c(\"tidyverse\", \"lubridate\", \"broom\", \"ggfortify\", \"ggrepel\", \"janitor\"))\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(broom)\nlibrary(ggfortify)\nlibrary(ggrepel)\nlibrary(janitor)\n\noptions(scipen = 999, digits = 4)\nset.seed(1234)\n\ntheme_set(theme_bw())\n\n\n\n\n\n\nread_csv(\"https://raw.githubusercontent.com/conorotompkins/pittsburgh_311/master/data/pittsburgh_311.csv\", progress = FALSE) %&gt;% \n  clean_names() %&gt;% \n  mutate(date = ymd(str_sub(created_on, 1, 10)),\n         month = month(date, label = TRUE)) %&gt;% \n  filter(date &lt; \"2018-07-19\") -&gt; df\n\n\n\n\nCreate a dataframe of the top request types\n\n(df %&gt;% \n  count(request_type, sort = TRUE) %&gt;% \n  filter(n &gt; 400)-&gt; df_top_requests)\n\n# A tibble: 84 × 2\n   request_type                             n\n   &lt;chr&gt;                                &lt;int&gt;\n 1 Potholes                             25202\n 2 Weeds/Debris                         16503\n 3 Building Maintenance                 10469\n 4 Snow/Ice removal                      7006\n 5 Refuse Violations                     6515\n 6 Abandoned Vehicle (parked on street)  5877\n 7 Missed Pick Up                        4689\n 8 Replace/Repair a Sign                 4445\n 9 Building Without a Permit             4404\n10 Litter                                4198\n# ℹ 74 more rows\n\n\nCount the number of requests per month by request type, filter for the top request types, and fill in gaps in the data\n\n(df %&gt;%\n  semi_join(df_top_requests) %&gt;% \n  group_by(request_type, month) %&gt;% \n  summarize(n = n()) %&gt;% \n  ungroup() %&gt;%\n  complete(request_type, month) %&gt;% \n  replace_na(replace = list(n = 0)) -&gt; df_months)\n\n# A tibble: 1,008 × 3\n   request_type                         month     n\n   &lt;chr&gt;                                &lt;ord&gt; &lt;int&gt;\n 1 Abandoned Vehicle (parked on street) Jan     523\n 2 Abandoned Vehicle (parked on street) Feb     427\n 3 Abandoned Vehicle (parked on street) Mar     452\n 4 Abandoned Vehicle (parked on street) Apr     417\n 5 Abandoned Vehicle (parked on street) May     488\n 6 Abandoned Vehicle (parked on street) Jun     466\n 7 Abandoned Vehicle (parked on street) Jul     457\n 8 Abandoned Vehicle (parked on street) Aug     596\n 9 Abandoned Vehicle (parked on street) Sep     525\n10 Abandoned Vehicle (parked on street) Oct     571\n# ℹ 998 more rows\n\n\nCalculate the percentage of a request type for each month\n\n(df_months %&gt;% \n  group_by(request_type) %&gt;% \n  mutate(request_type_total = sum(n),\n         month_percentage = n / request_type_total) -&gt; df_months)\n\n# A tibble: 1,008 × 5\n# Groups:   request_type [84]\n   request_type                  month     n request_type_total month_percentage\n   &lt;chr&gt;                         &lt;ord&gt; &lt;int&gt;              &lt;int&gt;            &lt;dbl&gt;\n 1 Abandoned Vehicle (parked on… Jan     523               5877           0.0890\n 2 Abandoned Vehicle (parked on… Feb     427               5877           0.0727\n 3 Abandoned Vehicle (parked on… Mar     452               5877           0.0769\n 4 Abandoned Vehicle (parked on… Apr     417               5877           0.0710\n 5 Abandoned Vehicle (parked on… May     488               5877           0.0830\n 6 Abandoned Vehicle (parked on… Jun     466               5877           0.0793\n 7 Abandoned Vehicle (parked on… Jul     457               5877           0.0778\n 8 Abandoned Vehicle (parked on… Aug     596               5877           0.101 \n 9 Abandoned Vehicle (parked on… Sep     525               5877           0.0893\n10 Abandoned Vehicle (parked on… Oct     571               5877           0.0972\n# ℹ 998 more rows\n\n\nCheck for bad data\n\ndf_months %&gt;% \n  filter(is.na(month_percentage) | is.nan(month_percentage))\n\n# A tibble: 0 × 5\n# Groups:   request_type [0]\n# ℹ 5 variables: request_type &lt;chr&gt;, month &lt;ord&gt;, n &lt;int&gt;,\n#   request_type_total &lt;int&gt;, month_percentage &lt;dbl&gt;\n\n\nSpread the data to turn the months into the columns\n\n(df_months %&gt;% \n  select(request_type, month, month_percentage) %&gt;% \n  spread(month, month_percentage) %&gt;% \n  ungroup() -&gt; df_months)\n\n# A tibble: 84 × 13\n   request_type     Jan     Feb    Mar    Apr    May    Jun    Jul    Aug    Sep\n   &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 Abandoned V… 0.0890  0.0727  0.0769 0.0710 0.0830 0.0793 0.0778 0.101  0.0893\n 2 Barking Dog  0.0563  0.0608  0.0608 0.0631 0.104  0.101  0.0788 0.113  0.124 \n 3 Board Up (P… 0.0395  0.0482  0.0658 0.0943 0.114  0.0899 0.110  0.123  0.0855\n 4 Broken Side… 0.0337  0.155   0.148  0.0872 0.105  0.0964 0.0696 0.0735 0.0528\n 5 Building Ma… 0.0708  0.0919  0.103  0.0739 0.0842 0.0829 0.0725 0.0919 0.0776\n 6 Building Wi… 0.0842  0.0697  0.0636 0.0577 0.105  0.0883 0.0924 0.0815 0.0829\n 7 Catch Basin… 0.0636  0.0377  0.0778 0.0748 0.0984 0.132  0.0825 0.127  0.105 \n 8 City Source… 0.00527 0.00246 0.0105 0.0428 0.196  0.213  0.195  0.164  0.0808\n 9 City Steps,… 0.0443  0.0180  0.0148 0.0197 0.116  0.216  0.203  0.146  0.118 \n10 City Steps,… 0.0265  0.0305  0.0713 0.0509 0.128  0.120  0.136  0.128  0.108 \n# ℹ 74 more rows\n# ℹ 3 more variables: Oct &lt;dbl&gt;, Nov &lt;dbl&gt;, Dec &lt;dbl&gt;\n\n\nCheck that they all add up to 1 across the rows\n\n(df_months %&gt;% \n  select(Jan:Dec) %&gt;% \n  mutate(row_sum = rowSums(.)) %&gt;% \n  select(row_sum, everything()) -&gt; test)\n\n# A tibble: 84 × 13\n   row_sum     Jan     Feb    Mar    Apr    May    Jun    Jul    Aug    Sep\n     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1       1 0.0890  0.0727  0.0769 0.0710 0.0830 0.0793 0.0778 0.101  0.0893\n 2       1 0.0563  0.0608  0.0608 0.0631 0.104  0.101  0.0788 0.113  0.124 \n 3       1 0.0395  0.0482  0.0658 0.0943 0.114  0.0899 0.110  0.123  0.0855\n 4       1 0.0337  0.155   0.148  0.0872 0.105  0.0964 0.0696 0.0735 0.0528\n 5       1 0.0708  0.0919  0.103  0.0739 0.0842 0.0829 0.0725 0.0919 0.0776\n 6       1 0.0842  0.0697  0.0636 0.0577 0.105  0.0883 0.0924 0.0815 0.0829\n 7       1 0.0636  0.0377  0.0778 0.0748 0.0984 0.132  0.0825 0.127  0.105 \n 8       1 0.00527 0.00246 0.0105 0.0428 0.196  0.213  0.195  0.164  0.0808\n 9       1 0.0443  0.0180  0.0148 0.0197 0.116  0.216  0.203  0.146  0.118 \n10       1 0.0265  0.0305  0.0713 0.0509 0.128  0.120  0.136  0.128  0.108 \n# ℹ 74 more rows\n# ℹ 3 more variables: Oct &lt;dbl&gt;, Nov &lt;dbl&gt;, Dec &lt;dbl&gt;\n\n\n\n\n\ndf_months %&gt;% \n  ggplot(aes(Jan, Jul)) +\n  geom_point()\n\n\n\n\n\n\n\n\nRemember that each dot represents a request type, and the month shows what % of that request type occurred that month\n\ndf_months %&gt;% \n  ggplot(aes(Apr, Oct)) +\n  geom_point()\n\n\n\n\n\n\n\n\nIt is not feasible to plot all the months against each other. PCA can help by condensing the columns and increasing the variance. PCA creates eigenvectors that represents the data in a concentrated way. Eigenvectors and eigenvalues do not represent observed data. They are calculated representations of the data. We will refer to eigenvectors as “principal components”.\nIn this case, where our data is measured by months in a year, each principal component could loosely be compared to a season.\n\n\n\n\nThe PCA function requires an all-numeric dataframe, so drop the request types into the dataframe metadata\n\n(df_months %&gt;% \n  ungroup() %&gt;% \n  remove_rownames() %&gt;% \n  column_to_rownames(var = \"request_type\") -&gt; df_months_pca1)\n\n                                                 Jan      Feb      Mar      Apr\nAbandoned Vehicle (parked on street)        0.088991 0.072656 0.076910 0.070955\nBarking Dog                                 0.056306 0.060811 0.060811 0.063063\nBoard Up (PLI referral to DPW)              0.039474 0.048246 0.065789 0.094298\nBroken Sidewalk                             0.033665 0.154552 0.147666 0.087223\nBuilding Maintenance                        0.070780 0.091890 0.103353 0.073933\nBuilding Without a Permit                   0.084242 0.069709 0.063579 0.057675\nCatch Basin, Clogged                        0.063642 0.037714 0.077784 0.074838\nCity Source (CDBG)                          0.005267 0.002458 0.010534 0.042837\nCity Steps, Need Cleared                    0.044262 0.018033 0.014754 0.019672\nCity Steps, Need Repaired                   0.026477 0.030550 0.071283 0.050916\nCollapsed Catch Basin                       0.064220 0.053899 0.061927 0.075688\nCommercial Refuse/Dumpsters                 0.079430 0.079430 0.089613 0.077393\nCurb /Broken/Deteriorated                   0.042373 0.048729 0.072034 0.110169\nCurb/Request for Asphalt Windrow            0.038660 0.020619 0.028351 0.085052\nDead Animal                                 0.038181 0.032566 0.043234 0.076923\nDead tree (Public property)                 0.034516 0.028763 0.049856 0.066155\nDrainage/Leak                               0.141304 0.050000 0.035870 0.083696\nDrug Enforcement                            0.077085 0.049755 0.065172 0.079187\nDumping, Private Property                   0.064315 0.076763 0.120332 0.093361\nDumpster (on Street)                        0.070866 0.048819 0.042520 0.088189\nEarly Set Out                               0.072444 0.069736 0.062288 0.063643\nExcessive Noise/Disturbances                0.057377 0.047131 0.056011 0.085383\nField                                       0.016432 0.014085 0.042254 0.110329\nFire Safety System Not Working              0.093750 0.185547 0.128906 0.077474\nGraffiti, Documentation                     0.057116 0.054307 0.103933 0.102060\nGraffiti, Removal                           0.088710 0.111290 0.098387 0.046774\nHydrant                                     0.121771 0.062731 0.075646 0.053506\nIllegal Dumping                             0.065672 0.057214 0.076617 0.106965\nIllegal Parking                             0.095682 0.075074 0.081943 0.079735\nJunk Vehicles                               0.079384 0.093602 0.114929 0.068720\nLeak                                        0.171456 0.097418 0.048709 0.044812\nLeaves/Street Cleaning                      0.028967 0.031486 0.030227 0.059194\nLitter                                      0.064316 0.064555 0.085755 0.093378\nLitter Can, Public                          0.064777 0.049393 0.060729 0.069636\nMaintenance Issue                           0.026455 0.031746 0.039153 0.078307\nMayor's Office                              0.158455 0.033289 0.023968 0.027963\nMissed Blue Bag                             0.094002 0.042077 0.051925 0.068935\nMissed Pick Up                              0.076775 0.048198 0.047345 0.058861\nNeed Potable Water                          0.002398 0.914868 0.001199 0.003597\nOperating Without a License                 0.041215 0.021692 0.149675 0.114967\nOvergrowth                                  0.005058 0.007867 0.006462 0.019106\nParking Authority                           0.086022 0.075269 0.064516 0.105376\nPatrol                                      0.063164 0.047816 0.071429 0.081464\nPaving Concern/Problem                      0.054819 0.043324 0.042440 0.071618\nPaving Request                              0.052950 0.047504 0.108321 0.114675\nPermit Parking (Residential Parking Permit) 0.107062 0.075171 0.063781 0.079727\nPlayground                                  0.015038 0.024436 0.043233 0.093985\nPotholes                                    0.123324 0.052972 0.105230 0.112570\nPruning (city tree)                         0.024019 0.025372 0.044317 0.057510\nPublic Right of Way                         0.033397 0.029580 0.020992 0.057252\nQuestion                                    0.079824 0.064739 0.043997 0.024513\nReferral                                    0.099161 0.050725 0.046148 0.049962\nRefuse Violations                           0.079202 0.059708 0.065848 0.084728\nReplace/Repair a Sign                       0.067492 0.053093 0.080315 0.084814\nRequest New Sign                            0.069169 0.049768 0.059047 0.097005\nRetaining Wall Maintenance                  0.066239 0.091880 0.115385 0.096154\nRodent control                              0.040957 0.033594 0.041417 0.052462\nRoot prune                                  0.022321 0.038690 0.053571 0.098214\nSidewalk Obstruction                        0.052799 0.042621 0.044529 0.052163\nSidewalk, Lack of Snow/Ice Removal          0.767726 0.090465 0.002445 0.002445\nSinkhole                                    0.103995 0.058973 0.062143 0.066582\nSmoke detectors                             0.118421 0.064145 0.062500 0.092105\nSnow/Ice removal                            0.681273 0.135027 0.005995 0.002712\nSpeeding                                    0.063973 0.060606 0.084175 0.094276\nStreet Cleaning/Sweeping                    0.027306 0.026790 0.035033 0.102009\nStreet Light - Repair                       0.078803 0.055112 0.067830 0.044888\nStreet Obstruction/Closure                  0.126273 0.040733 0.081466 0.061100\nThank you - DPW                             0.136264 0.046154 0.032967 0.065934\nTraffic                                     0.065356 0.053666 0.061637 0.073326\nTraffic or Pedestrian Signal, Repair        0.089659 0.047221 0.069934 0.069337\nTraffic or Pedestrian Signal, Request       0.056641 0.029297 0.099609 0.087891\nTree Fallen Across Road                     0.042589 0.031516 0.051959 0.034923\nTree Fallen Across Sidewalk                 0.034125 0.028190 0.044510 0.044510\nTree Issues                                 0.038384 0.056566 0.076768 0.048485\nTree Removal                                0.042949 0.036507 0.071582 0.074445\nUnpermitted Electrical Work                 0.145055 0.012088 0.030769 0.019780\nUnpermitted HVAC Work                       0.108516 0.045330 0.064560 0.048077\nUtility Cut - Other                         0.114889 0.067995 0.052755 0.059789\nUtility Cut - PWSA                          0.202261 0.050251 0.057789 0.075377\nUtility Pole                                0.065728 0.075117 0.049296 0.075117\nVacant Building                             0.088199 0.083230 0.096066 0.048861\nWeeds/Debris                                0.029631 0.024965 0.035751 0.045083\nWires                                       0.060651 0.071006 0.060651 0.078402\nZoning Issue                                0.055000 0.065000 0.080833 0.082500\n                                                  May       Jun      Jul\nAbandoned Vehicle (parked on street)        0.0830356 0.0792922 0.077761\nBarking Dog                                 0.1036036 0.1013514 0.078829\nBoard Up (PLI referral to DPW)              0.1140351 0.0899123 0.109649\nBroken Sidewalk                             0.1048202 0.0964040 0.069625\nBuilding Maintenance                        0.0841532 0.0829115 0.072500\nBuilding Without a Permit                   0.1049046 0.0883288 0.092416\nCatch Basin, Clogged                        0.0984090 0.1319976 0.082499\nCity Source (CDBG)                          0.1955758 0.2134831 0.195225\nCity Steps, Need Cleared                    0.1163934 0.2163934 0.203279\nCity Steps, Need Repaired                   0.1283096 0.1201629 0.136456\nCollapsed Catch Basin                       0.1100917 0.0917431 0.083716\nCommercial Refuse/Dumpsters                 0.0529532 0.1038697 0.105906\nCurb /Broken/Deteriorated                   0.1525424 0.1122881 0.116525\nCurb/Request for Asphalt Windrow            0.1430412 0.2113402 0.155928\nDead Animal                                 0.0713083 0.1021898 0.139809\nDead tree (Public property)                 0.1246405 0.1447747 0.154362\nDrainage/Leak                               0.0902174 0.1043478 0.102174\nDrug Enforcement                            0.0988087 0.1023125 0.088998\nDumping, Private Property                   0.0746888 0.0622407 0.064315\nDumpster (on Street)                        0.0787402 0.1354331 0.105512\nEarly Set Out                               0.0886933 0.0873392 0.111713\nExcessive Noise/Disturbances                0.0887978 0.0758197 0.075137\nField                                       0.1854460 0.1384977 0.150235\nFire Safety System Not Working              0.1139323 0.0572917 0.047526\nGraffiti, Documentation                     0.1207865 0.1254682 0.073970\nGraffiti, Removal                           0.0338710 0.0596774 0.091935\nHydrant                                     0.0922509 0.0571956 0.064576\nIllegal Dumping                             0.1000000 0.1228856 0.113930\nIllegal Parking                             0.0765456 0.0691855 0.059863\nJunk Vehicles                               0.0864929 0.0710900 0.104265\nLeak                                        0.0491963 0.0526059 0.057964\nLeaves/Street Cleaning                      0.0629723 0.0541562 0.021411\nLitter                                      0.0826584 0.0855169 0.098380\nLitter Can, Public                          0.0923077 0.0995951 0.127126\nMaintenance Issue                           0.1417989 0.1185185 0.135450\nMayor's Office                              0.0319574 0.1824234 0.065246\nMissed Blue Bag                             0.0841540 0.1020591 0.087735\nMissed Pick Up                              0.1027938 0.1123907 0.118789\nNeed Potable Water                          0.0011990 0.0023981 0.000000\nOperating Without a License                 0.4338395 0.0542299 0.028200\nOvergrowth                                  0.1219444 0.2489463 0.234335\nParking Authority                           0.0860215 0.0838710 0.081720\nPatrol                                      0.1015348 0.0879575 0.095041\nPaving Concern/Problem                      0.1114058 0.1255526 0.085765\nPaving Request                              0.1397882 0.1458396 0.118306\nPermit Parking (Residential Parking Permit) 0.0569476 0.0706150 0.079727\nPlayground                                  0.1184211 0.1691729 0.159774\nPotholes                                    0.1346322 0.1150702 0.107095\nPruning (city tree)                         0.1234777 0.1742219 0.168133\nPublic Right of Way                         0.1316794 0.1650763 0.154580\nQuestion                                    0.0483972 0.0936518 0.122564\nReferral                                    0.0362319 0.0846682 0.129291\nRefuse Violations                           0.0784344 0.0983883 0.100844\nReplace/Repair a Sign                       0.1196850 0.1113611 0.094713\nRequest New Sign                            0.0932096 0.0927879 0.097427\nRetaining Wall Maintenance                  0.0961538 0.0982906 0.085470\nRodent control                              0.0745513 0.1099862 0.141279\nRoot prune                                  0.1190476 0.1264881 0.163690\nSidewalk Obstruction                        0.0807888 0.1075064 0.123410\nSidewalk, Lack of Snow/Ice Removal          0.0097800 0.0000000 0.002445\nSinkhole                                    0.0786303 0.1122384 0.128725\nSmoke detectors                             0.0871711 0.1348684 0.046053\nSnow/Ice removal                            0.0008564 0.0001427 0.000000\nSpeeding                                    0.0976431 0.0909091 0.104377\nStreet Cleaning/Sweeping                    0.1257084 0.1298300 0.123132\nStreet Light - Repair                       0.0498753 0.0675810 0.097257\nStreet Obstruction/Closure                  0.0549898 0.0509165 0.087576\nThank you - DPW                             0.0769231 0.0967033 0.105495\nTraffic                                     0.0887354 0.0600425 0.054729\nTraffic or Pedestrian Signal, Repair        0.0854752 0.1040048 0.086671\nTraffic or Pedestrian Signal, Request       0.1074219 0.1191406 0.085938\nTree Fallen Across Road                     0.1345826 0.2206133 0.137990\nTree Fallen Across Sidewalk                 0.1424332 0.1958457 0.126113\nTree Issues                                 0.1010101 0.1010101 0.129293\nTree Removal                                0.1295634 0.1410165 0.118826\nUnpermitted Electrical Work                 0.1054945 0.1230769 0.085714\nUnpermitted HVAC Work                       0.1689560 0.0879121 0.085165\nUtility Cut - Other                         0.0797186 0.0738570 0.082063\nUtility Cut - PWSA                          0.1005025 0.0967337 0.095477\nUtility Pole                                0.1126761 0.1244131 0.107981\nVacant Building                             0.0683230 0.0608696 0.064182\nWeeds/Debris                                0.1373690 0.1666364 0.157062\nWires                                       0.0961538 0.1434911 0.087278\nZoning Issue                                0.0958333 0.0866667 0.103333\n                                                  Aug      Sep       Oct\nAbandoned Vehicle (parked on street)        0.1014123 0.089331 0.0971584\nBarking Dog                                 0.1126126 0.123874 0.1126126\nBoard Up (PLI referral to DPW)              0.1228070 0.085526 0.0877193\nBroken Sidewalk                             0.0734507 0.052793 0.0849273\nBuilding Maintenance                        0.0918903 0.077562 0.0787086\nBuilding Without a Permit                   0.0815168 0.082879 0.1128520\nCatch Basin, Clogged                        0.1266942 0.104891 0.0931055\nCity Source (CDBG)                          0.1639747 0.080758 0.0582865\nCity Steps, Need Cleared                    0.1459016 0.118033 0.0557377\nCity Steps, Need Repaired                   0.1283096 0.107943 0.0855397\nCollapsed Catch Basin                       0.1238532 0.113532 0.0986239\nCommercial Refuse/Dumpsters                 0.1344196 0.071283 0.0712831\nCurb /Broken/Deteriorated                   0.1186441 0.084746 0.0572034\nCurb/Request for Asphalt Windrow            0.1082474 0.079897 0.0605670\nDead Animal                                 0.1235261 0.112296 0.1274565\nDead tree (Public property)                 0.1236817 0.102589 0.0882071\nDrainage/Leak                               0.1076087 0.058696 0.0782609\nDrug Enforcement                            0.1135249 0.117730 0.0946041\nDumping, Private Property                   0.1327801 0.076763 0.1016598\nDumpster (on Street)                        0.1070866 0.086614 0.0881890\nEarly Set Out                               0.1570752 0.080569 0.0663507\nExcessive Noise/Disturbances                0.0881148 0.090164 0.1038251\nField                                       0.1197183 0.098592 0.0610329\nFire Safety System Not Working              0.0449219 0.050781 0.0651042\nGraffiti, Documentation                     0.0608614 0.067416 0.1207865\nGraffiti, Removal                           0.1048387 0.125806 0.0935484\nHydrant                                     0.1254613 0.077491 0.0867159\nIllegal Dumping                             0.0651741 0.068657 0.0726368\nIllegal Parking                             0.0969087 0.103042 0.0991168\nJunk Vehicles                               0.0912322 0.072275 0.0758294\nLeak                                        0.1066732 0.080857 0.0681929\nLeaves/Street Cleaning                      0.0440806 0.012594 0.0642317\nLitter                                      0.1202954 0.095760 0.0824202\nLitter Can, Public                          0.1246964 0.110121 0.0850202\nMaintenance Issue                           0.1174603 0.113228 0.1047619\nMayor's Office                              0.1118509 0.114514 0.0892144\nMissed Blue Bag                             0.1056401 0.089526 0.0957923\nMissed Pick Up                              0.1106846 0.098315 0.0835999\nNeed Potable Water                          0.0731415 0.000000 0.0000000\nOperating Without a License                 0.0390456 0.021692 0.0455531\nOvergrowth                                  0.1944366 0.104805 0.0396179\nParking Authority                           0.1032258 0.090323 0.0731183\nPatrol                                      0.1097993 0.103306 0.1151122\nPaving Concern/Problem                      0.0813439 0.089302 0.1432361\nPaving Request                              0.0928896 0.075340 0.0490166\nPermit Parking (Residential Parking Permit) 0.1321185 0.102506 0.0956720\nPlayground                                  0.1672932 0.090226 0.0733083\nPotholes                                    0.0759860 0.050155 0.0442425\nPruning (city tree)                         0.1586604 0.082206 0.0801759\nPublic Right of Way                         0.1832061 0.094466 0.0620229\nQuestion                                    0.1646763 0.065996 0.1451917\nReferral                                    0.1525553 0.089245 0.1060259\nRefuse Violations                           0.1122026 0.085035 0.0968534\nReplace/Repair a Sign                       0.0899888 0.088189 0.0758155\nRequest New Sign                            0.1100801 0.113454 0.0889920\nRetaining Wall Maintenance                  0.0769231 0.055556 0.1089744\nRodent control                              0.1385182 0.125633 0.1214910\nRoot prune                                  0.1056548 0.096726 0.0937500\nSidewalk Obstruction                        0.1246819 0.117048 0.0966921\nSidewalk, Lack of Snow/Ice Removal          0.0000000 0.002445 0.0000000\nSinkhole                                    0.1230184 0.088142 0.0798985\nSmoke detectors                             0.0756579 0.041118 0.1118421\nSnow/Ice removal                            0.0001427 0.000000 0.0001427\nSpeeding                                    0.1144781 0.112795 0.0791246\nStreet Cleaning/Sweeping                    0.1215868 0.098403 0.0963421\nStreet Light - Repair                       0.1057357 0.105237 0.1134663\nStreet Obstruction/Closure                  0.1201629 0.120163 0.0916497\nThank you - DPW                             0.1252747 0.112088 0.0879121\nTraffic                                     0.1046759 0.162062 0.1232731\nTraffic or Pedestrian Signal, Repair        0.1165571 0.105798 0.0854752\nTraffic or Pedestrian Signal, Request       0.0742188 0.111328 0.1035156\nTree Fallen Across Road                     0.1831346 0.064736 0.0442930\nTree Fallen Across Sidewalk                 0.2121662 0.063798 0.0474777\nTree Issues                                 0.1454545 0.094949 0.0848485\nTree Removal                                0.1460272 0.085183 0.0787402\nUnpermitted Electrical Work                 0.0571429 0.065934 0.1318681\nUnpermitted HVAC Work                       0.0824176 0.075549 0.0879121\nUtility Cut - Other                         0.1160610 0.110199 0.0937866\nUtility Cut - PWSA                          0.0716080 0.062814 0.0690955\nUtility Pole                                0.1197183 0.077465 0.0563380\nVacant Building                             0.0749482 0.077847 0.0815735\nWeeds/Debris                                0.1616070 0.097922 0.0718657\nWires                                       0.1316568 0.091716 0.0532544\nZoning Issue                                0.0908333 0.079167 0.0875000\n                                                 Nov      Dec\nAbandoned Vehicle (parked on street)        0.086098 0.076400\nBarking Dog                                 0.074324 0.051802\nBoard Up (PLI referral to DPW)              0.076754 0.065789\nBroken Sidewalk                             0.064269 0.030604\nBuilding Maintenance                        0.093419 0.078900\nBuilding Without a Permit                   0.086285 0.075613\nCatch Basin, Clogged                        0.064820 0.043606\nCity Source (CDBG)                          0.025632 0.005969\nCity Steps, Need Cleared                    0.022951 0.024590\nCity Steps, Need Repaired                   0.075356 0.038697\nCollapsed Catch Basin                       0.068807 0.053899\nCommercial Refuse/Dumpsters                 0.069246 0.065173\nCurb /Broken/Deteriorated                   0.046610 0.038136\nCurb/Request for Asphalt Windrow            0.047680 0.020619\nDead Animal                                 0.083661 0.048849\nDead tree (Public property)                 0.051774 0.030681\nDrainage/Leak                               0.070652 0.077174\nDrug Enforcement                            0.067274 0.045550\nDumping, Private Property                   0.074689 0.058091\nDumpster (on Street)                        0.083465 0.064567\nEarly Set Out                               0.069059 0.071090\nExcessive Noise/Disturbances                0.092213 0.140027\nField                                       0.032864 0.030516\nFire Safety System Not Working              0.051432 0.083333\nGraffiti, Documentation                     0.072097 0.041199\nGraffiti, Removal                           0.066129 0.079032\nHydrant                                     0.090406 0.092251\nIllegal Dumping                             0.083582 0.066667\nIllegal Parking                             0.088077 0.074828\nJunk Vehicles                               0.091232 0.050948\nLeak                                        0.097418 0.124696\nLeaves/Street Cleaning                      0.430730 0.159950\nLitter                                      0.070272 0.056694\nLitter Can, Public                          0.068826 0.047773\nMaintenance Issue                           0.053968 0.039153\nMayor's Office                              0.083888 0.077230\nMissed Blue Bag                             0.091316 0.086840\nMissed Pick Up                              0.070164 0.072084\nNeed Potable Water                          0.000000 0.001199\nOperating Without a License                 0.026030 0.023861\nOvergrowth                                  0.013768 0.003653\nParking Authority                           0.068817 0.081720\nPatrol                                      0.070838 0.052538\nPaving Concern/Problem                      0.108753 0.042440\nPaving Request                              0.034493 0.020877\nPermit Parking (Residential Parking Permit) 0.077449 0.059226\nPlayground                                  0.041353 0.003759\nPotholes                                    0.053289 0.025434\nPruning (city tree)                         0.041272 0.020636\nPublic Right of Way                         0.044847 0.022901\nQuestion                                    0.084852 0.061596\nReferral                                    0.081998 0.073989\nRefuse Violations                           0.067536 0.071220\nReplace/Repair a Sign                       0.070191 0.064342\nRequest New Sign                            0.076339 0.052720\nRetaining Wall Maintenance                  0.070513 0.038462\nRodent control                              0.071330 0.048780\nRoot prune                                  0.043155 0.038690\nSidewalk Obstruction                        0.086514 0.071247\nSidewalk, Lack of Snow/Ice Removal          0.000000 0.122249\nSinkhole                                    0.058339 0.039315\nSmoke detectors                             0.088816 0.077303\nSnow/Ice removal                            0.003711 0.169997\nSpeeding                                    0.052189 0.045455\nStreet Cleaning/Sweeping                    0.082947 0.030912\nStreet Light - Repair                       0.115960 0.098254\nStreet Obstruction/Closure                  0.089613 0.075356\nThank you - DPW                             0.065934 0.048352\nTraffic                                     0.088735 0.063762\nTraffic or Pedestrian Signal, Repair        0.084877 0.054991\nTraffic or Pedestrian Signal, Request       0.068359 0.056641\nTree Fallen Across Road                     0.036627 0.017036\nTree Fallen Across Sidewalk                 0.040059 0.020772\nTree Issues                                 0.070707 0.052525\nTree Removal                                0.047960 0.027201\nUnpermitted Electrical Work                 0.101099 0.121978\nUnpermitted HVAC Work                       0.074176 0.071429\nUtility Cut - Other                         0.069168 0.079719\nUtility Cut - PWSA                          0.081658 0.036432\nUtility Pole                                0.075117 0.061033\nVacant Building                             0.132091 0.123810\nWeeds/Debris                                0.045386 0.026722\nWires                                       0.076923 0.048817\nZoning Issue                                0.092500 0.080833\n\n\nCreate the PCA object\n\n(df_months_pca1 %&gt;% \n  prcomp(scale = TRUE) -&gt; pc)\n\nStandard deviations (1, .., p=12):\n [1] 2.0313544303132338165 1.5112299607905637089 1.3677583442481686671\n [4] 1.0647449915481708160 0.9373153843502739502 0.6612690017981475155\n [7] 0.6319678449167122070 0.5732234023111666410 0.4666060722915733039\n[10] 0.4192405535100036107 0.3847717270238655840 0.0000000000000002152\n\nRotation (n x k) = (12 x 12):\n        PC1     PC2       PC3      PC4      PC5        PC6      PC7      PC8\nJan -0.3509  0.2377  0.036588 -0.40554 -0.40259 -0.0878343  0.04648  0.10077\nFeb -0.2189  0.2230 -0.226391  0.69886  0.14610  0.1005237  0.15306 -0.04311\nMar -0.0235 -0.4858 -0.323760  0.11519 -0.19761 -0.1724337 -0.62037  0.31077\nApr  0.1329 -0.4686 -0.301500 -0.04306 -0.07313 -0.5014973  0.54199 -0.09929\nMay  0.2339 -0.1448 -0.445597 -0.33914  0.15507  0.5723879 -0.10942 -0.40042\nJun  0.4049  0.1866  0.002112 -0.22765  0.21210 -0.0386388  0.13438  0.42038\nJul  0.4322  0.1697  0.095923 -0.10888  0.01800 -0.1443989  0.02032  0.13081\nAug  0.3866  0.1805  0.189907  0.17598  0.04242 -0.2554247 -0.44835 -0.13536\nSep  0.2944 -0.1580  0.365255  0.16856 -0.42595 -0.0005222  0.04916 -0.53195\nOct  0.1150 -0.4130  0.389922  0.16708 -0.17452  0.5136274  0.18989  0.43757\nNov -0.1323 -0.3291  0.333424 -0.05855  0.69855 -0.1384698 -0.02866 -0.09659\nDec -0.3720 -0.1311  0.333522 -0.25017  0.03408 -0.0524049 -0.15824 -0.15902\n          PC9     PC10      PC11   PC12\nJan -0.119474 -0.19207  0.310620 0.5699\nFeb  0.085692  0.10149 -0.130098 0.5209\nMar  0.259371  0.01955  0.035606 0.1620\nApr -0.264116  0.01396 -0.143422 0.1379\nMay -0.106727  0.02849 -0.028027 0.2717\nJun  0.373786 -0.36939 -0.413396 0.2471\nJul  0.103405  0.77442  0.256091 0.2205\nAug -0.607417 -0.20278 -0.123033 0.1983\nSep  0.454920 -0.16862  0.035475 0.1519\nOct -0.304459  0.02271 -0.003209 0.1456\nNov  0.104258 -0.14115  0.387838 0.2472\nDec  0.008971  0.35391 -0.678722 0.1743\n\n\nInspect the PCA object with tidier functions from the broom library. These functions turn the PCA object into a tidy dataframe\n\npc %&gt;% \n  tidy() %&gt;% \n  head()\n\n# A tibble: 6 × 3\n  row                                     PC   value\n  &lt;chr&gt;                                &lt;dbl&gt;   &lt;dbl&gt;\n1 Abandoned Vehicle (parked on street)     1 -0.844 \n2 Abandoned Vehicle (parked on street)     2 -0.844 \n3 Abandoned Vehicle (parked on street)     3  0.383 \n4 Abandoned Vehicle (parked on street)     4  0.311 \n5 Abandoned Vehicle (parked on street)     5 -0.206 \n6 Abandoned Vehicle (parked on street)     6  0.0620\n\n\n\npc %&gt;% \n  tidy(\"pcs\")\n\n# A tibble: 12 × 4\n      PC  std.dev percent cumulative\n   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n 1     1 2.03e+ 0  0.344       0.344\n 2     2 1.51e+ 0  0.190       0.534\n 3     3 1.37e+ 0  0.156       0.690\n 4     4 1.06e+ 0  0.0945      0.785\n 5     5 9.37e- 1  0.0732      0.858\n 6     6 6.61e- 1  0.0364      0.894\n 7     7 6.32e- 1  0.0333      0.927\n 8     8 5.73e- 1  0.0274      0.955\n 9     9 4.67e- 1  0.0181      0.973\n10    10 4.19e- 1  0.0146      0.988\n11    11 3.85e- 1  0.0123      1    \n12    12 2.15e-16  0           1    \n\n\n\npc %&gt;% \n  augment(data = df_months) -&gt; au\n\nau %&gt;% \n  head()\n\n# A tibble: 6 × 26\n  .rownames request_type    Jan    Feb    Mar    Apr    May    Jun    Jul    Aug\n  &lt;chr&gt;     &lt;chr&gt;         &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 1         Abandoned V… 0.0890 0.0727 0.0769 0.0710 0.0830 0.0793 0.0778 0.101 \n2 2         Barking Dog  0.0563 0.0608 0.0608 0.0631 0.104  0.101  0.0788 0.113 \n3 3         Board Up (P… 0.0395 0.0482 0.0658 0.0943 0.114  0.0899 0.110  0.123 \n4 4         Broken Side… 0.0337 0.155  0.148  0.0872 0.105  0.0964 0.0696 0.0735\n5 5         Building Ma… 0.0708 0.0919 0.103  0.0739 0.0842 0.0829 0.0725 0.0919\n6 6         Building Wi… 0.0842 0.0697 0.0636 0.0577 0.105  0.0883 0.0924 0.0815\n# ℹ 16 more variables: Sep &lt;dbl&gt;, Oct &lt;dbl&gt;, Nov &lt;dbl&gt;, Dec &lt;dbl&gt;,\n#   .fittedPC1 &lt;dbl&gt;, .fittedPC2 &lt;dbl&gt;, .fittedPC3 &lt;dbl&gt;, .fittedPC4 &lt;dbl&gt;,\n#   .fittedPC5 &lt;dbl&gt;, .fittedPC6 &lt;dbl&gt;, .fittedPC7 &lt;dbl&gt;, .fittedPC8 &lt;dbl&gt;,\n#   .fittedPC9 &lt;dbl&gt;, .fittedPC10 &lt;dbl&gt;, .fittedPC11 &lt;dbl&gt;, .fittedPC12 &lt;dbl&gt;\n\n\nPlot how the PCA object explains the variance in the data\n\npc %&gt;% \n  tidy(\"pcs\") %&gt;%\n  select(-std.dev) %&gt;% \n  gather(measure, value, -PC) %&gt;% \n    ggplot(aes(PC, value)) +\n    geom_line() +\n    geom_point() +\n    facet_wrap(~measure) +\n    labs(title = \"Variance explained by each principal component\",\n         x = \"Principal Component\",\n         y = NULL) +\n    scale_x_continuous(breaks = 1:12)\n\n\n\n\n\n\n\n\nThe first two principal components explain most of the variance\nFor an in-depth plot we need to create the PCA object a different way\n\ndf_months %&gt;% \n  nest() %&gt;% \n  mutate(pca = map(data, ~ prcomp(.x %&gt;% select(-request_type), \n                                  center = TRUE, scale = TRUE)),\n         pca_aug = map2(pca, data, ~augment(.x, data = .y))) -&gt; df_months_pca2\n\nPlot the PCA data\n\ndf_months_pca2 %&gt;%\n  mutate(\n    pca_graph = map2(\n      .x = pca,\n      .y = data,\n      ~ autoplot(.x, loadings = TRUE, loadings.label = TRUE,\n                 loadings.label.repel = TRUE,\n                 data = .y) +\n        theme_bw() +\n        labs(x = \"Principal Component 1\",\n             y = \"Principal Component 2\",\n             title = \"First two principal components of PCA on 311 dataset\")\n    )\n  ) %&gt;%\n  pull(pca_graph)\n\n[[1]]\n\n\n\n\n\n\n\n\n\nThis shows that summer and winter explain a significant part of the variance\nPlot the data to show the outliers\n\nau %&gt;% \n  mutate(outlier = case_when(abs(.fittedPC1) &gt; 2 & abs(.fittedPC2) &gt; 1.5 ~ TRUE),\n         pothole = case_when(request_type == \"Potholes\" ~ \"Potholes\",\n                             request_type != \"Potholes\" ~ \"Other\")) -&gt; au\n\nau %&gt;% \nggplot(aes(.fittedPC1, .fittedPC2)) +\n  geom_point() +\n  geom_label_repel(data = au %&gt;% filter(outlier),\n             aes(label = request_type)) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nau %&gt;% \nggplot(aes(.fittedPC1, .fittedPC2)) +\n  geom_point(aes(color = pothole)) +\n  geom_label_repel(data = au %&gt;% filter(request_type == \"Potholes\"),\n             aes(label = request_type)) +\n  theme_bw() +\n  scale_color_manual(NULL, values = c(\"black\", \"red\"))"
  },
  {
    "objectID": "posts/exploring-311-data-with-pca/index.html#setup",
    "href": "posts/exploring-311-data-with-pca/index.html#setup",
    "title": "Exploring 311 Data With PCA",
    "section": "",
    "text": "install.packages(c(\"tidyverse\", \"lubridate\", \"broom\", \"ggfortify\", \"ggrepel\", \"janitor\"))\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(broom)\nlibrary(ggfortify)\nlibrary(ggrepel)\nlibrary(janitor)\n\noptions(scipen = 999, digits = 4)\nset.seed(1234)\n\ntheme_set(theme_bw())"
  },
  {
    "objectID": "posts/exploring-311-data-with-pca/index.html#load-the-data",
    "href": "posts/exploring-311-data-with-pca/index.html#load-the-data",
    "title": "Exploring 311 Data With PCA",
    "section": "",
    "text": "read_csv(\"https://raw.githubusercontent.com/conorotompkins/pittsburgh_311/master/data/pittsburgh_311.csv\", progress = FALSE) %&gt;% \n  clean_names() %&gt;% \n  mutate(date = ymd(str_sub(created_on, 1, 10)),\n         month = month(date, label = TRUE)) %&gt;% \n  filter(date &lt; \"2018-07-19\") -&gt; df"
  },
  {
    "objectID": "posts/exploring-311-data-with-pca/index.html#prep-the-data",
    "href": "posts/exploring-311-data-with-pca/index.html#prep-the-data",
    "title": "Exploring 311 Data With PCA",
    "section": "",
    "text": "Create a dataframe of the top request types\n\n(df %&gt;% \n  count(request_type, sort = TRUE) %&gt;% \n  filter(n &gt; 400)-&gt; df_top_requests)\n\n# A tibble: 84 × 2\n   request_type                             n\n   &lt;chr&gt;                                &lt;int&gt;\n 1 Potholes                             25202\n 2 Weeds/Debris                         16503\n 3 Building Maintenance                 10469\n 4 Snow/Ice removal                      7006\n 5 Refuse Violations                     6515\n 6 Abandoned Vehicle (parked on street)  5877\n 7 Missed Pick Up                        4689\n 8 Replace/Repair a Sign                 4445\n 9 Building Without a Permit             4404\n10 Litter                                4198\n# ℹ 74 more rows\n\n\nCount the number of requests per month by request type, filter for the top request types, and fill in gaps in the data\n\n(df %&gt;%\n  semi_join(df_top_requests) %&gt;% \n  group_by(request_type, month) %&gt;% \n  summarize(n = n()) %&gt;% \n  ungroup() %&gt;%\n  complete(request_type, month) %&gt;% \n  replace_na(replace = list(n = 0)) -&gt; df_months)\n\n# A tibble: 1,008 × 3\n   request_type                         month     n\n   &lt;chr&gt;                                &lt;ord&gt; &lt;int&gt;\n 1 Abandoned Vehicle (parked on street) Jan     523\n 2 Abandoned Vehicle (parked on street) Feb     427\n 3 Abandoned Vehicle (parked on street) Mar     452\n 4 Abandoned Vehicle (parked on street) Apr     417\n 5 Abandoned Vehicle (parked on street) May     488\n 6 Abandoned Vehicle (parked on street) Jun     466\n 7 Abandoned Vehicle (parked on street) Jul     457\n 8 Abandoned Vehicle (parked on street) Aug     596\n 9 Abandoned Vehicle (parked on street) Sep     525\n10 Abandoned Vehicle (parked on street) Oct     571\n# ℹ 998 more rows\n\n\nCalculate the percentage of a request type for each month\n\n(df_months %&gt;% \n  group_by(request_type) %&gt;% \n  mutate(request_type_total = sum(n),\n         month_percentage = n / request_type_total) -&gt; df_months)\n\n# A tibble: 1,008 × 5\n# Groups:   request_type [84]\n   request_type                  month     n request_type_total month_percentage\n   &lt;chr&gt;                         &lt;ord&gt; &lt;int&gt;              &lt;int&gt;            &lt;dbl&gt;\n 1 Abandoned Vehicle (parked on… Jan     523               5877           0.0890\n 2 Abandoned Vehicle (parked on… Feb     427               5877           0.0727\n 3 Abandoned Vehicle (parked on… Mar     452               5877           0.0769\n 4 Abandoned Vehicle (parked on… Apr     417               5877           0.0710\n 5 Abandoned Vehicle (parked on… May     488               5877           0.0830\n 6 Abandoned Vehicle (parked on… Jun     466               5877           0.0793\n 7 Abandoned Vehicle (parked on… Jul     457               5877           0.0778\n 8 Abandoned Vehicle (parked on… Aug     596               5877           0.101 \n 9 Abandoned Vehicle (parked on… Sep     525               5877           0.0893\n10 Abandoned Vehicle (parked on… Oct     571               5877           0.0972\n# ℹ 998 more rows\n\n\nCheck for bad data\n\ndf_months %&gt;% \n  filter(is.na(month_percentage) | is.nan(month_percentage))\n\n# A tibble: 0 × 5\n# Groups:   request_type [0]\n# ℹ 5 variables: request_type &lt;chr&gt;, month &lt;ord&gt;, n &lt;int&gt;,\n#   request_type_total &lt;int&gt;, month_percentage &lt;dbl&gt;\n\n\nSpread the data to turn the months into the columns\n\n(df_months %&gt;% \n  select(request_type, month, month_percentage) %&gt;% \n  spread(month, month_percentage) %&gt;% \n  ungroup() -&gt; df_months)\n\n# A tibble: 84 × 13\n   request_type     Jan     Feb    Mar    Apr    May    Jun    Jul    Aug    Sep\n   &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 Abandoned V… 0.0890  0.0727  0.0769 0.0710 0.0830 0.0793 0.0778 0.101  0.0893\n 2 Barking Dog  0.0563  0.0608  0.0608 0.0631 0.104  0.101  0.0788 0.113  0.124 \n 3 Board Up (P… 0.0395  0.0482  0.0658 0.0943 0.114  0.0899 0.110  0.123  0.0855\n 4 Broken Side… 0.0337  0.155   0.148  0.0872 0.105  0.0964 0.0696 0.0735 0.0528\n 5 Building Ma… 0.0708  0.0919  0.103  0.0739 0.0842 0.0829 0.0725 0.0919 0.0776\n 6 Building Wi… 0.0842  0.0697  0.0636 0.0577 0.105  0.0883 0.0924 0.0815 0.0829\n 7 Catch Basin… 0.0636  0.0377  0.0778 0.0748 0.0984 0.132  0.0825 0.127  0.105 \n 8 City Source… 0.00527 0.00246 0.0105 0.0428 0.196  0.213  0.195  0.164  0.0808\n 9 City Steps,… 0.0443  0.0180  0.0148 0.0197 0.116  0.216  0.203  0.146  0.118 \n10 City Steps,… 0.0265  0.0305  0.0713 0.0509 0.128  0.120  0.136  0.128  0.108 \n# ℹ 74 more rows\n# ℹ 3 more variables: Oct &lt;dbl&gt;, Nov &lt;dbl&gt;, Dec &lt;dbl&gt;\n\n\nCheck that they all add up to 1 across the rows\n\n(df_months %&gt;% \n  select(Jan:Dec) %&gt;% \n  mutate(row_sum = rowSums(.)) %&gt;% \n  select(row_sum, everything()) -&gt; test)\n\n# A tibble: 84 × 13\n   row_sum     Jan     Feb    Mar    Apr    May    Jun    Jul    Aug    Sep\n     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1       1 0.0890  0.0727  0.0769 0.0710 0.0830 0.0793 0.0778 0.101  0.0893\n 2       1 0.0563  0.0608  0.0608 0.0631 0.104  0.101  0.0788 0.113  0.124 \n 3       1 0.0395  0.0482  0.0658 0.0943 0.114  0.0899 0.110  0.123  0.0855\n 4       1 0.0337  0.155   0.148  0.0872 0.105  0.0964 0.0696 0.0735 0.0528\n 5       1 0.0708  0.0919  0.103  0.0739 0.0842 0.0829 0.0725 0.0919 0.0776\n 6       1 0.0842  0.0697  0.0636 0.0577 0.105  0.0883 0.0924 0.0815 0.0829\n 7       1 0.0636  0.0377  0.0778 0.0748 0.0984 0.132  0.0825 0.127  0.105 \n 8       1 0.00527 0.00246 0.0105 0.0428 0.196  0.213  0.195  0.164  0.0808\n 9       1 0.0443  0.0180  0.0148 0.0197 0.116  0.216  0.203  0.146  0.118 \n10       1 0.0265  0.0305  0.0713 0.0509 0.128  0.120  0.136  0.128  0.108 \n# ℹ 74 more rows\n# ℹ 3 more variables: Oct &lt;dbl&gt;, Nov &lt;dbl&gt;, Dec &lt;dbl&gt;\n\n\n\n\n\ndf_months %&gt;% \n  ggplot(aes(Jan, Jul)) +\n  geom_point()\n\n\n\n\n\n\n\n\nRemember that each dot represents a request type, and the month shows what % of that request type occurred that month\n\ndf_months %&gt;% \n  ggplot(aes(Apr, Oct)) +\n  geom_point()\n\n\n\n\n\n\n\n\nIt is not feasible to plot all the months against each other. PCA can help by condensing the columns and increasing the variance. PCA creates eigenvectors that represents the data in a concentrated way. Eigenvectors and eigenvalues do not represent observed data. They are calculated representations of the data. We will refer to eigenvectors as “principal components”.\nIn this case, where our data is measured by months in a year, each principal component could loosely be compared to a season."
  },
  {
    "objectID": "posts/exploring-311-data-with-pca/index.html#prep-the-data-for-pca",
    "href": "posts/exploring-311-data-with-pca/index.html#prep-the-data-for-pca",
    "title": "Exploring 311 Data With PCA",
    "section": "",
    "text": "The PCA function requires an all-numeric dataframe, so drop the request types into the dataframe metadata\n\n(df_months %&gt;% \n  ungroup() %&gt;% \n  remove_rownames() %&gt;% \n  column_to_rownames(var = \"request_type\") -&gt; df_months_pca1)\n\n                                                 Jan      Feb      Mar      Apr\nAbandoned Vehicle (parked on street)        0.088991 0.072656 0.076910 0.070955\nBarking Dog                                 0.056306 0.060811 0.060811 0.063063\nBoard Up (PLI referral to DPW)              0.039474 0.048246 0.065789 0.094298\nBroken Sidewalk                             0.033665 0.154552 0.147666 0.087223\nBuilding Maintenance                        0.070780 0.091890 0.103353 0.073933\nBuilding Without a Permit                   0.084242 0.069709 0.063579 0.057675\nCatch Basin, Clogged                        0.063642 0.037714 0.077784 0.074838\nCity Source (CDBG)                          0.005267 0.002458 0.010534 0.042837\nCity Steps, Need Cleared                    0.044262 0.018033 0.014754 0.019672\nCity Steps, Need Repaired                   0.026477 0.030550 0.071283 0.050916\nCollapsed Catch Basin                       0.064220 0.053899 0.061927 0.075688\nCommercial Refuse/Dumpsters                 0.079430 0.079430 0.089613 0.077393\nCurb /Broken/Deteriorated                   0.042373 0.048729 0.072034 0.110169\nCurb/Request for Asphalt Windrow            0.038660 0.020619 0.028351 0.085052\nDead Animal                                 0.038181 0.032566 0.043234 0.076923\nDead tree (Public property)                 0.034516 0.028763 0.049856 0.066155\nDrainage/Leak                               0.141304 0.050000 0.035870 0.083696\nDrug Enforcement                            0.077085 0.049755 0.065172 0.079187\nDumping, Private Property                   0.064315 0.076763 0.120332 0.093361\nDumpster (on Street)                        0.070866 0.048819 0.042520 0.088189\nEarly Set Out                               0.072444 0.069736 0.062288 0.063643\nExcessive Noise/Disturbances                0.057377 0.047131 0.056011 0.085383\nField                                       0.016432 0.014085 0.042254 0.110329\nFire Safety System Not Working              0.093750 0.185547 0.128906 0.077474\nGraffiti, Documentation                     0.057116 0.054307 0.103933 0.102060\nGraffiti, Removal                           0.088710 0.111290 0.098387 0.046774\nHydrant                                     0.121771 0.062731 0.075646 0.053506\nIllegal Dumping                             0.065672 0.057214 0.076617 0.106965\nIllegal Parking                             0.095682 0.075074 0.081943 0.079735\nJunk Vehicles                               0.079384 0.093602 0.114929 0.068720\nLeak                                        0.171456 0.097418 0.048709 0.044812\nLeaves/Street Cleaning                      0.028967 0.031486 0.030227 0.059194\nLitter                                      0.064316 0.064555 0.085755 0.093378\nLitter Can, Public                          0.064777 0.049393 0.060729 0.069636\nMaintenance Issue                           0.026455 0.031746 0.039153 0.078307\nMayor's Office                              0.158455 0.033289 0.023968 0.027963\nMissed Blue Bag                             0.094002 0.042077 0.051925 0.068935\nMissed Pick Up                              0.076775 0.048198 0.047345 0.058861\nNeed Potable Water                          0.002398 0.914868 0.001199 0.003597\nOperating Without a License                 0.041215 0.021692 0.149675 0.114967\nOvergrowth                                  0.005058 0.007867 0.006462 0.019106\nParking Authority                           0.086022 0.075269 0.064516 0.105376\nPatrol                                      0.063164 0.047816 0.071429 0.081464\nPaving Concern/Problem                      0.054819 0.043324 0.042440 0.071618\nPaving Request                              0.052950 0.047504 0.108321 0.114675\nPermit Parking (Residential Parking Permit) 0.107062 0.075171 0.063781 0.079727\nPlayground                                  0.015038 0.024436 0.043233 0.093985\nPotholes                                    0.123324 0.052972 0.105230 0.112570\nPruning (city tree)                         0.024019 0.025372 0.044317 0.057510\nPublic Right of Way                         0.033397 0.029580 0.020992 0.057252\nQuestion                                    0.079824 0.064739 0.043997 0.024513\nReferral                                    0.099161 0.050725 0.046148 0.049962\nRefuse Violations                           0.079202 0.059708 0.065848 0.084728\nReplace/Repair a Sign                       0.067492 0.053093 0.080315 0.084814\nRequest New Sign                            0.069169 0.049768 0.059047 0.097005\nRetaining Wall Maintenance                  0.066239 0.091880 0.115385 0.096154\nRodent control                              0.040957 0.033594 0.041417 0.052462\nRoot prune                                  0.022321 0.038690 0.053571 0.098214\nSidewalk Obstruction                        0.052799 0.042621 0.044529 0.052163\nSidewalk, Lack of Snow/Ice Removal          0.767726 0.090465 0.002445 0.002445\nSinkhole                                    0.103995 0.058973 0.062143 0.066582\nSmoke detectors                             0.118421 0.064145 0.062500 0.092105\nSnow/Ice removal                            0.681273 0.135027 0.005995 0.002712\nSpeeding                                    0.063973 0.060606 0.084175 0.094276\nStreet Cleaning/Sweeping                    0.027306 0.026790 0.035033 0.102009\nStreet Light - Repair                       0.078803 0.055112 0.067830 0.044888\nStreet Obstruction/Closure                  0.126273 0.040733 0.081466 0.061100\nThank you - DPW                             0.136264 0.046154 0.032967 0.065934\nTraffic                                     0.065356 0.053666 0.061637 0.073326\nTraffic or Pedestrian Signal, Repair        0.089659 0.047221 0.069934 0.069337\nTraffic or Pedestrian Signal, Request       0.056641 0.029297 0.099609 0.087891\nTree Fallen Across Road                     0.042589 0.031516 0.051959 0.034923\nTree Fallen Across Sidewalk                 0.034125 0.028190 0.044510 0.044510\nTree Issues                                 0.038384 0.056566 0.076768 0.048485\nTree Removal                                0.042949 0.036507 0.071582 0.074445\nUnpermitted Electrical Work                 0.145055 0.012088 0.030769 0.019780\nUnpermitted HVAC Work                       0.108516 0.045330 0.064560 0.048077\nUtility Cut - Other                         0.114889 0.067995 0.052755 0.059789\nUtility Cut - PWSA                          0.202261 0.050251 0.057789 0.075377\nUtility Pole                                0.065728 0.075117 0.049296 0.075117\nVacant Building                             0.088199 0.083230 0.096066 0.048861\nWeeds/Debris                                0.029631 0.024965 0.035751 0.045083\nWires                                       0.060651 0.071006 0.060651 0.078402\nZoning Issue                                0.055000 0.065000 0.080833 0.082500\n                                                  May       Jun      Jul\nAbandoned Vehicle (parked on street)        0.0830356 0.0792922 0.077761\nBarking Dog                                 0.1036036 0.1013514 0.078829\nBoard Up (PLI referral to DPW)              0.1140351 0.0899123 0.109649\nBroken Sidewalk                             0.1048202 0.0964040 0.069625\nBuilding Maintenance                        0.0841532 0.0829115 0.072500\nBuilding Without a Permit                   0.1049046 0.0883288 0.092416\nCatch Basin, Clogged                        0.0984090 0.1319976 0.082499\nCity Source (CDBG)                          0.1955758 0.2134831 0.195225\nCity Steps, Need Cleared                    0.1163934 0.2163934 0.203279\nCity Steps, Need Repaired                   0.1283096 0.1201629 0.136456\nCollapsed Catch Basin                       0.1100917 0.0917431 0.083716\nCommercial Refuse/Dumpsters                 0.0529532 0.1038697 0.105906\nCurb /Broken/Deteriorated                   0.1525424 0.1122881 0.116525\nCurb/Request for Asphalt Windrow            0.1430412 0.2113402 0.155928\nDead Animal                                 0.0713083 0.1021898 0.139809\nDead tree (Public property)                 0.1246405 0.1447747 0.154362\nDrainage/Leak                               0.0902174 0.1043478 0.102174\nDrug Enforcement                            0.0988087 0.1023125 0.088998\nDumping, Private Property                   0.0746888 0.0622407 0.064315\nDumpster (on Street)                        0.0787402 0.1354331 0.105512\nEarly Set Out                               0.0886933 0.0873392 0.111713\nExcessive Noise/Disturbances                0.0887978 0.0758197 0.075137\nField                                       0.1854460 0.1384977 0.150235\nFire Safety System Not Working              0.1139323 0.0572917 0.047526\nGraffiti, Documentation                     0.1207865 0.1254682 0.073970\nGraffiti, Removal                           0.0338710 0.0596774 0.091935\nHydrant                                     0.0922509 0.0571956 0.064576\nIllegal Dumping                             0.1000000 0.1228856 0.113930\nIllegal Parking                             0.0765456 0.0691855 0.059863\nJunk Vehicles                               0.0864929 0.0710900 0.104265\nLeak                                        0.0491963 0.0526059 0.057964\nLeaves/Street Cleaning                      0.0629723 0.0541562 0.021411\nLitter                                      0.0826584 0.0855169 0.098380\nLitter Can, Public                          0.0923077 0.0995951 0.127126\nMaintenance Issue                           0.1417989 0.1185185 0.135450\nMayor's Office                              0.0319574 0.1824234 0.065246\nMissed Blue Bag                             0.0841540 0.1020591 0.087735\nMissed Pick Up                              0.1027938 0.1123907 0.118789\nNeed Potable Water                          0.0011990 0.0023981 0.000000\nOperating Without a License                 0.4338395 0.0542299 0.028200\nOvergrowth                                  0.1219444 0.2489463 0.234335\nParking Authority                           0.0860215 0.0838710 0.081720\nPatrol                                      0.1015348 0.0879575 0.095041\nPaving Concern/Problem                      0.1114058 0.1255526 0.085765\nPaving Request                              0.1397882 0.1458396 0.118306\nPermit Parking (Residential Parking Permit) 0.0569476 0.0706150 0.079727\nPlayground                                  0.1184211 0.1691729 0.159774\nPotholes                                    0.1346322 0.1150702 0.107095\nPruning (city tree)                         0.1234777 0.1742219 0.168133\nPublic Right of Way                         0.1316794 0.1650763 0.154580\nQuestion                                    0.0483972 0.0936518 0.122564\nReferral                                    0.0362319 0.0846682 0.129291\nRefuse Violations                           0.0784344 0.0983883 0.100844\nReplace/Repair a Sign                       0.1196850 0.1113611 0.094713\nRequest New Sign                            0.0932096 0.0927879 0.097427\nRetaining Wall Maintenance                  0.0961538 0.0982906 0.085470\nRodent control                              0.0745513 0.1099862 0.141279\nRoot prune                                  0.1190476 0.1264881 0.163690\nSidewalk Obstruction                        0.0807888 0.1075064 0.123410\nSidewalk, Lack of Snow/Ice Removal          0.0097800 0.0000000 0.002445\nSinkhole                                    0.0786303 0.1122384 0.128725\nSmoke detectors                             0.0871711 0.1348684 0.046053\nSnow/Ice removal                            0.0008564 0.0001427 0.000000\nSpeeding                                    0.0976431 0.0909091 0.104377\nStreet Cleaning/Sweeping                    0.1257084 0.1298300 0.123132\nStreet Light - Repair                       0.0498753 0.0675810 0.097257\nStreet Obstruction/Closure                  0.0549898 0.0509165 0.087576\nThank you - DPW                             0.0769231 0.0967033 0.105495\nTraffic                                     0.0887354 0.0600425 0.054729\nTraffic or Pedestrian Signal, Repair        0.0854752 0.1040048 0.086671\nTraffic or Pedestrian Signal, Request       0.1074219 0.1191406 0.085938\nTree Fallen Across Road                     0.1345826 0.2206133 0.137990\nTree Fallen Across Sidewalk                 0.1424332 0.1958457 0.126113\nTree Issues                                 0.1010101 0.1010101 0.129293\nTree Removal                                0.1295634 0.1410165 0.118826\nUnpermitted Electrical Work                 0.1054945 0.1230769 0.085714\nUnpermitted HVAC Work                       0.1689560 0.0879121 0.085165\nUtility Cut - Other                         0.0797186 0.0738570 0.082063\nUtility Cut - PWSA                          0.1005025 0.0967337 0.095477\nUtility Pole                                0.1126761 0.1244131 0.107981\nVacant Building                             0.0683230 0.0608696 0.064182\nWeeds/Debris                                0.1373690 0.1666364 0.157062\nWires                                       0.0961538 0.1434911 0.087278\nZoning Issue                                0.0958333 0.0866667 0.103333\n                                                  Aug      Sep       Oct\nAbandoned Vehicle (parked on street)        0.1014123 0.089331 0.0971584\nBarking Dog                                 0.1126126 0.123874 0.1126126\nBoard Up (PLI referral to DPW)              0.1228070 0.085526 0.0877193\nBroken Sidewalk                             0.0734507 0.052793 0.0849273\nBuilding Maintenance                        0.0918903 0.077562 0.0787086\nBuilding Without a Permit                   0.0815168 0.082879 0.1128520\nCatch Basin, Clogged                        0.1266942 0.104891 0.0931055\nCity Source (CDBG)                          0.1639747 0.080758 0.0582865\nCity Steps, Need Cleared                    0.1459016 0.118033 0.0557377\nCity Steps, Need Repaired                   0.1283096 0.107943 0.0855397\nCollapsed Catch Basin                       0.1238532 0.113532 0.0986239\nCommercial Refuse/Dumpsters                 0.1344196 0.071283 0.0712831\nCurb /Broken/Deteriorated                   0.1186441 0.084746 0.0572034\nCurb/Request for Asphalt Windrow            0.1082474 0.079897 0.0605670\nDead Animal                                 0.1235261 0.112296 0.1274565\nDead tree (Public property)                 0.1236817 0.102589 0.0882071\nDrainage/Leak                               0.1076087 0.058696 0.0782609\nDrug Enforcement                            0.1135249 0.117730 0.0946041\nDumping, Private Property                   0.1327801 0.076763 0.1016598\nDumpster (on Street)                        0.1070866 0.086614 0.0881890\nEarly Set Out                               0.1570752 0.080569 0.0663507\nExcessive Noise/Disturbances                0.0881148 0.090164 0.1038251\nField                                       0.1197183 0.098592 0.0610329\nFire Safety System Not Working              0.0449219 0.050781 0.0651042\nGraffiti, Documentation                     0.0608614 0.067416 0.1207865\nGraffiti, Removal                           0.1048387 0.125806 0.0935484\nHydrant                                     0.1254613 0.077491 0.0867159\nIllegal Dumping                             0.0651741 0.068657 0.0726368\nIllegal Parking                             0.0969087 0.103042 0.0991168\nJunk Vehicles                               0.0912322 0.072275 0.0758294\nLeak                                        0.1066732 0.080857 0.0681929\nLeaves/Street Cleaning                      0.0440806 0.012594 0.0642317\nLitter                                      0.1202954 0.095760 0.0824202\nLitter Can, Public                          0.1246964 0.110121 0.0850202\nMaintenance Issue                           0.1174603 0.113228 0.1047619\nMayor's Office                              0.1118509 0.114514 0.0892144\nMissed Blue Bag                             0.1056401 0.089526 0.0957923\nMissed Pick Up                              0.1106846 0.098315 0.0835999\nNeed Potable Water                          0.0731415 0.000000 0.0000000\nOperating Without a License                 0.0390456 0.021692 0.0455531\nOvergrowth                                  0.1944366 0.104805 0.0396179\nParking Authority                           0.1032258 0.090323 0.0731183\nPatrol                                      0.1097993 0.103306 0.1151122\nPaving Concern/Problem                      0.0813439 0.089302 0.1432361\nPaving Request                              0.0928896 0.075340 0.0490166\nPermit Parking (Residential Parking Permit) 0.1321185 0.102506 0.0956720\nPlayground                                  0.1672932 0.090226 0.0733083\nPotholes                                    0.0759860 0.050155 0.0442425\nPruning (city tree)                         0.1586604 0.082206 0.0801759\nPublic Right of Way                         0.1832061 0.094466 0.0620229\nQuestion                                    0.1646763 0.065996 0.1451917\nReferral                                    0.1525553 0.089245 0.1060259\nRefuse Violations                           0.1122026 0.085035 0.0968534\nReplace/Repair a Sign                       0.0899888 0.088189 0.0758155\nRequest New Sign                            0.1100801 0.113454 0.0889920\nRetaining Wall Maintenance                  0.0769231 0.055556 0.1089744\nRodent control                              0.1385182 0.125633 0.1214910\nRoot prune                                  0.1056548 0.096726 0.0937500\nSidewalk Obstruction                        0.1246819 0.117048 0.0966921\nSidewalk, Lack of Snow/Ice Removal          0.0000000 0.002445 0.0000000\nSinkhole                                    0.1230184 0.088142 0.0798985\nSmoke detectors                             0.0756579 0.041118 0.1118421\nSnow/Ice removal                            0.0001427 0.000000 0.0001427\nSpeeding                                    0.1144781 0.112795 0.0791246\nStreet Cleaning/Sweeping                    0.1215868 0.098403 0.0963421\nStreet Light - Repair                       0.1057357 0.105237 0.1134663\nStreet Obstruction/Closure                  0.1201629 0.120163 0.0916497\nThank you - DPW                             0.1252747 0.112088 0.0879121\nTraffic                                     0.1046759 0.162062 0.1232731\nTraffic or Pedestrian Signal, Repair        0.1165571 0.105798 0.0854752\nTraffic or Pedestrian Signal, Request       0.0742188 0.111328 0.1035156\nTree Fallen Across Road                     0.1831346 0.064736 0.0442930\nTree Fallen Across Sidewalk                 0.2121662 0.063798 0.0474777\nTree Issues                                 0.1454545 0.094949 0.0848485\nTree Removal                                0.1460272 0.085183 0.0787402\nUnpermitted Electrical Work                 0.0571429 0.065934 0.1318681\nUnpermitted HVAC Work                       0.0824176 0.075549 0.0879121\nUtility Cut - Other                         0.1160610 0.110199 0.0937866\nUtility Cut - PWSA                          0.0716080 0.062814 0.0690955\nUtility Pole                                0.1197183 0.077465 0.0563380\nVacant Building                             0.0749482 0.077847 0.0815735\nWeeds/Debris                                0.1616070 0.097922 0.0718657\nWires                                       0.1316568 0.091716 0.0532544\nZoning Issue                                0.0908333 0.079167 0.0875000\n                                                 Nov      Dec\nAbandoned Vehicle (parked on street)        0.086098 0.076400\nBarking Dog                                 0.074324 0.051802\nBoard Up (PLI referral to DPW)              0.076754 0.065789\nBroken Sidewalk                             0.064269 0.030604\nBuilding Maintenance                        0.093419 0.078900\nBuilding Without a Permit                   0.086285 0.075613\nCatch Basin, Clogged                        0.064820 0.043606\nCity Source (CDBG)                          0.025632 0.005969\nCity Steps, Need Cleared                    0.022951 0.024590\nCity Steps, Need Repaired                   0.075356 0.038697\nCollapsed Catch Basin                       0.068807 0.053899\nCommercial Refuse/Dumpsters                 0.069246 0.065173\nCurb /Broken/Deteriorated                   0.046610 0.038136\nCurb/Request for Asphalt Windrow            0.047680 0.020619\nDead Animal                                 0.083661 0.048849\nDead tree (Public property)                 0.051774 0.030681\nDrainage/Leak                               0.070652 0.077174\nDrug Enforcement                            0.067274 0.045550\nDumping, Private Property                   0.074689 0.058091\nDumpster (on Street)                        0.083465 0.064567\nEarly Set Out                               0.069059 0.071090\nExcessive Noise/Disturbances                0.092213 0.140027\nField                                       0.032864 0.030516\nFire Safety System Not Working              0.051432 0.083333\nGraffiti, Documentation                     0.072097 0.041199\nGraffiti, Removal                           0.066129 0.079032\nHydrant                                     0.090406 0.092251\nIllegal Dumping                             0.083582 0.066667\nIllegal Parking                             0.088077 0.074828\nJunk Vehicles                               0.091232 0.050948\nLeak                                        0.097418 0.124696\nLeaves/Street Cleaning                      0.430730 0.159950\nLitter                                      0.070272 0.056694\nLitter Can, Public                          0.068826 0.047773\nMaintenance Issue                           0.053968 0.039153\nMayor's Office                              0.083888 0.077230\nMissed Blue Bag                             0.091316 0.086840\nMissed Pick Up                              0.070164 0.072084\nNeed Potable Water                          0.000000 0.001199\nOperating Without a License                 0.026030 0.023861\nOvergrowth                                  0.013768 0.003653\nParking Authority                           0.068817 0.081720\nPatrol                                      0.070838 0.052538\nPaving Concern/Problem                      0.108753 0.042440\nPaving Request                              0.034493 0.020877\nPermit Parking (Residential Parking Permit) 0.077449 0.059226\nPlayground                                  0.041353 0.003759\nPotholes                                    0.053289 0.025434\nPruning (city tree)                         0.041272 0.020636\nPublic Right of Way                         0.044847 0.022901\nQuestion                                    0.084852 0.061596\nReferral                                    0.081998 0.073989\nRefuse Violations                           0.067536 0.071220\nReplace/Repair a Sign                       0.070191 0.064342\nRequest New Sign                            0.076339 0.052720\nRetaining Wall Maintenance                  0.070513 0.038462\nRodent control                              0.071330 0.048780\nRoot prune                                  0.043155 0.038690\nSidewalk Obstruction                        0.086514 0.071247\nSidewalk, Lack of Snow/Ice Removal          0.000000 0.122249\nSinkhole                                    0.058339 0.039315\nSmoke detectors                             0.088816 0.077303\nSnow/Ice removal                            0.003711 0.169997\nSpeeding                                    0.052189 0.045455\nStreet Cleaning/Sweeping                    0.082947 0.030912\nStreet Light - Repair                       0.115960 0.098254\nStreet Obstruction/Closure                  0.089613 0.075356\nThank you - DPW                             0.065934 0.048352\nTraffic                                     0.088735 0.063762\nTraffic or Pedestrian Signal, Repair        0.084877 0.054991\nTraffic or Pedestrian Signal, Request       0.068359 0.056641\nTree Fallen Across Road                     0.036627 0.017036\nTree Fallen Across Sidewalk                 0.040059 0.020772\nTree Issues                                 0.070707 0.052525\nTree Removal                                0.047960 0.027201\nUnpermitted Electrical Work                 0.101099 0.121978\nUnpermitted HVAC Work                       0.074176 0.071429\nUtility Cut - Other                         0.069168 0.079719\nUtility Cut - PWSA                          0.081658 0.036432\nUtility Pole                                0.075117 0.061033\nVacant Building                             0.132091 0.123810\nWeeds/Debris                                0.045386 0.026722\nWires                                       0.076923 0.048817\nZoning Issue                                0.092500 0.080833\n\n\nCreate the PCA object\n\n(df_months_pca1 %&gt;% \n  prcomp(scale = TRUE) -&gt; pc)\n\nStandard deviations (1, .., p=12):\n [1] 2.0313544303132338165 1.5112299607905637089 1.3677583442481686671\n [4] 1.0647449915481708160 0.9373153843502739502 0.6612690017981475155\n [7] 0.6319678449167122070 0.5732234023111666410 0.4666060722915733039\n[10] 0.4192405535100036107 0.3847717270238655840 0.0000000000000002152\n\nRotation (n x k) = (12 x 12):\n        PC1     PC2       PC3      PC4      PC5        PC6      PC7      PC8\nJan -0.3509  0.2377  0.036588 -0.40554 -0.40259 -0.0878343  0.04648  0.10077\nFeb -0.2189  0.2230 -0.226391  0.69886  0.14610  0.1005237  0.15306 -0.04311\nMar -0.0235 -0.4858 -0.323760  0.11519 -0.19761 -0.1724337 -0.62037  0.31077\nApr  0.1329 -0.4686 -0.301500 -0.04306 -0.07313 -0.5014973  0.54199 -0.09929\nMay  0.2339 -0.1448 -0.445597 -0.33914  0.15507  0.5723879 -0.10942 -0.40042\nJun  0.4049  0.1866  0.002112 -0.22765  0.21210 -0.0386388  0.13438  0.42038\nJul  0.4322  0.1697  0.095923 -0.10888  0.01800 -0.1443989  0.02032  0.13081\nAug  0.3866  0.1805  0.189907  0.17598  0.04242 -0.2554247 -0.44835 -0.13536\nSep  0.2944 -0.1580  0.365255  0.16856 -0.42595 -0.0005222  0.04916 -0.53195\nOct  0.1150 -0.4130  0.389922  0.16708 -0.17452  0.5136274  0.18989  0.43757\nNov -0.1323 -0.3291  0.333424 -0.05855  0.69855 -0.1384698 -0.02866 -0.09659\nDec -0.3720 -0.1311  0.333522 -0.25017  0.03408 -0.0524049 -0.15824 -0.15902\n          PC9     PC10      PC11   PC12\nJan -0.119474 -0.19207  0.310620 0.5699\nFeb  0.085692  0.10149 -0.130098 0.5209\nMar  0.259371  0.01955  0.035606 0.1620\nApr -0.264116  0.01396 -0.143422 0.1379\nMay -0.106727  0.02849 -0.028027 0.2717\nJun  0.373786 -0.36939 -0.413396 0.2471\nJul  0.103405  0.77442  0.256091 0.2205\nAug -0.607417 -0.20278 -0.123033 0.1983\nSep  0.454920 -0.16862  0.035475 0.1519\nOct -0.304459  0.02271 -0.003209 0.1456\nNov  0.104258 -0.14115  0.387838 0.2472\nDec  0.008971  0.35391 -0.678722 0.1743\n\n\nInspect the PCA object with tidier functions from the broom library. These functions turn the PCA object into a tidy dataframe\n\npc %&gt;% \n  tidy() %&gt;% \n  head()\n\n# A tibble: 6 × 3\n  row                                     PC   value\n  &lt;chr&gt;                                &lt;dbl&gt;   &lt;dbl&gt;\n1 Abandoned Vehicle (parked on street)     1 -0.844 \n2 Abandoned Vehicle (parked on street)     2 -0.844 \n3 Abandoned Vehicle (parked on street)     3  0.383 \n4 Abandoned Vehicle (parked on street)     4  0.311 \n5 Abandoned Vehicle (parked on street)     5 -0.206 \n6 Abandoned Vehicle (parked on street)     6  0.0620\n\n\n\npc %&gt;% \n  tidy(\"pcs\")\n\n# A tibble: 12 × 4\n      PC  std.dev percent cumulative\n   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n 1     1 2.03e+ 0  0.344       0.344\n 2     2 1.51e+ 0  0.190       0.534\n 3     3 1.37e+ 0  0.156       0.690\n 4     4 1.06e+ 0  0.0945      0.785\n 5     5 9.37e- 1  0.0732      0.858\n 6     6 6.61e- 1  0.0364      0.894\n 7     7 6.32e- 1  0.0333      0.927\n 8     8 5.73e- 1  0.0274      0.955\n 9     9 4.67e- 1  0.0181      0.973\n10    10 4.19e- 1  0.0146      0.988\n11    11 3.85e- 1  0.0123      1    \n12    12 2.15e-16  0           1    \n\n\n\npc %&gt;% \n  augment(data = df_months) -&gt; au\n\nau %&gt;% \n  head()\n\n# A tibble: 6 × 26\n  .rownames request_type    Jan    Feb    Mar    Apr    May    Jun    Jul    Aug\n  &lt;chr&gt;     &lt;chr&gt;         &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 1         Abandoned V… 0.0890 0.0727 0.0769 0.0710 0.0830 0.0793 0.0778 0.101 \n2 2         Barking Dog  0.0563 0.0608 0.0608 0.0631 0.104  0.101  0.0788 0.113 \n3 3         Board Up (P… 0.0395 0.0482 0.0658 0.0943 0.114  0.0899 0.110  0.123 \n4 4         Broken Side… 0.0337 0.155  0.148  0.0872 0.105  0.0964 0.0696 0.0735\n5 5         Building Ma… 0.0708 0.0919 0.103  0.0739 0.0842 0.0829 0.0725 0.0919\n6 6         Building Wi… 0.0842 0.0697 0.0636 0.0577 0.105  0.0883 0.0924 0.0815\n# ℹ 16 more variables: Sep &lt;dbl&gt;, Oct &lt;dbl&gt;, Nov &lt;dbl&gt;, Dec &lt;dbl&gt;,\n#   .fittedPC1 &lt;dbl&gt;, .fittedPC2 &lt;dbl&gt;, .fittedPC3 &lt;dbl&gt;, .fittedPC4 &lt;dbl&gt;,\n#   .fittedPC5 &lt;dbl&gt;, .fittedPC6 &lt;dbl&gt;, .fittedPC7 &lt;dbl&gt;, .fittedPC8 &lt;dbl&gt;,\n#   .fittedPC9 &lt;dbl&gt;, .fittedPC10 &lt;dbl&gt;, .fittedPC11 &lt;dbl&gt;, .fittedPC12 &lt;dbl&gt;\n\n\nPlot how the PCA object explains the variance in the data\n\npc %&gt;% \n  tidy(\"pcs\") %&gt;%\n  select(-std.dev) %&gt;% \n  gather(measure, value, -PC) %&gt;% \n    ggplot(aes(PC, value)) +\n    geom_line() +\n    geom_point() +\n    facet_wrap(~measure) +\n    labs(title = \"Variance explained by each principal component\",\n         x = \"Principal Component\",\n         y = NULL) +\n    scale_x_continuous(breaks = 1:12)\n\n\n\n\n\n\n\n\nThe first two principal components explain most of the variance\nFor an in-depth plot we need to create the PCA object a different way\n\ndf_months %&gt;% \n  nest() %&gt;% \n  mutate(pca = map(data, ~ prcomp(.x %&gt;% select(-request_type), \n                                  center = TRUE, scale = TRUE)),\n         pca_aug = map2(pca, data, ~augment(.x, data = .y))) -&gt; df_months_pca2\n\nPlot the PCA data\n\ndf_months_pca2 %&gt;%\n  mutate(\n    pca_graph = map2(\n      .x = pca,\n      .y = data,\n      ~ autoplot(.x, loadings = TRUE, loadings.label = TRUE,\n                 loadings.label.repel = TRUE,\n                 data = .y) +\n        theme_bw() +\n        labs(x = \"Principal Component 1\",\n             y = \"Principal Component 2\",\n             title = \"First two principal components of PCA on 311 dataset\")\n    )\n  ) %&gt;%\n  pull(pca_graph)\n\n[[1]]\n\n\n\n\n\n\n\n\n\nThis shows that summer and winter explain a significant part of the variance\nPlot the data to show the outliers\n\nau %&gt;% \n  mutate(outlier = case_when(abs(.fittedPC1) &gt; 2 & abs(.fittedPC2) &gt; 1.5 ~ TRUE),\n         pothole = case_when(request_type == \"Potholes\" ~ \"Potholes\",\n                             request_type != \"Potholes\" ~ \"Other\")) -&gt; au\n\nau %&gt;% \nggplot(aes(.fittedPC1, .fittedPC2)) +\n  geom_point() +\n  geom_label_repel(data = au %&gt;% filter(outlier),\n             aes(label = request_type)) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nau %&gt;% \nggplot(aes(.fittedPC1, .fittedPC2)) +\n  geom_point(aes(color = pothole)) +\n  geom_label_repel(data = au %&gt;% filter(request_type == \"Potholes\"),\n             aes(label = request_type)) +\n  theme_bw() +\n  scale_color_manual(NULL, values = c(\"black\", \"red\"))"
  },
  {
    "objectID": "posts/visualizing-transit-connections-between-pittsburgh-census-tracts/index.html",
    "href": "posts/visualizing-transit-connections-between-pittsburgh-census-tracts/index.html",
    "title": "Visualizing Transit Connections Between Pittsburgh Census Tracts",
    "section": "",
    "text": "In this post I will use transit line and stop data from the WPRDC to map connections between census tracts. I access the census data via {tidycensus}, which contains information about the commuter connections between census tracts.\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tigris)\nlibrary(janitor)\nlibrary(tidycensus)\nlibrary(leaflet)\n\noptions(tigris_use_cache = TRUE,\n        scipen = 999,\n        digits = 2)\n\nThis code loads the transit line data from the WPRDC. I create the full_route_name_id column and set the coordinate reference system to 4326.\n\n##load transit data\ntransit_lines &lt;- st_read(\"post_data/shapefiles/transit_lines/PAAC_Routes_1909.shp\") %&gt;%\n  clean_names() %&gt;%\n  mutate_at(vars(-all_of(c(\"geometry\"))), as.character) %&gt;%\n  rename(route_id = route,\n         service_type = type_serv) %&gt;% \n  distinct(service_type, route_id, route_name, geometry) %&gt;%\n  mutate(full_route_name_id = str_c(route_id, route_name, sep = \" \")) %&gt;% \n  st_transform(3488)\n\nReading layer `PAAC_Routes_1909' from data source \n  `/Users/conorotompkins/Documents/github_repos/ctompkins_quarto_blog/posts/visualizing-transit-connections-between-pittsburgh-census-tracts/post_data/shapefiles/transit_lines/PAAC_Routes_1909.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 102 features and 13 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 1300000 ymin: 350000 xmax: 1400000 ymax: 490000\nProjected CRS: NAD83(2011) / Pennsylvania South (ftUS)\n\ntransit_lines\n\nSimple feature collection with 102 features and 4 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 3300000 ymin: 960000 xmax: 3300000 ymax: 1000000\nProjected CRS: NAD83(NSRS2007) / California Albers\nFirst 10 features:\n   service_type route_id               route_name\n1         Local       26                Chartiers\n2         Local       27                Fairywood\n3         Local       40           Mt. Washington\n4  Key Corridor      61C   McKeesport - Homestead\n5       Express       65            Squirrel Hill\n6  Key Corridor      71A                   Negley\n7  Key Corridor      71D                 Hamilton\n8         Local       74 Homewood - Squirrel Hill\n9         Local       83             Bedford Hill\n10        Local       89         Garfield Commons\n                         geometry          full_route_name_id\n1  MULTILINESTRING ((3293319 9...                26 Chartiers\n2  MULTILINESTRING ((3293319 9...                27 Fairywood\n3  MULTILINESTRING ((3294223 9...           40 Mt. Washington\n4  MULTILINESTRING ((3293614 9...  61C McKeesport - Homestead\n5  MULTILINESTRING ((3300806 9...            65 Squirrel Hill\n6  MULTILINESTRING ((3293614 9...                  71A Negley\n7  MULTILINESTRING ((3293614 9...                71D Hamilton\n8  MULTILINESTRING ((3298497 9... 74 Homewood - Squirrel Hill\n9  MULTILINESTRING ((3293794 9...             83 Bedford Hill\n10 MULTILINESTRING ((3298392 9...         89 Garfield Commons\n\n\nThis is what the transit lines look like on a basic map:\n\ntransit_lines %&gt;% \n  ggplot(aes(color = route_id)) + \n    geom_sf() +\n    guides(color = FALSE) +\n    theme_minimal()\n\n\n\n\n\n\n\n\nThis creates a table of route_id and service_type that I will join against later.\n\ndf_service_type &lt;- transit_lines %&gt;% \n  distinct(service_type, route_id, full_route_name_id) %&gt;% \n  st_drop_geometry()\n\nThis code loads the transit stop shapefile from the WPRDC:\n\ntransit_stops &lt;- st_read(\"post_data/shapefiles/transit_stops/PAAC_Stops_1909.shp\") %&gt;%\n  st_transform(3488) %&gt;% \n  clean_names() %&gt;% \n  mutate_at(vars(-all_of(c(\"geometry\", \"routes_cou\"))), as.character) %&gt;%\n  select(stop_name, routes_served = routes_ser, routes_cou, geometry) %&gt;% \n  distinct(stop_name, routes_served = routes_served, routes_cou, geometry)\n\nReading layer `PAAC_Stops_1909' from data source \n  `/Users/conorotompkins/Documents/github_repos/ctompkins_quarto_blog/posts/visualizing-transit-connections-between-pittsburgh-census-tracts/post_data/shapefiles/transit_stops/PAAC_Stops_1909.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 6946 features and 17 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 1300000 ymin: 350000 xmax: 1400000 ymax: 490000\nProjected CRS: NAD83(2011) / Pennsylvania South (ftUS)\n\ntransit_stops\n\nSimple feature collection with 6946 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3300000 ymin: 960000 xmax: 3300000 ymax: 1000000\nProjected CRS: NAD83(NSRS2007) / California Albers\nFirst 10 features:\n                             stop_name routes_served routes_cou\n1  26TH ST AT PENN AVE FS (SPRING WAY)    54, 88, 91          3\n2               28TH ST AT LIBERTY AVE            54          1\n3                32ND ST AT SPRING WAY    54, 88, 91          3\n4                 40TH ST AT BUTLER ST            93          1\n5            40TH ST AT DAVIDSON ST FS            93          1\n6              40TH ST OPP DAVIDSON ST        64, 93          2\n7                  4TH ST AT COREY AVE            59          1\n8               FIFTH AVE AT AIKEN AVE 28X, 71B, 71D          3\n9            FIFTH AVE AT AMBERSON AVE 28X, 71B, 71D          3\n10         FIFTH AVE AT BEECHWOOD BLVD      28X, 71D          2\n                 geometry\n1  POINT (3294762 976882)\n2  POINT (3294959 977172)\n3  POINT (3295178 977765)\n4  POINT (3295366 978861)\n5  POINT (3295502 978803)\n6  POINT (3295495 978788)\n7  POINT (3305461 976046)\n8  POINT (3298449 977796)\n9  POINT (3298126 977556)\n10 POINT (3299655 978942)\n\n\nMuch of the important data is stored in the routes_served column. This code pivots the data longer to make it easier to work with.\n\n#identify maximum number of routes served by a stop\nmax_routes_served &lt;- transit_stops %&gt;% \n  summarize(max_routes = max(routes_cou)) %&gt;% \n  pull(max_routes)\n\ntransit_stops %&gt;% \n  filter(routes_cou == max_routes_served)\n\nSimple feature collection with 1 feature and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3300000 ymin: 980000 xmax: 3300000 ymax: 980000\nProjected CRS: NAD83(NSRS2007) / California Albers\n                      stop_name\n1 EAST BUSWAY AT PENN STATION C\n                                                                                routes_served\n1 1, 11, 15, 19L, 39, 40, 44, 6, P1, P10, P12, P16, P17, P2, P67, P68, P69, P7, P71, P76, P78\n  routes_cou               geometry\n1         21 POINT (3294168 975311)\n\n\n\n#separate routes_served into multiple columns, one per route\ntransit_stops &lt;- transit_stops %&gt;% \n  separate(routes_served, sep = \", \", into = str_c(\"route_\", 1:max_routes_served), extra = \"merge\", fill = \"right\")\n\ntransit_stops\n\nSimple feature collection with 6946 features and 23 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3300000 ymin: 960000 xmax: 3300000 ymax: 1000000\nProjected CRS: NAD83(NSRS2007) / California Albers\nFirst 10 features:\n                             stop_name route_1 route_2 route_3 route_4 route_5\n1  26TH ST AT PENN AVE FS (SPRING WAY)      54      88      91    &lt;NA&gt;    &lt;NA&gt;\n2               28TH ST AT LIBERTY AVE      54    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;\n3                32ND ST AT SPRING WAY      54      88      91    &lt;NA&gt;    &lt;NA&gt;\n4                 40TH ST AT BUTLER ST      93    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;\n5            40TH ST AT DAVIDSON ST FS      93    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;\n6              40TH ST OPP DAVIDSON ST      64      93    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;\n7                  4TH ST AT COREY AVE      59    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;\n8               FIFTH AVE AT AIKEN AVE     28X     71B     71D    &lt;NA&gt;    &lt;NA&gt;\n9            FIFTH AVE AT AMBERSON AVE     28X     71B     71D    &lt;NA&gt;    &lt;NA&gt;\n10         FIFTH AVE AT BEECHWOOD BLVD     28X     71D    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;\n   route_6 route_7 route_8 route_9 route_10 route_11 route_12 route_13 route_14\n1     &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;\n2     &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;\n3     &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;\n4     &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;\n5     &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;\n6     &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;\n7     &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;\n8     &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;\n9     &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;\n10    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;\n   route_15 route_16 route_17 route_18 route_19 route_20 route_21 routes_cou\n1      &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;          3\n2      &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;          1\n3      &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;          3\n4      &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;          1\n5      &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;          1\n6      &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;          2\n7      &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;          1\n8      &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;          3\n9      &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;          3\n10     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;          2\n                 geometry\n1  POINT (3294762 976882)\n2  POINT (3294959 977172)\n3  POINT (3295178 977765)\n4  POINT (3295366 978861)\n5  POINT (3295502 978803)\n6  POINT (3295495 978788)\n7  POINT (3305461 976046)\n8  POINT (3298449 977796)\n9  POINT (3298126 977556)\n10 POINT (3299655 978942)\n\n\n\ntransit_stops %&gt;% \n  filter(routes_cou == max_routes_served)\n\nSimple feature collection with 1 feature and 23 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3300000 ymin: 980000 xmax: 3300000 ymax: 980000\nProjected CRS: NAD83(NSRS2007) / California Albers\n                      stop_name route_1 route_2 route_3 route_4 route_5 route_6\n1 EAST BUSWAY AT PENN STATION C       1      11      15     19L      39      40\n  route_7 route_8 route_9 route_10 route_11 route_12 route_13 route_14 route_15\n1      44       6      P1      P10      P12      P16      P17       P2      P67\n  route_16 route_17 route_18 route_19 route_20 route_21 routes_cou\n1      P68      P69       P7      P71      P76      P78         21\n                geometry\n1 POINT (3294168 975311)\n\n#pivot data longer\ntransit_stops &lt;- transit_stops %&gt;% \n  pivot_longer(cols = starts_with(\"route_\"), names_to = \"route_number\", values_to = \"route_id\") %&gt;% \n  st_as_sf()\n  \ntransit_stops\n\nSimple feature collection with 145866 features and 4 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3300000 ymin: 960000 xmax: 3300000 ymax: 1000000\nProjected CRS: NAD83(NSRS2007) / California Albers\n# A tibble: 145,866 × 5\n   stop_name          routes_cou         geometry route_number route_id\n   &lt;chr&gt;                   &lt;dbl&gt;      &lt;POINT [m]&gt; &lt;chr&gt;        &lt;chr&gt;   \n 1 26TH ST AT PENN A…          3 (3294762 976882) route_1      54      \n 2 26TH ST AT PENN A…          3 (3294762 976882) route_2      88      \n 3 26TH ST AT PENN A…          3 (3294762 976882) route_3      91      \n 4 26TH ST AT PENN A…          3 (3294762 976882) route_4      &lt;NA&gt;    \n 5 26TH ST AT PENN A…          3 (3294762 976882) route_5      &lt;NA&gt;    \n 6 26TH ST AT PENN A…          3 (3294762 976882) route_6      &lt;NA&gt;    \n 7 26TH ST AT PENN A…          3 (3294762 976882) route_7      &lt;NA&gt;    \n 8 26TH ST AT PENN A…          3 (3294762 976882) route_8      &lt;NA&gt;    \n 9 26TH ST AT PENN A…          3 (3294762 976882) route_9      &lt;NA&gt;    \n10 26TH ST AT PENN A…          3 (3294762 976882) route_10     &lt;NA&gt;    \n# ℹ 145,856 more rows\n\n\n\ntransit_stops &lt;- transit_stops %&gt;%\n  filter(!is.na(route_id)) %&gt;% \n  left_join(df_service_type)\n\nThis code loads the census tract data via {tidycensus}. I choose the census tracts 42003020100 (Downtown) and 42003070300 (Shadyside).\n\n#load tract data\nallegheny_tracts &lt;- get_decennial(geography = \"tract\",\n                                  variables = c(total_pop = \"P001001\"),\n                                  state = \"PA\",\n                                  county = \"Allegheny County\",\n                                  geometry = TRUE,\n                                  output = \"wide\",\n                                  year = 2010) %&gt;%\n  mutate(name = case_when(GEOID == \"42003020100\" ~ \"Downtown\",\n                          GEOID == \"42003070300\" ~ \"Shadyside\")) %&gt;% \n  st_transform(3488)\n\n#calculate centers of the tracts\nallegheny_tracts_centroid &lt;- allegheny_tracts %&gt;%\n  mutate(name = case_when(GEOID == \"42003020100\" ~ \"Downtown\",\n                          GEOID == \"42003070300\" ~ \"Shadyside\")) %&gt;% \n  mutate(lon = map_dbl(geometry, ~st_centroid(.x)[[1]]),\n         lat = map_dbl(geometry, ~st_centroid(.x)[[2]])) %&gt;% \n  st_drop_geometry() %&gt;% \n  st_as_sf(coords = c(\"lon\", \"lat\"), crs = 3488) %&gt;% \n  st_transform(3488)\n\n#creates table with geometry of the county border\nallegheny &lt;- allegheny_tracts %&gt;% \n  summarize()\n\n\ncommute_tracts &lt;- allegheny_tracts %&gt;% \n  filter(!is.na(name))\n\ncommute_centroids &lt;- allegheny_tracts_centroid %&gt;% \n  filter(!is.na(name))\n\nThis code uses st_is_within_distance to find the transit stops that are within 700 meters of the center of the Downtown AND Shadyside census tracts.\n\ndf_stops_joined_distance &lt;- transit_stops %&gt;% \n  st_join(commute_centroids, st_is_within_distance, dist = 700, left = TRUE) %&gt;% \n  arrange(route_id)\n\ndf_stops_joined_distance\n\nSimple feature collection with 12087 features and 10 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3300000 ymin: 960000 xmax: 3300000 ymax: 1000000\nProjected CRS: NAD83(NSRS2007) / California Albers\n# A tibble: 12,087 × 11\n   stop_name          routes_cou         geometry route_number route_id\n   &lt;chr&gt;                   &lt;dbl&gt;      &lt;POINT [m]&gt; &lt;chr&gt;        &lt;chr&gt;   \n 1 WEST MIFFLIN GARA…          4 (3302630 969221) route_2      \"\"      \n 2 LAYOVER PENN PARK…         12 (3294348 975649) route_2      \"\"      \n 3 LAYOVER PENN PARK…         12 (3294348 975649) route_6      \"\"      \n 4 LAYOVER PENN PARK…         12 (3294348 975649) route_8      \"\"      \n 5 LAYOVER PENN PARK…         12 (3294348 975649) route_11     \"\"      \n 6 4TH AVE AT 7TH ST           1 (3306225 995240) route_1      \"1\"     \n 7 E 4TH AVE AT BOYD…          2 (3305254 999390) route_1      \"1\"     \n 8 4TH AVE AT CENTRA…          1 (3306153 995343) route_1      \"1\"     \n 9 E 4TH AVE AT LOCK…          2 (3305377 999550) route_1      \"1\"     \n10 E 4TH AVE AT WOOD…          2 (3305323 999479) route_1      \"1\"     \n# ℹ 12,077 more rows\n# ℹ 6 more variables: service_type &lt;chr&gt;, full_route_name_id &lt;chr&gt;,\n#   GEOID &lt;chr&gt;, NAME &lt;chr&gt;, total_pop &lt;dbl&gt;, name &lt;chr&gt;\n\ndf_stops_joined_distance &lt;- df_stops_joined_distance %&gt;% \n  filter(!is.na(name))\n\ndf_stops_joined_distance\n\nSimple feature collection with 736 features and 10 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3300000 ymin: 970000 xmax: 3300000 ymax: 980000\nProjected CRS: NAD83(NSRS2007) / California Albers\n# A tibble: 736 × 11\n   stop_name          routes_cou         geometry route_number route_id\n * &lt;chr&gt;                   &lt;dbl&gt;      &lt;POINT [m]&gt; &lt;chr&gt;        &lt;chr&gt;   \n 1 7TH ST AT FT DUQU…          9 (3293481 975066) route_1      1       \n 2 7TH ST AT PENN AV…          4 (3293586 974931) route_1      1       \n 3 LIBERTY AVE AT 7T…          8 (3293638 974904) route_1      1       \n 4 LIBERTY AVE AT SM…         19 (3293846 975073) route_1      1       \n 5 LIBERTY AVE AT WI…         18 (3293943 975161) route_1      1       \n 6 LIBERTY AVE OPP 9…         20 (3293761 974994) route_1      1       \n 7 LIBERTY AVE OPP S…          8 (3293856 975107) route_1      1       \n 8 7TH ST AT FT DUQU…          9 (3293481 975066) route_2      11      \n 9 7TH ST AT PENN AV…          4 (3293586 974931) route_2      11      \n10 LIBERTY AVE AT 7T…          8 (3293638 974904) route_2      11      \n# ℹ 726 more rows\n# ℹ 6 more variables: service_type &lt;chr&gt;, full_route_name_id &lt;chr&gt;,\n#   GEOID &lt;chr&gt;, NAME &lt;chr&gt;, total_pop &lt;dbl&gt;, name &lt;chr&gt;\n\ndf_route_filter &lt;- df_stops_joined_distance %&gt;% \n  st_drop_geometry() %&gt;% \n  distinct(route_id, name) %&gt;% \n  group_by(route_id) %&gt;% \n  filter(n() &gt;= 2) %&gt;% \n  ungroup() %&gt;% \n  distinct(route_id)\n\ndf_route_filter\n\n# A tibble: 22 × 1\n   route_id\n   &lt;chr&gt;   \n 1 28X     \n 2 67      \n 3 69      \n 4 71A     \n 5 71B     \n 6 71C     \n 7 71D     \n 8 82      \n 9 86      \n10 P1      \n# ℹ 12 more rows\n\ndf_stops_joined_distance &lt;- df_stops_joined_distance %&gt;% \n  semi_join(df_route_filter, by = c(\"route_id\" = \"route_id\")) %&gt;% \n  left_join(df_service_type)\n\ndf_stops_joined_distance &lt;- df_stops_joined_distance %&gt;% \n  mutate(stop_name_route_id_route_name = str_c(full_route_name_id, str_to_title(stop_name), sep = \" - \"))\n\nTo help visualize how the join works, imagine a buffer around each point (transit stop). This shows the stops from 71A with a buffer:\n\nst_crs(transit_stops)$units\n\n[1] \"m\"\n\ntransit_stops %&gt;% \n  mutate(geometry_buffered = st_buffer(geometry, dist = 700)) %&gt;% \n  #st_transform(crs = 3488) %&gt;% \n  filter(route_id == \"71A\") %&gt;% \n  ggplot() +\n    geom_sf(data = commute_centroids,\n            color = \"blue\",\n            size = 3) +\n    geom_sf(color = \"red\", size = 1) +\n    geom_sf(aes(geometry = geometry_buffered),\n            fill = NA\n            ) +\n    #coord_sf(crs = 4326) +\n    theme_minimal()\n\n\n\n\n\n\n\n\nst_is_within_distance identifies which red points are within 700 meters of the blue points.\nThis shows the tracts, the tract centroids, and the transit stops within 700 meters of the centroids.\n\ndf_stops_joined_distance %&gt;% \n  ggplot() +\n    geom_sf(data = commute_tracts) +\n    geom_sf(data = commute_centroids, color = \"red\") +\n    geom_sf() +\n    theme_minimal()\n\n\n\n\n\n\n\n\nThis filters on the transit lines served by the stops that successfully joined against the Downtown and Shadyside centroids:\n\ncommuter_transit_lines &lt;- transit_lines %&gt;% \n  semi_join(df_route_filter, by = c(\"route_id\" = \"route_id\"))\n\ncommuter_transit_lines\n\nSimple feature collection with 22 features and 4 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 3300000 ymin: 970000 xmax: 3300000 ymax: 1000000\nProjected CRS: NAD83(NSRS2007) / California Albers\nFirst 10 features:\n   service_type route_id              route_name         full_route_name_id\n1  Key Corridor      71A                  Negley                 71A Negley\n2  Key Corridor      71D                Hamilton               71D Hamilton\n3         Rapid       P2       East Busway Short       P2 East Busway Short\n4       Express      P78           Oakmont Flyer          P78 Oakmont Flyer\n5         Rapid       P1 East Busway - All Stops P1 East Busway - All Stops\n6  Key Corridor      71B           Highland Park          71B Highland Park\n7         Local       69                Trafford                69 Trafford\n8         Local       86                 Liberty                 86 Liberty\n9       Express      P68    Braddock Hills Flyer   P68 Braddock Hills Flyer\n10      Express      P69          Trafford Flyer         P69 Trafford Flyer\n                         geometry\n1  MULTILINESTRING ((3293614 9...\n2  MULTILINESTRING ((3293614 9...\n3  MULTILINESTRING ((3293856 9...\n4  MULTILINESTRING ((3294045 9...\n5  MULTILINESTRING ((3294045 9...\n6  MULTILINESTRING ((3295547 9...\n7  MULTILINESTRING ((3310878 9...\n8  MULTILINESTRING ((3294134 9...\n9  MULTILINESTRING ((3294049 9...\n10 MULTILINESTRING ((3294049 9...\n\n\nThese are the transit lines that serve the two tracts:\n\ncommute_centroids %&gt;% \n  ggplot() +\n    geom_sf(size = 3) +\n    geom_sf(data = commuter_transit_lines, aes(color = route_id)) +\n    theme_void()\n\n\n\n\n\n\n\n\nThis sets the bounding box for the final static map:\n\ncommute_zoom &lt;- commute_tracts %&gt;% \n  st_buffer(dist = 700) %&gt;% \n  st_bbox()\n\nThis crops the transit lines to only include the parts within the bounding box:\n\ncommuter_transit_lines %&gt;% \n  st_crop(commute_zoom)\n\nSimple feature collection with 22 features and 4 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 3300000 ymin: 970000 xmax: 3300000 ymax: 980000\nProjected CRS: NAD83(NSRS2007) / California Albers\nFirst 10 features:\n   service_type route_id              route_name         full_route_name_id\n1  Key Corridor      71A                  Negley                 71A Negley\n2  Key Corridor      71D                Hamilton               71D Hamilton\n3         Rapid       P2       East Busway Short       P2 East Busway Short\n4       Express      P78           Oakmont Flyer          P78 Oakmont Flyer\n5         Rapid       P1 East Busway - All Stops P1 East Busway - All Stops\n6  Key Corridor      71B           Highland Park          71B Highland Park\n7         Local       69                Trafford                69 Trafford\n8         Local       86                 Liberty                 86 Liberty\n9       Express      P68    Braddock Hills Flyer   P68 Braddock Hills Flyer\n10      Express      P69          Trafford Flyer         P69 Trafford Flyer\n                         geometry\n1  MULTILINESTRING ((3293614 9...\n2  MULTILINESTRING ((3293614 9...\n3  MULTILINESTRING ((3293856 9...\n4  LINESTRING (3294045 975240,...\n5  MULTILINESTRING ((3294045 9...\n6  MULTILINESTRING ((3295547 9...\n7  MULTILINESTRING ((3299435 9...\n8  MULTILINESTRING ((3294134 9...\n9  MULTILINESTRING ((3294049 9...\n10 MULTILINESTRING ((3294049 9...\n\n\nThis plots the Downtown and Shadyside census tracts and the transit lines and stops that serve them:\n\np &lt;- commuter_transit_lines %&gt;% \n  st_crop(commute_zoom) %&gt;% \n  ggplot() +\n    geom_sf(data = allegheny, fill = NA) +\n    geom_sf(data = commute_tracts, aes(fill = name), size = 1, alpha = .5) +\n    geom_sf_label(data = commute_centroids, aes(label = name)) +\n    geom_sf(aes(color = route_id)) +\n    geom_sf(data = st_jitter(df_stops_joined_distance), aes(color = route_id), shape = 21, size = 3) +\n    geom_sf_label(aes(color = route_id, label = route_id)) +\n    coord_sf(xlim = c(commute_zoom[1], commute_zoom[3]),\n             ylim = c(commute_zoom[2], commute_zoom[4])) +\n    facet_wrap(~service_type) +\n    guides(color = FALSE,\n           fill = FALSE) +\n    theme_void() +\n    theme(panel.border = element_rect(color = \"black\", fill = NA))\n\np\n\n\n\n\n\n\n\n\nYou can use this interactive map made with leaflet to explore the transit lines and stops that connect Downtown and Shadyside:\n\nlibrary(widgetframe)\n#transform geometries to crs 4326\n\nallegheny &lt;- st_transform(allegheny, crs = 4326)\ncommute_tracts &lt;- st_transform(commute_tracts, crs = 4326)\ncommuter_transit_lines &lt;- st_transform(commuter_transit_lines, crs = 4326)\ndf_stops_joined_distance &lt;- st_transform(df_stops_joined_distance, crs = 4326)\ncommute_tracts &lt;- st_transform(commute_tracts, crs = 4326)\n\ncommute_zoom &lt;- commute_tracts %&gt;% \n  st_buffer(dist = .01) %&gt;% \n  st_bbox()\n\nnames(commute_zoom) &lt;- NULL\n\n###leaflet\ntransit_lines_palette &lt;- colorFactor(palette = \"Set1\", domain = commuter_transit_lines$full_route_name_id)\ntract_palette &lt;- colorFactor(palette = \"Set1\", domain = commute_tracts$GEOID)\n\ninteractive_map &lt;- leaflet() %&gt;% \n  addProviderTiles(providers$CartoDB.Positron) %&gt;% \n  addPolygons(data = allegheny,\n              color = \"#444444\",\n              stroke = TRUE,\n              fillOpacity = 0,\n              opacity = 1,\n              weight = 2,\n              group = \"Census tracts\") %&gt;%\n  addPolygons(data = commute_tracts,\n              #color\n              color = NA,\n              #fill\n              fillColor = ~tract_palette(GEOID),\n              fillOpacity = .3,\n              \n              #label\n              label = commute_tracts$name,\n              group = \"Census tracts\") %&gt;% \n  addPolylines(data = commuter_transit_lines,\n              color = ~transit_lines_palette(full_route_name_id),\n              label = commuter_transit_lines$full_route_name_id,\n              \n              #highlight\n              highlightOptions = highlightOptions(weight = 10, bringToFront = TRUE),\n              group = \"Transit lines and stops\"\n               ) %&gt;% \n  addCircles(data = df_stops_joined_distance,\n             radius = 3,\n             color = ~transit_lines_palette(full_route_name_id),\n             \n             #highlight\n             highlightOptions = highlightOptions(weight = 10, bringToFront = TRUE),\n             \n             #label\n             label = str_to_title(df_stops_joined_distance$stop_name_route_id_route_name),\n             group = \"Transit lines and stops\") %&gt;% \n  addLayersControl(overlayGroups = c(\"Census tracts\",\"Transit lines and stops\"), position = \"topleft\", \n                   options = layersControlOptions(collapsed = FALSE)) %&gt;% \n  addMiniMap() %&gt;% \n  fitBounds(lng1 = commute_zoom[[1]], lat1 = commute_zoom[[2]], lng2 = commute_zoom[[3]], lat2 = commute_zoom[[4]])\n\nframeWidget(interactive_map)"
  },
  {
    "objectID": "posts/clustering-bird-species-with-seasonality/index.html",
    "href": "posts/clustering-bird-species-with-seasonality/index.html",
    "title": "Clustering Bird Species with Seasonality",
    "section": "",
    "text": "In this post, I use k-means clustering to identify clusters of bird species based on frequency of observations per month. I use bird sightings in Allegheny County from eBird.\nLoad the relevant libraries:\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(janitor)\nlibrary(vroom)\nlibrary(broom)\nlibrary(hrbrthemes)\n\ntheme_set(theme_bw())\n\nset.seed(1234)\n\nLoad and filter the data:\n\ndf &lt;- vroom(\"post_data/ebd_US-PA-003_201001_202003_relFeb-2020.zip\", delim = \"\\t\") %&gt;% \n  clean_names() %&gt;% \n  mutate_at(vars(observer_id, locality, observation_date, time_observations_started, protocol_type), str_replace_na, \"NA\") %&gt;% \n  mutate(observation_count = as.numeric(str_replace(observation_count, \"X\", as.character(NA))),\n         observation_event_id = str_c(observer_id, locality, observation_date, time_observations_started, sep = \"-\"),\n         observation_date = ymd(observation_date)) %&gt;%\n  filter(all_species_reported == 1)\n\n\ndf_top_protocols &lt;- df %&gt;% \n  count(protocol_type, sort = TRUE) %&gt;% \n  slice(1:2)\n\ndf &lt;- df %&gt;% \n  semi_join(df_top_protocols) %&gt;% \n  filter(year(observation_date) &gt;= 2016)\n\n\ndf %&gt;% \n  select(common_name, observation_date, observation_count) %&gt;% \n  glimpse()\n\nRows: 533,493\nColumns: 3\n$ common_name       &lt;chr&gt; \"American Black Duck\", \"American Black Duck\", \"Ameri…\n$ observation_date  &lt;date&gt; 2016-01-31, 2016-01-24, 2016-01-30, 2016-01-31, 201…\n$ observation_count &lt;dbl&gt; 2, 2, 2, 3, 2, 3, 57, 7, 2, 4, 1, 5, 8, 1, 1, 4, 1, …\n\n\nThis graph shows general seasonality in bird observations:\n\ndf %&gt;% \n  count(observation_date) %&gt;% \n  ggplot(aes(observation_date, n)) +\n    geom_line() +\n    labs(x = \"Observation date\",\n         y = \"Observation events\") +\n    scale_y_comma()\n\n\n\n\n\n\n\n\nThis code chunk calculates the average number of observations by species and month. Then, it interpolates a value of 0 for birds where there were no sightings in a given month:\n\nmonths &lt;- df %&gt;% \n  mutate(observation_month = month(observation_date, label = TRUE)) %&gt;% \n  distinct(observation_month) %&gt;% \n  pull(observation_month)\n\ndf_seasonality &lt;- df %&gt;% \n  mutate(observation_month = month(observation_date, label = TRUE),\n         observation_year = year(observation_date)) %&gt;% \n  group_by(common_name, observation_year, observation_month) %&gt;% \n  summarize(observation_count = sum(observation_count, na.rm = TRUE)) %&gt;% \n  group_by(common_name, observation_month) %&gt;% \n  summarize(observation_count_mean = mean(observation_count) %&gt;% round(1)) %&gt;% \n  ungroup() %&gt;% \n  complete(common_name, observation_month = months) %&gt;% \n  replace_na(list(observation_count_mean = 0)) %&gt;% \n  arrange(common_name, observation_month)\n\nglimpse(df_seasonality)\n\nRows: 3,672\nColumns: 3\n$ common_name            &lt;chr&gt; \"Acadian Flycatcher\", \"Acadian Flycatcher\", \"Ac…\n$ observation_month      &lt;ord&gt; Jan, Feb, Mar, Apr, May, Jun, Jul, Aug, Sep, Oc…\n$ observation_count_mean &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 184.8, 98.5, 67.0, 19.0, 22…\n\n\nThis transforms the mean monthly observation into log10:\n\ndf_seasonality &lt;- df_seasonality %&gt;% \n  mutate(observation_count_mean_log10 = log10(observation_count_mean),\n         observation_count_mean_log10 = case_when(is.infinite(observation_count_mean_log10) ~ 0,\n                                                  TRUE ~ observation_count_mean_log10)) %&gt;% \n  select(-observation_count_mean)\n\nThese graphs show that observations generally increase in the spring and fall, but there is wide variation:\n\ndf_seasonality %&gt;% \n  ggplot(aes(observation_month, observation_count_mean_log10)) +\n    geom_boxplot() +\n    labs(x = \"Observation month\",\n         y = \"Mean observation count (log10)\")\n\n\n\n\n\n\n\n\nThis tile graph shows the seasonality trends per species. I sort the birds by ascending mean observation count by month. It shows there are birds that appear year-round, some that appear seasonally, and some that only appear sporadically:\n\nvec_common_name &lt;- df_seasonality %&gt;% \n  pivot_wider(names_from = observation_month, values_from = observation_count_mean_log10, names_prefix = \"month_\") %&gt;% \n  clean_names() %&gt;% \n  arrange(month_jan, month_feb, month_mar, month_apr, month_may, month_jun, month_jul, month_aug, month_sep, month_oct, month_nov, month_dec) %&gt;% \n  pull(common_name)\n\n\ndf_seasonality %&gt;%\n  mutate(common_name = factor(common_name, levels = vec_common_name)) %&gt;%\n  ggplot(aes(observation_month, common_name, fill = observation_count_mean_log10)) +\n    geom_tile() +\n    scale_fill_viridis_c(\"Mean observation count (log10)\") +\n    scale_x_discrete(expand = c(0,0)) +\n    scale_y_discrete(expand = c(0,0)) +\n    labs(x = \"Observation month\",\n         y = \"Species\") +\n    theme(panel.grid = element_blank(),\n          axis.text.y = element_blank(),\n          axis.ticks.y = element_blank())\n\n\n\n\n\n\n\n\nYou can subjectively see clusters of bird types in the above graph. I will use k-means to attempt to find those clusters.\nThis code chunk pivots the data wide to prepare it for clustering:\n\ndf_seasonality_wide &lt;- df_seasonality %&gt;% \n  select(common_name, observation_month, observation_count_mean_log10) %&gt;% \n  pivot_wider(names_from = observation_month, values_from = observation_count_mean_log10, names_prefix = \"month_\") %&gt;% \n  clean_names()\n\nglimpse(df_seasonality_wide)\n\nRows: 306\nColumns: 13\n$ common_name &lt;chr&gt; \"Acadian Flycatcher\", \"Accipiter sp.\", \"Alder Flycatcher\",…\n$ month_jan   &lt;dbl&gt; 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0…\n$ month_feb   &lt;dbl&gt; 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000…\n$ month_mar   &lt;dbl&gt; 0.0000000, 0.1760913, 0.0000000, 0.0000000, 0.0000000, 0.0…\n$ month_apr   &lt;dbl&gt; 0.0000000, 0.3010300, 0.0000000, 0.0000000, 0.0000000, 0.6…\n$ month_may   &lt;dbl&gt; 2.2667020, 0.3010300, 0.1760913, 0.4771213, 0.0000000, 0.0…\n$ month_jun   &lt;dbl&gt; 1.993436, 0.000000, 0.000000, 0.000000, 0.000000, 0.000000…\n$ month_jul   &lt;dbl&gt; 1.8260748, 0.0000000, 0.0000000, 0.4771213, 0.0000000, 0.0…\n$ month_aug   &lt;dbl&gt; 1.2787536, 0.0000000, 0.0000000, 0.3979400, 0.9030900, 0.0…\n$ month_sep   &lt;dbl&gt; 1.3424227, 0.0000000, 0.0000000, 1.0086002, 1.8450980, 0.0…\n$ month_oct   &lt;dbl&gt; 0.0000000, 0.3010300, 0.0000000, 0.0000000, 0.0000000, 0.0…\n$ month_nov   &lt;dbl&gt; 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0…\n$ month_dec   &lt;dbl&gt; 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0…\n\n\nThis uses purrr to cluster the data with varying numbers of clusters (1 to 9):\n\nkclusts &lt;- tibble(k = 1:9) %&gt;%\n  mutate(\n    kclust = map(k, ~kmeans(df_seasonality_wide %&gt;% select(-common_name), .x)),\n    tidied = map(kclust, tidy),\n    glanced = map(kclust, glance),\n    augmented = map(kclust, augment, df_seasonality_wide %&gt;% select(-common_name))\n  )\n\nkclusts\n\n# A tibble: 9 × 5\n      k kclust   tidied            glanced          augmented          \n  &lt;int&gt; &lt;list&gt;   &lt;list&gt;            &lt;list&gt;           &lt;list&gt;             \n1     1 &lt;kmeans&gt; &lt;tibble [1 × 15]&gt; &lt;tibble [1 × 4]&gt; &lt;tibble [306 × 13]&gt;\n2     2 &lt;kmeans&gt; &lt;tibble [2 × 15]&gt; &lt;tibble [1 × 4]&gt; &lt;tibble [306 × 13]&gt;\n3     3 &lt;kmeans&gt; &lt;tibble [3 × 15]&gt; &lt;tibble [1 × 4]&gt; &lt;tibble [306 × 13]&gt;\n4     4 &lt;kmeans&gt; &lt;tibble [4 × 15]&gt; &lt;tibble [1 × 4]&gt; &lt;tibble [306 × 13]&gt;\n5     5 &lt;kmeans&gt; &lt;tibble [5 × 15]&gt; &lt;tibble [1 × 4]&gt; &lt;tibble [306 × 13]&gt;\n6     6 &lt;kmeans&gt; &lt;tibble [6 × 15]&gt; &lt;tibble [1 × 4]&gt; &lt;tibble [306 × 13]&gt;\n7     7 &lt;kmeans&gt; &lt;tibble [7 × 15]&gt; &lt;tibble [1 × 4]&gt; &lt;tibble [306 × 13]&gt;\n8     8 &lt;kmeans&gt; &lt;tibble [8 × 15]&gt; &lt;tibble [1 × 4]&gt; &lt;tibble [306 × 13]&gt;\n9     9 &lt;kmeans&gt; &lt;tibble [9 × 15]&gt; &lt;tibble [1 × 4]&gt; &lt;tibble [306 × 13]&gt;\n\n\n\nclusters &lt;- kclusts %&gt;%\n  unnest(tidied)\n\nassignments &lt;- kclusts %&gt;% \n  unnest(augmented)\n\nclusterings &lt;- kclusts %&gt;%\n  unnest(glanced)\n\nThis scree plot shows that 2 clusters is probably optimal, but 4 could also be useful:\n\nggplot(clusterings, aes(k, tot.withinss)) +\n  geom_line() +\n  geom_vline(xintercept = 2, linetype = 2) +\n  geom_vline(xintercept = 4, linetype = 2) +\n  scale_x_continuous(breaks = seq(1:9)) +\n  labs(x = \"Number of clusters\")\n\n\n\n\n\n\n\n\nThis graph shows how the clustering performed by comparing the observation value in January to the value in the other months, for each k cluster value 1 through 4:\n\nassignments %&gt;% \n  select(k, .cluster, contains(\"month_\")) %&gt;% \n  mutate(id = row_number()) %&gt;% \n  pivot_longer(cols = contains(\"month_\"), names_to = \"observation_month\", values_to = \"observation_count_mean_log10\") %&gt;% \n  mutate(month_jan = case_when(observation_month == \"month_jan\" ~ observation_count_mean_log10,\n                               TRUE ~ as.numeric(NA))) %&gt;% \n  group_by(k, .cluster, id) %&gt;% \n  fill(month_jan, .direction = c(\"down\")) %&gt;% \n  ungroup() %&gt;% \n  filter(observation_month != \"month_jan\",\n         k &lt;= 4) %&gt;% \n  mutate(k = str_c(k, \"cluster(s)\", sep = \" \")) %&gt;% \n  ggplot(aes(observation_count_mean_log10, month_jan, color = .cluster)) +\n    geom_point() +\n    facet_grid(k ~ observation_month) +\n    labs(x = \"Observation month\",\n         y = \"January\")\n\n\n\n\n\n\n\n\nSubjectively, I think the optimal number of clusters is 4. It is noiser, but could show more interesting granularity in seasonality.\nThis clusters the data using 4 clusters:\n\ndf_kmeans &lt;- df_seasonality_wide %&gt;% \n  select(-common_name) %&gt;% \n  kmeans(centers = 4)\n\n\ndf_clustered &lt;- augment(df_kmeans, df_seasonality_wide) %&gt;% \n  select(common_name, .cluster)\n\ndf_clustered\n\n# A tibble: 306 × 2\n   common_name                                   .cluster\n   &lt;chr&gt;                                         &lt;fct&gt;   \n 1 Acadian Flycatcher                            4       \n 2 Accipiter sp.                                 3       \n 3 Alder Flycatcher                              3       \n 4 Alder/Willow Flycatcher (Traill's Flycatcher) 3       \n 5 American Avocet                               3       \n 6 American Bittern                              3       \n 7 American Black Duck                           2       \n 8 American Coot                                 2       \n 9 American Crow                                 1       \n10 American Golden-Plover                        3       \n# ℹ 296 more rows\n\n\nThis shows the same style of tile graph as shown previously, but facets it by cluster.\n\nvec_common_name_cluster &lt;- df_seasonality %&gt;%\n  left_join(df_clustered) %&gt;% \n  pivot_wider(names_from = observation_month, values_from = observation_count_mean_log10, names_prefix = \"month_\") %&gt;% \n  clean_names() %&gt;% \n  arrange(cluster, month_jan, month_feb, month_mar, month_apr, month_may, month_jun, month_jul, month_aug, month_sep, month_oct, month_nov, month_dec) %&gt;% \n  pull(common_name)\n\n\ndf_seasonality_clustered &lt;-  df_seasonality %&gt;%\n  left_join(df_clustered) %&gt;% \n  mutate(common_name = factor(common_name, levels = vec_common_name_cluster))\n\ndf_seasonality_clustered %&gt;% \n  mutate(.cluster = str_c(\"Cluster\", .cluster, sep = \" \")) %&gt;% \n  ggplot(aes(observation_month, common_name, fill = observation_count_mean_log10)) +\n    geom_tile() +\n    facet_wrap(~.cluster, scales = \"free_y\") +\n    scale_fill_viridis_c(\"Mean observation count (log10)\") +\n    scale_x_discrete(expand = c(0,0)) +\n    scale_y_discrete(expand = c(0,0)) +\n    labs(x = \"Observation month\",\n         y = \"Species\") +\n    theme(panel.grid = element_blank(),\n          axis.text.y = element_blank(),\n          axis.ticks.y = element_blank())\n\n\n\n\n\n\n\n\nCluster 1 shows birds that only appear sporadically. I think these are birds that migrate through Allegheny County, but do not stick around. Cluster 2 shows birds that are generally around all year. Cluster 3 shows birds that are seen mostly during the summer, and cluster 4 contains birds that appear in the winter.\nThis shows a sample of each cluster:\n\ndf_cluster_sample &lt;- df_clustered %&gt;% \n  group_by(.cluster) %&gt;% \n  sample_n(10, replace = FALSE) %&gt;% \n  ungroup()\n\ndf_seasonality_clustered %&gt;%\n  semi_join(df_cluster_sample) %&gt;% \n  mutate(.cluster = str_c(\"Cluster\", .cluster, sep = \" \")) %&gt;% \n  ggplot(aes(observation_month, common_name, fill = observation_count_mean_log10)) +\n    geom_tile() +\n    facet_wrap(~.cluster, scales = \"free_y\") +\n    scale_fill_viridis_c(\"Mean observation count (log10)\") +\n    scale_x_discrete(expand = c(0,0)) +\n    scale_y_discrete(expand = c(0,0)) +\n    labs(x = \"Observation month\",\n         y = NULL) +\n    theme(panel.grid = element_blank())"
  },
  {
    "objectID": "posts/compare-pre-post-covid/index.html",
    "href": "posts/compare-pre-post-covid/index.html",
    "title": "Comparing Healthy Ride Usage Pre And “Post” COVID-19",
    "section": "",
    "text": "Lawrence Andrews asked me on Twitter if there had been a change in Health Ride usage after COVID-19.\n\n\nWould be interested to see this @healthyridepgh data to compare pre-covid (2019) and during (2020)\n\n— Lawrence Andrews (@lawrenceandrews) August 13, 2020\n\n\nThe {tidyverts} universe of packages from Rob Hyndman provides a lot of tools that let you interrogate time series data. I will use some of these tools to decompose the Healthy Ride time series and see if there was a change.\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(janitor)\nlibrary(tsibble)\nlibrary(feasts)\nlibrary(hrbrthemes)\n\noptions(scipen = 999, digits = 4)\n\ntheme_set(theme_ipsum(base_size = 20, \n                      strip_text_size = 18, \n                      axis_title_size = 18))\n\nI had already combined the usage data from the WPRDC with list.files and map_df(read_csv), so I can just read in the combined CSV file:\n\ndata &lt;- read_csv(\"post_data/combined_ride_data.csv\")\n\nSummarizing the number of rides per day shows that the data is very seasonal. The red line is on March 6, which is the date of the first known positive COVID-19 case in the state.\n\ndata %&gt;% \n  count(date, name = \"number_of_rides\", sort = TRUE) %&gt;% \n  filter(!is.na(date)) %&gt;% \n  ggplot(aes(date, number_of_rides)) +\n  geom_point(alpha = .5, size = .5) +\n  geom_vline(xintercept = ymd(\"2020-03-06\"), color = \"red\")\n\n\n\n\n\n\n\n\nI use the {tsibble} package to make a time series tibble and fill in a few gaps in the data. Then I create 3 different models to decompose the time series. I will compare these 3 models to see which strips away the seasonality the best.\n\ndcmp &lt;- data %&gt;%\n  mutate(time = date) %&gt;% \n  count(time, name = \"number_of_rides\") %&gt;%\n  as_tsibble(index = time) %&gt;%\n  tsibble::fill_gaps(number_of_rides = 0) %&gt;% \n  model(STL(number_of_rides),\n        STL(number_of_rides ~ season(window = Inf)),\n        STL(number_of_rides ~ trend(window=7) + season(window='periodic'),\n            robust = TRUE))\n\ncomponents(dcmp) %&gt;% \n  glimpse()\n\nRows: 5,850\nColumns: 8\nKey: .model [3]\n: number_of_rides = trend + season_year + season_week + remainder\n$ .model          &lt;chr&gt; \"STL(number_of_rides)\", \"STL(number_of_rides)\", \"STL(n…\n$ time            &lt;date&gt; 2015-05-31, 2015-06-01, 2015-06-02, 2015-06-03, 2015-…\n$ number_of_rides &lt;dbl&gt; 480, 126, 139, 131, 213, 274, 380, 424, 124, 255, 267,…\n$ trend           &lt;dbl&gt; 249.2, 249.1, 249.1, 249.0, 248.9, 248.9, 248.8, 248.8…\n$ season_week     &lt;dbl&gt; 120.53, -87.43, -67.74, -17.65, -50.30, 16.87, 87.83, …\n$ season_year     &lt;dbl&gt; 132.54, 101.70, 103.87, 105.32, 131.12, 84.99, 20.29, …\n$ remainder       &lt;dbl&gt; -22.2488, -137.3844, -146.1889, -205.6720, -116.7592, …\n$ season_adjust   &lt;dbl&gt; 226.93, 111.73, 102.87, 43.33, 132.18, 172.13, 271.89,…\n\n\nThis code pivots the data long and plots the true number of rides per day and the estimate of the underlying trend per model. The “season_adjust” panel shows the number of rides adjusted for seasonal effects, the “trend” panel shows the underlying trend, and the “remainder” panel shows how much the seasonal adjustment missed by.\n\ncomponents(dcmp) %&gt;% \n  pivot_longer(cols = number_of_rides:season_adjust) %&gt;% \n  mutate(name = factor(name, levels = c(\"number_of_rides\", \"season_adjust\",\n                                        \"trend\", \"seasonal\",\n                                        \"season_year\", \"season_week\",\n                                        \"random\", \"remainder\"))) %&gt;% \n  filter(!is.na(value)) %&gt;% \n  filter(name == \"trend\" | name == \"season_adjust\" | name == \"number_of_rides\" | name == \"remainder\") %&gt;% \n  ggplot(aes(time, value, color = .model)) +\n  geom_point(alpha = .6, size = .6) +\n  annotate(geom = \"rect\", \n           xmin = ymd(\"2020-03-06\"), xmax = ymd(\"2020-12-31\"),\n           ymin = -Inf, ymax = Inf, \n           fill = \"red\", alpha = .1) +\n  facet_grid(name ~ .model, scales = \"free_y\", labeller = label_wrap_gen()) +\n  guides(color = FALSE)\n\n\n\n\n\n\n\n\nI am not a time series expert, but it appears that the most basic STL model STL(number_of_rides) does the best job because that model’s “trend” panel shows the least seasonality.\n\ncomponents(dcmp) %&gt;% \n  pivot_longer(cols = number_of_rides:season_adjust) %&gt;% \n  mutate(name = factor(name, levels = c(\"number_of_rides\", \"season_adjust\",\n                                        \"trend\", \"seasonal\",\n                                        \"season_year\", \"season_week\",\n                                        \"random\", \"remainder\"))) %&gt;% \n  filter(!is.na(value)) %&gt;% \n  filter(name == \"trend\" | name == \"season_adjust\" | name == \"number_of_rides\" | name == \"remainder\") %&gt;% \n  filter(.model == \"STL(number_of_rides)\") %&gt;% \n  ggplot(aes(time, value, color = .model)) +\n  geom_point(alpha = .6, size = .6, color = \"#619CFF\") +\n  annotate(geom = \"rect\", \n           xmin = ymd(\"2016-03-06\"), xmax = ymd(\"2016-03-30\"),\n           ymin = -Inf, ymax = Inf, \n           fill = \"black\", alpha = .3) +\n  annotate(geom = \"rect\", \n           xmin = ymd(\"2017-03-06\"), xmax = ymd(\"2017-03-30\"),\n           ymin = -Inf, ymax = Inf, \n           fill = \"black\", alpha = .3) +\n  annotate(geom = \"rect\", \n           xmin = ymd(\"2018-03-06\"), xmax = ymd(\"2018-03-30\"),\n           ymin = -Inf, ymax = Inf, \n           fill = \"black\", alpha = .3) +\n  annotate(geom = \"rect\", \n           xmin = ymd(\"2019-03-06\"), xmax = ymd(\"2019-03-30\"),\n           ymin = -Inf, ymax = Inf, \n           fill = \"black\", alpha = .3) +\n  annotate(geom = \"rect\", \n           xmin = ymd(\"2020-03-06\"), xmax = ymd(\"2020-03-06\") + 60,\n           ymin = -Inf, ymax = Inf, \n           fill = \"red\", alpha = .3) +\n  facet_grid(name ~ .model, scales = \"free_y\", labeller = label_wrap_gen()) +\n  guides(color = FALSE)\n\n\n\n\n\n\n\n\nFocusing on that model, it appears that the trend dropped in mid-March, but rebounded to normal levels quickly. I highlighted the data from previous Marches to see if there was a recurring dip in March."
  },
  {
    "objectID": "posts/volatile-zillow-house-value/index.html",
    "href": "posts/volatile-zillow-house-value/index.html",
    "title": "Graphing volatile home values in U.S. metro areas",
    "section": "",
    "text": "Zillow publishes a variety of cool data that I haven’t explored much yet. The first dataset that caught my eye was the Zillow Home Value Index (ZHVI). Zillow describes it as the “smoothed, seasonally adjusted measure of the typical home value and market changes across a given region and housing type”. In this post I will make a quick gganimate plot of the ZHVI of various metro areas in the U.S.\nThe code for this post was re-ran in 2024. The data was filtered to match the date of the original post.\nThe first thing I noticed about the data is that it is aggressively wide. There is a column for each year-month in the dataset. 293 columns is a lot to work with.\n\n#download housing data from https://www.zillow.com/research/data/\nzillow_house_value &lt;- read_csv(\"post_data/Metro_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv\")\n\ndim(zillow_house_value)\n\n[1] 895 293\n\n\nTo make the data more tidy, I use a regex to identify the columns that have a date in the name and pivot those longer. Now each row represents the ZHVI for a given region area on a given year-month.\n\nzillow_house_value &lt;- zillow_house_value %&gt;% \n  pivot_longer(cols = matches(\"\\\\d{4}-\\\\d{2}-\\\\d{2}\"),\n               names_to = \"date\", values_to = \"zhvi\") %&gt;% \n  clean_names() %&gt;% \n  mutate(date = ymd(date),\n         region_name = str_squish(region_name))\n\nglimpse(zillow_house_value)\n\nRows: 257,760\nColumns: 7\n$ region_id   &lt;dbl&gt; 102001, 102001, 102001, 102001, 102001, 102001, 102001, 10…\n$ size_rank   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ region_name &lt;chr&gt; \"United States\", \"United States\", \"United States\", \"United…\n$ region_type &lt;chr&gt; \"country\", \"country\", \"country\", \"country\", \"country\", \"co…\n$ state_name  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ date        &lt;date&gt; 2000-01-31, 2000-02-29, 2000-03-31, 2000-04-30, 2000-05-3…\n$ zhvi        &lt;dbl&gt; 118707, 118916, 119175, 119730, 120370, 121055, 121781, 12…\n\n\nOnce the data is tidy, it is easy to plot with ggplot2. In this graph, each line represents one metro area.\n\nzillow_house_value %&gt;% \n  ggplot(aes(date, zhvi, group = region_name)) +\n  geom_line(alpha = .1, size = .5)\n\n\n\n\n\n\n\n\nWhat struck me is that while most metro areas in the dataset start with ZHVI &lt; $300,000, many increase to 3x that, with many wild swings along the way due to housing bubbles, economic crashes, and housing scarcity. I will rank the metro areas by volatility (standard deviation of ZHVI) and use ggplot2 and gganimate to highlight the most volatile metro areas.\n\n#find most volatile regions\ndf_top_regions &lt;- zillow_house_value %&gt;% \n  group_by(region_name) %&gt;% \n  summarize(sd = sd(zhvi)) %&gt;% \n  ungroup() %&gt;% \n  arrange(desc(sd)) %&gt;% \n  slice(1:25) %&gt;% \n  mutate(region_name_rank = str_c(\"#\", row_number(), \" \", region_name, sep = \"\"))\n  \nregion_name_highlight_fct &lt;- df_top_regions %&gt;% \n  pull(region_name)\n\nregion_name_rank_fct &lt;- df_top_regions %&gt;% \n  pull(region_name_rank)\n\n\n#create highlight df\ndf_highlights &lt;- zillow_house_value %&gt;% \n  inner_join(df_top_regions) %&gt;% \n  mutate(region_name_highlight = region_name,\n         region_name_highlight = factor(region_name_highlight, levels = region_name_highlight_fct),\n         region_name_rank = factor(region_name_rank, levels = region_name_rank_fct))\n\nhousing_animation &lt;- zillow_house_value %&gt;% \n  ggplot() +\n  geom_line(aes(date, zhvi, group = region_name), alpha = .1, size = .5) +\n  geom_line(data = df_highlights,\n            aes(date, zhvi),\n            color = \"red\", size = 1.5) +\n  scale_y_continuous(labels = scales::dollar_format()) +\n  transition_manual(region_name_rank) +\n  labs(title = \"Top 25 most volatile housing markets 1996-2020\",\n       subtitle = \"Region: {current_frame}\",\n       x = NULL,\n       y = \"Zillow Housing Value Index\") +\n  theme(plot.subtitle = element_text(size = 15),\n        axis.title.y = element_text(size = 15))\n\nhousing_animation &lt;- animate(housing_animation, duration = 10, fps = 40)\n\nhousing_animation"
  },
  {
    "objectID": "posts/classifying-pittsburgh-city-boundary/index.html",
    "href": "posts/classifying-pittsburgh-city-boundary/index.html",
    "title": "Modeling the Pittsburgh City Boundary",
    "section": "",
    "text": "My goal is to create a classification model that can distinguish between census tracts that are inside the city or outside the City of Pittsburgh. The border is interrupted by rivers, has an enclave, and is very irregular in general, which made this an interesting intellectual exercise.\n\n\nWhile Pittsburgh was founded in 1758, the city’s borders have changed many times due to annexation of surrounding municipalities. This map shows that what we call the North Side was previously Allegheny City, and was annexed into the city in 1907.\n\nMt. Oliver is a geographic enclave that is completely surrounded by the City of Pittsburgh, but is a separate municipality. The borough has resisted multiple annexation attempts.\n\n\n\n\nThis code loads the packages I need, configures some options, and sets the seed.\n\n#set up environment\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(janitor)\nlibrary(tidycensus)\nlibrary(sf)\nlibrary(hrbrthemes)\nlibrary(GGally)\n\ntheme_set(theme_ipsum(base_size = 15))\n\noptions(scipen = 999, digits = 4, tigris_use_cache = TRUE)\n\nset.seed(1234)\n\nI created a small Shiny app that let me select which tracts are inside the city borders. I will go over that in a future post. This loads the tracts from the Census API and pulls the results from the Shiny app.\n\n#load data about census tracts\ntracts &lt;- get_decennial(year = 2010, state = \"PA\", county = \"Allegheny County\", \n                        variables = \"P001001\",\n                        geography = \"tract\", geometry = TRUE)\n\ncity_tracts &lt;- read_csv(\"post_data/selected_tracts.csv\", col_types = cols(\"c\", \"l\")) %&gt;% \n  filter(selected == TRUE)\n\nglimpse(city_tracts)\n\nRows: 137\nColumns: 2\n$ GEOID    &lt;chr&gt; \"42003563000\", \"42003562800\", \"42003563100\", \"42003281500\", \"…\n$ selected &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, T…\n\n\nThis reads in the boundary shapefile and graphs it to show which tracts are in the city.\n\npgh_official_boundary &lt;- st_read(\"post_data/Pittsburgh_City_Boundary-shp\") %&gt;% \n  mutate(geography = \"City boundary\") %&gt;% \n  st_transform(crs = \"NAD83\") %&gt;% \n  st_cast(\"POLYGON\") %&gt;% \n  filter(FID != 7)\n\nReading layer `City_Boundary' from data source \n  `/Users/conorotompkins/Documents/github_repos/ctompkins_quarto_blog/posts/classifying-pittsburgh-city-boundary/post_data/Pittsburgh_City_Boundary-shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 8 features and 8 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 1316000 ymin: 381900 xmax: 1380000 ymax: 433400\nProjected CRS: NAD83 / Pennsylvania South (ftUS)\n\ntracts %&gt;% \n  left_join(city_tracts) %&gt;% \n  mutate(type = case_when(selected == TRUE ~ \"city\",\n                          is.na(selected) ~ \"non_city\")) %&gt;% \n  ggplot() +\n  geom_sf(aes(fill = type), size = .1) +\n  geom_sf(data = pgh_official_boundary, color = \"white\", linetype = 2, alpha = 0) +\n  theme_void()\n\n\n\n\n\n\n\n\nThis reads in the data I will consider for the model. All the data is from the 2010 Census.\n\nall_data &lt;- read_csv(\"post_data/combined_census_data_tract.csv\", col_types = cols(.default = \"c\")) %&gt;%\n  left_join(city_tracts) %&gt;% \n  mutate(type = case_when(selected == TRUE ~ \"city\",\n                          is.na(selected) ~ \"non_city\")) %&gt;% \n  mutate(across(pct_units_owned_loan:housed_population_density_pop_per_square_km, as.numeric)) %&gt;% \n  select(GEOID, type, everything()) %&gt;%\n  select(-c(selected, total_population_housed))\n\nglimpse(all_data)\n\nRows: 402\nColumns: 13\n$ GEOID                                       &lt;chr&gt; \"42003010300\", \"4200302010…\n$ type                                        &lt;chr&gt; \"city\", \"city\", \"city\", \"c…\n$ pct_units_owned_loan                        &lt;dbl&gt; 0.137755, 0.119779, 0.0762…\n$ pct_units_owned_entire                      &lt;dbl&gt; 0.18367, 0.06619, 0.02110,…\n$ pct_units_rented                            &lt;dbl&gt; 0.6786, 0.8140, 0.9026, 0.…\n$ total_population                            &lt;dbl&gt; 6600, 3629, 616, 2256, 260…\n$ pct_white                                   &lt;dbl&gt; 0.658333, 0.745660, 0.7564…\n$ pct_black                                   &lt;dbl&gt; 0.31167, 0.15982, 0.15584,…\n$ pct_asian                                   &lt;dbl&gt; 0.0110606, 0.0471204, 0.06…\n$ pct_hispanic                                &lt;dbl&gt; 0.024394, 0.032791, 0.0162…\n$ workers                                     &lt;dbl&gt; 0, 963, 408, 1056, 537, 47…\n$ jobs                                        &lt;dbl&gt; 0, 79639, 8301, 4633, 1962…\n$ housed_population_density_pop_per_square_km &lt;dbl&gt; 682.7, 1511.3, 386.7, 3205…\n\n\nThis plot compares all of the variables against each other. I used this to identify covariance and determine which variables should be excluded.\n\n#eda\npairwise_plot &lt;- all_data %&gt;% \n  select(-c(GEOID)) %&gt;% \n  ggpairs(aes(color = type)) +\n  theme(axis.text = element_text(size = 8))\n\n After reviewing this graph and considering things like covariance and zero-variance, I will use these variables in the model:\n\nHousing\n\nPercent of housing units owned outright\nPercent of housing units owned with a mortgage\nPercent of housing units rented\n\nDemographics\n\nPercent of people in the tract that are\n\nWhite\nBlack\n\n\nPopulation density\nEconomic data\n\nNumber of workers that live in the tract\nNumber of jobs in the tract\n\n\nNote that I am intentionally excluding any geographic data about the tracts. I am more interested in how “city-like” a given tract is than how close it is to the geographic center of the city.\nThis finalizes the data I will use to build the model.\n\ncensus_combined &lt;- all_data %&gt;% \n  select(GEOID, type, \n         pct_units_owned_loan, pct_units_owned_entire, pct_units_rented,\n         housed_population_density_pop_per_square_km,\n         pct_white, pct_black,\n         workers, jobs)\n\nglimpse(census_combined)\n\nRows: 402\nColumns: 10\n$ GEOID                                       &lt;chr&gt; \"42003010300\", \"4200302010…\n$ type                                        &lt;chr&gt; \"city\", \"city\", \"city\", \"c…\n$ pct_units_owned_loan                        &lt;dbl&gt; 0.137755, 0.119779, 0.0762…\n$ pct_units_owned_entire                      &lt;dbl&gt; 0.18367, 0.06619, 0.02110,…\n$ pct_units_rented                            &lt;dbl&gt; 0.6786, 0.8140, 0.9026, 0.…\n$ housed_population_density_pop_per_square_km &lt;dbl&gt; 682.7, 1511.3, 386.7, 3205…\n$ pct_white                                   &lt;dbl&gt; 0.658333, 0.745660, 0.7564…\n$ pct_black                                   &lt;dbl&gt; 0.31167, 0.15982, 0.15584,…\n$ workers                                     &lt;dbl&gt; 0, 963, 408, 1056, 537, 47…\n$ jobs                                        &lt;dbl&gt; 0, 79639, 8301, 4633, 1962…\n\n\n34% of the tracts are in the city, which is a slightly unbalanced dataset.\n\ncensus_combined %&gt;% \n  tabyl(type)\n\n     type   n percent\n     city 137  0.3408\n non_city 265  0.6592\n\n\nSince the total amount of data available is small, I will bootstrap the data to try to achieve more stable results from the model. Bootstrapping resamples the data with replacement, which creates multiple replicates of the original dataset with some variation due to sampling. I created a meme of my dog Quincy to illustrate the effect: \nI stratify by type so that each bootstrap has ~34% city tracts. This generates 50 sets of data for the model to work with.\n\ntract_boot &lt;- bootstraps(census_combined, strata = type, times = 50)\n\ntract_boot\n\n# Bootstrap sampling using stratification \n# A tibble: 50 × 2\n   splits            id         \n   &lt;list&gt;            &lt;chr&gt;      \n 1 &lt;split [402/152]&gt; Bootstrap01\n 2 &lt;split [402/145]&gt; Bootstrap02\n 3 &lt;split [402/147]&gt; Bootstrap03\n 4 &lt;split [402/150]&gt; Bootstrap04\n 5 &lt;split [402/144]&gt; Bootstrap05\n 6 &lt;split [402/148]&gt; Bootstrap06\n 7 &lt;split [402/152]&gt; Bootstrap07\n 8 &lt;split [402/149]&gt; Bootstrap08\n 9 &lt;split [402/143]&gt; Bootstrap09\n10 &lt;split [402/156]&gt; Bootstrap10\n# ℹ 40 more rows\n\n\nThis code chunk prepares the data to be modeled. I define the formula and scale all the numeric variables to have a mean of 0 and standard deviation of 1.\n\n#recipe\nmodel_recipe &lt;- recipe(type ~ ., data = census_combined) %&gt;% \n  update_role(GEOID, new_role = \"id\") %&gt;% \n  step_normalize(all_predictors())\n\nmodel_recipe_prep &lt;- model_recipe %&gt;%\n  prep(strings_as_factors = FALSE)\n\nmodel_recipe %&gt;% \n  summary()\n\n# A tibble: 10 × 4\n   variable                                    type      role      source  \n   &lt;chr&gt;                                       &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n 1 GEOID                                       &lt;chr [3]&gt; id        original\n 2 pct_units_owned_loan                        &lt;chr [2]&gt; predictor original\n 3 pct_units_owned_entire                      &lt;chr [2]&gt; predictor original\n 4 pct_units_rented                            &lt;chr [2]&gt; predictor original\n 5 housed_population_density_pop_per_square_km &lt;chr [2]&gt; predictor original\n 6 pct_white                                   &lt;chr [2]&gt; predictor original\n 7 pct_black                                   &lt;chr [2]&gt; predictor original\n 8 workers                                     &lt;chr [2]&gt; predictor original\n 9 jobs                                        &lt;chr [2]&gt; predictor original\n10 type                                        &lt;chr [3]&gt; outcome   original\n\n\nThis creates the model specifications for the two types of models I will use.\n\n#logistic regression\nlm_model &lt;- logistic_reg(mode = \"classification\") %&gt;% \n  set_engine(\"glm\")\n\n#random forest\nranger_model &lt;- rand_forest(trees = 1000, mode = \"classification\") %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\")\n\nThis sets up a workflow object to fit a logistic regression model against the bootstrap resamples I created earlier.\n\n#logistic regression\nlm_workflow &lt;- workflow() %&gt;% \n  add_recipe(model_recipe) %&gt;% \n  add_model(lm_model)\n\nlm_res &lt;- lm_workflow %&gt;% \n  fit_resamples(resamples = tract_boot) %&gt;% \n  mutate(model = \"lm\")\n\nlm_res %&gt;% \n  collect_metrics()\n\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary     0.783    49 0.00383 Preprocessor1_Model1\n2 roc_auc  binary     0.856    49 0.00394 Preprocessor1_Model1\n\n\nThe logistic regression gets ~76% accuracy, which is pretty good, but I want to know if a random forest could do better. This creates a workflow to fit a random forest model, and saves the predictions so I can use them later.\n\n#rf\nrf_workflow &lt;- workflow() %&gt;% \n  add_recipe(model_recipe_prep) %&gt;% \n  add_model(ranger_model)\n\nrf_res &lt;- rf_workflow %&gt;% \n  fit_resamples(resamples = tract_boot,\n                control = control_resamples(save_pred = TRUE)) %&gt;% \n  mutate(model = \"rf\")\n\nrf_res %&gt;% \n  collect_metrics()\n\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary     0.816    50 0.00360 Preprocessor1_Model1\n2 roc_auc  binary     0.894    50 0.00289 Preprocessor1_Model1\n\n\nIf you compare the results of the two models, the random forest model does much better than the logistic regression model.\n\ncombined_res &lt;- bind_rows(rf_res, lm_res)\n\ncombined_res %&gt;% \n  unnest(.metrics) %&gt;% \n  ggplot(aes(.estimate, color = model, fill = model)) +\n  geom_density(alpha = .5) +\n  facet_wrap(~.metric)\n\n\n\n\n\n\n\n\nThis graph shows that the random forest model’s false negative and false positive rates are about the same.\n\nrf_res %&gt;% \n  collect_predictions() %&gt;% \n  count(type, .pred_class) %&gt;% \n  ggplot(aes(type, .pred_class, fill = n)) +\n  geom_tile() +\n  labs(x = \"Truth\",\n       y = \"Prediction\",\n       fill = \"Number of observations\") +\n  scale_fill_continuous(label = scales::comma) +\n  coord_equal() +\n  theme(panel.grid.major = element_blank())\n\n\n\n\n\n\n\n\nThis fits the random forest model against the entire dataset to extract the variable importance metrics.\n\n#variable importance\nvar_imp &lt;- rf_workflow %&gt;% \n  fit(juice(model_recipe_prep)) %&gt;% \n  pull_workflow_fit() %&gt;% \n  vip::vi()\n\nvar_imp %&gt;%\n  mutate(Variable = fct_reorder(Variable, Importance)) %&gt;% \n  ggplot(aes(Importance, Variable)) +\n  geom_point()\n\n\n\n\n\n\n\n\nThe top 3 variables are the percent of a tract’s population that is White, the percent of housing units that are owned with a loan, and the population density. This matches my subjective model of city vs. non-city characteristics. Areas that are low density with majority White demographics where people own their homes are typically outside of the city. This dynamic is probably connected to the history of segregation and redlining in majority African American communities in Pittsburgh.\nSince the random forest model was fit against multiple bootstraps, I have multiple predictions per tract. I stratified the bootstraps by type, so the city and non_city tracts were sampled about the same number of times.\n\n#extract probabilities from bootstrap resamples\nfull_predictions &lt;- rf_res %&gt;% \n  collect_predictions() %&gt;% \n  mutate(correct = type == .pred_class) %&gt;%\n  left_join(census_combined %&gt;%\n              mutate(.row = row_number()))\n\nfull_predictions %&gt;% \n  count(type, GEOID) %&gt;% \n  ggplot(aes(n, fill = type, color = type)) +\n  geom_density(alpha = .3) +\n  labs(x = \"Number of observations of a tract\")\n\n\n\n\n\n\n\n\nI am not solely interested in the top-line accuracy of the model. Since the city border is a geographic phenomenon, there may be interesting patterns in the geographic distribution of the model’s predictions that can be shown on a map.\nTo map the data, I calculate the following metrics per tract:\n\nthe percent of predictions that were correct\naverage city classification %\naverage non-city classification %\nthe number of times the tract was sampled\n\n\nfull_predictions_pct &lt;- full_predictions %&gt;% \n  group_by(GEOID) %&gt;% \n  summarize(pct_correct = mean(correct),\n            mean_city = mean(.pred_city),\n            mean_non_city = mean(.pred_non_city),\n            n = n())\n\nglimpse(full_predictions_pct)\n\nRows: 402\nColumns: 5\n$ GEOID         &lt;chr&gt; \"42003010300\", \"42003020100\", \"42003020300\", \"4200303050…\n$ pct_correct   &lt;dbl&gt; 1.00000, 1.00000, 1.00000, 1.00000, 1.00000, 1.00000, 1.…\n$ mean_city     &lt;dbl&gt; 0.7656, 0.7912, 0.8174, 0.7668, 0.7078, 0.8055, 0.9276, …\n$ mean_non_city &lt;dbl&gt; 0.23436, 0.20878, 0.18263, 0.23318, 0.29219, 0.19454, 0.…\n$ n             &lt;int&gt; 23, 13, 15, 21, 19, 17, 16, 19, 11, 20, 22, 21, 18, 22, …\n\n\nThis shows the % of correct predictions per tract. The model was very successful with the outlying tracts in the county, but struggled in Mt. Washington/Beechview/Brookline, the Hazelwood/Greenfield area, and Forest Hills towards Monroeville.\n\ntracts %&gt;% \n  left_join(full_predictions_pct) %&gt;% \n  ggplot() +\n  geom_sf(aes(fill = pct_correct), lwd = 0) +\n  geom_sf(data = pgh_official_boundary, alpha = 0, color = \"black\", lwd = 2) +\n  geom_sf(data = pgh_official_boundary, alpha = 0, color = \"yellow\", lwd = .3) +\n  scale_fill_viridis_c(labels = scales::percent) +\n  labs(fill = \"% predictions correct\") +\n  theme_void()\n\n\n\n\n\n\n\n\nThis shows the average % that the model classified a tract as being in the city. The model was very confident that Oakland, Shadyside, and Squirrel Hill are in the city. The model also thought that many communities to the east and communities along the Monongahela River are in the city, specifically McKeesport and West Mifflin.\n\ntracts %&gt;% \n  left_join(full_predictions_pct) %&gt;% \n  ggplot() +\n  geom_sf(aes(fill = mean_city), lwd = 0) +\n  geom_sf(data = pgh_official_boundary, alpha = 0, color = \"black\", lwd = 2) +\n  geom_sf(data = pgh_official_boundary, alpha = 0, color = \"yellow\", lwd = .3) +\n  scale_fill_viridis_c(labels = scales::percent) +\n  labs(fill = \"Average city classification %\") +\n  theme_void()\n\n\n\n\n\n\n\n\nTo review, I think it would be difficult to create a model that almost perfectly captures the current city border, which is a result of political decisions, court cases, and other non-deterministic phenomena. In addition, white flight and the relative expansion of the suburbs during the collapse of the steel industry reshaped the Pittsburgh metro area. City borders are defined by people and politicians, not a clustering algorithm based on Census data (although that would be interesting). My experience is that many people that don’t technically live within the border consider themselves to be Pittsburghers. So what is a border, anyways?"
  },
  {
    "objectID": "posts/classifying-pittsburgh-city-boundary/index.html#introduction",
    "href": "posts/classifying-pittsburgh-city-boundary/index.html#introduction",
    "title": "Modeling the Pittsburgh City Boundary",
    "section": "",
    "text": "My goal is to create a classification model that can distinguish between census tracts that are inside the city or outside the City of Pittsburgh. The border is interrupted by rivers, has an enclave, and is very irregular in general, which made this an interesting intellectual exercise.\n\n\nWhile Pittsburgh was founded in 1758, the city’s borders have changed many times due to annexation of surrounding municipalities. This map shows that what we call the North Side was previously Allegheny City, and was annexed into the city in 1907.\n\nMt. Oliver is a geographic enclave that is completely surrounded by the City of Pittsburgh, but is a separate municipality. The borough has resisted multiple annexation attempts.\n\n\n\n\nThis code loads the packages I need, configures some options, and sets the seed.\n\n#set up environment\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(janitor)\nlibrary(tidycensus)\nlibrary(sf)\nlibrary(hrbrthemes)\nlibrary(GGally)\n\ntheme_set(theme_ipsum(base_size = 15))\n\noptions(scipen = 999, digits = 4, tigris_use_cache = TRUE)\n\nset.seed(1234)\n\nI created a small Shiny app that let me select which tracts are inside the city borders. I will go over that in a future post. This loads the tracts from the Census API and pulls the results from the Shiny app.\n\n#load data about census tracts\ntracts &lt;- get_decennial(year = 2010, state = \"PA\", county = \"Allegheny County\", \n                        variables = \"P001001\",\n                        geography = \"tract\", geometry = TRUE)\n\ncity_tracts &lt;- read_csv(\"post_data/selected_tracts.csv\", col_types = cols(\"c\", \"l\")) %&gt;% \n  filter(selected == TRUE)\n\nglimpse(city_tracts)\n\nRows: 137\nColumns: 2\n$ GEOID    &lt;chr&gt; \"42003563000\", \"42003562800\", \"42003563100\", \"42003281500\", \"…\n$ selected &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, T…\n\n\nThis reads in the boundary shapefile and graphs it to show which tracts are in the city.\n\npgh_official_boundary &lt;- st_read(\"post_data/Pittsburgh_City_Boundary-shp\") %&gt;% \n  mutate(geography = \"City boundary\") %&gt;% \n  st_transform(crs = \"NAD83\") %&gt;% \n  st_cast(\"POLYGON\") %&gt;% \n  filter(FID != 7)\n\nReading layer `City_Boundary' from data source \n  `/Users/conorotompkins/Documents/github_repos/ctompkins_quarto_blog/posts/classifying-pittsburgh-city-boundary/post_data/Pittsburgh_City_Boundary-shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 8 features and 8 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 1316000 ymin: 381900 xmax: 1380000 ymax: 433400\nProjected CRS: NAD83 / Pennsylvania South (ftUS)\n\ntracts %&gt;% \n  left_join(city_tracts) %&gt;% \n  mutate(type = case_when(selected == TRUE ~ \"city\",\n                          is.na(selected) ~ \"non_city\")) %&gt;% \n  ggplot() +\n  geom_sf(aes(fill = type), size = .1) +\n  geom_sf(data = pgh_official_boundary, color = \"white\", linetype = 2, alpha = 0) +\n  theme_void()\n\n\n\n\n\n\n\n\nThis reads in the data I will consider for the model. All the data is from the 2010 Census.\n\nall_data &lt;- read_csv(\"post_data/combined_census_data_tract.csv\", col_types = cols(.default = \"c\")) %&gt;%\n  left_join(city_tracts) %&gt;% \n  mutate(type = case_when(selected == TRUE ~ \"city\",\n                          is.na(selected) ~ \"non_city\")) %&gt;% \n  mutate(across(pct_units_owned_loan:housed_population_density_pop_per_square_km, as.numeric)) %&gt;% \n  select(GEOID, type, everything()) %&gt;%\n  select(-c(selected, total_population_housed))\n\nglimpse(all_data)\n\nRows: 402\nColumns: 13\n$ GEOID                                       &lt;chr&gt; \"42003010300\", \"4200302010…\n$ type                                        &lt;chr&gt; \"city\", \"city\", \"city\", \"c…\n$ pct_units_owned_loan                        &lt;dbl&gt; 0.137755, 0.119779, 0.0762…\n$ pct_units_owned_entire                      &lt;dbl&gt; 0.18367, 0.06619, 0.02110,…\n$ pct_units_rented                            &lt;dbl&gt; 0.6786, 0.8140, 0.9026, 0.…\n$ total_population                            &lt;dbl&gt; 6600, 3629, 616, 2256, 260…\n$ pct_white                                   &lt;dbl&gt; 0.658333, 0.745660, 0.7564…\n$ pct_black                                   &lt;dbl&gt; 0.31167, 0.15982, 0.15584,…\n$ pct_asian                                   &lt;dbl&gt; 0.0110606, 0.0471204, 0.06…\n$ pct_hispanic                                &lt;dbl&gt; 0.024394, 0.032791, 0.0162…\n$ workers                                     &lt;dbl&gt; 0, 963, 408, 1056, 537, 47…\n$ jobs                                        &lt;dbl&gt; 0, 79639, 8301, 4633, 1962…\n$ housed_population_density_pop_per_square_km &lt;dbl&gt; 682.7, 1511.3, 386.7, 3205…\n\n\nThis plot compares all of the variables against each other. I used this to identify covariance and determine which variables should be excluded.\n\n#eda\npairwise_plot &lt;- all_data %&gt;% \n  select(-c(GEOID)) %&gt;% \n  ggpairs(aes(color = type)) +\n  theme(axis.text = element_text(size = 8))\n\n After reviewing this graph and considering things like covariance and zero-variance, I will use these variables in the model:\n\nHousing\n\nPercent of housing units owned outright\nPercent of housing units owned with a mortgage\nPercent of housing units rented\n\nDemographics\n\nPercent of people in the tract that are\n\nWhite\nBlack\n\n\nPopulation density\nEconomic data\n\nNumber of workers that live in the tract\nNumber of jobs in the tract\n\n\nNote that I am intentionally excluding any geographic data about the tracts. I am more interested in how “city-like” a given tract is than how close it is to the geographic center of the city.\nThis finalizes the data I will use to build the model.\n\ncensus_combined &lt;- all_data %&gt;% \n  select(GEOID, type, \n         pct_units_owned_loan, pct_units_owned_entire, pct_units_rented,\n         housed_population_density_pop_per_square_km,\n         pct_white, pct_black,\n         workers, jobs)\n\nglimpse(census_combined)\n\nRows: 402\nColumns: 10\n$ GEOID                                       &lt;chr&gt; \"42003010300\", \"4200302010…\n$ type                                        &lt;chr&gt; \"city\", \"city\", \"city\", \"c…\n$ pct_units_owned_loan                        &lt;dbl&gt; 0.137755, 0.119779, 0.0762…\n$ pct_units_owned_entire                      &lt;dbl&gt; 0.18367, 0.06619, 0.02110,…\n$ pct_units_rented                            &lt;dbl&gt; 0.6786, 0.8140, 0.9026, 0.…\n$ housed_population_density_pop_per_square_km &lt;dbl&gt; 682.7, 1511.3, 386.7, 3205…\n$ pct_white                                   &lt;dbl&gt; 0.658333, 0.745660, 0.7564…\n$ pct_black                                   &lt;dbl&gt; 0.31167, 0.15982, 0.15584,…\n$ workers                                     &lt;dbl&gt; 0, 963, 408, 1056, 537, 47…\n$ jobs                                        &lt;dbl&gt; 0, 79639, 8301, 4633, 1962…\n\n\n34% of the tracts are in the city, which is a slightly unbalanced dataset.\n\ncensus_combined %&gt;% \n  tabyl(type)\n\n     type   n percent\n     city 137  0.3408\n non_city 265  0.6592\n\n\nSince the total amount of data available is small, I will bootstrap the data to try to achieve more stable results from the model. Bootstrapping resamples the data with replacement, which creates multiple replicates of the original dataset with some variation due to sampling. I created a meme of my dog Quincy to illustrate the effect: \nI stratify by type so that each bootstrap has ~34% city tracts. This generates 50 sets of data for the model to work with.\n\ntract_boot &lt;- bootstraps(census_combined, strata = type, times = 50)\n\ntract_boot\n\n# Bootstrap sampling using stratification \n# A tibble: 50 × 2\n   splits            id         \n   &lt;list&gt;            &lt;chr&gt;      \n 1 &lt;split [402/152]&gt; Bootstrap01\n 2 &lt;split [402/145]&gt; Bootstrap02\n 3 &lt;split [402/147]&gt; Bootstrap03\n 4 &lt;split [402/150]&gt; Bootstrap04\n 5 &lt;split [402/144]&gt; Bootstrap05\n 6 &lt;split [402/148]&gt; Bootstrap06\n 7 &lt;split [402/152]&gt; Bootstrap07\n 8 &lt;split [402/149]&gt; Bootstrap08\n 9 &lt;split [402/143]&gt; Bootstrap09\n10 &lt;split [402/156]&gt; Bootstrap10\n# ℹ 40 more rows\n\n\nThis code chunk prepares the data to be modeled. I define the formula and scale all the numeric variables to have a mean of 0 and standard deviation of 1.\n\n#recipe\nmodel_recipe &lt;- recipe(type ~ ., data = census_combined) %&gt;% \n  update_role(GEOID, new_role = \"id\") %&gt;% \n  step_normalize(all_predictors())\n\nmodel_recipe_prep &lt;- model_recipe %&gt;%\n  prep(strings_as_factors = FALSE)\n\nmodel_recipe %&gt;% \n  summary()\n\n# A tibble: 10 × 4\n   variable                                    type      role      source  \n   &lt;chr&gt;                                       &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n 1 GEOID                                       &lt;chr [3]&gt; id        original\n 2 pct_units_owned_loan                        &lt;chr [2]&gt; predictor original\n 3 pct_units_owned_entire                      &lt;chr [2]&gt; predictor original\n 4 pct_units_rented                            &lt;chr [2]&gt; predictor original\n 5 housed_population_density_pop_per_square_km &lt;chr [2]&gt; predictor original\n 6 pct_white                                   &lt;chr [2]&gt; predictor original\n 7 pct_black                                   &lt;chr [2]&gt; predictor original\n 8 workers                                     &lt;chr [2]&gt; predictor original\n 9 jobs                                        &lt;chr [2]&gt; predictor original\n10 type                                        &lt;chr [3]&gt; outcome   original\n\n\nThis creates the model specifications for the two types of models I will use.\n\n#logistic regression\nlm_model &lt;- logistic_reg(mode = \"classification\") %&gt;% \n  set_engine(\"glm\")\n\n#random forest\nranger_model &lt;- rand_forest(trees = 1000, mode = \"classification\") %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\")\n\nThis sets up a workflow object to fit a logistic regression model against the bootstrap resamples I created earlier.\n\n#logistic regression\nlm_workflow &lt;- workflow() %&gt;% \n  add_recipe(model_recipe) %&gt;% \n  add_model(lm_model)\n\nlm_res &lt;- lm_workflow %&gt;% \n  fit_resamples(resamples = tract_boot) %&gt;% \n  mutate(model = \"lm\")\n\nlm_res %&gt;% \n  collect_metrics()\n\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary     0.783    49 0.00383 Preprocessor1_Model1\n2 roc_auc  binary     0.856    49 0.00394 Preprocessor1_Model1\n\n\nThe logistic regression gets ~76% accuracy, which is pretty good, but I want to know if a random forest could do better. This creates a workflow to fit a random forest model, and saves the predictions so I can use them later.\n\n#rf\nrf_workflow &lt;- workflow() %&gt;% \n  add_recipe(model_recipe_prep) %&gt;% \n  add_model(ranger_model)\n\nrf_res &lt;- rf_workflow %&gt;% \n  fit_resamples(resamples = tract_boot,\n                control = control_resamples(save_pred = TRUE)) %&gt;% \n  mutate(model = \"rf\")\n\nrf_res %&gt;% \n  collect_metrics()\n\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary     0.816    50 0.00360 Preprocessor1_Model1\n2 roc_auc  binary     0.894    50 0.00289 Preprocessor1_Model1\n\n\nIf you compare the results of the two models, the random forest model does much better than the logistic regression model.\n\ncombined_res &lt;- bind_rows(rf_res, lm_res)\n\ncombined_res %&gt;% \n  unnest(.metrics) %&gt;% \n  ggplot(aes(.estimate, color = model, fill = model)) +\n  geom_density(alpha = .5) +\n  facet_wrap(~.metric)\n\n\n\n\n\n\n\n\nThis graph shows that the random forest model’s false negative and false positive rates are about the same.\n\nrf_res %&gt;% \n  collect_predictions() %&gt;% \n  count(type, .pred_class) %&gt;% \n  ggplot(aes(type, .pred_class, fill = n)) +\n  geom_tile() +\n  labs(x = \"Truth\",\n       y = \"Prediction\",\n       fill = \"Number of observations\") +\n  scale_fill_continuous(label = scales::comma) +\n  coord_equal() +\n  theme(panel.grid.major = element_blank())\n\n\n\n\n\n\n\n\nThis fits the random forest model against the entire dataset to extract the variable importance metrics.\n\n#variable importance\nvar_imp &lt;- rf_workflow %&gt;% \n  fit(juice(model_recipe_prep)) %&gt;% \n  pull_workflow_fit() %&gt;% \n  vip::vi()\n\nvar_imp %&gt;%\n  mutate(Variable = fct_reorder(Variable, Importance)) %&gt;% \n  ggplot(aes(Importance, Variable)) +\n  geom_point()\n\n\n\n\n\n\n\n\nThe top 3 variables are the percent of a tract’s population that is White, the percent of housing units that are owned with a loan, and the population density. This matches my subjective model of city vs. non-city characteristics. Areas that are low density with majority White demographics where people own their homes are typically outside of the city. This dynamic is probably connected to the history of segregation and redlining in majority African American communities in Pittsburgh.\nSince the random forest model was fit against multiple bootstraps, I have multiple predictions per tract. I stratified the bootstraps by type, so the city and non_city tracts were sampled about the same number of times.\n\n#extract probabilities from bootstrap resamples\nfull_predictions &lt;- rf_res %&gt;% \n  collect_predictions() %&gt;% \n  mutate(correct = type == .pred_class) %&gt;%\n  left_join(census_combined %&gt;%\n              mutate(.row = row_number()))\n\nfull_predictions %&gt;% \n  count(type, GEOID) %&gt;% \n  ggplot(aes(n, fill = type, color = type)) +\n  geom_density(alpha = .3) +\n  labs(x = \"Number of observations of a tract\")\n\n\n\n\n\n\n\n\nI am not solely interested in the top-line accuracy of the model. Since the city border is a geographic phenomenon, there may be interesting patterns in the geographic distribution of the model’s predictions that can be shown on a map.\nTo map the data, I calculate the following metrics per tract:\n\nthe percent of predictions that were correct\naverage city classification %\naverage non-city classification %\nthe number of times the tract was sampled\n\n\nfull_predictions_pct &lt;- full_predictions %&gt;% \n  group_by(GEOID) %&gt;% \n  summarize(pct_correct = mean(correct),\n            mean_city = mean(.pred_city),\n            mean_non_city = mean(.pred_non_city),\n            n = n())\n\nglimpse(full_predictions_pct)\n\nRows: 402\nColumns: 5\n$ GEOID         &lt;chr&gt; \"42003010300\", \"42003020100\", \"42003020300\", \"4200303050…\n$ pct_correct   &lt;dbl&gt; 1.00000, 1.00000, 1.00000, 1.00000, 1.00000, 1.00000, 1.…\n$ mean_city     &lt;dbl&gt; 0.7656, 0.7912, 0.8174, 0.7668, 0.7078, 0.8055, 0.9276, …\n$ mean_non_city &lt;dbl&gt; 0.23436, 0.20878, 0.18263, 0.23318, 0.29219, 0.19454, 0.…\n$ n             &lt;int&gt; 23, 13, 15, 21, 19, 17, 16, 19, 11, 20, 22, 21, 18, 22, …\n\n\nThis shows the % of correct predictions per tract. The model was very successful with the outlying tracts in the county, but struggled in Mt. Washington/Beechview/Brookline, the Hazelwood/Greenfield area, and Forest Hills towards Monroeville.\n\ntracts %&gt;% \n  left_join(full_predictions_pct) %&gt;% \n  ggplot() +\n  geom_sf(aes(fill = pct_correct), lwd = 0) +\n  geom_sf(data = pgh_official_boundary, alpha = 0, color = \"black\", lwd = 2) +\n  geom_sf(data = pgh_official_boundary, alpha = 0, color = \"yellow\", lwd = .3) +\n  scale_fill_viridis_c(labels = scales::percent) +\n  labs(fill = \"% predictions correct\") +\n  theme_void()\n\n\n\n\n\n\n\n\nThis shows the average % that the model classified a tract as being in the city. The model was very confident that Oakland, Shadyside, and Squirrel Hill are in the city. The model also thought that many communities to the east and communities along the Monongahela River are in the city, specifically McKeesport and West Mifflin.\n\ntracts %&gt;% \n  left_join(full_predictions_pct) %&gt;% \n  ggplot() +\n  geom_sf(aes(fill = mean_city), lwd = 0) +\n  geom_sf(data = pgh_official_boundary, alpha = 0, color = \"black\", lwd = 2) +\n  geom_sf(data = pgh_official_boundary, alpha = 0, color = \"yellow\", lwd = .3) +\n  scale_fill_viridis_c(labels = scales::percent) +\n  labs(fill = \"Average city classification %\") +\n  theme_void()\n\n\n\n\n\n\n\n\nTo review, I think it would be difficult to create a model that almost perfectly captures the current city border, which is a result of political decisions, court cases, and other non-deterministic phenomena. In addition, white flight and the relative expansion of the suburbs during the collapse of the steel industry reshaped the Pittsburgh metro area. City borders are defined by people and politicians, not a clustering algorithm based on Census data (although that would be interesting). My experience is that many people that don’t technically live within the border consider themselves to be Pittsburghers. So what is a border, anyways?"
  },
  {
    "objectID": "posts/classifying-pittsburgh-city-boundary/index.html#references",
    "href": "posts/classifying-pittsburgh-city-boundary/index.html#references",
    "title": "Modeling the Pittsburgh City Boundary",
    "section": "References",
    "text": "References\n\nhttps://juliasilge.com/blog/multinomial-volcano-eruptions/\nhttp://www.rebeccabarter.com/blog/2020-03-25_machine_learning/#split-into-traintest\nhttps://www.brodrigues.co/blog/2018-11-25-tidy_cv/\nhttps://agailloty.rbind.io/en/post/tidymodels/\nhttps://alison.rbind.io/post/2020-02-27-better-tidymodels/\nhttps://hansjoerg.me/2020/02/09/tidymodels-for-machine-learning/\nhttps://towardsdatascience.com/modelling-with-tidymodels-and-parsnip-bae2c01c131c\nhttps://www.benjaminsorensen.me/post/modeling-with-parsnip-and-tidymodels/\nhttps://rviews.rstudio.com/2019/06/19/a-gentle-intro-to-tidymodels/\nhttps://en.wikipedia.org/wiki/Allegheny,_Pennsylvania"
  },
  {
    "objectID": "posts/remodeling-the-office/index.html",
    "href": "posts/remodeling-the-office/index.html",
    "title": "(re)Modeling the Office",
    "section": "",
    "text": "The goal for this analysis is to determine which characters, directors, and writers from The Office most influence an episode’s IMDB rating. My hypothesis is that IMDB rating is largely driven by a few show personnel. I also briefly walk through the data cleaning and modeling processes. This analysis is based on code from Julia Silge’s Tidy Tuesdays writeup She does a very good job of explaining the modeling aspects of this. She uses LASSO regression, which is very similar to the ridge regression I use.\nThe steps in the analysis are:\nI use these variables in the model:"
  },
  {
    "objectID": "posts/remodeling-the-office/index.html#basic-eda",
    "href": "posts/remodeling-the-office/index.html#basic-eda",
    "title": "(re)Modeling the Office",
    "section": "Basic EDA",
    "text": "Basic EDA\nThe boxplot shows that season may have an impact on the rating, so I will include that in the model.\n\ndf %&gt;% \n  distinct(air_date, season, imdb_rating) %&gt;% \n  ggplot(aes(air_date, imdb_rating, fill = as.factor(season))) +\n    geom_boxplot() +\n    labs(x = \"Air date\",\n         y = \"IMDB rating\",\n         fill = \"Season\") +\n    theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nThis creates a table with IMDB ratings and season premier/finale flags. This will be the table I join the personnel data to.\n\ndf_imdb &lt;- df %&gt;% \n  distinct(season, episode, imdb_rating) %&gt;% \n  group_by(season) %&gt;% \n  mutate(flag_premier = episode == first(episode),\n         flag_finale = episode == last(episode)) %&gt;% \n  ungroup() %&gt;% \n  mutate(across(contains(\"flag\"), as.numeric))\n\nglimpse(df_imdb)\n\nRows: 186\nColumns: 5\n$ season       &lt;int&gt; 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ episode      &lt;int&gt; 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, …\n$ imdb_rating  &lt;dbl&gt; 7.6, 8.3, 7.9, 8.1, 8.4, 7.8, 8.7, 8.2, 8.4, 8.4, 8.2, 8.…\n$ flag_premier &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ flag_finale  &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n\n\n\nDirectors\nSome episodes have more than one director, so I move them into separate rows.\n\ndf_directors &lt;- df %&gt;% \n  distinct(season, episode, director) %&gt;% \n  separate_rows(director, sep = \";\")\n\nThe original data contains misspellings of director names, which will cause issues when I filter out less common directors. This code fixes those misspellings.\n\ndf_director_fix &lt;- tibble(director_good = c(\"Charles McDougall\",\n                                            \"Claire Scanlon\",\n                                            \"Greg Daniels\",\n                                            \"Ken Whittingham\",\n                                            \"Paul Lieberstein\"),\n                          director_bad = c(\"Charles McDougal\",\n                                           \"Claire Scanlong\",\n                                           \"Greg Daneils\",\n                                           \"Ken Wittingham\",\n                                           \"Paul Lieerstein\"))\n\ndf_directors &lt;- df_directors %&gt;% \n  left_join(df_director_fix, by = c(\"director\" = \"director_bad\")) %&gt;% \n  mutate(director = case_when(!is.na(director_good) ~ director_good,\n                              is.na(director_good) ~ director)) %&gt;% \n  mutate(director = str_c(\"director\", director, sep = \"_\")) %&gt;% \n  select(-director_good)\n\nThis cleans up the director names and selects only directors that were involved in more than 2 episodes.\n\ndf_directors &lt;- df_directors %&gt;%  \n  mutate(director = str_remove_all(director, \"\\\\.\"),\n         director = str_replace_all(director, \"\\\\-\", \"_\"),\n         director = str_replace_all(director, \" \", \"_\")) %&gt;% \n  add_count(director) %&gt;% \n  filter(n &gt; 2) %&gt;% \n  select(-n)\n\nThis pivots the data wide so it can be used with the regression model.\n\ndf_directors &lt;- df_directors %&gt;% \n  mutate(flag = 1) %&gt;% \n  pivot_wider(id_cols = c(season, episode), names_from = director, values_from = flag, values_fill = list(flag = 0))\n\ndf_directors %&gt;% \n  select(1:20) %&gt;% \n  glimpse()\n\nRows: 139\nColumns: 20\n$ season                     &lt;int&gt; 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ episode                    &lt;int&gt; 1, 2, 3, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, …\n$ director_Ken_Kwapis        &lt;dbl&gt; 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1…\n$ director_Ken_Whittingham   &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ director_Greg_Daniels      &lt;dbl&gt; 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0…\n$ director_Paul_Feig         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0…\n$ director_Charles_McDougall &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0…\n$ director_Randall_Einhorn   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ director_Tucker_Gates      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ director_Jeffrey_Blitz     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ director_Harold_Ramis      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ director_Paul_Lieberstein  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ director_Jennifer_Celotta  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ director_David_Rogers      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ director_Steve_Carell      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ director_Brent_Forrester   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ director_BJ_Novak          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ director_John_Krasinski    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ director_Matt_Sohn         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ director_Rainn_Wilson      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n\n\n\n\nWriters\nThis separates out where more than one writer was involved in an episode, filters on writers that were involved in more than 2 episodes, and pivots the data wide.\n\ndf_writers &lt;- df %&gt;% \n  distinct(season, episode, writer) %&gt;% \n  separate_rows(writer, sep = \";\") %&gt;% \n  add_count(writer) %&gt;% \n  filter(n &gt; 2)\n\ndf_writers &lt;- df_writers %&gt;% \n  mutate(writer = str_remove_all(writer, \"\\\\.\"),\n         writer = str_replace_all(writer, \"\\\\-\", \"_\"),\n         writer = str_replace_all(writer, \" \", \"_\")) %&gt;% \n  mutate(writer = str_c(\"writer\", writer, sep = \"_\"))\n\ndf_writers &lt;- df_writers %&gt;% \n  mutate(flag = 1) %&gt;% \n  pivot_wider(id_cols = c(season, episode), names_from = writer, values_from = flag, values_fill = list(flag = 0))\n\ndf_writers %&gt;% \n  select(1:20) %&gt;% \n  glimpse()\n\nRows: 157\nColumns: 20\n$ season                    &lt;int&gt; 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2,…\n$ episode                   &lt;int&gt; 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 9, 10…\n$ writer_Greg_Daniels       &lt;dbl&gt; 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,…\n$ writer_BJ_Novak           &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,…\n$ writer_Paul_Lieberstein   &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,…\n$ writer_Michael_Schur      &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,…\n$ writer_Mindy_Kaling       &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ writer_Gene_Stupnitsky    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,…\n$ writer_Lee_Eisenberg      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,…\n$ writer_Jennifer_Celotta   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,…\n$ writer_Brent_Forrester    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ writer_Justin_Spitzer     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ writer_Aaron_Shure        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ writer_Charlie_Grandy     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ writer_Warren_Lieberstein &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ writer_Halsted_Sullivan   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ writer_Daniel_Chun        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ writer_Carrie_Kemper      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ writer_Steve_Hely         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ writer_Robert_Padnick     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n\n\n\n\nCharacters\nSome of the characters are named inconsistently, so this fixes the cases I identified.\n\ndf_characters &lt;- df %&gt;% \n  select(season, episode, character) %&gt;% \n  mutate(character = case_when(season == 7 & episode == 18 & character == \"Todd\" ~ \"Todd Packer\",\n                               TRUE ~ character)) %&gt;% \n  mutate(character = case_when(season == 7 & episode == 14 & character == \"David\" ~ character,\n                               character == \"David\" ~ \"David Wallace\",\n                               TRUE ~ character)) %&gt;% \n  mutate(character = case_when(character == \"DeAngelo\" ~ \"Deangelo\",\n                               TRUE ~ character))\n\nSome of the values contain odd characters that need to be removed. This also counts how many lines a character had in an episode.\n\ndf_characters &lt;- df_characters %&gt;%\n  mutate(character = str_replace_all(character, \" & \", \" and \"),\n         character = str_replace_all(character, \"/\", \" and \"),\n         character = str_replace_all(character, \",\", \" and \"),\n         character = str_trim(character),\n         character = str_remove_all(character, \"#\"),\n         character = str_remove_all(character, \"-\"),\n         character = str_remove_all(character, \"'\"),\n         character = str_remove_all(character, '\"'),\n         character = str_remove_all(character, \"\\\\[\"),\n         character = str_remove_all(character, \"\\\\]\"),\n         character = str_remove_all(character, \"\\\\(\"),\n         character = str_remove_all(character, \"\\\\)\"),\n         character = str_replace_all(character, \" \", \"_\")) %&gt;%\n  count(season, episode, character, name = \"line_count\")\n\nThis selects only the characters that were involved in more than 20 episodes and pivots the data wide. The value in each character column shows how many lines they had in the episode.\n\ndf_top_characters &lt;- df_characters %&gt;% \n  count(character, sort = TRUE) %&gt;% \n  filter(n &gt;= 20) %&gt;% \n  select(character)\n\ndf_characters_main &lt;- df_characters %&gt;% \n  semi_join(df_top_characters) %&gt;% \n  pivot_wider(id_cols = c(season, episode), \n              names_from = character, \n              names_prefix = \"cast_\", \n              values_from = line_count, \n              values_fill = list(line_count = 0))\n\ndf_characters_main %&gt;% \n  select(1:20) %&gt;% \n  glimpse()\n\nRows: 186\nColumns: 20\n$ season        &lt;int&gt; 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,…\n$ episode       &lt;int&gt; 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,…\n$ cast_Angela   &lt;int&gt; 1, 4, 5, 7, 3, 3, 1, 2, 6, 17, 13, 3, 0, 5, 13, 9, 1, 5,…\n$ cast_Dwight   &lt;int&gt; 29, 17, 62, 47, 25, 28, 32, 11, 55, 65, 33, 64, 22, 42, …\n$ cast_Jan      &lt;int&gt; 12, 0, 18, 0, 0, 8, 9, 11, 0, 0, 0, 0, 46, 52, 0, 0, 0, …\n$ cast_Jim      &lt;int&gt; 36, 25, 42, 49, 21, 55, 32, 16, 55, 51, 30, 49, 40, 26, …\n$ cast_Kevin    &lt;int&gt; 1, 8, 6, 3, 1, 5, 1, 6, 9, 5, 2, 3, 1, 4, 8, 11, 0, 2, 8…\n$ cast_Michael  &lt;int&gt; 81, 75, 56, 68, 104, 106, 96, 100, 83, 69, 108, 85, 73, …\n$ cast_Oscar    &lt;int&gt; 3, 13, 9, 14, 2, 1, 2, 0, 10, 4, 7, 0, 4, 1, 6, 8, 1, 3,…\n$ cast_Pam      &lt;int&gt; 41, 12, 32, 22, 14, 45, 41, 27, 33, 22, 27, 25, 32, 30, …\n$ cast_Phyllis  &lt;int&gt; 2, 0, 0, 5, 4, 0, 10, 6, 2, 6, 3, 0, 4, 1, 4, 5, 4, 3, 0…\n$ cast_Roy      &lt;int&gt; 5, 0, 0, 3, 12, 14, 6, 14, 0, 6, 0, 0, 1, 0, 0, 8, 11, 0…\n$ cast_Ryan     &lt;int&gt; 8, 4, 1, 4, 8, 12, 2, 1, 5, 40, 1, 18, 6, 1, 2, 15, 2, 1…\n$ cast_Stanley  &lt;int&gt; 5, 5, 6, 2, 3, 3, 8, 1, 3, 5, 3, 3, 0, 4, 2, 5, 8, 4, 3,…\n$ cast_Kelly    &lt;int&gt; 0, 2, 0, 0, 0, 0, 7, 0, 0, 4, 3, 3, 1, 2, 1, 8, 4, 1, 5,…\n$ cast_Toby     &lt;int&gt; 0, 2, 0, 4, 0, 7, 0, 26, 0, 0, 0, 5, 1, 1, 0, 3, 0, 7, 3…\n$ cast_Meredith &lt;int&gt; 0, 0, 3, 10, 0, 0, 0, 1, 1, 4, 0, 0, 0, 0, 0, 10, 3, 1, …\n$ cast_Darryl   &lt;int&gt; 0, 0, 0, 0, 15, 0, 1, 9, 0, 0, 0, 0, 0, 0, 0, 11, 3, 0, …\n$ cast_Everyone &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,…\n$ cast_Creed    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 16, 0, 0, 1, 0, 4, 0, 1, 3…"
  },
  {
    "objectID": "posts/remodeling-the-office/index.html#prepare-data-for-modeling",
    "href": "posts/remodeling-the-office/index.html#prepare-data-for-modeling",
    "title": "(re)Modeling the Office",
    "section": "Prepare data for modeling",
    "text": "Prepare data for modeling\nThis combines all the personnel tables and creates an episode_id variable. I also replace missing values with 0.\n\ndf_office &lt;- df_imdb %&gt;% \n  left_join(df_directors) %&gt;% \n  left_join(df_writers) %&gt;% \n  left_join(df_characters_main) %&gt;% \n  mutate(episode_id = str_c(season, episode, sep = \"_\")) %&gt;% \n  mutate(across(contains(\"director\"), coalesce, 0),\n         across(contains(\"writer\"), coalesce, 0)) %&gt;% \n  select(-episode)\n\ndf_office %&gt;% \n  glimpse()\n\nRows: 186\nColumns: 72\n$ season                     &lt;int&gt; 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ imdb_rating                &lt;dbl&gt; 7.6, 8.3, 7.9, 8.1, 8.4, 7.8, 8.7, 8.2, 8.4…\n$ flag_premier               &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0…\n$ flag_finale                &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ director_Ken_Kwapis        &lt;dbl&gt; 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0…\n$ director_Ken_Whittingham   &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ director_Greg_Daniels      &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0…\n$ director_Paul_Feig         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1…\n$ director_Charles_McDougall &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ director_Randall_Einhorn   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ director_Tucker_Gates      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ director_Jeffrey_Blitz     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ director_Harold_Ramis      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ director_Paul_Lieberstein  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ director_Jennifer_Celotta  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ director_David_Rogers      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ director_Steve_Carell      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ director_Brent_Forrester   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ director_BJ_Novak          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ director_John_Krasinski    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ director_Matt_Sohn         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ director_Rainn_Wilson      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ director_Troy_Miller       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ writer_Greg_Daniels        &lt;dbl&gt; 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0…\n$ writer_BJ_Novak            &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0…\n$ writer_Paul_Lieberstein    &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0…\n$ writer_Michael_Schur       &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0…\n$ writer_Mindy_Kaling        &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0…\n$ writer_Gene_Stupnitsky     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0…\n$ writer_Lee_Eisenberg       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0…\n$ writer_Jennifer_Celotta    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1…\n$ writer_Brent_Forrester     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ writer_Justin_Spitzer      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ writer_Aaron_Shure         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ writer_Charlie_Grandy      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ writer_Warren_Lieberstein  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ writer_Halsted_Sullivan    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ writer_Daniel_Chun         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ writer_Carrie_Kemper       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ writer_Steve_Hely          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ writer_Robert_Padnick      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ writer_Allison_Silverman   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ writer_Owen_Ellickson      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ cast_Angela                &lt;int&gt; 1, 4, 5, 7, 3, 3, 1, 2, 6, 17, 13, 3, 0, 5,…\n$ cast_Dwight                &lt;int&gt; 29, 17, 62, 47, 25, 28, 32, 11, 55, 65, 33,…\n$ cast_Jan                   &lt;int&gt; 12, 0, 18, 0, 0, 8, 9, 11, 0, 0, 0, 0, 46, …\n$ cast_Jim                   &lt;int&gt; 36, 25, 42, 49, 21, 55, 32, 16, 55, 51, 30,…\n$ cast_Kevin                 &lt;int&gt; 1, 8, 6, 3, 1, 5, 1, 6, 9, 5, 2, 3, 1, 4, 8…\n$ cast_Michael               &lt;int&gt; 81, 75, 56, 68, 104, 106, 96, 100, 83, 69, …\n$ cast_Oscar                 &lt;int&gt; 3, 13, 9, 14, 2, 1, 2, 0, 10, 4, 7, 0, 4, 1…\n$ cast_Pam                   &lt;int&gt; 41, 12, 32, 22, 14, 45, 41, 27, 33, 22, 27,…\n$ cast_Phyllis               &lt;int&gt; 2, 0, 0, 5, 4, 0, 10, 6, 2, 6, 3, 0, 4, 1, …\n$ cast_Roy                   &lt;int&gt; 5, 0, 0, 3, 12, 14, 6, 14, 0, 6, 0, 0, 1, 0…\n$ cast_Ryan                  &lt;int&gt; 8, 4, 1, 4, 8, 12, 2, 1, 5, 40, 1, 18, 6, 1…\n$ cast_Stanley               &lt;int&gt; 5, 5, 6, 2, 3, 3, 8, 1, 3, 5, 3, 3, 0, 4, 2…\n$ cast_Kelly                 &lt;int&gt; 0, 2, 0, 0, 0, 0, 7, 0, 0, 4, 3, 3, 1, 2, 1…\n$ cast_Toby                  &lt;int&gt; 0, 2, 0, 4, 0, 7, 0, 26, 0, 0, 0, 5, 1, 1, …\n$ cast_Meredith              &lt;int&gt; 0, 0, 3, 10, 0, 0, 0, 1, 1, 4, 0, 0, 0, 0, …\n$ cast_Darryl                &lt;int&gt; 0, 0, 0, 0, 15, 0, 1, 9, 0, 0, 0, 0, 0, 0, …\n$ cast_Everyone              &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0…\n$ cast_Creed                 &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 16, 0, 0, 1, …\n$ cast_All                   &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ cast_David_Wallace         &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ cast_Andy                  &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ cast_Karen                 &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ cast_Pete                  &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ cast_Erin                  &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ cast_Gabe                  &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ cast_Clark                 &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ cast_Robert                &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ cast_Nellie                &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ episode_id                 &lt;chr&gt; \"1_1\", \"1_2\", \"1_3\", \"1_4\", \"1_5\", \"1_6\", \"…\n\n\nThis splits the data into the training and testing sets that will be used to model the data. I stratify by season because it may have an effect on imdb_rating that I want to capture.\n\noffice_split &lt;- initial_split(df_office, strata = season)\noffice_train &lt;- training(office_split)\noffice_test &lt;- testing(office_split)\n\nThis creates a tidymodels recipe that removes zero-variance variables and normalizes the predictor variables.\n\noffice_rec &lt;- recipe(imdb_rating ~ ., data = office_train) %&gt;%\n  update_role(episode_id, new_role = \"ID\") %&gt;%\n  step_zv(all_numeric(), -all_outcomes()) %&gt;%\n  step_normalize(all_numeric(), -all_outcomes())\n\noffice_prep &lt;- office_rec %&gt;%\n  prep(strings_as_factors = FALSE)"
  },
  {
    "objectID": "posts/remodeling-the-office/index.html#modeling",
    "href": "posts/remodeling-the-office/index.html#modeling",
    "title": "(re)Modeling the Office",
    "section": "Modeling",
    "text": "Modeling\nI will use a linear model with ridge regression to penalize extreme coefficients. I bootstrap the training data and use tune() to find the optimal value for penalty.\n\nwf &lt;- workflow() %&gt;%\n  add_recipe(office_rec)\n\noffice_boot &lt;- bootstraps(office_train, strata = season)\n\ntune_spec &lt;- linear_reg(penalty = tune(), mixture = 0) %&gt;%\n  set_engine(\"glmnet\")\n\nlambda_grid &lt;- grid_regular(penalty(), levels = 50)\n\nridge_grid &lt;- tune_grid(\n  wf %&gt;% add_model(tune_spec),\n  resamples = office_boot,\n  grid = lambda_grid)\n\nlowest_rmse searches through the bootstrapped models to find the penalty that gives the lowest RMSE (root mean squared error). This graph shows that increasing the penalty increases performance, but has diminishing returns.\n\nlowest_rmse &lt;- ridge_grid %&gt;%\n  select_best(\"rmse\")\n\n#graph metrics\nridge_grid %&gt;%\n  collect_metrics() %&gt;%\n  ggplot(aes(penalty, mean, color = .metric, fill = .metric)) +\n  geom_ribbon(aes(ymin = mean - std_err,\n                  ymax = mean + std_err),\n              alpha = 0.5) +\n  geom_line(size = 1.5) +\n  geom_vline(xintercept = lowest_rmse$penalty, linetype = 2) +\n  facet_wrap(~.metric, scales = \"free\", nrow = 2) +\n  scale_x_log10() +\n  labs(title = \"Ridge regression lambda values\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nThis fits the model with the best value for penalty.\n\nfinal_ridge &lt;- finalize_workflow(wf %&gt;% add_model(tune_spec), lowest_rmse)\n\n\nAssess model\nThe model generally overrates episodes with low ratings and underrates episodes with high ratings.\n\nfinal_ridge %&gt;% \n  fit(office_train) %&gt;% \n  predict(office_train) %&gt;% \n  bind_cols(office_train) %&gt;% \n  ggplot(aes(imdb_rating, .pred)) +\n    geom_abline(linetype = 2) +\n    geom_point(alpha = .2) +\n    geom_smooth() +\n    coord_equal() +\n    labs(x = \"IMDB rating\",\n         y = \"Predicted rating\")\n\n\n\n\n\n\n\n\nExamining the data by season shows that the model predicted imdb_rating reasonably well for most seasons. It overestimated season 1 and underestimated season 3.\n\nfinal_ridge %&gt;% \n  fit(office_train) %&gt;% \n  predict(office_train) %&gt;% \n  bind_cols(office_train) %&gt;% \n  separate(episode_id, into = c(\"season\", \"episode\"), sep = \"_\") %&gt;% \n  mutate(.resid = imdb_rating - .pred) %&gt;% \n  select(season, episode, .resid) %&gt;% \n  ggplot(aes(season, .resid)) +\n    geom_boxplot() +\n    geom_hline(yintercept = 0, linetype = 2, color = \"red\") +\n    labs(y = \"Residual\",\n         title = \"Actual minus predicted rating\")\n\n\n\n\n\n\n\n\nThis graph shows the variable importance, split by role:\n\ndf_vi &lt;- final_ridge %&gt;%\n  fit(office_train) %&gt;%\n  pull_workflow_fit() %&gt;%\n  vi(lambda = lowest_rmse$penalty) %&gt;%\n  mutate(Importance = case_when(Sign == \"NEG\" ~ Importance * -1,\n                                TRUE ~ Importance)) %&gt;%\n  mutate(Variable = case_when(str_detect(Variable, \"writer|director|cast\") ~ Variable,\n                              TRUE ~ str_c(\"other_\", Variable))) %&gt;% \n  mutate(Variable = fct_reorder(Variable, Importance)) %&gt;%\n  separate(Variable, sep = \"_\", into = c(\"role\", \"person\"), extra = \"merge\") %&gt;% \n  mutate(person = str_replace_all(person, \"_\", \" \"))\n\ndf_vi %&gt;% \n  mutate(person = tidytext::reorder_within(x = person, by = Importance, within = role)) %&gt;% \n  ggplot(aes(x = Importance, y = person, fill = Importance)) +\n  geom_col(color = \"black\") +\n  facet_wrap(~role, scales = \"free_y\") +\n  scale_fill_viridis_c() +\n  scale_y_reordered() +\n  labs(y = NULL)\n\n\n\n\n\n\n\n\nThe importance of characters is much more evenly distributed than I thought it would be. Stanley is the cast MVP (non-Michael division) based on this model. The character isn’t usually the focus of an episode, but when he has a lot of lines, the episode gets better ratings.\n\n\n\n\n\nThe high values for “All” show that scenes where the entire office is involved are highly associated with increased ratings.\n\n\n\n\n\nI’m impressed by the high ratings for Michael and Jim, who carried a lot of the workload in terms of lines delivered. Despite this, the model still considers the number of lines they deliver to be important.\nCarell’s directorship is significantly more important than the other directors. I was definitely surprised by this, since Carell only directed a few episodes, and I expected the ridge regression to penalize his director coefficient heavily.\nThe model has a dim view of Nellie and Robert, who were brought in fill the gap left by Carell’s departure from the show.\n\n\n\n\n\nIn the “other” variables, the model thinks the show gets lower ratings as the seasons go on. Finales and season premiers are positive influences.\nSplitting the model inputs by role means I can compare how impactful a person was across roles. For example, the showrunner Greg Daniels was relatively more important as as writer than a director.\n\ndf_vi %&gt;%  \n  filter(person == \"Greg Daniels\")\n\n# A tibble: 2 × 4\n  role     person       Importance Sign \n  &lt;chr&gt;    &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;\n1 writer   Greg Daniels     0.0328 POS  \n2 director Greg Daniels     0.0123 POS  \n\n\nRainn Wilson was much more important as a cast member than as a director.\n\ndf_vi %&gt;% \n  filter(person == \"Dwight\" | person == \"Rainn Wilson\")\n\n# A tibble: 2 × 4\n  role     person       Importance Sign \n  &lt;chr&gt;    &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;\n1 cast     Dwight           0.0197 POS  \n2 director Rainn Wilson    -0.0189 NEG  \n\n\n\n\n\n\n\nFinally, this tests how the model performs on test data that it has not seen. I think this is reasonably good, considering that TV show quality is driven by chemistry between the cast, which is hard to quantify.\n\nlast_fit(final_ridge, office_split) %&gt;%\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       0.430 Preprocessor1_Model1\n2 rsq     standard       0.324 Preprocessor1_Model1"
  },
  {
    "objectID": "posts/house_price_estimator/index.html",
    "href": "posts/house_price_estimator/index.html",
    "title": "House Price Estimator Dashboard",
    "section": "",
    "text": "Click here to view the full dashboard"
  },
  {
    "objectID": "posts/house_price_estimator/index.html#training-set-metrics-75-of-total-observations",
    "href": "posts/house_price_estimator/index.html#training-set-metrics-75-of-total-observations",
    "title": "House Price Estimator Dashboard",
    "section": "Training set metrics (75% of total observations)",
    "text": "Training set metrics (75% of total observations)\nI used 10-fold cross-validation to assess model performance against the training set:\n\ntrain_metrics %&gt;% \n  select(model_name, id, .metric, .estimate) %&gt;% \n  pivot_wider(names_from = .metric, values_from = .estimate) %&gt;% \n  ggplot(aes(rmse, rsq, color = model_name)) +\n  geom_point() +\n  scale_x_continuous(label = dollar) +\n  labs(x = \"Root Mean Squared Error\",\n       y = \"R^2\")"
  },
  {
    "objectID": "posts/house_price_estimator/index.html#test-set-metrics-25-of-total-observations",
    "href": "posts/house_price_estimator/index.html#test-set-metrics-25-of-total-observations",
    "title": "House Price Estimator Dashboard",
    "section": "Test set metrics (25% of total observations)",
    "text": "Test set metrics (25% of total observations)\n\ntest_metrics %&gt;% \n  select(.metric, .estimate)\n\n# A tibble: 3 × 2\n  .metric   .estimate\n  &lt;chr&gt;         &lt;dbl&gt;\n1 rmse     251406.   \n2 rsq           0.607\n3 mape    3724488.   \n\n\n\nmodel_results %&gt;% \n  ggplot(aes(.resid)) +\n  geom_density() +\n  geom_vline(xintercept = 0, lty = 2) +\n  scale_x_continuous(label = label_dollar())\n\n\n\n\n\n\n\n\n\nmodel_results %&gt;% \n  ggplot(aes(sale_price_adj, .pred_dollar)) +\n  geom_density_2d_filled(contour_var = \"count\") +\n  scale_x_log10(label = label_dollar()) +\n  scale_y_log10(label = label_dollar()) +\n  guides(fill = guide_coloursteps()) +\n  labs(x = \"Inflation-adjusted sale price log10 scale\",\n       y = \"Prediction\",\n       fill = \"Sales\")\n\n\n\n\n\n\n\n\nThe model becomes less effective as the actual sale price increases.\n\nmodel_results %&gt;% \n  ggplot(aes(sale_price_adj, .resid)) +\n  geom_point(alpha = .01) +\n  scale_x_log10(label = dollar) +\n  scale_y_continuous(label = dollar) +\n  labs(x = \"Inflation-adjusted sale price log10 scale\",\n       y = \"Residual\")\n\n\n\n\n\n\n\n\n\ngeo_ids &lt;- st_read(\"post_data/unified_geo_ids/unified_geo_ids.shp\",\n                   quiet = T)\n\ngeo_id_median_resid &lt;- model_results %&gt;% \n  group_by(geo_id) %&gt;% \n  summarize(median_resid = median(.resid))\n\npal &lt;- colorNumeric(\n  palette = \"viridis\",\n  domain = geo_id_median_resid$median_resid)\n\ngeo_ids %&gt;% \n  left_join(geo_id_median_resid) %&gt;% \n  leaflet() %&gt;% \n  addProviderTiles(providers$OpenStreetMap.Mapnik,\n                   options = providerTileOptions(noWrap = TRUE,\n                                                 minZoom = 9\n                                                 #maxZoom = 8\n                   )) %&gt;%\n  addPolygons(popup = ~ str_c(geo_id, \" \", \"median residual: \", round(median_resid, 2), sep = \"\"),\n              fillColor = ~pal(median_resid),\n              fillOpacity = .7,\n              color = \"black\",\n              weight = 3) %&gt;% \n  addLegend(\"bottomright\", pal = pal, values = ~median_resid,\n            title = \"Median of residual\",\n            opacity = 1)\n\nJoining with `by = join_by(geo_id)`\n\n\n\n\n\n\n\nmodel_results %&gt;% \n  add_count(geo_id) %&gt;% \n  mutate(geo_id = fct_reorder(geo_id, .resid, .fun = median)) %&gt;% \n  ggplot(aes(.resid, geo_id, fill = n)) +\n  geom_boxplot(color = \"grey\",\n               outlier.alpha = 0) +\n  geom_vline(xintercept = 0, lty = 2, color = \"red\") +\n  scale_fill_viridis_c() +\n  coord_cartesian(xlim = c(-10^5, 10^5)) +\n  labs(fill = \"Sales\")\n\n\n\n\n\n\n\n\n\nmodel_results %&gt;% \n  add_count(style_desc) %&gt;% \n  mutate(style_desc = fct_reorder(style_desc, .resid, .fun = median)) %&gt;% \n  ggplot(aes(.resid, style_desc, fill = n)) +\n  geom_boxplot(color = \"grey\",\n               outlier.alpha = 0) +\n  geom_vline(xintercept = 0, lty = 2, color = \"red\") +\n  coord_cartesian(xlim = c(-10.5^5, 10.5^5)) +\n  scale_x_continuous(labels = label_dollar()) +\n  scale_fill_viridis_c() +\n  labs(fill = \"Sales\",\n       x = \"Residual\",\n       y = \"House style\")\n\n\n\n\n\n\n\n\n\nmodel_results %&gt;% \n  add_count(grade_desc) %&gt;% \n  mutate(grade_desc = fct_reorder(grade_desc, .resid, .fun = median)) %&gt;% \n  ggplot(aes(.resid, grade_desc, fill = n)) +\n  geom_boxplot(color = \"grey\",\n               outlier.alpha = 0) +\n  scale_fill_viridis_c() +\n  scale_x_continuous(labels = label_dollar()) +\n  coord_cartesian(xlim = c(-10^5, 10.5^6)) +\n  labs(x = \"Residual\",\n       y = \"Grade\",\n       fill = \"Sales\")\n\n\n\n\n\n\n\n\n\nmodel_results %&gt;% \n  add_count(condition_desc) %&gt;% \n  mutate(condition_desc = fct_explicit_na(condition_desc),\n         condition_desc = fct_reorder(condition_desc, .resid, .fun = median)) %&gt;% \n  ggplot(aes(.resid, condition_desc, fill = n)) +\n  geom_boxplot(color = \"grey\",\n               outlier.alpha = 0) +\n  geom_vline(xintercept = 0, lty = 2, color = \"red\") +\n  scale_fill_viridis_c() +\n  scale_x_continuous(labels = label_dollar()) +\n  coord_cartesian(xlim = c(-10^5, 10.5^5)) +\n  labs(x = \"Residual\",\n       y = \"Condition\",\n       fill = \"Sales\")\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `condition_desc = fct_explicit_na(condition_desc)`.\nCaused by warning:\n! `fct_explicit_na()` was deprecated in forcats 1.0.0.\nℹ Please use `fct_na_value_to_level()` instead.\n\n\n\n\n\n\n\n\n\n\nmodel_results %&gt;% \n  ggplot(aes(finished_living_area, .resid)) +\n  geom_point(alpha = .1) +\n  scale_x_log10() +\n  scale_y_continuous(label = dollar) +\n  labs(x = \"Finished Living Area sq. ft. log10 scale\",\n       y = \"Residual\")\n\n\n\n\n\n\n\n\n\nmodel_results %&gt;% \n  ggplot(aes(lot_area, .resid)) +\n  geom_point(alpha = .1) +\n  scale_x_log10(labels = label_comma()) +\n  scale_y_continuous(labels = label_dollar()) +\n  labs(x = \"Lot Area sq. ft. log10 scale\",\n       y = \"Residual\")\n\nWarning: Transformation introduced infinite values in continuous x-axis\n\n\n\n\n\n\n\n\n\n\nmodel_results %&gt;% \n  group_by(house_age_at_sale) %&gt;% \n  rmse(truth = sale_price_adj, estimate = .pred_dollar) %&gt;% \n  ggplot(aes(house_age_at_sale, .estimate)) +\n  geom_point(alpha = .5) +\n  scale_y_continuous(labels = label_dollar()) +\n  labs(x = \"House age at sale\",\n       y = \"RMSE\")\n\n\n\n\n\n\n\n\nThe model is best at predicting the sale price of houses built in the 1940s to 1980s. This is when most of the houses in the county were built.\n\nmodel_results %&gt;% \n  group_by(year_built) %&gt;% \n  rmse(truth = sale_price_adj, estimate = .pred_dollar) %&gt;% \n  ggplot(aes(year_built, .estimate)) +\n  geom_point(alpha = .5) +\n  scale_y_continuous(labels = label_dollar()) +\n  labs(x = \"Year Built\",\n       y = \"RMSE\")\n\n\n\n\n\n\n\n\n\nmodel_results %&gt;% \n  add_count(bedrooms) %&gt;% \n  ggplot(aes(.resid, bedrooms, group = bedrooms, fill = n)) +\n  geom_boxplot(color = \"grey\",\n               outlier.alpha = 0) +\n  geom_vline(xintercept = 0, lty = 2, color = \"red\") +\n  scale_y_continuous(breaks = c(0:15)) +\n  scale_fill_viridis_c() +\n  scale_x_continuous(labels = label_dollar()) +\n  coord_cartesian(xlim = c(-10^5, 10^5)) +\n  labs(x = \"Residual\",\n       y = \"Bedrooms\",\n       fill = \"Sales\")\n\nWarning: Removed 2 rows containing missing values (`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\nmodel_results %&gt;% \n  add_count(full_baths) %&gt;% \n  ggplot(aes(.resid, full_baths, group = full_baths, fill = n)) +\n  geom_boxplot(color = \"grey\",\n               outlier.alpha = 0) +\n  geom_vline(xintercept = 0, lty = 2, color = \"red\") +\n  scale_y_continuous(breaks = c(0:12)) +\n  scale_fill_viridis_c() +\n  scale_x_continuous(label = dollar) +\n  coord_cartesian(xlim = c(-10^5, 750000)) +\n  labs(x = \"Residual\",\n       y = \"Full bathrooms\",\n       fill = \"Sales\")\n\nWarning: Removed 14 rows containing missing values (`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\nmodel_results %&gt;% \n  add_count(half_baths) %&gt;% \n  ggplot(aes(.resid, half_baths, group = half_baths, fill = n)) +\n  geom_boxplot(color = \"grey\",\n               outlier.alpha = 0) +\n  geom_vline(xintercept = 0, lty = 2, color = \"red\") +\n  scale_y_continuous(breaks = c(0:8)) +\n  scale_x_continuous(labels = label_dollar()) +\n  scale_fill_viridis_c() +\n  coord_cartesian(xlim = c(-10^5, 10^5)) +\n  labs(x = \"Residual\",\n       y = \"Half bathrooms\",\n       fill = \"Sales\")\n\nWarning: Removed 785 rows containing missing values (`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\nmodel_results %&gt;% \n  group_by(sale_year) %&gt;% \n  rmse(truth = sale_price_adj, estimate = .pred_dollar) %&gt;% \n  ggplot(aes(sale_year, .estimate)) +\n  geom_line() +\n  scale_y_continuous(label = dollar) +\n  labs(x = \"Sale year\",\n       y = \"RMSE\")"
  },
  {
    "objectID": "posts/forecasting-healthy-ride-ridership-with-prophet/index.html",
    "href": "posts/forecasting-healthy-ride-ridership-with-prophet/index.html",
    "title": "Forecasting Healthy Ride Ridership With Prophet",
    "section": "",
    "text": "This post is about predicting demand for the Healthy Ride bike system in Pittsburgh. I wanted to try out Facebook’s prophet package and try to do some time series forecasting.\nAs usual, load the required packages and set up the environment:\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(prophet)\nlibrary(janitor)\nlibrary(hrbrthemes)\n\noptions(scipen = 999)\n\ntheme_set(theme_bw())\ndf &lt;- read_csv(\"post_data/combined_ride_data.csv\") %&gt;%\n  filter(date &lt; \"2019-08-01\") %&gt;% \n  count(date)\nThis code loads the data and formats the date column so the prophet package can interface with it. I use dir() to find all the CSV files in the folder and then set_names() and map_df(read_csv()) to read each of the CSV files into memory.\nfiles &lt;- dir(\"data\", \".csv\")\n\ndata &lt;- str_c(\"data/\", files) %&gt;% \n  set_names() %&gt;% \n  map_df(read_csv) %&gt;% \n  clean_names()\n\ndf &lt;- data %&gt;% \n  filter(date &lt; \"2019-08-01\") %&gt;% \n  count(date)\n\n# df %&gt;% \n#   ggplot(aes(date, n)) +\n#   geom_point()\n# \n# last(df$date)\nThe data I will use contains the number of rides per day and also includes the month and year. prophet will identify the time series patterns (“seasonality”) in the data and identify the “true” pattern\nprophet has a plug-and-play workflow that is easy to use, but it has more stringent requirements for how the data has to be shaped. The date data has to be named ds and the target variable has to be named y. I set the floor to zero because there cannot be fewer than 0 rides in a day. prophet requires a cap\nmax_rides &lt;- df %&gt;% \n  summarize(max_rides = max(n) * 3) %&gt;% \n  pull()\n\ndf &lt;- df %&gt;% \n  mutate(n = log(n),\n         cap = log(max_rides)) %&gt;% \n  filter(!is.na(date)) %&gt;% \n  rename(ds = date,\n         y = n) %&gt;% \n  mutate(floor = 0)\n\n\n\ndf %&gt;% \n  filter(is.na(ds))\n\n# A tibble: 0 × 4\n# ℹ 4 variables: ds &lt;date&gt;, y &lt;dbl&gt;, cap &lt;dbl&gt;, floor &lt;dbl&gt;\nglimpse(df)\n\nRows: 1,518\nColumns: 4\n$ ds    &lt;date&gt; 2015-05-31, 2015-06-01, 2015-06-02, 2015-06-03, 2015-06-04, 201…\n$ y     &lt;dbl&gt; 6.173786, 4.836282, 4.934474, 4.875197, 5.361292, 5.613128, 5.94…\n$ cap   &lt;dbl&gt; 8.013343, 8.013343, 8.013343, 8.013343, 8.013343, 8.013343, 8.01…\n$ floor &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\nThis creates the set of holidays I use in the model.\nus_holidays &lt;- prophet::generated_holidays %&gt;% \n  as_tibble() %&gt;% \n  filter(country == \"US\") %&gt;% \n  mutate(ds = as.Date(ds))\nThis code fits a model to the dataset.\nm &lt;- prophet(df, growth = 'logistic', holidays = us_holidays)\nmake_future_dataframe() creates the dataframe that prophet uses to make its forecast. In this case, I have it create a dataframe with 365 days of additional rows to predict onto.\nfuture &lt;- make_future_dataframe(m, periods = 365, freq = \"day\") %&gt;% \n  mutate(floor = 0,\n         cap = unique(df$cap))\nThis code performs the forecast on the future dataset.\nforecast &lt;- predict(m, future) %&gt;% \n  as_tibble()\nThe output is a dataframe with the date, the predicted ridership, and the upper and lower bounds of the prediction.\nforecast\n\n# A tibble: 1,883 × 66\n   ds                  trend   cap floor additive_terms additive_terms_lower\n   &lt;dttm&gt;              &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;          &lt;dbl&gt;                &lt;dbl&gt;\n 1 2015-05-31 00:00:00  5.13  8.01     0          0.720                0.720\n 2 2015-06-01 00:00:00  5.13  8.01     0          0.658                0.658\n 3 2015-06-02 00:00:00  5.13  8.01     0          0.724                0.724\n 4 2015-06-03 00:00:00  5.13  8.01     0          0.766                0.766\n 5 2015-06-04 00:00:00  5.13  8.01     0          0.733                0.733\n 6 2015-06-05 00:00:00  5.13  8.01     0          0.784                0.784\n 7 2015-06-06 00:00:00  5.13  8.01     0          0.844                0.844\n 8 2015-06-07 00:00:00  5.13  8.01     0          0.754                0.754\n 9 2015-06-08 00:00:00  5.13  8.01     0          0.691                0.691\n10 2015-06-09 00:00:00  5.13  8.01     0          0.755                0.755\n# ℹ 1,873 more rows\n# ℹ 60 more variables: additive_terms_upper &lt;dbl&gt;, `Christmas Day` &lt;dbl&gt;,\n#   `Christmas Day_lower` &lt;dbl&gt;, `Christmas Day_upper` &lt;dbl&gt;,\n#   `Christmas Day (Observed)` &lt;dbl&gt;, `Christmas Day (Observed)_lower` &lt;dbl&gt;,\n#   `Christmas Day (Observed)_upper` &lt;dbl&gt;, `Columbus Day` &lt;dbl&gt;,\n#   `Columbus Day_lower` &lt;dbl&gt;, `Columbus Day_upper` &lt;dbl&gt;, holidays &lt;dbl&gt;,\n#   holidays_lower &lt;dbl&gt;, holidays_upper &lt;dbl&gt;, `Independence Day` &lt;dbl&gt;, …\nplot automatically plots the forecast data:\nplot(m, forecast)\nprophet also decomposes the various seasonal effects.\nprophet_plot_components(m, forecast)\nWe can of course use ggplot to manually plot the data.\ndf_aug &lt;- forecast %&gt;% \n  mutate(ds = ymd(ds)) %&gt;% \n  left_join(df) %&gt;% \n  mutate(yhat = exp(yhat),\n         yhat_lower = exp(yhat_lower),\n         yhat_upper = exp(yhat_upper),\n         y = exp(y))\n\ndf_aug %&gt;% \n  ggplot(aes(x = ds)) +\n    geom_ribbon(data = df_aug %&gt;% filter(ds &gt; last(df$ds)), \n                aes(ymin = yhat_lower, ymax = yhat_upper), alpha = .2, fill = \"blue\") +\n    geom_line(data = df_aug %&gt;% filter(ds &gt; last(df$ds)), \n              aes(y = yhat), color = \"blue\") +\n    geom_point(aes(y = y), alpha = .5) +\n    geom_hline(aes(yintercept = unique(floor)), linetype = 2) +\n    labs(x = NULL,\n         y = \"Number of rides\") +\n    scale_y_comma() +\n  theme_bw(base_size = 20)\nprophet also provides functions for cross-validation.\ndf_cv &lt;- cross_validation(m, horizon = 30, units = 'days')\n\nperformance_metrics(df_cv) %&gt;% \n  as_tibble() %&gt;% \n  gather(metric, measure, -horizon) %&gt;% \n  ggplot(aes(horizon, measure)) +\n  geom_line() +\n  facet_wrap(~metric, scales = \"free_y\",\n             ncol = 1) +\n  labs(x = \"Horizon\",\n       y = \"Measure\") +\n  theme_bw()\nWe can also inspect the impact of holidays on the prediction.\ndf_holiday_impact &lt;- forecast %&gt;% \n  clean_names() %&gt;% \n  select(ds, christmas_day:washingtons_birthday) %&gt;% \n  select(-contains(\"upper\"), -contains(\"lower\")) %&gt;% \n  pivot_longer(-ds, names_to = \"holiday\", values_to = \"value\") %&gt;% \n  mutate(year = as.factor(year(ds))) %&gt;% \n  filter(holiday != \"holidays\",\n         value != 0)\n\ndf_holiday_impact %&gt;% \n  arrange(ds) %&gt;% \n  mutate(holiday = as.factor(holiday)) %&gt;% \n  ggplot(aes(holiday, value, color = year)) +\n  geom_hline(yintercept = 0, linetype = 2) +\n  geom_jitter() +\n  coord_flip() +\n  labs(x = \"Holiday\",\n       y = \"Impact\") +\n  scale_color_discrete(\"Year\") +\n  theme_ipsum()"
  },
  {
    "objectID": "posts/forecasting-healthy-ride-ridership-with-prophet/index.html#documentation-and-references",
    "href": "posts/forecasting-healthy-ride-ridership-with-prophet/index.html#documentation-and-references",
    "title": "Forecasting Healthy Ride Ridership With Prophet",
    "section": "Documentation and references",
    "text": "Documentation and references\n\nhttps://facebook.github.io/prophet/\nhttps://data.wprdc.org/dataset/healthyride-trip-data"
  },
  {
    "objectID": "posts/map-census-data-with-r/index.html",
    "href": "posts/map-census-data-with-r/index.html",
    "title": "Map Census Data With R",
    "section": "",
    "text": "This talk was presented on May 30th, 2019 at Code For Pittsburgh.\nBefore we dive in, this presentation assumes that the user has basic familiarity with tidyverse, mainly dplyr. Knowing how to use %&gt;% will be very helpful.\nHow to install packages:\ninstall.packages(\"package_name\")\nGet your census API key: https://api.census.gov/data/key_signup.html\nConfigure environment:\nlibrary(tidycensus)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tigris)\nlibrary(ggmap)\nlibrary(janitor)\n\ntheme_set(theme_bw())\n\noptions(tigris_use_cache = TRUE,\n        scipen = 4,\n        digits = 3)"
  },
  {
    "objectID": "posts/map-census-data-with-r/index.html#packages",
    "href": "posts/map-census-data-with-r/index.html#packages",
    "title": "Map Census Data With R",
    "section": "Packages",
    "text": "Packages\n\n{tidycensus}\ntidycensus gives access to the Census API and makes it easy to plot data on a map.\nData\n\nDemographics\n\nDecennial Census\nAmerican Community Survey (ACS)\nerror estimates\n\nGeometries\n\ncountry\ncounty\nzip code\nblocks\ntracts\nand more\n\n\n\n\n{sf}\nsimple features makes it easy to work with polygon data in R. It uses the familiar tidyverse framework: everything is a tibble, and it uses %&gt;%.\nggplot2::geom_sf() makes it easy to plot sf polygons.\nsf can also do spatial calculations such as st_contains, st_intersects, and st_boundary.\n\n\n{ggmap}\nUses Google Maps API to get basemaps. The API now requires a credit card, but it has a fairly generous “free” tier."
  },
  {
    "objectID": "posts/map-census-data-with-r/index.html#using-tidycensus",
    "href": "posts/map-census-data-with-r/index.html#using-tidycensus",
    "title": "Map Census Data With R",
    "section": "Using {tidycensus}",
    "text": "Using {tidycensus}\n\ncensus_api_key(\"your_key_here\")\n\nThis loads the variables from the Decennial Census in 2010:\n\nvariables_dec &lt;- load_variables(year = 2010, dataset = \"sf1\", cache = TRUE)\n\n\n\n# A tibble: 8,959 × 3\n   name    label                                concept         \n   &lt;chr&gt;   &lt;chr&gt;                                &lt;chr&gt;           \n 1 H001001 Total                                HOUSING UNITS   \n 2 H002001 Total                                URBAN AND RURAL \n 3 H002002 Total!!Urban                         URBAN AND RURAL \n 4 H002003 Total!!Urban!!Inside urbanized areas URBAN AND RURAL \n 5 H002004 Total!!Urban!!Inside urban clusters  URBAN AND RURAL \n 6 H002005 Total!!Rural                         URBAN AND RURAL \n 7 H002006 Total!!Not defined for this file     URBAN AND RURAL \n 8 H003001 Total                                OCCUPANCY STATUS\n 9 H003002 Total!!Occupied                      OCCUPANCY STATUS\n10 H003003 Total!!Vacant                        OCCUPANCY STATUS\n# ℹ 8,949 more rows\n\n\nThis loads the ACS variables for 2017:\n\nvariables_acs &lt;- load_variables(year = 2017, dataset = \"acs5\", cache = TRUE)\n\n\n\n# A tibble: 25,071 × 4\n   name        label                                  concept          geography\n   &lt;chr&gt;       &lt;chr&gt;                                  &lt;chr&gt;            &lt;chr&gt;    \n 1 B00001_001  Estimate!!Total                        UNWEIGHTED SAMP… block gr…\n 2 B00002_001  Estimate!!Total                        UNWEIGHTED SAMP… block gr…\n 3 B01001A_001 Estimate!!Total                        SEX BY AGE (WHI… tract    \n 4 B01001A_002 Estimate!!Total!!Male                  SEX BY AGE (WHI… tract    \n 5 B01001A_003 Estimate!!Total!!Male!!Under 5 years   SEX BY AGE (WHI… tract    \n 6 B01001A_004 Estimate!!Total!!Male!!5 to 9 years    SEX BY AGE (WHI… tract    \n 7 B01001A_005 Estimate!!Total!!Male!!10 to 14 years  SEX BY AGE (WHI… tract    \n 8 B01001A_006 Estimate!!Total!!Male!!15 to 17 years  SEX BY AGE (WHI… tract    \n 9 B01001A_007 Estimate!!Total!!Male!!18 and 19 years SEX BY AGE (WHI… tract    \n10 B01001A_008 Estimate!!Total!!Male!!20 to 24 years  SEX BY AGE (WHI… tract    \n# ℹ 25,061 more rows\n\n\n\nMap total population in the U.S.\nUse View() to browse the variables\n\nvariables_dec %&gt;% \n  filter(str_detect(concept, \"POPULATION\")) %&gt;% \n  View()\n\nP001001 has the data we are looking for.\nQuery the total population of the continental U.S. states:\n\nstates &lt;- get_decennial(geography = \"state\",\n                        variables = c(total_pop = \"P001001\"),\n                        geometry = TRUE,\n                        output = \"wide\",\n                        year = 2010)\n\nThe states tibble contains the census data and the polygons for the geometries.\n\n\nSimple feature collection with 52 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -179 ymin: 17.9 xmax: 180 ymax: 71.4\nGeodetic CRS:  NAD83\n# A tibble: 52 × 4\n   GEOID NAME           total_pop                                       geometry\n   &lt;chr&gt; &lt;chr&gt;              &lt;dbl&gt;                             &lt;MULTIPOLYGON [°]&gt;\n 1 23    Maine            1328361 (((-67.6 44.5, -67.6 44.5, -67.6 44.5, -67.6 …\n 2 25    Massachusetts    6547629 (((-70.8 41.6, -70.8 41.6, -70.8 41.6, -70.8 …\n 3 26    Michigan         9883640 (((-88.7 48.1, -88.7 48.1, -88.7 48.1, -88.7 …\n 4 30    Montana           989415 (((-104 45, -104 45, -104 45, -104 45, -105 4…\n 5 32    Nevada           2700551 (((-114 37, -114 37, -114 36.8, -114 36.7, -1…\n 6 34    New Jersey       8791894 (((-75.5 39.7, -75.5 39.7, -75.5 39.7, -75.5 …\n 7 36    New York        19378102 (((-71.9 41.3, -71.9 41.3, -71.9 41.3, -71.9 …\n 8 37    North Carolina   9535483 (((-82.6 36, -82.6 36, -82.6 36, -82.6 36, -8…\n 9 39    Ohio            11536504 (((-82.8 41.7, -82.8 41.7, -82.8 41.7, -82.8 …\n10 42    Pennsylvania    12702379 (((-75.4 39.8, -75.4 39.8, -75.5 39.8, -75.5 …\n# ℹ 42 more rows\n\n\nMake a bar graph with the data:\n\nstates %&gt;% \n  mutate(NAME = fct_reorder(NAME, total_pop)) %&gt;% \n  ggplot(aes(NAME, total_pop)) +\n    geom_col() +\n    coord_flip()\n\n\n\n\n\n\n\n\nPlot the same data on a map:\n\nstates %&gt;% \n  filter(NAME != \"Alaska\",\n         NAME != \"Hawaii\",\n         !str_detect(NAME, \"Puerto\")) %&gt;% \n  ggplot(aes(fill = total_pop)) +\n    geom_sf() +\n    scale_fill_viridis_c(\"Total Population\")\n\n\n\n\n\n\n\n\nPull the total population of each county in PA and plot it:\n\npennsylvania &lt;- get_decennial(geography = \"county\",\n                              variables = c(total_pop = \"P001001\"),\n                              state = \"PA\",\n                              geometry = TRUE,\n                              output = \"wide\",\n                              year = 2010)\npennsylvania %&gt;% \n  ggplot(aes(fill = total_pop)) +\n    geom_sf() +\n    scale_fill_viridis_c()\n\n\n\n\n\n\n\n\nggplot2 intelligently handles cases when we don’t have data for a certain polygon:\n\npennsylvania %&gt;% \n  mutate(total_pop = case_when(NAME == \"Allegheny County, Pennsylvania\" ~ NA_real_,\n                               NAME != \"Allegheny County, Pennsylvania\" ~ total_pop)) %&gt;% \n  ggplot(aes(fill = total_pop)) +\n    geom_sf() +\n    scale_fill_viridis_c()\n\n\n\n\n\n\n\n\nWe can stack multiple polygons in the same graph to highlight Allegheny County:\n\nallegheny &lt;- pennsylvania %&gt;% \n  filter(str_detect(NAME, \"Allegheny\"))\n\npennsylvania %&gt;% \n  ggplot() +\n    geom_sf(aes(fill = total_pop)) +\n    geom_sf(data = allegheny, color = \"white\", linetype = 2, size = 1, alpha = 0) +\n    scale_fill_viridis_c()\n\n\n\n\n\n\n\n\nWe can also use tidycensus to download demographic data for census tracts.\nSet the variables we want to use first:\n\nracevars &lt;- c(White = \"P005003\", \n              Black = \"P005004\", \n              Asian = \"P005006\", \n              Hispanic = \"P004003\")\n#note that this data is long, not wide\nallegheny_tracts &lt;- get_decennial(geography = \"tract\", \n                                  variables = racevars, \n                                  state = \"PA\", \n                                  county = \"Allegheny County\", \n                                  geometry = TRUE,\n                                  summary_var = \"P001001\",\n                                  year = 2010) \n\nCalculate as a percentage of tract population:\n\nallegheny_tracts &lt;- allegheny_tracts %&gt;% \n  mutate(pct = 100 * value / summary_value)\n\nFacet by variable and map the data:\n\nallegheny_tracts %&gt;% \n  ggplot(aes(fill = pct)) +\n    geom_sf(color = NA) +\n    facet_wrap(~variable) +\n    scale_fill_viridis_c()\n\n\n\n\n\n\n\n\nWe can overlay the boundaries of Pittsburgh over the same graph.\nDownload the boundary shapefile and use sf::st_read to read it into R:\n\ncity_pgh &lt;- st_read(\"post_data/Pittsburgh_City_Boundary-shp/City_Boundary.shp\")\n\nReading layer `City_Boundary' from data source \n  `/Users/conorotompkins/Documents/github_repos/ctompkins_quarto_blog/posts/map-census-data-with-r/post_data/Pittsburgh_City_Boundary-shp/City_Boundary.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 8 features and 8 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 1320000 ymin: 382000 xmax: 1380000 ymax: 433000\nProjected CRS: NAD83 / Pennsylvania South (ftUS)\n\nallegheny_tracts %&gt;% \n  ggplot() +\n    geom_sf(aes(fill = pct), color = NA) +\n    geom_sf(data = city_pgh, color = \"white\", linetype = 2, size = 1, alpha = 0) +\n    facet_wrap(~variable) + \n    scale_fill_viridis_c()\n\n\n\n\n\n\n\n\n\n\nWorking with other data\n\nWPRDC 311 data and city wards\nWe can also download the shapefile for the City of Pittsburgh wards. The 311 dataset is tagged with the ward the request originated from, so we can use that to aggregate and map the total number of 311 requests per ward.\n\ndf_311 &lt;- read_csv(\"https://data.wprdc.org/datastore/dump/76fda9d0-69be-4dd5-8108-0de7907fc5a4\") %&gt;% \n  clean_names()\n\n\n\n\n\n\nrequest_id\ncreated_on\nrequest_type\nrequest_origin\nstatus\ndepartment\nneighborhood\ncouncil_district\nward\n\n\n\n\n203364\n2017-12-15 14:53:00\nStreet Obstruction/Closure\nCall Center\n1\nDOMI - Permits\nCentral Northside\n1\n22\n\n\n200800\n2017-11-29 09:54:00\nGraffiti\nControl Panel\n1\nPolice - Zones 1-6\nSouth Side Flats\n3\n16\n\n\n201310\n2017-12-01 13:23:00\nLitter\nCall Center\n1\nDPW - Street Maintenance\nTroy Hill\n1\n24\n\n\n200171\n2017-11-22 14:54:00\nWater Main Break\nCall Center\n1\nPittsburgh Water and Sewer Authority\nBanksville\n2\n20\n\n\n193043\n2017-10-12 12:46:00\nGuide Rail\nCall Center\n1\nDPW - Construction Division\nEast Hills\n9\n13\n\n\n196521\n2017-10-31 15:17:00\nGuide Rail\nCall Center\n1\nDPW - Construction Division\nEast Hills\n9\n13\n\n\n193206\n2017-10-13 09:18:00\nSidewalk/Curb/HC Ramp Maintenance\nCall Center\n1\nDOMI - Permits\nMount Washington\n4\n19\n\n\n195917\n2017-10-27 10:23:00\nManhole Cover\nCall Center\n1\nDOMI - Permits\nBluff\n6\n1\n\n\n179176\n2017-08-14 14:00:00\nNeighborhood Issues\nControl Panel\n0\nNA\nMiddle Hill\n6\n5\n\n\n190422\n2017-09-29 11:46:00\nMayor's Office\nWebsite\n1\n311\nNorth Oakland\n8\n4\n\n\n\n\n\n\n\n\n\nSimple feature collection with 32 features and 1 field\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 1320000 ymin: 382000 xmax: 1380000 ymax: 433000\nProjected CRS: NAD83 / Pennsylvania South (ftUS)\n# A tibble: 32 × 2\n    ward                                                                geometry\n   &lt;int&gt;                                              &lt;POLYGON [US_survey_foot]&gt;\n 1     1 ((1344377 410658, 1344401 410655, 1344473 410650, 1344497 410648, 1344…\n 2     2 ((1349657 415566, 1349615 415539, 1349572 415508, 1349487 415433, 1349…\n 3     3 ((1348490 410322, 1348432 410326, 1348333 410334, 1348260 410339, 1348…\n 4     4 ((1357003 413341, 1357009 413322, 1357024 413281, 1357058 413173, 1357…\n 5     5 ((1354794 418150, 1354861 418062, 1354918 417978, 1354931 417956, 1355…\n 6     6 ((1354713 418633, 1354629 418544, 1354609 418522, 1354575 418487, 1354…\n 7     7 ((1364905 417408, 1364974 417343, 1365180 417147, 1365249 417082, 1365…\n 8     8 ((1357423 420022, 1357491 420008, 1357693 419965, 1357760 419952, 1357…\n 9     9 ((1357423 420022, 1357369 420033, 1357314 420045, 1357208 420069, 1357…\n10    10 ((1365337 428177, 1365357 428160, 1365370 428141, 1365383 428126, 1365…\n# ℹ 22 more rows\n\n\nPlot the ward polygons:\n\nwards %&gt;% \n  ggplot() +\n  geom_sf()\n\n\n\n\n\n\n\n\nCalculate the center of each ward. We will use this to label the wards on the map:\n\nward_labels &lt;- wards %&gt;% \n  st_centroid() %&gt;% \n  st_coordinates() %&gt;%\n  as_tibble() %&gt;%\n  clean_names() %&gt;%\n  mutate(ward = wards$ward)\n\nward_labels_transformed &lt;- wards %&gt;% \n  st_transform(4326) |&gt; \n  st_centroid() %&gt;% \n  st_coordinates() %&gt;%\n  as_tibble() %&gt;%\n  clean_names() %&gt;%\n  mutate(ward = wards$ward)\n\n\n\n\n\n\nx\ny\nward\n\n\n\n\n1343990\n410154\n1\n\n\n1345190\n413807\n2\n\n\n1346380\n411764\n3\n\n\n1353781\n410344\n4\n\n\n1351582\n414257\n5\n\n\n\n\n\n\n\nCount the number of requests per ward:\n\ndf_311_count &lt;- df_311 %&gt;% \n  count(ward, sort = TRUE)\n\nUse left_join to join the count data with the coordinates:\n\nward_311 &lt;- wards %&gt;% \n  left_join(df_311_count) %&gt;%\n  mutate(ward_label = ward_labels$ward)\n\n\n\nSimple feature collection with 32 features and 3 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 1320000 ymin: 382000 xmax: 1380000 ymax: 433000\nProjected CRS: NAD83 / Pennsylvania South (ftUS)\n# A tibble: 32 × 4\n    ward                                               geometry     n ward_label\n * &lt;dbl&gt;                             &lt;POLYGON [US_survey_foot]&gt; &lt;int&gt;      &lt;int&gt;\n 1     1 ((1344377 410658, 1344401 410655, 1344473 410650, 134…  3821          1\n 2     2 ((1349657 415566, 1349615 415539, 1349572 415508, 134…  7188          2\n 3     3 ((1348490 410322, 1348432 410326, 1348333 410334, 134…  2327          3\n 4     4 ((1357003 413341, 1357009 413322, 1357024 413281, 135… 12187          4\n 5     5 ((1354794 418150, 1354861 418062, 1354918 417978, 135…  8514          5\n 6     6 ((1354713 418633, 1354629 418544, 1354609 418522, 135…  6672          6\n 7     7 ((1364905 417408, 1364974 417343, 1365180 417147, 136…  6896          7\n 8     8 ((1357423 420022, 1357491 420008, 1357693 419965, 135…  9128          8\n 9     9 ((1357423 420022, 1357369 420033, 1357314 420045, 135…  8321          9\n10    10 ((1365337 428177, 1365357 428160, 1365370 428141, 136… 14032         10\n# ℹ 22 more rows\n\n\nPlot the data:\n\nward_311 %&gt;% \n  ggplot() +\n    geom_sf(aes(fill = n), color = NA) +\n    geom_label(data = ward_labels, aes(x, y, label = ward), size = 3, inherit.aes = FALSE) +\n    scale_fill_viridis_c(\"Number of 311 requests\")\n\n\n\n\n\n\n\n\n\n\nWPRDC overdose data\nWe can use the census data to adjust other data for per capita rates. For example, the WPRDC’s overdose data has the zip code that the overdose occurred in.\nFirst, download the overdose dataset and pull the population data for each zip code:\n\ndf_overdose &lt;- read_csv(\"https://data.wprdc.org/datastore/dump/1c59b26a-1684-4bfb-92f7-205b947530cf\") %&gt;% \n  clean_names() %&gt;% \n  mutate(incident_zip = str_sub(incident_zip, 1, 5))\n\n\nall_zips &lt;- get_acs(geography = \"zip code tabulation area\",\n                    variables = c(total_pop = \"B01003_001\"),\n                    geometry = TRUE,\n                    output = \"wide\",\n                    year = 2018)\n\n\ndf_overdose &lt;- read_csv(\"post_data/1c59b26a-1684-4bfb-92f7-205b947530cf.csv\") %&gt;% \n  clean_names() %&gt;% \n  mutate(incident_zip = str_sub(incident_zip, 1, 5)) |&gt; \n  filter(death_date_and_time &lt;= \"2019-06-01\")\n\nThen, aggregate the overdose data to the zip code and join the datasets:\n\ndf_overdose &lt;- df_overdose %&gt;% \n  count(incident_zip, sort = TRUE)\n\nattempt1 &lt;- all_zips %&gt;%\n  semi_join(df_overdose, by = c(\"GEOID\" = \"incident_zip\")) %&gt;% \n  left_join(df_overdose, by = c(\"GEOID\" = \"incident_zip\"))\n\nattempt1 %&gt;% \n  ggplot() +\n    geom_sf()\n\n\n\n\n\n\n\n\nUnfortunately the data is kind of messy and includes zip codes that aren’t in Allegheny County.\nWe can use st_intersection to exclude all of the zip code polygons that do not fall within the allegheny county tibble we made earlier:\n\nallegheny %&gt;% \n  ggplot() +\n    geom_sf()\n\n\n\n\n\n\n\n\nThen, join the aggregated overdose data with left_join:\n\ndf_allegheny_overdose &lt;- st_intersection(allegheny, all_zips) %&gt;% \n  left_join(df_overdose, by = c(\"GEOID.1\" = \"incident_zip\"))\n\nNow we can calculate the per 1,000 overdose rate and plot the data:\n\ndf_allegheny_overdose %&gt;% \n  filter(total_popE &gt;= 400) %&gt;% \n  mutate(overdoses_per_capita = n / total_popE * 1000) %&gt;% \n  ggplot(aes(fill = overdoses_per_capita)) +\n    geom_sf(color = NA) +\n    scale_fill_viridis_c()\n\n\n\n\n\n\n\n\n\n\n\n{ggmap} basemaps\nWe can use ggmap to request a basemap from the Google Maps API. Get your API key here\n\nregister_google(key = \"Your key here\")\n\n\npgh_map &lt;- get_map(location = c(lat = 40.445315, lon = -79.977104), zoom = 12)\n\nggmap(pgh_map)\n\n\n\n\n\n\n\n\nThere are multiple basemap styles available:\n\nget_map(location = c(lat = 40.445315, lon = -79.977104), zoom = 12, maptype = \"satellite\", source = \"google\") %&gt;% \n  ggmap()\n\n\n\n\n\n\n\n\n\nget_map(location = c(lat = 40.445315, lon = -79.977104), zoom = 12, maptype = \"roadmap\", source = \"google\") %&gt;% \n  ggmap()\n\n\n\n\n\n\n\n\nCombining maps from different systems requires us to use the same map projection. Google uses 4326. Use coord_sf to set the projection:\n\nward_labels_sf &lt;- ward_labels\n\nggmap(pgh_map) +\n  geom_sf(data = ward_311, aes(fill = n), inherit.aes = FALSE, color = NA, alpha = .7) +\n  geom_label(data = ward_labels_transformed, aes(x, y, label = ward), size = 3) +\n  coord_sf(crs = st_crs(4326)) +\n  scale_fill_viridis_c(\"Number of 311 requests\")"
  },
  {
    "objectID": "posts/map-census-data-with-r/index.html#links",
    "href": "posts/map-census-data-with-r/index.html#links",
    "title": "Map Census Data With R",
    "section": "Links",
    "text": "Links\n\nhttps://walkerke.github.io/tidycensus/articles/basic-usage.html\nhttps://walkerke.github.io/tidycensus/reference/get_acs.html\nhttps://walkerke.github.io/tidycensus/articles/spatial-data.html\nhttps://walkerke.github.io/tidycensus/articles/other-datasets.html\nhttps://cengel.github.io/R-spatial/mapping.html\nhttps://www.r-spatial.org/r/2018/10/25/ggplot2-sf.html\nhttps://www.r-spatial.org/r/2018/10/25/ggplot2-sf-2.html\nhttps://www.r-spatial.org/r/2018/10/25/ggplot2-sf-3.html\ngoogle maps API key: https://cloud.google.com/maps-platform/\nhttps://lucidmanager.org/geocoding-with-ggmap/\nhttps://github.com/rstudio/cheatsheets/blob/master/sf.pdf"
  },
  {
    "objectID": "posts/car-crashes-in-allegheny-county/index.html",
    "href": "posts/car-crashes-in-allegheny-county/index.html",
    "title": "Car Crashes in Allegheny County",
    "section": "",
    "text": "WPRDC has published a dataset on car crashes in Allegheny County from 2004-2017. I was interested to see if there were any patterns or interesting trends in the data."
  },
  {
    "objectID": "posts/car-crashes-in-allegheny-county/index.html#load-data",
    "href": "posts/car-crashes-in-allegheny-county/index.html#load-data",
    "title": "Car Crashes in Allegheny County",
    "section": "Load data",
    "text": "Load data\nThe data was difficult to work with, so I condensed my data munging and cleansing workflow into the following scripts. I may write a post about that process in the future.\n\nsource(\"https://raw.githubusercontent.com/conorotompkins/allegheny_crashes/master/scripts/02_factorize_columns.R\")\nsource(\"https://raw.githubusercontent.com/conorotompkins/allegheny_crashes/master/scripts/03_clean_data.R\")\n\ndf &lt;- data %&gt;% \n  mutate(casualty_count = injury_count + fatal_count)\n\nrm(\"data\", \"df_combined_allegheny_county_crash_data_2004_2017_factorized\", \"df_dictionary\")\n\nThis graph shows that the number of crashes per year is stable, with some year-to-year variation.\n\ndf %&gt;% \n  mutate(crash_year = factor(crash_year)) %&gt;% \n  count(crash_year) %&gt;% \n  ggplot(aes(crash_year, n, group = 1)) +\n  geom_line() +\n  geom_point() +\n  scale_y_continuous(limits = c(0, 13000),\n                     label=comma) +\n  labs(title = \"Crashes per year\",\n       subtitle = my_subtitle,\n       x = NULL,\n       y = \"Number of crashes\",\n       caption = my_caption) +\n    theme(axis.text.x = element_text(angle = 75, hjust = 1))\n\n\nThis shows that the number of crashes per month has varied similarly over the years:\n\ndf %&gt;% \n  mutate(crash_year = factor(crash_year)) %&gt;% \n  count(crash_year, crash_month) %&gt;% \n  ggplot(aes(crash_month, n)) +\n    geom_smooth(aes(group = 1)) +\n    geom_jitter(aes(color = crash_month),\n                height = 0,\n                width = .25,\n                alpha = .5,\n                show.legend = F) +\n    scale_y_continuous(label = comma) +\n    scale_color_viridis(\"Month\",\n                        discrete = TRUE) +\n    labs(title = \"Crashes per month\",\n         subtitle = my_subtitle,\n         x = \"1 dot = Month/Year. Jitter applied\",\n         y = \"Number of crashes\",\n         caption = my_caption) +\n  theme(axis.title.x = element_text(size = 12))\n\n\nThis shows that there is much greater variation between weekdays, though there is still a perceptible pattern.\n\ndf %&gt;% \n  count(crash_year, crash_month, day_of_week) -&gt; df_months_year_dow\n\ndf_months_year_dow %&gt;% \n  group_by(day_of_week) %&gt;% \n  summarize(median = median(n)) -&gt; df_dow\n\ndf_months_year_dow %&gt;% \n  ggplot(aes(day_of_week, n)) +\n  geom_jitter(aes(color = day_of_week), \n              height = 0,\n              alpha = .3,\n              show.legend = F) +\n  geom_point(data = df_dow,\n             aes(x = day_of_week,\n                 y = median,\n                 fill = day_of_week),\n             color = \"black\",\n             size = 4,\n             shape = 21,\n             show.legend = F) +\n  scale_color_viridis(discrete = TRUE) +\n  scale_fill_viridis(discrete = TRUE) +\n  scale_y_continuous(label = comma) +\n  labs(title = \"Crashes per weekday\",\n         subtitle = my_subtitle,\n         x = \"Large dot = median, small dot = Weekday/Month/Year. Jitter applied\",\n         y = \"Number of crashes\",\n         caption = my_caption) +\n  theme(axis.title.x = element_text(size = 12),\n        axis.text.x = element_text(size = 12))\n\n\nThis shows that the number of crashes increases in the fall and winter.\n\ndf %&gt;% \n  mutate(crash_month = fct_rev(crash_month)) %&gt;% \n  count(crash_year, crash_month) %&gt;% \n  ggplot(aes(crash_year, crash_month, fill = n)) +\n    geom_tile() +\n    coord_equal() +\n    scale_x_continuous(expand = c(0,0),\n                       breaks = c(2004:2017)) +\n    scale_y_discrete(expand = c(0,0)) +\n    scale_fill_viridis(\"Number of crashes\",\n                       labels = comma) +\n    labs(title = \"Crash heatmap\",\n         subtitle = my_subtitle,\n         x = NULL,\n         y = NULL,\n         caption = my_caption) +\n    theme(panel.grid = element_blank(),\n          axis.text.x = element_text(angle = 75, hjust = 1))\n\n\nThese plots show how the number of crashes changes throughout the day.\n\ndf %&gt;% \n  mutate(day_of_week = fct_rev(day_of_week)) %&gt;% \n  filter(!hour_of_day &gt; 24,\n         !is.na(day_of_week)) %&gt;% \n  count(day_of_week, hour_of_day) %&gt;% \n  ggplot(aes(hour_of_day, day_of_week, fill = n)) +\n  geom_tile() +\n  coord_equal() +\n  labs(title = \"Crash heatmap\",\n       subtitle = my_subtitle,\n       x = \"Hour of day\",\n       y = \"\",\n       caption = my_caption) +\n  scale_y_discrete(expand = c(0,0)) +\n  scale_x_continuous(expand = c(0,0)) +\n  scale_fill_viridis(labels = comma,\n                     \"Number of crashes\") +\n  theme(legend.position = \"bottom\",\n        legend.direction = \"horizontal\",\n        legend.text = element_text(size = 8, \n                                   angle = 300))\n\n\nThis shows a more granular view:\n\ndf %&gt;% \n  select(day_of_week, time_of_day) %&gt;% \n  filter(!time_of_day &gt; 2400,\n         !is.na(day_of_week)) %&gt;% \n  mutate(day_of_week = fct_rev(day_of_week),\n         hour = time_of_day %/% 100,\n         minute = time_of_day %% 100) %&gt;% \n  count(day_of_week, hour, minute) %&gt;% \n  complete(day_of_week, hour = 0:23, minute = 0:60) %&gt;% \n  replace_na(list(n = 0)) %&gt;% \n  mutate(time = make_datetime(hour = hour, min = minute),\n         time = round_date(time, unit = \"15 minutes\")) %&gt;%\n  group_by(day_of_week, time) %&gt;% \n  summarize(n = sum(n)) -&gt; df_time_rounded\n\ndf_time_rounded %&gt;% \n  ggplot(aes(time, day_of_week, fill = n)) +\n  geom_tile() +\n  scale_y_discrete(expand = c(0,0)) +\n  scale_x_datetime(date_labels = (\"%H:%M\"),\n                   expand = c(0,0)) +\n  scale_fill_viridis(\"Number of crashes\") +\n  labs(title = \"Crash heatmap\",\n       subtitle = my_subtitle,\n       x = \"Time (rounded to nearest 15 minutes)\",\n       y = \"\",\n       caption = my_caption) +\n  theme(legend.position = \"bottom\",\n        legend.direction = \"horizontal\",\n        legend.text = element_text(size = 8, angle = 300))\n\n\nThis is a different veiew of the same data. Saturday and Sunday behave differently than the weekdays.\n\ndf %&gt;%\n  select(time_of_day, day_of_week) %&gt;% \n  filter(!time_of_day &gt; 2400,\n         !is.na(day_of_week)) %&gt;% \n  mutate(hour = time_of_day %/% 100,\n         minute = time_of_day %% 100,\n         time = make_datetime(hour = hour, min = minute),\n         time = round_date(time, unit = \"15 minutes\")) %&gt;% \n  ggplot(aes(time, color = day_of_week)) +\n  geom_freqpoly(size = 2) +\n  scale_color_viridis(\"Weekday\", \n                      discrete = TRUE) +\n  scale_x_datetime(labels = date_format(\"%H:%M\")) +\n  scale_y_continuous(labels = comma) +\n  labs(title = \"Number of crashes\",\n       subtitle = my_subtitle,\n       x = \"Time (rounded to nearest 15 minutes)\",\n       y = \"Number of crashes\",\n       caption = my_caption)\n\n\nThis shows that there are more casualties (injuries and fatalities) per person involved in the crash in the early morning.\n\ndf %&gt;%\n  mutate(day_of_week = fct_rev(day_of_week)) %&gt;% \n  filter(hour_of_day &lt; 24) %&gt;% \n  group_by(day_of_week, hour_of_day) %&gt;% \n  summarize(person_sum = sum(person_count, na.rm = TRUE),\n            casualties_sum = sum(casualty_count, na.rm = TRUE),\n            casualties_per_person = casualties_sum / person_sum) %&gt;% \n  ggplot(aes(hour_of_day, day_of_week, fill = casualties_per_person)) +\n  geom_tile() +\n  coord_equal() +\n  scale_y_discrete(expand = c(0,0)) +\n  scale_x_continuous(expand = c(0,0)) +\n  scale_fill_viridis(\"Casualties per person\") +\n  labs(title = \"Casualties per person\",\n       subtitle = my_subtitle,\n       x = \"Hour\",\n       y = \"\",\n       caption = my_caption) +\n  theme(legend.direction = \"horizontal\",\n        legend.position = \"bottom\",\n        legend.text = element_text(size = 8, \n                                   angle = 300))\n\n\nThe number of injuries and fatalities follow the same general pattern, but it is less pronunced in the fatality data.\n\ndf %&gt;% \n  select(crash_year, crash_month, injury_count, fatal_count) %&gt;% \n  gather(measure, value, -c(crash_year, crash_month)) %&gt;% \n  mutate(measure = factor(measure, \n                          levels = c(\"injury_count\", \"fatal_count\"),\n                          labels = c(\"Injuries\", \"Fatalities\"))) %&gt;% \n  group_by(crash_year, crash_month, measure) %&gt;% \n  summarize(value = sum(value, na.rm = TRUE)) %&gt;% \n  ggplot(aes(crash_month, value, color = crash_month)) +\n    geom_jitter(alpha = .75,\n                height = 0,\n                width = .25,\n                show.legend = FALSE) +\n    facet_wrap(~measure,\n             ncol = 1,\n             scales = \"free\") +\n    labs(title = \"Injuries and fatalities\",\n       subtitle = my_subtitle,\n       x = \"Jitter applied\",\n       y = \"Sum\")\n\n\nThis shows the number of pedestrian fatalities by month.\n\ndf %&gt;% \n  select(crash_year, crash_month, ped_death_count) %&gt;%\n  group_by(crash_year, crash_month) %&gt;% \n  summarize(ped_death_count = sum(ped_death_count, na.rm = TRUE)) %&gt;% \n  ggplot(aes(crash_month, ped_death_count, color = crash_month)) +\n    geom_jitter(height = .15,\n                width = .25,\n                show.legend = FALSE) +\n    scale_color_viridis(\"Month\",\n                      discrete = TRUE) +\n    labs(title = \"Pedestrian fatalities\",\n       subtitle = my_subtitle,\n       x = \"One dot = Month/Year. Jitter applied\",\n       y = \"Sum\")\n\n\nThe rate of increase in the number of fatalities among belted vehicle occupants has been decreasing, while the rate among unbelted occupants has been increasing.\n\ndf %&gt;% \n  select(crash_year, crash_month, belted_death_count, unb_death_count) %&gt;% \n  arrange(crash_year, crash_month) %&gt;%\n  mutate(time_period = make_date(year = crash_year, month = crash_month)) %&gt;%\n  group_by(time_period, crash_year, crash_month) %&gt;% \n  summarize(belted_death_count = sum(belted_death_count),\n            unb_death_count = sum(unb_death_count)) %&gt;% \n  gather(death_type, death_count, -c(time_period, crash_year, crash_month)) %&gt;% \n  arrange(death_type, time_period) %&gt;% \n  group_by(death_type) %&gt;% \n  mutate(death_count_cum = cumsum(death_count)) %&gt;% \n  ungroup() %&gt;% \n  mutate(death_type = factor(death_type,\n                             levels = c(\"belted_death_count\", \"unb_death_count\"),\n                             labels = c(\"Belted deaths\", \"Unbelted deaths\"))) -&gt; df_belted_unbelted\n\ndf_belted_unbelted %&gt;% \n  ggplot(aes(time_period, death_count_cum, color = death_type, group = death_type)) +\n  geom_line(size = 2) +\n  scale_color_viridis(\"\", discrete = TRUE) +\n  scale_y_continuous(label = comma) +\n  labs(title = \"Car occupant fatalities\",\n       subtitle = my_subtitle,\n       x = \"\",\n       y = \"Cumulative sum of deaths\",\n       caption = my_caption) +\n  theme(panel.grid = element_blank())"
  },
  {
    "objectID": "posts/ac_driving_commuter_routes/index.html",
    "href": "posts/ac_driving_commuter_routes/index.html",
    "title": "Analyzing major commuter routes in Allegheny County",
    "section": "",
    "text": "In this post I will use the Mapbox API to calculate metrics for major commuter routes in Allegheny County. The API will provide the distance and duration of the trip, as well as turn-by-turn directions. The route duration should be considered a “minimum duration” because it does not consider traffic. Then I will estimate the duration of the trips with a linear model and compare that to the actual duration from the Mapbox API. I will use the difference between the actual and estimated duration to identify neighborhoods that experience longer or shorter commutes than expected.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(mapboxapi)\nlibrary(tidycensus)\nlibrary(janitor)\nlibrary(lehdr)\nlibrary(tigris)\nlibrary(sf)\nlibrary(hrbrthemes)\n\noptions(tigris_use_cache = TRUE,\n        scipen = 999,\n        digits = 4)\n\ntheme_set(theme_ipsum())\n\nsf_use_s2(FALSE)"
  },
  {
    "objectID": "posts/ac_driving_commuter_routes/index.html#intro",
    "href": "posts/ac_driving_commuter_routes/index.html#intro",
    "title": "Analyzing major commuter routes in Allegheny County",
    "section": "",
    "text": "In this post I will use the Mapbox API to calculate metrics for major commuter routes in Allegheny County. The API will provide the distance and duration of the trip, as well as turn-by-turn directions. The route duration should be considered a “minimum duration” because it does not consider traffic. Then I will estimate the duration of the trips with a linear model and compare that to the actual duration from the Mapbox API. I will use the difference between the actual and estimated duration to identify neighborhoods that experience longer or shorter commutes than expected.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(mapboxapi)\nlibrary(tidycensus)\nlibrary(janitor)\nlibrary(lehdr)\nlibrary(tigris)\nlibrary(sf)\nlibrary(hrbrthemes)\n\noptions(tigris_use_cache = TRUE,\n        scipen = 999,\n        digits = 4)\n\ntheme_set(theme_ipsum())\n\nsf_use_s2(FALSE)"
  },
  {
    "objectID": "posts/ac_driving_commuter_routes/index.html#gather-data",
    "href": "posts/ac_driving_commuter_routes/index.html#gather-data",
    "title": "Analyzing major commuter routes in Allegheny County",
    "section": "Gather data",
    "text": "Gather data\nThe first step is to download the census tract shapefiles for the county:\n\n#get tracts\nallegheny_county_tracts &lt;- tracts(state = \"PA\", county = \"Allegheny\", cb = TRUE) %&gt;% \n  select(GEOID)\n\nst_erase &lt;- function(x, y) {\n  st_difference(x, st_union(y))\n}\n\nac_water &lt;- area_water(\"PA\", \"Allegheny\", class = \"sf\")\n\nallegheny_county_tracts &lt;- st_erase(allegheny_county_tracts, ac_water)\n\nThen I download the “Origin-Destination” LODES file from the Census for Pennsylvania in 2017 and subset it to commuters within Allegheny County.\n\n#load od tract-level data\nlodes_od_ac_main &lt;- grab_lodes(state = \"pa\", year = 2017, \n                               lodes_type = \"od\", job_type = \"JT00\", \n                               segment = \"S000\", state_part = \"main\", \n                               agg_geo = \"tract\", use_cache = TRUE) %&gt;%\n  select(state, w_tract, h_tract, S000, year) %&gt;% \n  rename(commuters = S000) %&gt;% \n  mutate(intra_tract_commute = h_tract == w_tract) %&gt;% \n  semi_join(allegheny_county_tracts, by = c(\"w_tract\" = \"GEOID\")) %&gt;% \n  semi_join(allegheny_county_tracts, by = c(\"h_tract\" = \"GEOID\"))\n\nThis analysis only considers routes where the commuter changed census tracts. 96% of commuters in Allegheny County change census tracts.\n\nlodes_od_ac_main %&gt;% \n  group_by(intra_tract_commute) %&gt;% \n  summarize(commuters = sum(commuters)) %&gt;% \n  ungroup() %&gt;% \n  mutate(pct = commuters / sum(commuters))\n\n# A tibble: 2 × 3\n  intra_tract_commute commuters    pct\n  &lt;lgl&gt;                   &lt;dbl&gt;  &lt;dbl&gt;\n1 FALSE                  465637 0.963 \n2 TRUE                    18127 0.0375"
  },
  {
    "objectID": "posts/ac_driving_commuter_routes/index.html#get-directions",
    "href": "posts/ac_driving_commuter_routes/index.html#get-directions",
    "title": "Analyzing major commuter routes in Allegheny County",
    "section": "Get directions",
    "text": "Get directions\nThis is the code that identifies the center of each tract, geocodes those centroids to get an address, and gets the turn-by-turn directions and route data for each pair of home and work addresses. I will focus on the top 20% of these routes (in terms of cumulative percent of commuters) because the Mapbox API is not designed for the size of query I would need to get directions for all combinations of census tracts.\nNote that I manually replaced the geocoded address for the Wexford and Swissvale areas because the geocoder returned results outside of the county, probably because the center of those tracts intersect with highways.\n\n#filter out rows where commuter doesn't change tracts\ncombined_tract_sf &lt;- lodes_od_ac_main %&gt;%\n  arrange(desc(commuters)) %&gt;% \n  filter(w_tract != h_tract)\n\n#calculate cumulative pct of commuters, keep only top 20%\ncombined_tract_sf_small &lt;- combined_tract_sf %&gt;% \n  select(h_tract, w_tract, commuters) %&gt;% \n  arrange(desc(commuters)) %&gt;% \n  mutate(id = row_number(),\n         pct_commuters = commuters / sum(commuters),\n         cumulative_pct_commuters = cumsum(pct_commuters)) %&gt;%\n  filter(cumulative_pct_commuters &lt; .2) %&gt;%\n  select(h_tract, w_tract, commuters)\n\n#add census centroid geometry\ncombined_tract_sf_small &lt;- combined_tract_sf_small %&gt;% \n  left_join(st_centroid(allegheny_county_tracts), by = c(\"h_tract\" = \"GEOID\")) %&gt;% \n  rename(h_tract_geo = geometry) %&gt;% \n  left_join(st_centroid(allegheny_county_tracts), by = c(\"w_tract\" = \"GEOID\")) %&gt;% \n  rename(w_tract_geo = geometry) %&gt;% \n  select(h_tract, h_tract_geo, w_tract, w_tract_geo, commuters)\n\ncombined_tract_sf_small |&gt; \n  st_sf() |&gt; \n  ggplot() +\n  geom_sf(data = allegheny_county_tracts) +\n  geom_sf()\n\n#get addresses for tract centroids\ntract_od_directions &lt;- combined_tract_sf_small %&gt;%\n  mutate(home_address = map_chr(h_tract_geo, mb_reverse_geocode),\n         work_address = map_chr(w_tract_geo, mb_reverse_geocode))\n\n#replace bad address with good address\nwexford_good_address &lt;- \"3321 Wexford Rd, Gibsonia, PA 15044\"\nswissvale_good_address &lt;- \"1118 S Braddock Ave, Swissvale, PA 15218\"\n\ntract_od_directions &lt;- tract_od_directions %&gt;% \n  #fix wexford address\n  mutate(home_address = case_when(h_tract == \"42003409000\" ~ wexford_good_address,\n                                  h_tract != \"42003409000\" ~ home_address),\n         work_address = case_when(w_tract == \"42003409000\" ~ wexford_good_address,\n                                  w_tract != \"42003409000\" ~ work_address)) |&gt; \n  #fix swissvale address\n  mutate(home_address = case_when(h_tract == \"42003515401\" ~ swissvale_good_address,\n                                  TRUE ~ home_address))\n\n#define error-safe mb_directions function\nmb_directions_possibly &lt;- possibly(mb_directions, otherwise = NA)\n\n#geocode addresses, get directions\ntract_od_directions &lt;- tract_od_directions %&gt;% \n  mutate(home_address_location_geocoded = map(home_address, mb_geocode),\n         work_address_location_geocoded = map(work_address, mb_geocode)) %&gt;% \n  mutate(directions = map2(home_address, work_address, ~ mb_directions_possibly(origin = .x,\n                                                                       destination = .y,\n                                                                       steps = TRUE,\n                                                                       profile = \"driving\"))) %&gt;% \n  select(h_tract, h_tract_geo, home_address, home_address_location_geocoded,\n         w_tract, w_tract_geo, work_address, work_address_location_geocoded,\n         directions, commuters)\n\nThe core of the above code is combining map2 and mb_directions_possibly. This maps the mb_directions_possibly function against two inputs (the home address and work address).\nThe result is a dataframe with a row per turn-by-turn direction for each commuter route.\nThis summarizes the data so there is one row per commuter route and creates summarized route data.\n\n#summarize direction data\ntract_od_stats &lt;- tract_od_directions %&gt;% \n  group_by(h_tract, home_address, w_tract, work_address) %&gt;%\n  summarize(duration = sum(duration),\n            distance = sum(distance),\n            steps = n(),\n            commuters = unique(commuters)) %&gt;% \n  ungroup()\n\nAs expected, route duration and distance are highly correlated. The median duration of a trip is 16.7 minutes.\n\n#graph od stats\ntract_od_stats %&gt;% \n  ggplot(aes(distance, duration, size = commuters)) +\n  geom_point(alpha = .3) +\n  geom_abline(linetype = 2, color = \"red\") +\n  coord_equal() +\n  theme_ipsum() +\n  labs(title = \"Commutes between census tracts\",\n       subtitle = \"Allegheny County, PA\",\n       x = \"Distance in KM\",\n       y = \"Duration in minutes\",\n       size = \"Commuters\")\n\n\n\n\n\n\n\n\n\nmedian_duration &lt;- tract_od_stats %&gt;% \n  uncount(weights = commuters) %&gt;% \n  summarize(median_duration = median(duration)) %&gt;% \n  pull(median_duration)\n\ntract_od_stats %&gt;% \n  uncount(weights = commuters) %&gt;% \n  ggplot(aes(duration)) +\n  geom_density(fill = \"grey\") +\n  geom_vline(xintercept = median_duration, lty = 2, color = \"red\") +\n  annotate(\"text\", x = 21, y = .05, label = \"median\", color = \"red\") +\n  theme_ipsum() +\n  labs(title = \"Trip duration\",\n       x = \"Duration in minutes\",\n       y = \"Density of observations\")\n\n\n\n\n\n\n\n\nThis map shows the main roads that commuter routes use I-376, I-279, and Route 28 are major arteries, as expected.\n\n#map routes\ntract_od_stats %&gt;% \n  ggplot() +\n  geom_sf(data = allegheny_county_tracts, linewidth = .1, fill = \"black\") +\n  geom_sf(aes(alpha = commuters, linewidth = commuters), color = \"#ffcc01\", alpha = .1) +\n  guides(linewidth = guide_legend(override.aes= list(alpha = 1))) +\n  scale_linewidth_continuous(range = c(.1, 5)) +\n  theme_void() +\n  labs(title = \"Commuter routes between Allegheny County census tracts\",\n       subtitle = \"Driving routes\",\n       linewidth = \"Commuters\")\n\n\n\n\n\n\n\n\nA high-resolution image of this map is available here. An animation of the routes is here.\nPeople that live closer to downtown Pittsburgh have shorter commutes, on average.\n\nallegheny_county_tracts %&gt;% \n  st_drop_geometry() %&gt;% \n  left_join(tract_od_stats %&gt;% \n              st_drop_geometry() |&gt; \n              select(h_tract, w_tract, duration) %&gt;% \n              pivot_longer(contains(\"tract\")) %&gt;% \n              group_by(name, value) %&gt;% \n              summarize(avg_duration = mean(duration)) %&gt;% \n              ungroup(),\n            by = c(\"GEOID\" = \"value\")) %&gt;% \n  complete(GEOID, name) %&gt;% \n  filter(!is.na(name)) %&gt;% \n  left_join(allegheny_county_tracts, by = \"GEOID\") %&gt;%\n  mutate(name = case_when(name == \"h_tract\" ~ \"Origin tract\",\n                          name == \"w_tract\" ~ \"Destination tract\"),\n         name = as.factor(name) %&gt;% fct_rev()) %&gt;% \n  st_sf() %&gt;% \n  ggplot() +\n  geom_sf(aes(fill = avg_duration), size = .1) +\n  facet_wrap(~name, ncol = 1) +\n  scale_fill_viridis_c(na.value = \"grey90\") +\n  labs(title = \"Average commute duration\",\n       fill = \"Minutes\") +\n  theme_void()"
  },
  {
    "objectID": "posts/ac_driving_commuter_routes/index.html#model",
    "href": "posts/ac_driving_commuter_routes/index.html#model",
    "title": "Analyzing major commuter routes in Allegheny County",
    "section": "Model",
    "text": "Model\nThe next step is to create a model that estimates the duration of a given commute. I will use the number of steps in the turn-by-turn directions and the distance as predictors. Additionally, I will calculate which rivers a commute route crosses and use those as logical variables in the model.\nThis collects the geometry for the main rivers in the county.\n\nmain_rivers &lt;- ac_water %&gt;% \n  group_by(FULLNAME) %&gt;% \n  summarize(AWATER = sum(AWATER)) %&gt;% \n  arrange(desc(AWATER)) %&gt;% \n  slice(1:4)\n\nThis code calculates whether a given commuter route crosses a river.\n\ntract_od_stats_rivers &lt;- tract_od_stats %&gt;% \n  mutate(intersects_ohio = st_intersects(., main_rivers %&gt;% \n                                           filter(FULLNAME == \"Ohio Riv\")) %&gt;% as.logical(),\n         intersects_allegheny = st_intersects(., main_rivers %&gt;% \n                                                filter(FULLNAME == \"Allegheny Riv\")) %&gt;% as.logical(),\n         intersects_monongahela = st_intersects(., main_rivers %&gt;% \n                                                  filter(FULLNAME == \"Monongahela Riv\")) %&gt;% as.logical(),\n         intersects_youghiogheny = st_intersects(., main_rivers %&gt;% \n                                                   filter(FULLNAME == \"Youghiogheny Riv\")) %&gt;% as.logical()) %&gt;% \n  replace_na(list(intersects_ohio = FALSE,\n                  intersects_allegheny = FALSE,\n                  intersects_monongahela = FALSE,\n                  intersects_youghiogheny = FALSE)) %&gt;% \n  st_drop_geometry()\n\nglimpse(tract_od_stats_rivers)\n\nRows: 780\nColumns: 12\n$ h_tract                 &lt;chr&gt; \"42003020100\", \"42003020300\", \"42003030500\", \"…\n$ home_address            &lt;chr&gt; \"445 Wood Street, Pittsburgh, Pennsylvania 152…\n$ w_tract                 &lt;chr&gt; \"42003982200\", \"42003020100\", \"42003020100\", \"…\n$ work_address            &lt;chr&gt; \"4215 Fifth Avenue, Pittsburgh, Pennsylvania 1…\n$ duration                &lt;dbl&gt; 14.087, 9.225, 7.460, 10.951, 2.510, 10.517, 1…\n$ distance                &lt;dbl&gt; 5.6640, 2.6518, 1.5922, 3.4968, 0.7169, 4.2762…\n$ steps                   &lt;int&gt; 8, 7, 7, 5, 6, 6, 9, 7, 10, 11, 14, 10, 10, 8,…\n$ commuters               &lt;dbl&gt; 58, 106, 287, 68, 81, 129, 87, 121, 116, 59, 1…\n$ intersects_ohio         &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS…\n$ intersects_allegheny    &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS…\n$ intersects_monongahela  &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS…\n$ intersects_youghiogheny &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS…\n\ntract_od_stats_rivers &lt;- tract_od_stats_rivers %&gt;% \n  mutate(od_id = str_c(\"h_tract: \", h_tract, \", \", \"w_tract: \", w_tract, sep = \"\"))\n\nFirst I set the seed and split the data into training and testing sets.\n\nset.seed(1234)\n\n#split data\nsplits &lt;- initial_split(tract_od_stats_rivers, prop = .75)\n\ntraining_data &lt;- training(splits)\ntesting_data &lt;- testing(splits)\n\nThen I use {tidymodels} to define a linear model, cross-validate it, and extract the coefficients.\n\n#recipe\nmodel_recipe &lt;- recipe(duration ~ ., \n                       data = training_data) %&gt;% \n  update_role(od_id, new_role = \"id\") %&gt;%\n  step_rm(h_tract, home_address, w_tract, work_address, commuters) %&gt;% \n  step_normalize(distance, steps) %&gt;% \n  step_zv(all_predictors())\n\nmodel_recipe %&gt;% \n  prep() %&gt;% \n  summary()\n\n# A tibble: 8 × 4\n  variable                type      role      source  \n  &lt;chr&gt;                   &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n1 distance                &lt;chr [2]&gt; predictor original\n2 steps                   &lt;chr [2]&gt; predictor original\n3 intersects_ohio         &lt;chr [1]&gt; predictor original\n4 intersects_allegheny    &lt;chr [1]&gt; predictor original\n5 intersects_monongahela  &lt;chr [1]&gt; predictor original\n6 intersects_youghiogheny &lt;chr [1]&gt; predictor original\n7 od_id                   &lt;chr [3]&gt; id        original\n8 duration                &lt;chr [2]&gt; outcome   original\n\nmodel_recipe_prep &lt;- model_recipe %&gt;% \n  prep()\n\n\n#apply cv to training data\ntraining_vfold &lt;- vfold_cv(training_data, v = 10, repeats = 2)\n\n\n#model specification\nlm_model &lt;- linear_reg(mode = \"regression\") %&gt;% \n  set_engine(\"lm\")\n\n#linear regression workflow\nlm_workflow &lt;- workflow() %&gt;% \n  add_recipe(model_recipe) %&gt;% \n  add_model(lm_model)\n\n#fit against training resamples\nkeep_pred &lt;- control_resamples(save_pred = TRUE)\n\nlm_training_fit &lt;- lm_workflow %&gt;% \n  fit_resamples(training_vfold, control = keep_pred) %&gt;% \n  mutate(model = \"lm\")\n\n#get results from training cv\nlm_training_fit %&gt;% \n  collect_metrics()\n\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   2.88     20 0.0590  Preprocessor1_Model1\n2 rsq     standard   0.870    20 0.00571 Preprocessor1_Model1\n\n\nThe model averaged an R-squared of .82 on the training data, which is pretty good.\nThe predictions from the training set fit the actual duration pretty well.\n\n#graph predictions from assessment sets\nlm_training_fit %&gt;% \n  collect_predictions() %&gt;% \n  ggplot(aes(duration, .pred)) +\n  geom_point(alpha = .3) +\n  geom_abline(linetype = 2, color = \"red\") +\n  coord_equal() +\n  labs(x = \"Actual duration\",\n       y = \"Predicted duration\")\n\n\n\n\n\n\n\n\nNext I fit the model against the test data to extract the coefficients. Holding the other variables constant, distance is by far the most influential variable in the model. For every kilometer increase in distance, the duration of the commute can be expected to increase by around 5 minutes. Crossing the Monongahela will add around 2 minutes to a commute, while crossing the Allegheny and Ohio actually decrease commute times. This is probably related to the bridge that the commuter uses.\n\n#variable importance\nlm_workflow %&gt;% \n  fit(testing_data) %&gt;% \n  pull_workflow_fit() %&gt;% \n  tidy() %&gt;% \n  filter(term != \"(Intercept)\") %&gt;% \n  mutate(term = fct_reorder(term, estimate)) %&gt;% \n  ggplot(aes(estimate, term)) +\n  geom_col(fill = \"grey\", color = \"black\")\n\n\n\n\n\n\n\n\nThis fits the model to the full dataset and plots the predicted duration against the actual duration. The fit is tighter than just plotting distance vs. duration.\n\n#final model\ntract_od_pred &lt;- lm_workflow %&gt;% \n  fit(testing_data) %&gt;% \n  predict(tract_od_stats_rivers) %&gt;% \n  bind_cols(tract_od_stats_rivers) %&gt;% \n  select(h_tract, w_tract, distance, steps, duration, .pred, commuters)\n\ntract_od_pred %&gt;% \n  ggplot(aes(duration, .pred, size = commuters)) +\n  geom_point(alpha = .3) +\n  geom_abline(lty = 2, color = \"red\") +\n  coord_equal() +\n  labs(x = \"Duration in minutes\",\n       y = \"Predicted duration\",\n       size = \"Number of commuters\")\n\n\n\n\n\n\n\n\nThis calculates how far off the model’s estimation of duration was for each census tract in the dataset (origin and destination). Commuters originating from neighborhoods between State Route 51 and the Monongahela River experience longer than expected commutes.\n\nallegheny_county_tracts %&gt;% \n  st_drop_geometry() %&gt;% \n  left_join(tract_od_pred %&gt;% \n              mutate(.resid = duration - .pred) %&gt;% \n              select(h_tract, w_tract, .resid) %&gt;% \n              pivot_longer(contains(\"tract\")) %&gt;% \n              group_by(name, value) %&gt;% \n              summarize(avg_resid = mean(.resid)) %&gt;% \n              ungroup(),\n            by = c(\"GEOID\" = \"value\")) %&gt;% \n  complete(GEOID, name) %&gt;% \n  filter(!is.na(name)) %&gt;% \n  left_join(allegheny_county_tracts) %&gt;%\n  mutate(name = case_when(name == \"h_tract\" ~ \"Origin tract\",\n                          name == \"w_tract\" ~ \"Destination tract\"),\n         name = as.factor(name) %&gt;% fct_rev()) %&gt;% \n  st_sf() %&gt;% \n  ggplot() +\n  geom_sf(aes(fill = avg_resid), size = .1) +\n  facet_wrap(~name, ncol = 1) +\n  scale_fill_viridis_c(na.value = \"grey90\") +\n  labs(title = \"Commute duration above/below expected\",\n       fill = \"Minutes\") +\n  theme_void()"
  },
  {
    "objectID": "posts/networking-usl-clubs-with-euclidean-distance/index.html",
    "href": "posts/networking-usl-clubs-with-euclidean-distance/index.html",
    "title": "Networking USL Club Similarity With Euclidean Distance",
    "section": "",
    "text": "Euclidean distance is a simple way to measure the distance between two points. It can also be used to measure how similar two sports teams are, given a set of variables. In this post, I use Euclidean distance to calculate the similarity between USL clubs and map that data to a network graph. I will use the 538 Soccer Power Index data to calculate the distance."
  },
  {
    "objectID": "posts/networking-usl-clubs-with-euclidean-distance/index.html#setup",
    "href": "posts/networking-usl-clubs-with-euclidean-distance/index.html#setup",
    "title": "Networking USL Club Similarity With Euclidean Distance",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(ggraph)\nlibrary(tidygraph)\nlibrary(viridis)\n\nset_graph_style()\n\nset.seed(1234)"
  },
  {
    "objectID": "posts/networking-usl-clubs-with-euclidean-distance/index.html#download-data",
    "href": "posts/networking-usl-clubs-with-euclidean-distance/index.html#download-data",
    "title": "Networking USL Club Similarity With Euclidean Distance",
    "section": "Download data",
    "text": "Download data\nThis code downloads the data from 538’s GitHub repo and does some light munging.\n\nread_csv(\"https://projects.fivethirtyeight.com/soccer-api/club/spi_global_rankings.csv\", progress = FALSE) %&gt;% \n  filter(league == \"United Soccer League\") %&gt;% \n  mutate(name = str_replace(name, \"Arizona United\", \"Phoenix Rising\")) -&gt; df\n\ndf"
  },
  {
    "objectID": "posts/networking-usl-clubs-with-euclidean-distance/index.html#calculate-euclidean-distance",
    "href": "posts/networking-usl-clubs-with-euclidean-distance/index.html#calculate-euclidean-distance",
    "title": "Networking USL Club Similarity With Euclidean Distance",
    "section": "Calculate Euclidean distance",
    "text": "Calculate Euclidean distance\nThis is the code that measures the distance between the clubs. It uses the 538 offensive and defensive ratings.\n\ndf %&gt;% \n  select(name, off, def) %&gt;% \n  column_to_rownames(var = \"name\") -&gt; df_dist\n\n#df_dist\n#rownames(df_dist) %&gt;% \n#  head()\n\ndf_dist &lt;- dist(df_dist, \"euclidean\", upper = FALSE, diag = FALSE)\n#head(df_dist)\n\ndf_dist %&gt;% \n  tidy() %&gt;% \n  arrange(desc(distance)) -&gt; df_dist\n\n#df_dist %&gt;% \n#  count(item1, sort = TRUE) %&gt;% \n#  ggplot(aes(item1, n)) +\n#  geom_point() +\n#  coord_flip() +\n#  theme_bw()"
  },
  {
    "objectID": "posts/networking-usl-clubs-with-euclidean-distance/index.html#network-graph",
    "href": "posts/networking-usl-clubs-with-euclidean-distance/index.html#network-graph",
    "title": "Networking USL Club Similarity With Euclidean Distance",
    "section": "Network graph",
    "text": "Network graph\nIn this snippet I set a threshhold for how similar clubs need to be to warrant a connection. Then I graph it using tidygraph and ggraph. Teams that are closer together on the graph are more similar. Darker and thicker lines indicate higher similarity.\n\ndistance_filter &lt;- .5\n\ndf_dist %&gt;% \n  mutate(distance = distance^2) %&gt;% \n  filter(distance &lt;= distance_filter) %&gt;%\n  as_tbl_graph(directed = FALSE) %&gt;% \n  mutate(community = as.factor(group_edge_betweenness())) %&gt;%\n  ggraph(layout = \"kk\", maxiter = 1000) +\n    geom_edge_fan(aes(edge_alpha = distance, edge_width = distance)) + \n    geom_node_label(aes(label = name, color = community), size = 5) +\n    scale_color_discrete(\"Group\") +\n    scale_edge_alpha_continuous(\"Euclidean distance ^2\", range = c(.2, 0)) +\n    scale_edge_width_continuous(\"Euclidean distance ^2\", range = c(2, 0)) +\n    labs(title = \"United Soccer League clubs\",\n       subtitle = \"Euclidean distance (offensive rating, defensive rating)^2\",\n       x = NULL,\n       y = NULL,\n       caption = \"538 data, @conor_tompkins\")"
  },
  {
    "objectID": "posts/modeling-pittsburgh-house-sales-linear/index.html",
    "href": "posts/modeling-pittsburgh-house-sales-linear/index.html",
    "title": "Modeling Pittsburgh House Sales Linear",
    "section": "",
    "text": "In this post I will be modeling house (land parcel) sales in Pittsburgh. The data is from the WPRDC’s Parcels n’at dashboard or here.\nThe goal is to use linear modeling to predict the sale price of a house using features of the house and the property.\nThis code sets up the environment and loads the libraries I will use.\n\n#load libraries\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(caret)\nlibrary(broom)\nlibrary(modelr)\nlibrary(rsample)\nlibrary(janitor)\nlibrary(vroom)\n\n#set up environment\noptions(scipen = 999, digits = 5)\n\ntheme_set(theme_bw())\n\nThis reads the data and engineers some features.\n\n#read in data\ndf &lt;- vroom(\"post_data/allegheny_county_master_file.csv\", col_types = cols(.default = \"c\")) %&gt;% \n  clean_names() %&gt;% \n  mutate(across(.cols = c(saleprice, finishedlivingarea, lotarea, yearblt,\n                          bedrooms, fullbaths, halfbaths), parse_number))\n\n#glimpse(df)\n\n\n# df %&gt;% \n#   select(saleprice, finishedlivingarea, lotarea, yearblt, bedrooms, fullbaths, halfbaths) %&gt;% \n#   glimpse()\n# \n# df %&gt;% \n#   select(contains(\"muni\"))\n\n\ndf &lt;- df %&gt;% \n  mutate(munidesc = str_replace(munidesc, \" - PITTSBURGH\", \"\")) %&gt;% \n  mutate(finishedlivingarea_log10 = log10(finishedlivingarea),\n         lotarea_log10 = log10(lotarea),\n         saleprice_log10 = log10(saleprice)) %&gt;% \n  select(parid, classdesc, munidesc, schooldesc, neighdesc, taxdesc,\n         usedesc, homesteadflag, farmsteadflag, styledesc,\n         yearblt, extfinish_desc, roofdesc,  basementdesc,\n         gradedesc, conditiondesc, stories, totalrooms, bedrooms,\n         fullbaths, halfbaths, heatingcoolingdesc, fireplaces, \n         bsmtgarage, finishedlivingarea, finishedlivingarea_log10,\n         lotarea, lotarea_log10, saledate,\n         saleprice, saleprice_log10)\n\n#create grade vectors\ngrades_standard &lt;- c(\"average -\", \"average\", \"average +\",\n                     \"good -\", \"good\", \"good +\",\n                     \"very good -\", \"very good\", \"very good +\")\n\ngrades_below_average_or_worse &lt;- c(\"poor -\", \"poor\", \"poor +\",\n                                   \"below average -\", \"below average\", \"below average +\")\n\ngrades_excellent_or_better &lt;- c(\"excellent -\", \"excellent\", \"excellent +\",\n                                \"highest cost -\", \"highest cost\", \"highest cost +\")\n\n#subset data and engineer features\ndf &lt;- df %&gt;% \n  filter(classdesc == \"RESIDENTIAL\",\n         saleprice &gt; 100,\n         str_detect(munidesc, \"Ward\"),\n         finishedlivingarea &gt; 0,\n         lotarea &gt; 0) %&gt;% \n  select(parid, munidesc, schooldesc, neighdesc, taxdesc,\n         usedesc, homesteadflag, farmsteadflag, styledesc,\n         yearblt, extfinish_desc, roofdesc,  basementdesc, \n         heatingcoolingdesc, gradedesc, conditiondesc, stories, \n         totalrooms, bedrooms, fullbaths, halfbaths, fireplaces, \n         bsmtgarage, finishedlivingarea_log10, lotarea_log10, \n         saleprice_log10, saledate) %&gt;% \n  mutate(usedesc = fct_lump(usedesc, n = 5),\n         styledesc = fct_lump(styledesc, n = 10),\n         #clean up and condense gradedesc\n         gradedesc = str_to_lower(gradedesc),\n         gradedesc = case_when(gradedesc %in% grades_below_average_or_worse ~ \"below average + or worse\",\n                                    gradedesc %in% grades_excellent_or_better ~ \"excellent - or better\",\n                                    gradedesc %in% grades_standard ~ gradedesc),\n         gradedesc = fct_relevel(gradedesc, c(\"below average + or worse\", \"average -\", \"average\", \"average +\",\n                                                        \"good -\", \"good\", \"good +\",\n                                                        \"very good -\", \"very good\", \"very good +\", \"excellent - or better\")))\n\n#replace missing character rows with \"missing\", change character columns to factor\ndf &lt;- df %&gt;% \n  mutate_if(is.character, replace_na, \"missing\") %&gt;% \n  mutate_if(is.character, as.factor)\n\n#select response and features\ndf &lt;- df %&gt;% \n  select(munidesc, usedesc, styledesc, conditiondesc, gradedesc,\n         finishedlivingarea_log10, lotarea_log10, yearblt, bedrooms, \n         fullbaths, halfbaths, saleprice_log10) %&gt;% \n  na.omit()\n\n#muni_desc_levels &lt;- levels(df$munidesc)\n\n#view data\nglimpse(df)\n\nRows: 74,687\nColumns: 12\n$ munidesc                 &lt;fct&gt; 1st Ward , 1st Ward , 1st Ward , 1st Ward , 1…\n$ usedesc                  &lt;fct&gt; Other, SINGLE FAMILY, Other, Other, Other, Ot…\n$ styledesc                &lt;fct&gt; Other, TOWNHOUSE, Other, Other, Other, Other,…\n$ conditiondesc            &lt;fct&gt; AVERAGE, EXCELLENT, GOOD, AVERAGE, AVERAGE, G…\n$ gradedesc                &lt;fct&gt; very good +, excellent - or better, very good…\n$ finishedlivingarea_log10 &lt;dbl&gt; 3.0993, 3.6170, 3.1844, 3.2057, 3.1173, 3.159…\n$ lotarea_log10            &lt;dbl&gt; 3.0993, 3.0461, 3.2355, 3.1584, 3.1173, 3.159…\n$ yearblt                  &lt;dbl&gt; 2007, 2012, 2007, 2007, 2015, 1905, 1905, 190…\n$ bedrooms                 &lt;dbl&gt; 2, 3, 2, 2, 2, 1, 2, 2, 1, 2, 4, 3, 3, 4, 4, …\n$ fullbaths                &lt;dbl&gt; 2, 3, 2, 1, 2, 1, 2, 2, 1, 2, 2, 2, 2, 3, 2, …\n$ halfbaths                &lt;dbl&gt; 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, …\n$ saleprice_log10          &lt;dbl&gt; 6.0414, 6.2544, 5.8543, 5.8876, 5.6767, 5.531…\n\n\nAs shown in the data above, the model uses the following features to predict sale price:\n\nmunicipality name\nprimary use of the parcel\nstyle of building\ncondition of the structure\ngrade of construction\nliving area in square feet\nlot area in square feet\nyear the house was built\nnumber of bedrooms\nnumber of full baths\nnumber of half-baths\n\nThis code sets up the data for cross validation.\n\n#create initial split object\ndf_split &lt;- initial_split(df, prop = .75)\n\n#extract training dataframe\ntraining_data_full &lt;- training(df_split)\n\n#extract testing dataframe\ntesting_data &lt;- testing(df_split)\n\ndistinct(training_data_full, munidesc)|&gt; \n  anti_join(distinct(testing_data, munidesc))\n\n# A tibble: 1 × 1\n  munidesc              \n  &lt;fct&gt;                 \n1 1st Ward  - McKEESPORT\n\n#find dimensions of training_data_full and testing_data\ndim(training_data_full)\n\n[1] 56015    12\n\ndim(testing_data)\n\n[1] 18672    12\n\n\nThis code divides the data into training and testing sets.\n\nset.seed(42)\n\n#prep the df with the cross validation partitions\ncv_split &lt;- vfold_cv(training_data_full, v = 5)\n\ncv_data &lt;- cv_split %&gt;% \n  mutate(\n    #extract train dataframe for each split\n    train = map(splits, ~training(.x)), \n    #extract validate dataframe for each split\n    validate = map(splits, ~testing(.x))\n  )\n\n#view df\ncv_data\n\n#  5-fold cross-validation \n# A tibble: 5 × 4\n  splits                id    train                  validate              \n  &lt;list&gt;                &lt;chr&gt; &lt;list&gt;                 &lt;list&gt;                \n1 &lt;split [44812/11203]&gt; Fold1 &lt;tibble [44,812 × 12]&gt; &lt;tibble [11,203 × 12]&gt;\n2 &lt;split [44812/11203]&gt; Fold2 &lt;tibble [44,812 × 12]&gt; &lt;tibble [11,203 × 12]&gt;\n3 &lt;split [44812/11203]&gt; Fold3 &lt;tibble [44,812 × 12]&gt; &lt;tibble [11,203 × 12]&gt;\n4 &lt;split [44812/11203]&gt; Fold4 &lt;tibble [44,812 × 12]&gt; &lt;tibble [11,203 × 12]&gt;\n5 &lt;split [44812/11203]&gt; Fold5 &lt;tibble [44,812 × 12]&gt; &lt;tibble [11,203 × 12]&gt;\n\n\nThis builds the model to predict house sale price.\n\n#build model using the train data for each fold of the cross validation\ncv_models_lm &lt;- cv_data %&gt;% \n  mutate(model = map(train, ~lm(formula = saleprice_log10 ~ ., data = .x)))\n\ncv_models_lm\n\n#  5-fold cross-validation \n# A tibble: 5 × 5\n  splits                id    train                  validate model \n  &lt;list&gt;                &lt;chr&gt; &lt;list&gt;                 &lt;list&gt;   &lt;list&gt;\n1 &lt;split [44812/11203]&gt; Fold1 &lt;tibble [44,812 × 12]&gt; &lt;tibble&gt; &lt;lm&gt;  \n2 &lt;split [44812/11203]&gt; Fold2 &lt;tibble [44,812 × 12]&gt; &lt;tibble&gt; &lt;lm&gt;  \n3 &lt;split [44812/11203]&gt; Fold3 &lt;tibble [44,812 × 12]&gt; &lt;tibble&gt; &lt;lm&gt;  \n4 &lt;split [44812/11203]&gt; Fold4 &lt;tibble [44,812 × 12]&gt; &lt;tibble&gt; &lt;lm&gt;  \n5 &lt;split [44812/11203]&gt; Fold5 &lt;tibble [44,812 × 12]&gt; &lt;tibble&gt; &lt;lm&gt;  \n\n#problem with factors split across training/validation\n#https://stats.stackexchange.com/questions/235764/new-factors-levels-not-present-in-training-data\n\nThis is where I begin to calculate metrics to judge how well my model is doing.\n\ncv_prep_lm &lt;- cv_models_lm %&gt;% \n  mutate(\n    #extract actual sale price for the records in the validate dataframes\n    validate_actual = map(validate, ~.x$saleprice_log10),\n    #predict response variable for each validate set using its corresponding model\n    validate_predicted = map2(.x = model, .y = validate, ~predict(.x, .y))\n  )\n\n#View data\ncv_prep_lm\n\n#  5-fold cross-validation \n# A tibble: 5 × 7\n  splits                id    train    validate model  validate_actual\n  &lt;list&gt;                &lt;chr&gt; &lt;list&gt;   &lt;list&gt;   &lt;list&gt; &lt;list&gt;         \n1 &lt;split [44812/11203]&gt; Fold1 &lt;tibble&gt; &lt;tibble&gt; &lt;lm&gt;   &lt;dbl [11,203]&gt; \n2 &lt;split [44812/11203]&gt; Fold2 &lt;tibble&gt; &lt;tibble&gt; &lt;lm&gt;   &lt;dbl [11,203]&gt; \n3 &lt;split [44812/11203]&gt; Fold3 &lt;tibble&gt; &lt;tibble&gt; &lt;lm&gt;   &lt;dbl [11,203]&gt; \n4 &lt;split [44812/11203]&gt; Fold4 &lt;tibble&gt; &lt;tibble&gt; &lt;lm&gt;   &lt;dbl [11,203]&gt; \n5 &lt;split [44812/11203]&gt; Fold5 &lt;tibble&gt; &lt;tibble&gt; &lt;lm&gt;   &lt;dbl [11,203]&gt; \n# ℹ 1 more variable: validate_predicted &lt;list&gt;\n\n\n\n#calculate fit metrics for each validate fold       \ncv_eval_lm &lt;- cv_prep_lm %&gt;% \n  mutate(validate_rmse = map2_dbl(model, validate, modelr::rmse),\n         validate_mae = map2_dbl(model, validate, modelr::mae))\n\ncv_eval_lm &lt;- cv_eval_lm %&gt;% \n  mutate(fit = map(model, ~glance(.x))) %&gt;% \n  unnest(fit)\n\n\n#view data\ncv_eval_lm %&gt;% \n  select(id, validate_mae, validate_rmse, adj.r.squared)\n\n# A tibble: 5 × 4\n  id    validate_mae validate_rmse adj.r.squared\n  &lt;chr&gt;        &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n1 Fold1        0.328         0.447         0.428\n2 Fold2        0.330         0.450         0.428\n3 Fold3        0.329         0.448         0.430\n4 Fold4        0.328         0.448         0.431\n5 Fold5        0.326         0.442         0.428\n\n\nFinally, this calculates how well the model did on the validation set.\n\n#summarize fit metrics on cross-validated dfs\ncv_eval_lm %&gt;% \n  select(validate_mae, validate_rmse, adj.r.squared) %&gt;% \n  summarize_all(mean)\n\n# A tibble: 1 × 3\n  validate_mae validate_rmse adj.r.squared\n         &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n1        0.328         0.447         0.429\n\n\n\n#fit model on full training set\ntrain_df &lt;- cv_data %&gt;% \n  select(train) %&gt;% \n  unnest(train)\n\nmodel_train &lt;- lm(formula = saleprice_log10 ~ ., data = train_df)\n\nmodel_train %&gt;% \n  glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df   logLik     AIC     BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     0.430         0.430 0.446     1941.       0    87 -137142. 274462. 275381.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nThis is the RMSE on the training set\n\n#calculate rmse on training set\nrmse(model_train, train_df)\n\n[1] 0.44626\n\n\nThis shows the impact each term of the model has on the response variable. This is for the training data.\n\n#visualize estimates for terms\nmodel_train %&gt;% \n  tidy() %&gt;% \n  filter(term != \"(Intercept)\") %&gt;% \n  mutate(term = fct_reorder(term, estimate)) %&gt;% \n  ggplot(aes(term, estimate)) +\n  geom_hline(yintercept = 0, linetype = 2, color = \"red\") +\n  geom_point() +\n  coord_flip()\n\n\n\n\n\n\n\n\nNext, I apply the model to the testing data to see how the model does out-of-sample.\n\n#create dfs for train_data_small and validate_data\n#train_data_small &lt;- cv_prep_lm %&gt;% \n#  unnest(train) %&gt;% \n#  select(-id)\n\nvalidate_df &lt;- cv_prep_lm %&gt;% \n  select(validate) %&gt;% \n  unnest()\n\nThis creates the augmented dataframe and plots the actual price vs. the fitted price.\n\n#visualize model on validate data\naugment_validate &lt;- augment(model_train, newdata = validate_df) %&gt;% \n  mutate(.resid = saleprice_log10 - .fitted)\n\n#actual vs. fitted\ncv_prep_lm %&gt;% \n  unnest(validate_actual, validate_predicted) %&gt;% \n  ggplot(aes(validate_actual, validate_predicted)) +\n  geom_abline() +\n  stat_density_2d(aes(fill = stat(level)), geom = \"polygon\") +\n  geom_smooth(method = \"lm\") +\n  scale_x_continuous(limits = c(2, 7)) +\n  scale_y_continuous(limits = c(2, 7)) +\n  coord_equal() +\n  scale_fill_viridis_c()\n\n\n\n\n\n\n\n\nThis distribution shows that the model overestimates the prices on many houses.\n\n#distribution of residuals\naugment_validate %&gt;% \n  ggplot(aes(.resid)) +\n  geom_density() +\n  geom_vline(xintercept = 0, color = \"red\", linetype = 2)\n\n\n\n\n\n\n\n\nThis shows that the residuals are correlated with the actual price, which indicates that the model is failing to account for some dynamic in the sale process.\n\n#sale price vs. residuals\naugment_validate %&gt;% \n  ggplot(aes(.resid, saleprice_log10)) +\n  stat_density_2d(aes(fill = stat(level)), geom = \"polygon\") +\n  geom_vline(xintercept = 0, color = \"red\", linetype = 2) +\n  scale_fill_viridis_c()\n\n\n\n\n\n\n\n\nThis calculates how well the model predicted sale price on out-of-sample testing data.\n\n#calculate fit of model on test data\nrmse(model_train, validate_df)\n\n[1] 0.44626\n\nmae(model_train, validate_df)\n\n[1] 0.32743\n\nrsquare(model_train, validate_df)\n\n[1] 0.42988"
  },
  {
    "objectID": "posts/exploring-allegheny-county-with-census-data/index.html",
    "href": "posts/exploring-allegheny-county-with-census-data/index.html",
    "title": "Exploring Allegheny County With Census Data",
    "section": "",
    "text": "This post explores Allegheny County and Pennsylvania through census data. I use the tidycensus and sf packages to collect data from the census API and draw maps with the data."
  },
  {
    "objectID": "posts/exploring-allegheny-county-with-census-data/index.html#setup",
    "href": "posts/exploring-allegheny-county-with-census-data/index.html#setup",
    "title": "Exploring Allegheny County With Census Data",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(tidycensus)\nlibrary(tigris)\nlibrary(sf)\nlibrary(broom)\nlibrary(ggfortify)\nlibrary(viridis)\nlibrary(scales)\nlibrary(janitor)\n\noptions(tigris_use_cache = TRUE)\n\ntheme_set(theme_bw())\n\ncensus_vars &lt;- load_variables(2010, \"sf1\", cache = TRUE)"
  },
  {
    "objectID": "posts/exploring-allegheny-county-with-census-data/index.html#collect-data",
    "href": "posts/exploring-allegheny-county-with-census-data/index.html#collect-data",
    "title": "Exploring Allegheny County With Census Data",
    "section": "Collect data",
    "text": "Collect data\ntidycensus provides a wrapper for the U.S. Census API. You can request a wide variety of data, from economic measures to information about demography. The API also includes data about geographic regions.\nThis code creates a dataframe of some of the variables available through the census API.\n\nvars &lt;- load_variables(2016, \"acs5\", cache = TRUE)\n\nvars\n\n# A tibble: 22,816 × 4\n   name        label                                  concept          geography\n   &lt;chr&gt;       &lt;chr&gt;                                  &lt;chr&gt;            &lt;chr&gt;    \n 1 B00001_001  Estimate!!Total                        UNWEIGHTED SAMP… block gr…\n 2 B00002_001  Estimate!!Total                        UNWEIGHTED SAMP… block gr…\n 3 B01001A_001 Estimate!!Total                        SEX BY AGE (WHI… tract    \n 4 B01001A_002 Estimate!!Total!!Male                  SEX BY AGE (WHI… tract    \n 5 B01001A_003 Estimate!!Total!!Male!!Under 5 years   SEX BY AGE (WHI… tract    \n 6 B01001A_004 Estimate!!Total!!Male!!5 to 9 years    SEX BY AGE (WHI… tract    \n 7 B01001A_005 Estimate!!Total!!Male!!10 to 14 years  SEX BY AGE (WHI… tract    \n 8 B01001A_006 Estimate!!Total!!Male!!15 to 17 years  SEX BY AGE (WHI… tract    \n 9 B01001A_007 Estimate!!Total!!Male!!18 and 19 years SEX BY AGE (WHI… tract    \n10 B01001A_008 Estimate!!Total!!Male!!20 to 24 years  SEX BY AGE (WHI… tract    \n# ℹ 22,806 more rows\n\n\nThis code requests information about the median income of census tracts in Allegheny County. The “geography” argument sets the level of geographic granularity.\n\nallegheny &lt;- get_acs(state = \"PA\", \n                     county = \"Allegheny County\", \n                     geography = \"tract\", \n                     variables = c(median_income = \"B19013_001\"),\n                     year = 2018,\n                     geometry = TRUE,\n                     cb = FALSE)\n\nhead(allegheny)\n\nSimple feature collection with 6 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -80.16148 ymin: 40.41478 xmax: -79.88377 ymax: 40.57546\nGeodetic CRS:  NAD83\n        GEOID                                              NAME      variable\n1 42003563200 Census Tract 5632, Allegheny County, Pennsylvania median_income\n2 42003980000 Census Tract 9800, Allegheny County, Pennsylvania median_income\n3 42003564100 Census Tract 5641, Allegheny County, Pennsylvania median_income\n4 42003461000 Census Tract 4610, Allegheny County, Pennsylvania median_income\n5 42003437000 Census Tract 4370, Allegheny County, Pennsylvania median_income\n6 42003981800 Census Tract 9818, Allegheny County, Pennsylvania median_income\n  estimate   moe                       geometry\n1    29750  8141 MULTIPOLYGON (((-80.00469 4...\n2       NA    NA MULTIPOLYGON (((-79.90168 4...\n3   145179 11268 MULTIPOLYGON (((-80.09943 4...\n4    39063  6923 MULTIPOLYGON (((-80.16148 4...\n5   106250 11871 MULTIPOLYGON (((-80.12246 4...\n6       NA    NA MULTIPOLYGON (((-79.90822 4...\n\n\nThis code maps the data onto the census tracts.\n\nallegheny %&gt;%\n  ggplot(aes(fill = estimate, color = estimate)) + \n  geom_sf() + \n  scale_fill_viridis(\"Median household income\", option = \"magma\", labels = comma) +\n  scale_color_viridis(\"Median household income\", option = \"magma\", labels = comma) +\n  labs(title = \"Allegheny County\",\n       subtitle = \"American Community Survey\") +\n  theme(axis.text = element_blank())\n\n\n\n\n\n\n\n\nThis code requests information about the ethnicities within each census tract. Then, it calculates the percentage of the entire population of a tract that each ethnicity makes up.\n\nracevars &lt;- c(White = \"P005003\", \n              Black = \"P005004\", \n              Asian = \"P005006\", \n              Hispanic = \"P004003\")\n\nget_decennial(geography = \"tract\", \n              variables = racevars,\n              state = \"PA\", \n              county = \"Allegheny\", \n              geometry = TRUE,\n              summary_var = \"P001001\",\n              year = 2010,) %&gt;% \n  mutate(value = value / summary_value,\n         variable = str_c(\"percent_\", tolower(variable))) -&gt; allegheny_race\n\nhead(allegheny_race)\n\nSimple feature collection with 6 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -80.12431 ymin: 40.54225 xmax: -79.99058 ymax: 40.61431\nGeodetic CRS:  NAD83\n# A tibble: 6 × 6\n  GEOID       NAME      variable   value summary_value                  geometry\n  &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt;      &lt;dbl&gt;         &lt;dbl&gt;        &lt;MULTIPOLYGON [°]&gt;\n1 42003412002 Census T… percent… 0.916            4865 (((-80.07936 40.58043, -…\n2 42003412002 Census T… percent… 0.00843          4865 (((-80.07936 40.58043, -…\n3 42003412002 Census T… percent… 0.0580           4865 (((-80.07936 40.58043, -…\n4 42003412002 Census T… percent… 0.0103           4865 (((-80.07936 40.58043, -…\n5 42003413100 Census T… percent… 0.878            6609 (((-80.06788 40.60846, -…\n6 42003413100 Census T… percent… 0.0172           6609 (((-80.06788 40.60846, -…\n\n\nThis code maps that data. The facet_wrap function creates a map for each ethnicity.\n\n#allegheny_race &lt;- st_erase(allegheny_race, allegheny_water)\n\nallegheny_race %&gt;%\n  ggplot(aes(fill = value, color = value)) +\n  facet_wrap(~variable) +\n  geom_sf() +\n  scale_fill_viridis(\"Percent\", option = \"magma\", labels = percent) +\n  scale_color_viridis(\"Percent\", option = \"magma\", labels = percent) +\n  theme(axis.text = element_blank())\n\n\n\n\n\n\n\n\nYou can also request data for an entire state. This code requests the median income for each census tract in Pennsylvania.\n\npa &lt;- get_acs(state = \"PA\",\n              geography = \"tract\", \n              variables = c(median_income = \"B19013_001\"), \n              year = 2018,\n              geometry = TRUE)\n\nhead(pa)\n\nSimple feature collection with 6 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -80.14905 ymin: 40.32178 xmax: -75.6175 ymax: 41.07083\nGeodetic CRS:  NAD83\n        GEOID                                           NAME      variable\n1 42019910500 Census Tract 9105, Butler County, Pennsylvania median_income\n2 42019912200 Census Tract 9122, Butler County, Pennsylvania median_income\n3 42021000100   Census Tract 1, Cambria County, Pennsylvania median_income\n4 42021012600 Census Tract 126, Cambria County, Pennsylvania median_income\n5 42025020700  Census Tract 207, Carbon County, Pennsylvania median_income\n6 42027010800  Census Tract 108, Centre County, Pennsylvania median_income\n  estimate   moe                       geometry\n1       NA    NA MULTIPOLYGON (((-80.04897 4...\n2    93446 11356 MULTIPOLYGON (((-80.14905 4...\n3    12907  1274 MULTIPOLYGON (((-78.92583 4...\n4    47143  9880 MULTIPOLYGON (((-78.73584 4...\n5    57939  4427 MULTIPOLYGON (((-75.71378 4...\n6    53569  4123 MULTIPOLYGON (((-77.55509 4...\n\n\n\npa %&gt;%\n  ggplot(aes(fill = estimate, color = estimate)) + \n  geom_sf() + \n  scale_fill_viridis(\"Estimated median income\", option = \"magma\", label = comma) + \n  scale_color_viridis(\"Estimated median income\", option = \"magma\", label = comma) +\n  theme(axis.text = element_blank())\n\n\n\n\n\n\n\n\nThis code requests ethnicity data for each tract in Pennsylvania.\n\nracevars &lt;- c(White = \"P005003\", \n              Black = \"P005004\", \n              Asian = \"P005006\", \n              Hispanic = \"P004003\")\n\nget_decennial(geography = \"tract\", \n              variables = racevars,\n              state = \"PA\",\n              year = 2010,\n              geometry = TRUE,\n              summary_var = \"P001001\") %&gt;% \n  mutate(value = value / summary_value,\n         variable = str_c(\"percent_\", tolower(variable))) -&gt; pa_race\n\nhead(pa_race)\n\nSimple feature collection with 6 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -80.12431 ymin: 40.54225 xmax: -79.99058 ymax: 40.61431\nGeodetic CRS:  NAD83\n# A tibble: 6 × 6\n  GEOID       NAME      variable   value summary_value                  geometry\n  &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt;      &lt;dbl&gt;         &lt;dbl&gt;        &lt;MULTIPOLYGON [°]&gt;\n1 42003412002 Census T… percent… 0.916            4865 (((-80.07936 40.58043, -…\n2 42003412002 Census T… percent… 0.00843          4865 (((-80.07936 40.58043, -…\n3 42003412002 Census T… percent… 0.0580           4865 (((-80.07936 40.58043, -…\n4 42003412002 Census T… percent… 0.0103           4865 (((-80.07936 40.58043, -…\n5 42003413100 Census T… percent… 0.878            6609 (((-80.06788 40.60846, -…\n6 42003413100 Census T… percent… 0.0172           6609 (((-80.06788 40.60846, -…\n\n\n\npa_race %&gt;%\n  ggplot(aes(fill = value, color = value)) +\n  facet_wrap(~variable) +\n  geom_sf() +\n  labs(title = \"Major ethncities in Pennsylvania\",\n       subtitle = \"Census data\") +\n  scale_fill_viridis(\"Percent\", option = \"magma\", label = percent) +\n  scale_color_viridis(\"Percent\", option = \"magma\", label = percent) +\n  theme(axis.text = element_blank())\n\n\n\n\n\n\n\n\nResources used:\n\nhttp://strimas.com/r/tidy-sf/\nhttps://walkerke.github.io/tidycensus/articles/spatial-data.html\nhttps://walkerke.github.io/tidycensus/index.html\nhttps://walkerke.github.io/2017/06/comparing-metros/"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/healthy_ride_access_pittsburgh/index.html",
    "href": "posts/healthy_ride_access_pittsburgh/index.html",
    "title": "Bike rental access in Pittsburgh",
    "section": "",
    "text": "This is an interactive Leaflet map of Healthy Ride access in Pittsburgh. It counts how many Healthy Ride stations are within a 10 minute bike ride of a given location. This gives an estimation of how accessible the Healthy Ride service is in a given neighborhood (lighter green = more accessible). As you zoom in, individual bike stations will appear. Click the “full screen” button on the left to maximize your view.\nThere are some obvious cases (like the Wabash Tunnel) where the API doesn’t know that a bicyclist shouldn’t go in there, but overall it is accurate.\nThis map was built with the Mapbox API and was inspired by the Penn MUSA Masterclass 2020 talk that Kyle Walker gave."
  },
  {
    "objectID": "posts/healthy_ride_access_pittsburgh/index.html#build-one-isochrone",
    "href": "posts/healthy_ride_access_pittsburgh/index.html#build-one-isochrone",
    "title": "Bike rental access in Pittsburgh",
    "section": "Build one isochrone",
    "text": "Build one isochrone\nThis takes the first station in the dataframe and uses the Mapbox API to make a test isochrone that shows how far a bicyclist can go in a given period of time (5, 10, 15 minutes).\n\ntest_isochrone_data &lt;- stations %&gt;% \n  slice(1) %&gt;% \n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326) %&gt;% \n  mb_isochrone(profile = \"cycling\", time = c(5, 10, 15))\n\ntest_isochrone_map &lt;- mapbox_map %&gt;% \n  mapdeck() %&gt;% \n  add_polygon(data = test_isochrone_data,\n              fill_colour = \"time\",\n              fill_opacity = 0.5,\n              legend = TRUE) %&gt;% \n  add_scatterplot(data = stations %&gt;% \n                          slice(1) %&gt;% \n                          st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326),\n                  radius = 100,\n                  fill_colour = \"#ffffff\") %&gt;% \n  mapdeck_view(location = c(pgh_coords[1], pgh_coords[2]), zoom = 11)\n\ntest_isochrone_map\n\n\n\n\n\nThe same graph can be made in ggplot2:\n\ntest_isochrone_data %&gt;% \n  ggplot() +\n  geom_sf(aes(fill = time)) +\n  scale_fill_viridis_c() +\n  theme_void()\n\n\n\n\n\n\n\n\nThis calculates the isochrones for all the stations and transforms them into a projected coordinate system:\n\nstation_isochrone &lt;- stations %&gt;%\n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326) %&gt;%\n  mb_isochrone(profile = \"cycling\", time = c(10)) %&gt;%\n  st_transform(3857)\n\nThis shows the overlap between all the isochrones. Interesting to look at, but not very informative.\n\nstation_isochrone %&gt;% \n  ggplot() +\n  geom_sf(fill = \"black\", lwd = 0, alpha = .05) +\n  theme_void()"
  },
  {
    "objectID": "posts/healthy_ride_access_pittsburgh/index.html#build-raster",
    "href": "posts/healthy_ride_access_pittsburgh/index.html#build-raster",
    "title": "Bike rental access in Pittsburgh",
    "section": "Build Raster",
    "text": "Build Raster\nThis builds a raster object that calculates how many isochrones overlap on a given area:\n\n#raster\npolygons_proj &lt;- station_isochrone %&gt;%\n  mutate(test_id = 1) %&gt;% \n  filter(time == 10) %&gt;% \n  st_transform(3857)\n\ntemplate &lt;- raster(polygons_proj, resolution = 25)\n\nraster_surface &lt;- fasterize(polygons_proj, template, field = \"test_id\", fun = \"sum\")\n\nraster_values &lt;- tibble(values = values(raster_surface)) %&gt;% \n  filter(!is.na(values)) %&gt;% \n  distinct(values) %&gt;% \n  pull(values)\n\nplot(raster_surface)"
  },
  {
    "objectID": "posts/healthy_ride_access_pittsburgh/index.html#build-interactive-map",
    "href": "posts/healthy_ride_access_pittsburgh/index.html#build-interactive-map",
    "title": "Bike rental access in Pittsburgh",
    "section": "Build interactive Map",
    "text": "Build interactive Map\nThis builds out the interactive map using leaflet and Mapbox libraries:\n\ncustom_pal &lt;- colorNumeric(\"viridis\", \n                           #0:max_bike_stations, \n                           raster_values,\n                           na.color = \"transparent\")\n\npopup_labels &lt;- sprintf(\"%s \n                        &lt;br&gt;Number of bike racks: %s\",\n                        stations$station_name, stations$number_of_racks) %&gt;% \n  map(htmltools::HTML)\n\nhealth_ride_icon &lt;- makeIcon(\n  iconUrl = \"https://healthyridepgh.com/wp-content/uploads/sites/3/2019/05/NEXTBIKE-LOGO-01.png\",\n  #iconUrl = \"https://healthyridepgh.com/wp-content/uploads/sites/3/2016/09/Healthy-Ride-Logo.Stacked-01.png\",\n  iconWidth = 50, iconHeight = 50,\n  iconAnchorX = 0, iconAnchorY = 0\n)\n\nstation_heatmap &lt;- mapbox_map %&gt;%\n  addPolygons(data = city_boundary,\n              opacity = 1,\n              color = \"black\",\n              fillColor = \"#ffffff\",\n              group = \"City boundary\") %&gt;% \n  addRasterImage(raster_surface, colors = custom_pal, opacity = .75,\n                 group = \"Raster\") %&gt;% \n  addLegend(pal = custom_pal, \n            values = raster_values,\n            title = \"Number of stations&lt;br&gt;within 10-minute bike ride\") %&gt;% \n  addMarkers(data = stations, lng = ~longitude, lat = ~latitude,\n             popup = popup_labels,\n             icon = health_ride_icon,\n             clusterOptions = markerClusterOptions(),\n             group = \"Stations\") %&gt;% \n  addLayersControl(overlayGroups = c(\"City boundary\", \"Raster\", \"Stations\"),\n                   options = layersControlOptions(collapsed = FALSE)) %&gt;% \n  addFullscreenControl() %&gt;% \n  setView(lng = pgh_coords[1], lat = pgh_coords[2], zoom = 12)\n\nframeWidget(station_heatmap, options=frameOptions(allowfullscreen = TRUE))"
  },
  {
    "objectID": "posts/mapping-boswash-commuter-patterns/index.html",
    "href": "posts/mapping-boswash-commuter-patterns/index.html",
    "title": "Mapping BosWash commuter patterns with Flowmap.blue",
    "section": "",
    "text": "This map shows the commuter patterns in the Northeast Megalopolis/Acela Corridor/BosWash metro area. I pulled the data from the Census Longitudinal Employer-Household Dynamics (LODES) system via the {lehdr} package. The map was created through the Flowmap.blue tool, which makes interactive maps of movement between areas. Flowmap.blue also exposes a bunch of cool features, like animating and clustering connections, among others.\nYou can view a full version of the map here."
  },
  {
    "objectID": "posts/mapping-boswash-commuter-patterns/index.html#code",
    "href": "posts/mapping-boswash-commuter-patterns/index.html#code",
    "title": "Mapping BosWash commuter patterns with Flowmap.blue",
    "section": "Code",
    "text": "Code\nThis code is what I used to query the LODES data and aggregate it. First, load the required libraries.\n\nlibrary(tidyverse)\nlibrary(tidycensus)\nlibrary(janitor)\nlibrary(lehdr)\nlibrary(tigris)\nlibrary(sf)\nlibrary(ggraph)\nlibrary(tidygraph)\nlibrary(googlesheets4)\nlibrary(googledrive)\n\noptions(tigris_use_cache = TRUE,\n        scipen = 999,\n        digits = 4)\n\nThis code gets the main and aux LODES data for each state that I name in the states object. I then combine the data into lodes_combined and check that there are no duplicate origin-destination pairs. Be warned that these files are large (100-500MB each), and can take a bit to read into R, depending on your machine.\n\n#get lodes\nstates &lt;- c(\"pa\", \"wv\", \"va\", \"dc\", \"de\",\n            \"md\", \"ny\", \"ri\", \"ct\", \"ma\", \"vt\", \"nh\", \"me\")\n\nlodes_od_main &lt;- grab_lodes(state = states, year = 2017, \n                            lodes_type = \"od\", job_type = \"JT00\", \n                            segment = \"S000\", state_part = \"main\", \n                            agg_geo = \"county\",\n                            use_cache = TRUE) %&gt;% \n  select(state, w_county, h_county, S000, year) %&gt;% \n  rename(commuters = S000)\n\nlodes_od_aux &lt;- grab_lodes(state = states, year = 2017, \n                           lodes_type = \"od\", job_type = \"JT00\", \n                           segment = \"S000\", state_part = \"aux\", \n                           agg_geo = \"county\",\n                           use_cache = TRUE) %&gt;% \n  select(state, w_county, h_county, S000, year) %&gt;% \n  rename(commuters = S000)\n\nlodes_combined &lt;- bind_rows(lodes_od_main, lodes_od_aux)\n\nThis code pulls the geometry for the states from the TIGER shapefile API:\n\ncounties_combined &lt;- tigris::counties(state = c(\"PA\", \"NY\", \"NJ\", \"MD\", \n                                                \"WV\", \"DE\", \"VA\", \n                                                \"DC\", \"MA\", \"CT\", \"VT\", \n                                                \"RI\", \"NH\", \"ME\"), \n                                      cb = TRUE) %&gt;% \n  arrange(STATEFP) %&gt;% \n  left_join(fips_codes %&gt;% distinct(state_code, state_name), by = c(\"STATEFP\" = \"state_code\"))\n\ncounties_combined %&gt;% \n  ggplot() +\n  geom_sf(aes(fill = state_name))\n\n\n\n\n\n\n\n\nThe next step is to calculate the centroid of each county that will be used in the final map.\n\nnode_pos &lt;- counties_combined %&gt;% \n  mutate(centroid = map(geometry, st_centroid),\n         x = map_dbl(centroid, 1),\n         y = map_dbl(centroid, 2)) %&gt;% \n  select(GEOID, NAME, x, y) %&gt;% \n  arrange(GEOID) %&gt;% \n  st_drop_geometry() %&gt;% \n  as_tibble() %&gt;% \n  select(-NAME) %&gt;%\n  rename(lon = x,\n         lat = y) %&gt;% \n  mutate(id = row_number()) %&gt;% \n  select(id, GEOID, lat, lon)\n\nThen I add the county and state name to the node positions so the name is intelligible.\n\nnode_pos &lt;- node_pos %&gt;% \n  left_join(st_drop_geometry(counties_combined), by = c(\"GEOID\" = \"GEOID\")) %&gt;% \n  mutate(county_name = str_c(NAME, \"County\", sep = \" \"),\n         name = str_c(county_name, state_name, sep = \", \"))\n\nnode_pos &lt;- node_pos %&gt;% \n  select(id, name, lat, lon, GEOID)\n\nnode_pos\n\n\n\nRows: 433\nColumns: 5\n$ id    &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 1…\n$ name  &lt;chr&gt; \"Fairfield County, Connecticut\", \"Hartford County, Connecticut\",…\n$ lat   &lt;dbl&gt; 41.27, 41.81, 41.79, 41.46, 41.41, 41.49, 41.86, 41.83, 39.09, 3…\n$ lon   &lt;dbl&gt; -73.39, -72.73, -73.25, -72.54, -72.93, -72.10, -72.34, -71.99, …\n$ GEOID &lt;chr&gt; \"09001\", \"09003\", \"09005\", \"09007\", \"09009\", \"09011\", \"09013\", \"…\n\n\nThis processes the LODES origin-destination data and creates the node-edge network graph object that will be fed into the Flowmap.blue service.\n\nnetwork_graph &lt;- lodes_combined %&gt;%\n  semi_join(counties_combined, by = c(\"w_county\" = \"GEOID\")) %&gt;%\n  semi_join(counties_combined, by = c(\"h_county\" = \"GEOID\")) %&gt;%\n  select(h_county, w_county, commuters) %&gt;% \n  as_tbl_graph(directed = TRUE) %&gt;% \n  activate(edges) %&gt;% \n  filter(commuters &gt;= 500,\n         #!edge_is_loop()\n  ) %&gt;%\n  activate(nodes) %&gt;%\n  arrange(name)\n\n\nnodes &lt;- network_graph %&gt;%\n  activate(nodes) %&gt;%\n  as_tibble()\n\nglimpse(nodes)\n\n\n\nRows: 433\nColumns: 1\n$ name &lt;chr&gt; \"09001\", \"09003\", \"09005\", \"09007\", \"09009\", \"09011\", \"09013\", \"0…\n\n\n\nedges &lt;- network_graph %&gt;% \n  activate(edges) %&gt;% \n  as_tibble() %&gt;% \n  rename(origin = from,\n         dest = to,\n         count = commuters) %&gt;% \n  arrange(desc(count))\n\nglimpse(edges)\n\n\n\nRows: 3,309\nColumns: 3\n$ origin &lt;dbl&gt; 128, 161, 121, 149, 61, 121, 138, 210, 112, 2, 127, 125, 138, 1…\n$ dest   &lt;dbl&gt; 128, 161, 128, 149, 61, 121, 128, 210, 112, 2, 127, 125, 138, 1…\n$ count  &lt;dbl&gt; 558308, 483764, 475073, 468885, 450975, 398535, 384941, 371075,…\n\n\nFinally, this code checks that the node position data matches up with the nodes from the network object. If these checks fail, the origin-destination pairs will be mapped to the wrong geographic coordinates.\n\n#check that nodes match up\nall(node_pos$GEOID == nodes$name)\n\n[1] TRUE\n\nidentical(node_pos$GEOID, nodes$name)\n\n[1] TRUE\n\nlength(node_pos$GEOID) == length(nodes$name)\n\n[1] TRUE\n\n\nThis code creates the metadata that Flowmap.blue requires and loads the data into Google Sheets.\n\nmy_properties &lt;- c(\n  \"title\"=\"BosWash regional US commuter flow\",\n  \"description\"=\"Miniumum 500 commuters per origin-destination pair\",\n  \"source.name\"=\"2017 US Census LODES\",\n  \"source.url\"=\"https://lehd.ces.census.gov/data/\",\n  \"createdBy.name\"=\"Conor Tompkins\",\n  \"createdBy.url\"=\"https://ctompkins.netlify.app/\",\n  \"mapbox.mapStyle\"=NA,\n  \"flows.sheets\" = \"flows\",\n  \"colors.scheme\"=\"interpolateViridis\",\n  \"colors.darkMode\"=\"yes\",\n  \"animate.flows\"=\"no\",\n  \"clustering\"=\"yes\"\n)\n\nproperties &lt;- tibble(property=names(my_properties)) %&gt;%\n  mutate(value=my_properties[property])\n\ngs4_auth()\n\ngoogledrive::drive_auth()\n\ndrive_trash(\"lodes_flowmapblue\")\n\nss &lt;- gs4_create(\"lodes_flowmapblue\", sheets = list(properties = properties,\n                                                    locations = node_pos,\n                                                    flows = edges))\n\nThe final step is to allow the Google Sheet to be read by anyone with the link, and copy the Sheet’s link to Flowmap.blue"
  },
  {
    "objectID": "posts/mapping-boswash-commuter-patterns/index.html#references",
    "href": "posts/mapping-boswash-commuter-patterns/index.html#references",
    "title": "Mapping BosWash commuter patterns with Flowmap.blue",
    "section": "References",
    "text": "References\n\nhttps://doodles.mountainmath.ca/blog/2020/01/06/flow-maps/\nhttps://jamgreen.github.io/lehdr/articles/getting_started.html"
  },
  {
    "objectID": "posts/cleaning-creatively-formatted-u-s-census-bureau-migration-data/index.html",
    "href": "posts/cleaning-creatively-formatted-u-s-census-bureau-migration-data/index.html",
    "title": "Cleaning Creatively Formatted US Census Bureau Migration Data",
    "section": "",
    "text": "I recently ran across the U.S. Census Bureau page for state-to-state migration flows. There’s a lot of interesting analyses that can be done with a dataset like this. Unfortunately, when I opened up the .xls file, I realized that the data is not formatted in a way that makes it easy to analyze.\n\nThe data is extremely wide and contains a header, merged cells, multiple layers of column names, duplicate rows, and a footer. I assume there is a good reason for this format, but it does not make it easy for analysts to use.\nThe readxl package makes working with Excel files and the attendant formatting issues very easy. It supports the legacy .xls format and has many useful functions that help you transform Excel sheets designed for presentation into data that is ready for analysis. The goal is to have from_state (where the migration flowed from), columns for summary statistics about the from_state, to_state (where the migration flowed to), and migration_amount (how many people migrated).\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(readxl)\nlibrary(janitor)\nlibrary(tidytext)\n\ntheme_set(theme_bw())\n\nThe first step is to read in the data with read_xls. I opened the .xls file to find the range that contained the data, and set Excel’s “N/A” value to be NA.\n\ndf &lt;- read_xls(\"post_data/State_to_State_Migrations_Table_2017.xls\", range = \"A7:DZ78\", na = \"N/A\")\n\n\nThe column headers are clearly malformed and there are rows of useless or blank values. The data is wide, and each from_state has its own column for Estimate and MOE (Margin of Error). For my purposes, I only want to keep the Estimate data for each from_state.\nNext, I identify the colums that contain the summary statistics I want to preserve. I identify the columns with their numeric index since the column names are mangled.\n\ndf &lt;- df %&gt;% \n    rename(state_from = 1,\n           state_population_same_one_year_plus = 2,\n           state_population_same_house_one_year = 4,\n           state_population_same_state_one_year = 6,\n           state_population_different_state_residence_one_year = 8)\n\n\nNext, I drop the columns that contain the MOEs and other duplicate columns. read_xls did a good job identifying those automatically, so I can just search for \"..\", the string that the function uses to name columns it didn’t find a name for.\n\ndf &lt;- df %&gt;% \n    select(-contains(\"..\")) %&gt;%\n    clean_names() %&gt;%\n    filter_all(any_vars(!str_detect(., \"Estimate\"))) %&gt;%\n    filter(!str_detect(state_from, \"residence\"))\n\n\nThis is much closer to a dataset that can be used for analysis. The remaining steps are pretty standard data munging activities. I turn the relevant columns from character to numeric type, filter out data I don’t need, and pivot the data from wide to long. I aso compute the total migration population by state.\n\ndf &lt;- df %&gt;%\n  mutate_at(vars(2:ncol(df)), as.numeric) %&gt;%\n  filter(!str_detect(state_from, \"United States\")) %&gt;%\n  pivot_longer(cols = 6:ncol(df), names_to = \"state_to\", values_to = \"migration\") %&gt;%\n  group_by(state_from) %&gt;%\n  mutate(total_migration_population = sum(migration, na.rm = TRUE)) %&gt;%\n  ungroup() \n\n\nFinally, I calculate what % of a from_state’s total outward migration goes to each to_state and clean up the to_state names.\n\ndf &lt;- df %&gt;% \n  mutate(pct_migrated = migration / total_migration_population,\n        state_to = str_replace_all(state_to, \"_\", \" \"),\n        state_to = str_to_title(state_to),\n        state_to = str_remove_all(state_to, \"[[:digit:]]\"))\n\nThis is the final dataset: \nI turned the above steps into a function called clean_census_migration_data. This lets me read in and clean the data in one line.\n\nclean_census_migration_data &lt;- function(data){\n  \n  message(str_c(\"Reading in:\", data, sep = \" \"))\n  \n  df &lt;- suppressMessages(read_xls(data, range = \"A7:DZ78\", na = \"N/A\")) %&gt;%\n    rename(state_from = 1,\n           state_population_same_one_year_plus = 2,\n           state_population_same_house_one_year = 4,\n           state_population_same_state_one_year = 6,\n           state_population_different_state_residence_one_year = 8) %&gt;%\n    select(-contains(\"..\")) %&gt;%\n    clean_names() %&gt;%\n    filter_all(any_vars(!str_detect(., \"Estimate\"))) %&gt;%\n    filter(!str_detect(state_from, \"residence\"))\n\n  message(str_c(\"Rows:\", nrow(df), sep = \" \"))\n  message(str_c(\"Columns:\", ncol(df), sep = \" \"))\n          \n\n  df &lt;- df %&gt;%\n    mutate_at(vars(2:ncol(df)), as.numeric) %&gt;%\n    filter(!str_detect(state_from, \"United States\")) %&gt;%\n    pivot_longer(cols = 6:ncol(df), names_to = \"state_to\", values_to = \"migration\") %&gt;%\n    group_by(state_from) %&gt;%\n    mutate(total_migration_population = sum(migration, na.rm = TRUE)) %&gt;%\n    ungroup() %&gt;%\n    mutate(pct_migrated = migration / total_migration_population,\n           state_to = str_replace_all(state_to, \"_\", \" \"),\n           state_to = str_to_title(state_to),\n           state_to = str_remove_all(state_to, \"[[:digit:]]\"),\n           state_to = str_replace(state_to, \" Of \", \" of \"))\n\n  return(df)\n}\n\nSince the Census Bureau provides a file for each year, I can map clean_census_migration_data to multiple files in a list and combine them. This allows me to compare the data longitudinally.\n\nmigration_files &lt;- list.files(\"post_data\", full.names = TRUE) %&gt;% \n  keep(str_detect(., \".xls$\"))\n\nmigration_files\n\n[1] \"post_data/state_to_state_migrations_table_2010.xls\"\n[2] \"post_data/state_to_state_migrations_table_2011.xls\"\n[3] \"post_data/state_to_state_migrations_table_2012.xls\"\n[4] \"post_data/state_to_state_migrations_table_2013.xls\"\n[5] \"post_data/State_to_State_Migrations_Table_2014.xls\"\n[6] \"post_data/State_to_State_Migrations_Table_2015.xls\"\n[7] \"post_data/State_to_State_Migrations_Table_2016.xls\"\n[8] \"post_data/State_to_State_Migrations_Table_2017.xls\"\n[9] \"post_data/State_to_State_Migrations_Table_2018.xls\"\n\n\n\ndf_migration &lt;- migration_files %&gt;% \n  set_names() %&gt;% \n  map_dfr(clean_census_migration_data, .id = \"file_name\") %&gt;% \n  mutate(year = str_extract(file_name, \"[[:digit:]]{4}\") %&gt;% as.numeric) %&gt;% \n  select(year, everything(), -file_name)\n\nReading in: post_data/state_to_state_migrations_table_2010.xls\n\n\nRows: 53\n\n\nColumns: 59\n\n\nReading in: post_data/state_to_state_migrations_table_2011.xls\n\n\nRows: 53\n\n\nColumns: 59\n\n\nReading in: post_data/state_to_state_migrations_table_2012.xls\n\n\nRows: 53\n\n\nColumns: 59\n\n\nReading in: post_data/state_to_state_migrations_table_2013.xls\n\n\nRows: 53\n\n\nColumns: 59\n\n\nReading in: post_data/State_to_State_Migrations_Table_2014.xls\n\n\nRows: 53\n\n\nColumns: 59\n\n\nReading in: post_data/State_to_State_Migrations_Table_2015.xls\n\n\nRows: 53\n\n\nColumns: 59\n\n\nReading in: post_data/State_to_State_Migrations_Table_2016.xls\n\n\nRows: 53\n\n\nColumns: 59\n\n\nReading in: post_data/State_to_State_Migrations_Table_2017.xls\n\n\nRows: 53\n\n\nColumns: 59\n\n\nReading in: post_data/State_to_State_Migrations_Table_2018.xls\n\n\nRows: 53\n\n\nColumns: 59\n\n\n\nNote: the above function will not work with the 2009 migration file because the Census Bureau did not include summary statistics about the from_state for that year. You can read in the 2009 file separately with a modified clean_census_migration_data function and join that back to the rest of the data."
  },
  {
    "objectID": "posts/riverhounds_lilley/index.html",
    "href": "posts/riverhounds_lilley/index.html",
    "title": "Pittsburgh Riverhounds under Coach Lilley",
    "section": "",
    "text": "I have been a season-ticket holder with the Pittsburgh Riverhounds for a couple seasons now. The stadium has a great fan experience, and the team has gotten a lot better over the past few years. A major part of that is the head coach, Bob Lilley. I will use some data from American Soccer Analysis to show how the Riverhounds have improved. Their website has an explainer on expected goals and other metrics they calculate.\nLoad libraries and configure settings:\n\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(hrbrthemes)\nlibrary(ggrepel)\n\ntheme_set(theme_ipsum(base_size = 18))\n\n#source https://app.americansocceranalysis.com/#!/\n\nI pulled a CSV of team-level goal metrics for the last 4 USL seasons from the ASA website. This shows the available data:\n\nusl &lt;- read_csv(\"post_data/american_soccer_analysis_uslc_xgoals_teams_2023-10-15.csv\") %&gt;% \n  clean_names() %&gt;% \n  select(-x1) %&gt;% \n  mutate(coach = case_when(team == \"PIT\" & season &gt;= 2018 ~ \"Lilley\",\n                           team == \"PIT\" & season &lt; 2018 ~ \"Brandt\",\n                           TRUE ~ NA_character_)) |&gt; \n  filter(season &lt; 2021)\n\nglimpse(usl)\n\nRows: 134\nColumns: 15\n$ team    &lt;chr&gt; \"PHX\", \"CIN\", \"RNO\", \"LOU\", \"HFD\", \"PIT\", \"SLC\", \"SA\", \"TBR\", …\n$ season  &lt;dbl&gt; 2019, 2018, 2020, 2020, 2020, 2020, 2017, 2020, 2020, 2020, 20…\n$ games   &lt;dbl&gt; 34, 34, 16, 16, 16, 16, 32, 16, 16, 16, 34, 15, 16, 34, 34, 34…\n$ sht_f   &lt;dbl&gt; 16.79, 12.68, 16.06, 15.06, 10.31, 10.81, 11.88, 14.81, 12.63,…\n$ sht_a   &lt;dbl&gt; 13.32, 15.15, 14.94, 9.56, 12.19, 7.81, 11.41, 12.44, 9.38, 13…\n$ gf      &lt;dbl&gt; 2.53, 2.06, 2.69, 1.75, 1.88, 2.38, 1.84, 1.88, 1.56, 2.75, 1.…\n$ ga      &lt;dbl&gt; 1.00, 0.97, 1.31, 0.75, 1.44, 0.63, 0.91, 0.75, 0.63, 1.19, 0.…\n$ gd      &lt;dbl&gt; 1.53, 1.09, 1.38, 1.00, 0.44, 1.75, 0.94, 1.13, 0.94, 1.56, 0.…\n$ x_gf    &lt;dbl&gt; 2.08, 1.43, 2.25, 1.48, 1.32, 1.69, 1.46, 1.58, 1.63, 2.39, 1.…\n$ x_ga    &lt;dbl&gt; 1.37, 1.26, 1.53, 1.01, 1.35, 0.94, 1.34, 1.17, 0.84, 1.30, 0.…\n$ x_gd    &lt;dbl&gt; 0.71, 0.17, 0.72, 0.47, -0.03, 0.75, 0.12, 0.42, 0.79, 1.10, 0…\n$ gd_x_gd &lt;dbl&gt; 0.82, 0.92, 0.65, 0.53, 0.46, 1.00, 0.81, 0.71, 0.15, 0.47, 0.…\n$ pts     &lt;dbl&gt; 2.29, 2.26, 2.25, 2.19, 2.19, 2.13, 2.09, 2.06, 2.06, 2.00, 2.…\n$ x_pts   &lt;dbl&gt; 1.80, 1.49, 1.83, 1.70, 1.39, 1.86, 1.44, 1.64, 1.85, 1.98, 1.…\n$ coach   &lt;chr&gt; NA, NA, NA, NA, NA, \"Lilley\", NA, NA, NA, NA, \"Lilley\", NA, NA…\n\n\nThe Riverhound’s statistics show clear improvement in 2018 when Lilley took over from Brandt. The team immediately began scoring more than they allowed. The team’s expected goals for and against also improved, which shows that the improvement wasn’t a matter of luck.\n\ngoal_data &lt;- usl %&gt;% \n  filter(team == \"PIT\") %&gt;% \n  select(team, season, gf, x_gf, ga, x_ga) %&gt;% \n  pivot_longer(cols = c(gf, x_gf, ga, x_ga), names_to = \"g_type\", values_to = \"g_value\") %&gt;%\n  mutate(goal_type = case_when(str_detect(g_type, \"gf$\") ~ \"For\",\n                               TRUE ~ \"Against\")) %&gt;% \n  mutate(metric_type = case_when(str_detect(g_type, \"^x_\") ~ \"Expected\",\n                                 TRUE ~ \"Actual\"))\n\ngoal_data %&gt;% \n  ggplot(aes(season, g_value, color = goal_type, lty = metric_type)) +\n  geom_line(size = 1.5) +\n  geom_point(data = filter(goal_data, metric_type == \"Actual\"), size = 2) +\n  labs(title = \"Pittsburgh Riverhounds\",\n       subtitle = \"Expected and Actual Goals per game\",\n       x = \"Season\",\n       y = \"Goals\",\n       color = \"Goal Type\",\n       lty = \"Metric Type\")\n\n\n\n\n\n\n\n\nThis shows that in terms of expected goal difference, the Riverhounds became one of the top teams in the USL once Lilley took over.\n\nusl %&gt;% \n  ggplot(aes(season, x_gd, group = team)) +\n  geom_hline(yintercept = 0, size = 1, lty = 2) +\n  geom_line(color = \"black\", alpha = .2) +\n  geom_line(data = filter(usl, team == \"PIT\"), \n            color = \"gold\", size = 2) +\n  geom_point(data = filter(usl, team == \"PIT\"),\n             aes(fill = coach),\n             shape = 21, size = 4) +\n  scale_fill_manual(values = c(\"grey\", \"gold\")) +\n  #coord_fixed(ratio = .5) +\n  labs(title = \"xG difference per game\",\n       x = \"Season\",\n       y = \"xG Difference\",\n       fill = \"Riverhounds Coach\",\n       caption = \"Grey lines show other USL teams\")\n\n\n\n\n\n\n\n\nLilley’s Riverhounds are consistently better than league average in terms of expected goals.\n\nusl %&gt;% \n  ggplot(aes(x_gd)) +\n  #geom_histogram(binwidth = .2) +\n  geom_vline(data = filter(usl, team == \"PIT\"), aes(xintercept = x_gd), size = 3) +\n  geom_vline(data = filter(usl, team == \"PIT\"), aes(xintercept = x_gd, color = coach),\n             size = 2.5, key_glyph = \"rect\") +\n  geom_density(aes(y = ..count.. * .2), fill = \"white\", alpha = 1) +\n  geom_vline(xintercept = 0, lty = 2) +\n  geom_hline(yintercept = 0) +\n  scale_color_manual(values = c(\"grey\", \"gold\")) +\n  scale_x_continuous(expand = c(0,0)) +\n  scale_y_continuous(expand = c(0,0)) +\n  coord_cartesian(ylim = c(0, 25)) +\n  #coord_fixed(ratio = .1) +\n  labs(title = \"xG Difference Per Game\",\n       subtitle = \"Distribution of all USL teams 2017-2020\",\n       x = \"xG\",\n       y = \"Number of teams\",\n       color = \"Riverhounds Coach\") +\n  theme(legend.key = element_rect(color = \"black\"))\n\n\n\n\n\n\n\n\nWhile the 2020 Riverhounds were a very good team, they were not quite as good as their plain goals for/against would show. This graph shows that they were fortunate to do as well as they did (which, again, was very well).\n\nusl %&gt;% \n  mutate(logo = case_when(team == \"PIT\" ~ \"post_data/pit_logo.png\",\n                          TRUE ~ NA_character_)) %&gt;% \n  ggplot(aes(x_gd, gd)) +\n  geom_abline(lty = 2) +\n  geom_point(alpha = .3) +\n  ggimage::geom_image(aes(image = logo)) +\n  geom_label_repel(data = filter(usl, team == \"PIT\"),\n                   aes(label = season, fill = coach),\n                   force = 5,\n                   key_glyph = \"rect\") +\n  annotate(\"text\", label = \"Under-performing\",\n           x = .75, y = -1.5) +\n  annotate(\"text\", label = \"Over-performing\",\n           x = -1, y = 1.5) +\n  tune::coord_obs_pred() +\n  scale_fill_manual(values = c(\"grey\", \"gold\")) +\n  labs(title = \"Goal and xG difference per game\",\n       x = \"xG Difference\",\n       y = \"Goal Difference\",\n       fill = \"Riverhounds Coach\") +\n  theme(legend.key = element_rect(color = \"black\"))\n\n\n\n\n\n\n\n\nThis shows that the 2020 Riverhounds were probably one of the most fortunate teams in the league, in addition to being very good.\n\nusl %&gt;% \n  ggplot(aes(season, gd_x_gd, group = team)) +\n  geom_hline(yintercept = 0, lty = 2) +\n  geom_line(color = \"black\", alpha = .2) +\n  geom_line(data = filter(usl, team == \"PIT\"),\n            color = \"gold\", size = 2) +\n  geom_point(data = filter(usl, team == \"PIT\"),\n             aes(fill = coach, group = team),\n             shape = 21, size = 4, color = \"black\") +\n  scale_fill_manual(values = c(\"grey\", \"gold\")) +\n  coord_cartesian(ylim = c(-1.5, 1.5)) +\n  #coord_fixed(ratio = .5) +\n  labs(title = \"Goal difference - xG difference\",\n       subtitle = \"Per game\",\n       x = \"Season\",\n       y = substitute(paste(\"\" %&lt;-% \"\", \"Under-performing\", \"  |  \", \"Over-performing\", \"\" %-&gt;% \"\")),\n       fill = \"Riverhounds Coach\",\n       caption = \"Grey lines show other USL teams\")\n\n\n\n\n\n\n\n\nIn FiveThirtyEights’ Global Soccer Power Index, the Riverhounds will begin the 2021 season ranked around #460 out of 639 teams."
  },
  {
    "objectID": "posts/working-with-paycheck-protection-program-data-in-r/index.html",
    "href": "posts/working-with-paycheck-protection-program-data-in-r/index.html",
    "title": "Working with Paycheck Protection Program data in R",
    "section": "",
    "text": "In this post, I walk through the process of reading in the data from the Paycheck Protection Program, and show some basic analyses that can be done.\nThis post was migrated to Quarto in April 2024. The SBA significantly restructured how the data is structured between 2020-2024, so some of the older analyis is no longer applicable.\nLoad packages and set up environment:\nlibrary(tidyverse)\nlibrary(vroom)\nlibrary(sf)\nlibrary(tigris)\nlibrary(lubridate)\nlibrary(janitor)\nlibrary(hrbrthemes)\nlibrary(scales)\n\ntheme_set(theme_ipsum(base_size = 18))\n\noptions(scipen = 999, digits = 4, tigris_use_cache = TRUE)"
  },
  {
    "objectID": "posts/working-with-paycheck-protection-program-data-in-r/index.html#load-and-clean-data",
    "href": "posts/working-with-paycheck-protection-program-data-in-r/index.html#load-and-clean-data",
    "title": "Working with Paycheck Protection Program data in R",
    "section": "Load and clean data",
    "text": "Load and clean data\nThe data is available at this site: https://data.sba.gov/dataset/ppp-foia\nThe data exists in numerous CSV files, separated by state and sometimes loan amount.\nThis code chunk identifies all the CSV files in the folder, reads them in, and combines them.\n\npath &lt;- \"posts/working-with-paycheck-protection-program-data-in-r/post_data/ppp\"\n\n#find all files in the folder that end in .csv\nppp_files &lt;- list.files(path, full.names = TRUE, recursive = TRUE) %&gt;% \n  keep(str_detect(., \".csv$\")) %&gt;% \n  #read each file with read_csv and combine them\n  set_names()\n\nppp_files |&gt; \n  pluck(1) |&gt; \n  read_csv(n_max = 100) |&gt; \n  glimpse()\n\nread_and_filter &lt;- function(x){\n  \n  print(x)\n  \n  read_csv(x, col_types = cols(.default = \"c\")) |&gt; \n    filter(mdy(DateApproved) &lt; \"2020-07-01\") |&gt; \n    clean_names()\n  \n}\n\nppp_data &lt;- ppp_files |&gt; \n  map(read_and_filter)\n\nppp_data |&gt;\n  pluck(1)\n\nglimpse(ppp_data)\n\nlist_rbind(ppp_data, names_to = \"file_name\") |&gt; \n  write_csv(\"posts/working-with-paycheck-protection-program-data-in-r/post_data/combined_ppp.csv\")\n\n\nppp_combined &lt;- vroom(\"post_data/combined_ppp.csv\", delim = \",\") %&gt;%\n  mutate(date_approved = mdy(date_approved)) |&gt; \n  mutate(loan_range = cut(current_approval_amount,\n                          breaks = c(0, 150000, 350000, 1000000, 2000000, 5000000, 10000000),\n                          labels = c(\"Under $150k\", \"$150k-$350k\", \"$350k-$1 million\", \"$1 million-$2 million\", \"$2 million-$5 million\", \"$5 million-$10 million\"))) |&gt; \n  separate(cd, into = c(\"state_2\", \"district\"), remove = FALSE)\n\nThis changes how the columns are interpreted, and separates the congressional district column into state and district. If the district value is blank, I replace it with NA.\nThere are cases where the value in the state column doesn’t match the state embedded in the congressional district cd column.\n\n#preview data where state doesn't match congressional district\nppp_combined %&gt;% \n  count(file_name, project_state, cd, state_2, sort = TRUE) %&gt;% \n  mutate(flag_match = project_state == state_2) %&gt;% \n  filter(flag_match == FALSE) %&gt;% \n  slice(1:5)\n\n# A tibble: 5 × 6\n  file_name                         project_state cd    state_2     n flag_match\n  &lt;chr&gt;                             &lt;chr&gt;         &lt;chr&gt; &lt;chr&gt;   &lt;int&gt; &lt;lgl&gt;     \n1 posts/working-with-paycheck-prot… MI            MT-02 MT         18 FALSE     \n2 posts/working-with-paycheck-prot… MN            MT-02 MT         15 FALSE     \n3 posts/working-with-paycheck-prot… NE            SD-   SD          4 FALSE     \n4 posts/working-with-paycheck-prot… NY            NJ-05 NJ          4 FALSE     \n5 posts/working-with-paycheck-prot… TX            NM-03 NM          3 FALSE     \n\n\nThere are only 423 rows where this occurs, and it is less than 1% of the dataset.\n\n#summarize cases where mismatch occurs\nppp_combined %&gt;% \n  mutate(flag_match = project_state == state_2) %&gt;% \n  count(flag_match) %&gt;% \n  mutate(pct_mismatch = n / sum(n),\n         pct_mismatch = round(pct_mismatch, 4)) %&gt;% \n  filter(flag_match == FALSE) %&gt;% \n  arrange(-pct_mismatch)\n\n# A tibble: 1 × 3\n  flag_match     n pct_mismatch\n  &lt;lgl&gt;      &lt;int&gt;        &lt;dbl&gt;\n1 FALSE        423       0.0001\n\n\nI filter out the rows where the state doesn’t match the congressional district.\n\nppp_combined |&gt; \n  filter(project_state != state_2) |&gt; \n  distinct(project_state, state_2)\n\n# A tibble: 236 × 2\n   project_state state_2\n   &lt;chr&gt;         &lt;chr&gt;  \n 1 AL            WA     \n 2 AL            FL     \n 3 AZ            TX     \n 4 CA            ME     \n 5 CA            FL     \n 6 CA            AP     \n 7 CT            NJ     \n 8 FL            TX     \n 9 FL            WV     \n10 FL            CA     \n# ℹ 226 more rows\n\nppp_combined &lt;- ppp_combined %&gt;% \n  filter(project_state == state_2)\n\nThis shows that there are some negative values in loan_amount. I filter out those values.\n\nppp_combined %&gt;% \n  mutate(loan_type = current_approval_amount &gt; 0) %&gt;% \n  count(loan_type)\n\n# A tibble: 2 × 2\n  loan_type       n\n  &lt;lgl&gt;       &lt;int&gt;\n1 FALSE           1\n2 TRUE      4811540\n\nppp_combined &lt;- ppp_combined %&gt;% \n  filter(current_approval_amount &gt; 0, !is.na(current_approval_amount))"
  },
  {
    "objectID": "posts/working-with-paycheck-protection-program-data-in-r/index.html#analysis",
    "href": "posts/working-with-paycheck-protection-program-data-in-r/index.html#analysis",
    "title": "Working with Paycheck Protection Program data in R",
    "section": "Analysis",
    "text": "Analysis\n\nLoan amount\nThe first step is to split the data into 2 buckets. For loans above $150k, the SBA binned the loan amount instead of reporting the actual value.\n\nppp_under_150 &lt;- ppp_combined %&gt;% \n  filter(!str_detect(file_name, \"150k_plus\"))\n\nppp_over_150 &lt;- ppp_combined %&gt;% \n  filter(str_detect(file_name, \"150k_plus\"))\n\nAmong loans less than 150k, most are less than 21k, and the distribution is very heavily skewed to the right.\n\nquantiles &lt;- ppp_under_150 %&gt;% \n  reframe(quantiles = quantile(current_approval_amount, probs = c(.25, .50, .75)), \n            probability = c(\"25th\", \"50th\", \"75th\")) %&gt;% \n  mutate(probability = as.factor(probability))\n\nppp_under_150 %&gt;% \n  ggplot(aes(current_approval_amount)) +\n  geom_histogram() +\n  geom_vline(data = quantiles, aes(xintercept = quantiles, color = probability)) +\n  scale_y_comma() +\n  scale_x_comma(labels = scales::dollar) +\n  labs(title = \"SBA PPP Loans\",\n       subtitle = \"Among loans less than $150k\",\n       x = \"Loan amount\",\n       y = \"Number of loans\",\n       color = \"Quantile\")\n\n\n\n\n\n\n\n\nLoans under $150k make up the vast majority of all PPP loans.\n\nppp_combined |&gt; \n  count(loan_range) %&gt;%   \n  mutate(loan_range = fct_reorder(loan_range, n)) %&gt;% \n  ggplot(aes(n, loan_range)) +\n  geom_col() +\n  labs(title = \"SBA PPP Loans\",\n       subtitle = \"All loans\",\n       x = \"Number of loans\",\n       y = \"Loan range\") +\n  scale_x_comma()\n\n\n\n\n\n\n\n\nLoan approvals peaked in late April 2020 when the program began accepting applications for the second round, and picked up in July. There is extreme weekday-weekend seasonality in the data after April. The “U” shape from May to July generally coincides with the effort to “reopen” economies nationwide.\n\nppp_combined %&gt;% \n  count(date_approved) %&gt;% \n  ggplot(aes(date_approved, n)) +\n  geom_point() +\n  geom_vline(xintercept = ymd(\"2020-04-16\"), linetype = 2) +\n  labs(title = \"SBA PPP Loans\",\n       x = NULL,\n       y = \"Number of loans\") +\n  scale_y_comma()\n\n\n\n\n\n\n\n\nThis shows that bigger loans tended to be approved earlier in the program, which was a major criticism.\n\nppp_combined %&gt;% \n  count(date_approved, loan_range) %&gt;% \n  mutate(loan_range = fct_reorder(loan_range, n) %&gt;% fct_rev) %&gt;% \n  group_by(date_approved) %&gt;% \n  mutate(pct_of_loans = n / sum(n)) %&gt;% \n  ungroup() %&gt;% \n  ggplot(aes(date_approved, pct_of_loans, color = loan_range)) +\n  geom_line() +\n  scale_x_date(expand = c(0,0)) +\n  scale_y_percent(limits = c(0,1.1)) +\n  scale_color_viridis_d() +\n  labs(title = \"SBA PPP Loans\",\n       subtitle = \"% of loans approved on a date\",\n       x = NULL,\n       y = \"% of loans\",\n       fill = \"Loan range\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nMost states received similar proportions of small and large loans. Territories received more small loans and fewer large loans.\n\nppp_combined %&gt;% \n  mutate(loan_range = fct_inorder(loan_range)) %&gt;% \n  count(project_state, loan_range) %&gt;% \n  group_by(project_state) %&gt;% \n  mutate(pct_of_loans = n / sum(n)) %&gt;% \n  ungroup() %&gt;% \n  filter(!is.na(project_state)) %&gt;% \n  mutate(project_state = fct_reorder(project_state, n, .fun = sum)) %&gt;% \n  ggplot(aes(pct_of_loans, project_state, fill = loan_range)) +\n  geom_col() +\n  scale_fill_viridis_d(direction = -1) +\n  scale_x_percent() +\n  labs(title = \"SBA PPP Loans\",\n       subtitle = \"% of loan range by state\",\n       x = \"% of loans\",\n       y = NULL,\n       fill = \"Loan range\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\nJobs\nBigger loans tended to retain more jobs.\n\nppp_combined %&gt;% \n  filter(!is.na(jobs_reported)) %&gt;% \n  mutate(loan_range = fct_inorder(loan_range) %&gt;% fct_rev()) %&gt;% \n  ggplot(aes(jobs_reported, fill = loan_range)) +\n  #geom_histogram(bins = 200) +\n  geom_density() +\n  facet_wrap(~loan_range, ncol = 2, scales = \"free\") +\n  scale_x_comma() +\n  #scale_y_comma() +\n  scale_fill_viridis_d() +\n  labs(title = \"SBA PPP Loan Program\",\n       subtitle = \"Jobs retained\",\n       x = \"Number of jobs retained\",\n       y = \"Density\",\n       fill = \"Loan range\")\n\n\n\n\n\n\n\n\nThe timeline of jobs_reported closely matches the trend of when loans were approved.\n\nppp_combined %&gt;% \n  filter(!is.na(jobs_reported)) %&gt;% \n  group_by(date_approved) %&gt;% \n  summarize(jobs_reported = sum(jobs_reported)) %&gt;% \n  ungroup() %&gt;% \n  ggplot(aes(date_approved, jobs_reported)) +\n  geom_point() +\n  scale_y_comma() +\n  labs(title = \"SBA PPP Loan Program\",\n       subtitle = \"Jobs Retained\",\n       x = NULL,\n       y = \"Jobs retained\")\n\n\n\n\n\n\n\n\nAmong loans less than 150k and where the data was reported, the median loan retained 7 jobs per 50k spent. Systemic reporting bias probably makes this number less reliable.\n\nppp_combined %&gt;% \n  filter(loan_range == \"Under $150k\",\n         !is.na(jobs_reported),\n         !is.na(current_approval_amount)) %&gt;% \n  select(jobs_reported, current_approval_amount) %&gt;% \n  mutate(jobs_retained_per_50k = (jobs_reported / current_approval_amount) * 50000) %&gt;% \n  summarize(jobs_retained_per_50k = median(jobs_retained_per_50k))\n\n# A tibble: 1 × 1\n  jobs_retained_per_50k\n                  &lt;dbl&gt;\n1                  7.11\n\n\nIn the same loan range, bigger loans generally meant more jobs retained.\n\ntest &lt;- ppp_combined %&gt;% \n  filter(loan_range == \"Under $150k\") %&gt;% \n  count(current_approval_amount, jobs_reported, sort = TRUE) %&gt;%\n  slice(1:5000)\n\ntest %&gt;% \n  ggplot(aes(current_approval_amount, jobs_reported)) +\n  geom_density_2d_filled() +\n  scale_x_comma(labels = scales::dollar) +\n  labs(title = \"SBA PPP Loan Program\",\n       subtitle = NULL,\n       x = \"Loan amount\",\n       y = \"Jobs retained\",\n       fill = \"Density of observations\")\n\n\n\n\n\n\n\n\n\n\nMapping the data\nThe dataset identifies which federal congressional district the applicant is from. I use tigris to retrieve the shapefiles for the most recent districts.\n\ncongress_districts &lt;- suppressMessages(congressional_districts(cb = TRUE, resolution = \"500k\", year = 2020)) %&gt;% \n  st_as_sf() %&gt;% \n  clean_names() %&gt;% \n  filter(statefp == 42)\n\nThis counts how many of each loan_range a district received. Note that there are missing values and what appear to be defunct district IDs in the district column.\n\nppp_pa_districts_loan_range &lt;- ppp_combined %&gt;% \n  filter(project_state == \"PA\") %&gt;% \n  count(cd, loan_range, sort = TRUE)\n\nppp_pa_districts_loan_range %&gt;% \n  mutate(cd = fct_explicit_na(cd),\n         cd = fct_reorder(cd, n, .fun = sum)) %&gt;% \n  ggplot(aes(n, cd, fill = loan_range)) +\n  geom_col() +\n  scale_x_comma() +\n  scale_fill_viridis_d() +\n  labs(title = \"SBA PPP Loan Program\",\n       subtitle = \"Pennsylvania\",\n       x = \"Number of loans\",\n       y = \"Congressional District\",\n       fill =  \"Loan range\")\n\n\n\n\n\n\n\n\nDistricts in eastern Pennsylvania near Philadelphia received more loans from the program.\n\nppp_pa_districts &lt;- ppp_combined %&gt;% \n  filter(project_state == \"PA\") %&gt;% \n  count(district, sort = TRUE)\n\ncongress_districts %&gt;% \n  right_join(ppp_pa_districts, by = c(\"cd116fp\" = \"district\")) %&gt;% \n  ggplot() +\n  geom_sf(aes(fill = n)) +\n  scale_fill_viridis_c(option = \"A\", labels = scales::comma) +\n  labs(title = \"SBA PPP Loan Program\",\n       subtitle = \"Pennsylvania\",\n       fill = \"Number of loans\") +\n  theme_void()"
  },
  {
    "objectID": "posts/graphing-allegheny-county-covid-19-data/index.html",
    "href": "posts/graphing-allegheny-county-covid-19-data/index.html",
    "title": "Graphing Allegheny County COVID-19 data",
    "section": "",
    "text": "In this post, I review the process I use to make daily graphs from data published by the Allegheny County Health Department. I use the data posted by Franklin Chen, who scrapes the data from the County’s email updates.\nFirst, load the required packages and set up the environment.\n\n#load libraries\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(tidyquant)\nlibrary(hrbrthemes)\n\n#settings\ntheme_set(theme_ipsum(base_size = 15, strip_text_size = 15, axis_title_size = 15))\n\noptions(scipen = 999, digits = 4)\n\nThis reads in the raw data from the GitHub repository.\n\n#read in data\ndf &lt;- read_csv(\"https://raw.githubusercontent.com/FranklinChen/covid-19-allegheny-county/master/covid-19-allegheny-county.csv\") %&gt;% \n  mutate(state = \"Pennsylvania\",\n         county = \"Allegheny County\") %&gt;% \n  filter(date &lt; \"2020-07-08\")\n\nglimpse(df)\n\nOn July 7th, the County added deaths that occurred between April and June, but marked them as reported on July 7th. For the purposes of visualization, I remove those deaths.\n\n#remove deaths from July 7th\n#The deaths reported today are from the state’s use of the Electronic Data Reporting System (EDRS) and include #deaths from April 5 – June 13, all decedents were 65 or older.\n#https://twitter.com/HealthAllegheny/status/1280517051589722117?s=20\n\ndf &lt;- df %&gt;% \n  mutate(deaths = case_when(date == \"2020-07-07\" ~ NA_real_,\n                            date != \"2020-07-07\" ~ deaths))\n\nThis calculates new cases, hospitalizations, and deaths.\n\ndf &lt;- df %&gt;% \n  mutate(cases_new = cases - lag(cases),\n         hospitalizations_new = hospitalizations - lag(hospitalizations),\n         deaths_new = deaths - lag(deaths))\n\nThere are instances where the number of cumulative hospitalizations or deaths decreases.\n\ndf %&gt;% \n  mutate(hospitalizations_lag = lag(hospitalizations)) %&gt;% \n  select(date, date, hospitalizations, hospitalizations_lag, hospitalizations_new) %&gt;% \n  filter(hospitalizations_new &lt; 0)\n\n# A tibble: 3 × 4\n  date       hospitalizations hospitalizations_lag hospitalizations_new\n  &lt;date&gt;                &lt;dbl&gt;                &lt;dbl&gt;                &lt;dbl&gt;\n1 2020-04-27              213                  214                   -1\n2 2020-05-01              235                  236                   -1\n3 2020-05-14              283                  285                   -2\n\n\n\ndf %&gt;% \n  mutate(deaths_lag = lag(deaths)) %&gt;% \n  select(date, date, deaths, deaths_lag, deaths_new) %&gt;% \n  filter(deaths_new &lt; 0)\n\n# A tibble: 2 × 4\n  date       deaths deaths_lag deaths_new\n  &lt;date&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 2020-04-23     69         74         -5\n2 2020-04-29     86         87         -1\n\n\nIn these cases, I remove the values and do not replace them.\n\n#when new cases/hospitalizations/deaths is negative, replace with NA\ndf &lt;- df %&gt;% \n  mutate(cases_new = case_when(cases_new &lt; 0 ~ NA_real_,\n                               cases_new &gt;= 0 ~ cases_new),\n         hospitalizations_new = case_when(hospitalizations_new &lt; 0 ~ NA_real_,\n                                          hospitalizations_new &gt;= 0 ~ hospitalizations_new),\n         deaths_new = case_when(deaths_new &lt; 0 ~ NA_real_,\n                                deaths_new &gt;= 0 ~ deaths_new))\n\nThis calculates rolling 14-day averages for new cases/hospitalizations/deaths.\n\n#calculate rolling 14 day averages for cases/hospitalizations/deaths\ndf &lt;- df %&gt;% \n  tq_mutate(\n    # tq_mutate args\n    select     = cases_new,\n    mutate_fun = rollapply, \n    # rollapply args\n    width      = 14,\n    align      = \"right\",\n    FUN        = mean,\n    # mean args\n    na.rm      = TRUE,\n    # tq_mutate args\n    col_rename = \"cases_new_rolling_14\"\n  ) %&gt;% \n  tq_mutate(\n    # tq_mutate args\n    select     = hospitalizations_new,\n    mutate_fun = rollapply, \n    # rollapply args\n    width      = 14,\n    align      = \"right\",\n    FUN        = mean,\n    # mean args\n    na.rm      = TRUE,\n    # tq_mutate args\n    col_rename = \"hospitalizations_new_rolling_14\"\n  ) %&gt;% \n  tq_mutate(\n    # tq_mutate args\n    select     = deaths_new,\n    mutate_fun = rollapply, \n    # rollapply args\n    width      = 14,\n    align      = \"right\",\n    FUN        = mean,\n    # mean args\n    na.rm      = TRUE,\n    # tq_mutate args\n    col_rename = \"deaths_new_rolling_14\"\n  ) %&gt;% \n  select(state, county, date, contains(\"_new\"), contains(\"rolling\"))\n\nglimpse(df)\n\nRows: 126\nColumns: 9\n$ state                           &lt;chr&gt; \"Pennsylvania\", \"Pennsylvania\", \"Penns…\n$ county                          &lt;chr&gt; \"Allegheny County\", \"Allegheny County\"…\n$ date                            &lt;date&gt; 2020-03-04, 2020-03-05, 2020-03-06, 2…\n$ cases_new                       &lt;dbl&gt; NA, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2…\n$ hospitalizations_new            &lt;dbl&gt; NA, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ deaths_new                      &lt;dbl&gt; NA, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ cases_new_rolling_14            &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ hospitalizations_new_rolling_14 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ deaths_new_rolling_14           &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n\n\nTo prepare the data for visualization in ggplot2, I pivot the rolling averages longer and move them into a separate table.\n\n#pivot rolling average data longer\ndf_rolling &lt;- df %&gt;% \n  select(state, county, date, contains(\"rolling\")) %&gt;% \n  pivot_longer(cols = contains(\"rolling\"), names_to = \"metric\") %&gt;% \n  mutate(metric = case_when(str_detect(metric, \"cases\") ~ \"New cases\",\n                            str_detect(metric, \"deaths\") ~ \"New deaths\",\n                            str_detect(metric, \"hospitalizations\") ~ \"New hospitalizations\")) %&gt;% \n  mutate(metric = factor(metric, levels = c(\"New cases\", \"New hospitalizations\", \"New deaths\")))\n\nglimpse(df_rolling)\n\nRows: 378\nColumns: 5\n$ state  &lt;chr&gt; \"Pennsylvania\", \"Pennsylvania\", \"Pennsylvania\", \"Pennsylvania\",…\n$ county &lt;chr&gt; \"Allegheny County\", \"Allegheny County\", \"Allegheny County\", \"Al…\n$ date   &lt;date&gt; 2020-03-04, 2020-03-04, 2020-03-04, 2020-03-05, 2020-03-05, 20…\n$ metric &lt;fct&gt; New cases, New hospitalizations, New deaths, New cases, New hos…\n$ value  &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n\n\nI do the same for the daily data.\n\n#pivot daily data longer\ndf_new &lt;- df %&gt;% \n  select(state, county, date, !contains(\"rolling\")) %&gt;% \n  pivot_longer(cols = contains(\"_new\"), names_to = \"metric\") %&gt;% \n  mutate(metric = case_when(str_detect(metric, \"cases\") ~ \"New cases\",\n                            str_detect(metric, \"deaths\") ~ \"New deaths\",\n                            str_detect(metric, \"hospitalizations\") ~ \"New hospitalizations\")) %&gt;% \n  mutate(metric = factor(metric, levels = c(\"New cases\", \"New hospitalizations\", \"New deaths\")))\n\nglimpse(df_new)\n\nRows: 378\nColumns: 5\n$ state  &lt;chr&gt; \"Pennsylvania\", \"Pennsylvania\", \"Pennsylvania\", \"Pennsylvania\",…\n$ county &lt;chr&gt; \"Allegheny County\", \"Allegheny County\", \"Allegheny County\", \"Al…\n$ date   &lt;date&gt; 2020-03-04, 2020-03-04, 2020-03-04, 2020-03-05, 2020-03-05, 20…\n$ metric &lt;fct&gt; New cases, New hospitalizations, New deaths, New cases, New hos…\n$ value  &lt;dbl&gt; NA, NA, NA, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n\n\nIn the daily data, I remove rows where the date is before the first non-zero value of that metric.\n\n#identify first non-zero value in each metric.\n##filter out rows before first non-zero value\ndf_new &lt;- df_new %&gt;% \n  arrange(state, county, metric, date) %&gt;% \n  group_by(state, county, metric) %&gt;% \n  filter(row_number() != 1) %&gt;% \n  mutate(first_non_zero_value = cumsum(coalesce(value, 0) &gt; 0) &gt;= 1) %&gt;% \n  ungroup() %&gt;% \n  filter(first_non_zero_value == TRUE)\n\nThese graphs show the rolling and daily new data:\n\n#preview rolling data\ndf_rolling %&gt;% \n  ggplot(aes(date, value)) +\n  geom_line() +\n  facet_wrap(~metric, ncol = 1, scales = \"free_y\")\n\n\n\n\n\n\n\n\n\n#preview daily data\ndf_new %&gt;% \n  ggplot(aes(date, value)) +\n  geom_point() +\n  facet_wrap(~metric, ncol = 1, scales = \"free_y\")\n\n\n\n\n\n\n\n\nThis finds the most recent date in the data, which I insert into the final graph.\n\n#find most recent date\nlast_updated &lt;- last(df_rolling$date)\n\nThis creates the final graph. Since I pivoted the data longer, I can facet by metric, which lets me show cases/hospitalizations/deaths on separate y axes. I insert colored rectanges behind the data to show the stages of state intervention.\n\n#make graph\nallegheny_county_timeline &lt;- df_rolling %&gt;% \n  filter(!is.na(value)) %&gt;% \n  ggplot(aes(x = date, y = value)) +\n  #create colored rectangles showing various government intervention timelines\n  annotate(geom = \"rect\", xmin = ymd(\"2020-03-23\"), xmax = ymd(\"2020-05-15\"), ymin = as.Date(-Inf), ymax = as.Date(Inf), \n           fill = \"red\", alpha = .3) +\n  annotate(geom = \"rect\", xmin = ymd(\"2020-05-15\"), xmax = ymd(\"2020-06-05\"), ymin = as.Date(-Inf), ymax = as.Date(Inf), \n           fill = \"yellow\", alpha = .3) +\n  annotate(geom = \"rect\", xmin = ymd(\"2020-06-05\"), xmax = ymd(\"2020-06-28\"), ymin = as.Date(-Inf), ymax = as.Date(Inf), \n           fill = \"green\", alpha = .3) +\n  annotate(geom = \"rect\", xmin = ymd(\"2020-06-28\"), xmax = as.Date(Inf), ymin = as.Date(-Inf), ymax = as.Date(Inf),\n           fill = \"#aaff00\", alpha = .3) +\n  #plot daily data as points, rolling average as lines\n  geom_point(data = df_new, aes(y = value), alpha = .3)+\n  geom_line(size = 1.5) +\n  #facet by metric\n  facet_wrap(~metric, ncol = 1, scales = \"free_y\") +\n  labs(title = str_c(\"Allegheny County COVID-19 response timeline (last updated \", last_updated, \")\"),\n       x = NULL,\n       y = NULL,\n       subtitle = \"14-day rolling average\",\n       caption = \"@conor_tompkins, data from Allegheny County via Franklin Chen\")\n\nallegheny_county_timeline"
  },
  {
    "objectID": "posts/effect-of-geographic-resolution-on-ebirdst-abundance/index.html",
    "href": "posts/effect-of-geographic-resolution-on-ebirdst-abundance/index.html",
    "title": "Effect of Geographic Resolution on ebirdst Abundance",
    "section": "",
    "text": "While exploring some of the citizen science bird observation data available through ebirdst, I was confused by how to understand the calculation of ebirdst’s abundance metric.\nThe ebirdst documentation (?ebirdst::load_raster) defines abundance as:\nI had seen some weird results when trying to manually calculate abundance as occurrence * count. My initial attempt had aggregated the results by month.\nThe underlying problem is that abundance and count are the results of models, and are subject to model error. I also believe that the data outputted from load_raster lacks the necessary significant digits to accurately recreate abundance. Lowering the resolution or aggregating the data will exacerbate this issue.\nThis code loads my convenience function to retrieve a metric for a species at a given geographic resolution. This gets occurrence, count, and abundance for the Northern Cardinal at high (3 km), medium (9 km), and low resolutions (27 km). The function also crops the underlying raster data to Pennsylvania.\nlibrary(here)\nlibrary(hrbrthemes)\nlibrary(patchwork)\n\nsource(\"https://raw.githubusercontent.com/conorotompkins/ebird_shiny_app/main/scripts/functions/get_species_metric.R\")\n\ntheme_set(theme_ipsum())\n\nspecies_table &lt;- crossing(location = \"Pennsylvania\",\n                          species = c(\"Northern Cardinal\"),\n                          metric = c(\"occurrence\", \"count\", \"abundance\"),\n                          resolution = c(\"hr\", \"mr\", \"lr\"))\nspecies_table\n\n# A tibble: 9 × 4\n  location     species           metric     resolution\n  &lt;chr&gt;        &lt;chr&gt;             &lt;chr&gt;      &lt;chr&gt;     \n1 Pennsylvania Northern Cardinal abundance  hr        \n2 Pennsylvania Northern Cardinal abundance  lr        \n3 Pennsylvania Northern Cardinal abundance  mr        \n4 Pennsylvania Northern Cardinal count      hr        \n5 Pennsylvania Northern Cardinal count      lr        \n6 Pennsylvania Northern Cardinal count      mr        \n7 Pennsylvania Northern Cardinal occurrence hr        \n8 Pennsylvania Northern Cardinal occurrence lr        \n9 Pennsylvania Northern Cardinal occurrence mr\nspecies_metrics &lt;- species_table %&gt;% \n  mutate(data = pmap(list(location, species, metric, resolution), ~get_species_metric(..1, ..2, ..2, ..3, ..4))) %&gt;% \n  mutate(resolution = fct_relevel(resolution, c(\"hr\", \"mr\", \"lr\"))) %&gt;% \n  arrange(species, metric, resolution) |&gt; \n  unnest(data) %&gt;% \n  unnest(data)\nspecies_metrics\n\n# A tibble: 1,243,320 × 13\n   location species metric resolution family_common_name common_name metric_desc\n   &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;              &lt;chr&gt;       &lt;chr&gt;      \n 1 Pennsyl… Northe… abund… hr         Northern Cardinal  Northern C… abundance  \n 2 Pennsyl… Northe… abund… hr         Northern Cardinal  Northern C… abundance  \n 3 Pennsyl… Northe… abund… hr         Northern Cardinal  Northern C… abundance  \n 4 Pennsyl… Northe… abund… hr         Northern Cardinal  Northern C… abundance  \n 5 Pennsyl… Northe… abund… hr         Northern Cardinal  Northern C… abundance  \n 6 Pennsyl… Northe… abund… hr         Northern Cardinal  Northern C… abundance  \n 7 Pennsyl… Northe… abund… hr         Northern Cardinal  Northern C… abundance  \n 8 Pennsyl… Northe… abund… hr         Northern Cardinal  Northern C… abundance  \n 9 Pennsyl… Northe… abund… hr         Northern Cardinal  Northern C… abundance  \n10 Pennsyl… Northe… abund… hr         Northern Cardinal  Northern C… abundance  \n# ℹ 1,243,310 more rows\n# ℹ 6 more variables: date &lt;date&gt;, value &lt;dbl&gt;, month &lt;chr&gt;, region &lt;chr&gt;,\n#   x &lt;dbl&gt;, y &lt;dbl&gt;\nThis unnests the data and recalculates abundance (abundance_test) and the difference between actual abundance and abundance_test.\nspecies_table_unnested &lt;- species_metrics %&gt;%\n  select(species, resolution, date, month, x, y, metric_desc, value) %&gt;% \n  pivot_wider(id_cols = c(species, resolution, date, month, x, y),\n              names_from = metric_desc,\n              values_from = value) %&gt;% \n  select(species, resolution, date, month, x, y, count, occurrence, abundance) %&gt;% \n  mutate(abundance_test = count * occurrence,\n         diff = abundance - abundance_test)\nGrouping by month to get to the county level changes the grain of the data so much that abundance_test undershoots abundance by 20%. This occurs at all resolutions.\nspecies_metrics %&gt;%\n  select(species, resolution, date, month, x, y, metric_desc, value) %&gt;% \n  pivot_wider(id_cols = c(species, resolution, date, month, x, y),\n              names_from = metric_desc,\n              values_from = value) %&gt;% \n  select(species, resolution, date, month, x, y, count, occurrence, abundance) %&gt;% \n  group_by(species, month, resolution) %&gt;% \n  summarize(occurrence = mean(occurrence, na.rm = T),\n            count = mean(count, na.rm = T),\n            abundance = mean(abundance, na.rm = T)) %&gt;% \n  ungroup() %&gt;% \n  mutate(abundance_test = count * occurrence,\n         diff = abundance - abundance_test) %&gt;% \n  ggplot(aes(abundance, abundance_test)) +\n  geom_abline() +\n  geom_point() +\n  facet_wrap(~resolution) +\n  tune::coord_obs_pred()\n\n`summarise()` has grouped output by 'species', 'month'. You can override using\nthe `.groups` argument.\nTotally un-aggregated, abundance_test closely resembles abundance, but degrades as resolution decreases.\nspecies_table_unnested %&gt;% \n  select(abundance, abundance_test, resolution) %&gt;% \n  drop_na() %&gt;% \n  ggplot(aes(abundance, abundance_test)) +\n  geom_density_2d_filled(contour_var = \"ndensity\") +\n  geom_abline(color = \"white\") +\n  facet_wrap(~resolution) +\n  tune::coord_obs_pred() +\n  coord_cartesian(xlim = c(0, 4),\n                  ylim = c(0, 4)) +\n  guides(fill = guide_colorsteps())\n\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\nAt lower resolutions, the difference is positively skewed, which means that abundance is higher than abundance_test.\nspecies_table_unnested %&gt;% \n  drop_na(diff) %&gt;% \n  ggplot(aes(diff)) +\n  geom_histogram() +\n  facet_wrap(~resolution, scale = \"free_y\", ncol = 1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\nAt the highest resolution, diff is heteroskedastic. At lower resolutions, there are patterns to the error.\nspecies_table_unnested %&gt;% \n  drop_na(occurrence, diff) %&gt;% \n  ggplot(aes(occurrence, diff)) +\n  geom_density_2d_filled(contour_var = \"ndensity\") +\n  facet_wrap(~resolution) + \n  scale_x_percent() +\n  guides(fill = guide_colorsteps())\nThis was a useful exercise for me to understand how the geographic resolution and other aggregation of the data can affect estimated metrics, specifically in the citizen science context."
  },
  {
    "objectID": "posts/effect-of-geographic-resolution-on-ebirdst-abundance/index.html#update",
    "href": "posts/effect-of-geographic-resolution-on-ebirdst-abundance/index.html#update",
    "title": "Effect of Geographic Resolution on ebirdst Abundance",
    "section": "Update",
    "text": "Update\nI made an issue on the ebirdst Github page and talked to one of the maintainers about their definitions of count and abundance. I now have a much stronger understanding of these variables.\nThe following code reproduces the graph I attached to the issue:\n\nlibrary(hrbrthemes)\ntheme_set(theme_ipsum())\n\nnorcar_table &lt;- crossing(location = \"Pennsylvania\",\n                         species = c(\"Northern Cardinal\"),\n                         metric = c(\"occurrence\", \"count\", \"abundance\"),\n                         resolution = c(\"hr\"))\n\n\nnorcar_metrics &lt;- norcar_table %&gt;% \n  mutate(data = pmap(list(location, species, metric, resolution), ~get_species_metric(..1, ..2, ..2, ..3, ..4))) %&gt;% \n  mutate(resolution = fct_relevel(resolution, c(\"hr\", \"mr\", \"lr\"))) %&gt;% \n  arrange(species, metric, resolution) %&gt;%\n  unnest(data) |&gt; \n  unnest(data)\n\n\nnorcar_metrics_wide &lt;- norcar_metrics %&gt;% \n  select(species, date, x, y, metric_desc, value) %&gt;% \n  pivot_wider(names_from = metric_desc,\n              values_from = value)\n\nplot_1 &lt;- norcar_metrics_wide %&gt;% \n  drop_na(occurrence, count) %&gt;% \n  ggplot(aes(occurrence, count)) +\n  geom_density_2d_filled(contour_var = \"ndensity\") +\n  scale_x_percent() +\n  guides(fill = \"none\") +\n  theme_bw()\n\nplot_2 &lt;- norcar_metrics_wide %&gt;% \n  drop_na() %&gt;% \n  ggplot(aes(occurrence, abundance)) +\n  geom_density_2d_filled(contour_var = \"ndensity\") +\n  scale_x_percent() +\n  guides(fill = \"none\") +\n  theme_bw()\n\nplot_3 &lt;- norcar_metrics_wide %&gt;%\n  drop_na() %&gt;% \n  ggplot(aes(count, abundance)) +\n  geom_density_2d_filled(contour_var = \"ndensity\") +\n  geom_abline() +\n  guides(fill = \"none\") +\n  theme_bw()\n\nlayout &lt;- \"\nAACC\nBBCC\n\"\n\nplot_1 + plot_2 + plot_3 + \n  plot_layout(guides = 'collect', design = layout) +\n  plot_annotation(title = \"Northern Cardinal in Pennsylvania\")"
  },
  {
    "objectID": "posts/effect-of-geographic-resolution-on-ebirdst-abundance/index.html#citations",
    "href": "posts/effect-of-geographic-resolution-on-ebirdst-abundance/index.html#citations",
    "title": "Effect of Geographic Resolution on ebirdst Abundance",
    "section": "Citations",
    "text": "Citations\nFink, D., T. Auer, A. Johnston, M. Strimas-Mackey, O. Robinson, S. Ligocki, W. Hochachka, C. Wood, I. Davies, M. Iliff, L. Seitz. 2020. eBird Status and Trends, Data Version: 2019; Released: 2020 Cornell Lab of Ornithology, Ithaca, New York. https://doi.org/10.2173/ebirdst.2019"
  },
  {
    "objectID": "posts/roughly-calculating-allegheny-county-transit-efficiency/index.html",
    "href": "posts/roughly-calculating-allegheny-county-transit-efficiency/index.html",
    "title": "Roughly Calculating Allegheny County Transit Efficiency",
    "section": "",
    "text": "As part my work on transit lines in Allegheny County, I am interested in which transit lines are most efficient, in terms of residents and jobs served. This is possible with the Port Authority transit line datasets hosted on the WPRDC and data from the Census.\nLoad libraries and set up the environment:\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(janitor)\nlibrary(ggrepel)\nlibrary(hrbrthemes)\n\noptions(scipen = 999, digits = 2,\n        fig.width = 9,\n        fig.height = 9)\n\ntheme_set(theme_bw())\n\nThis loads the summarized LODES census tract data (munging script here)\n\ndf_tract_centroid_summary &lt;- st_read(\"post_data/shapefiles/tract_centroid_summary/tract_centroid_summary.shp\")\n\nTo recap, the distribution of jobs and residents across census tracts is vaguely normal. The main outlier in the “jobs” measure is the census tract for the Golden Triangle (Downtown).\n\ndf_tract_centroid_summary %&gt;% \n  select(GEOID, residents, jobs) %&gt;% \n  st_drop_geometry() %&gt;% \n  pivot_longer(cols = c(residents, jobs), names_to = \"measure\", values_to = \"value\") %&gt;% \n  ggplot(aes(value, fill = measure)) +\n    geom_density() +\n    facet_wrap(~str_to_title(measure), ncol = 1, scales = \"free\") +\n    scale_x_log10() +\n    guides(fill = \"none\") +\n    labs(x = \"Log 10 scale\",\n         y = \"Density\")\n\n\n\n\n\n\n\n\n\ndf_tract_centroid_summary %&gt;% \n  ggplot(aes(residents, jobs)) +\n    geom_point() +\n    geom_label_repel(data = df_tract_centroid_summary %&gt;% filter(jobs == max(jobs)),\n                     label = \"Downtown\") +\n    scale_y_comma() +\n    scale_x_comma() +\n    labs(x = \"Residents\",\n         y = \"Jobs\")\n\n\n\n\n\n\n\n\nThis code grabs the shapefile with the transit route stats and stop geometry. This code:\n\ncalculates how many residents and jobs are in each census tract\ncalculates which transit lines stops serve which census tracts\nsummarizes how many residents and jobs a transit line servers\n\n\ndf_route_stats &lt;- st_read(\"post_data/shapefiles/route_stats/route_stats.shp\") %&gt;% \n  rename(route_id = route_d,\n         service_type = srvc_ty,\n         residents = resdnts,\n         stop_count = stp_cnt,\n         route_name = rout_nm,\n         route_length_miles = rt_lng_,\n         stops_per_mile = stps_p_)\n\n\ndf_route_stats\n\nSimple feature collection with 102 features and 8 fields\nGeometry type: MULTIPOINT\nDimension:     XY\nBounding box:  xmin: -80 ymin: 40 xmax: -80 ymax: 41\nGeodetic CRS:  WGS 84\nFirst 10 features:\n   route_id service_type   jobs residents stop_count       route_name\n1         1        Local  21282     10119        224    Freeport Road\n2        11        Local    998      4102         62         Fineview\n3        12        Local   4640      2406        112         McKnight\n4        13        Local  26667     15659        140         Bellevue\n5        14        Local  31869     10017        140      Ohio Valley\n6        15        Local  21376      3934         98          Charles\n7        16 Key Corridor  22588      7712        124         Brighton\n8        17        Local  30226     10285        108        Shadeland\n9        18        Local   8380      4746         47       Manchester\n10      19L      Express 294211     18845         94 Emsworth Limited\n   route_length_miles stops_per_mile                       geometry\n1                44.4            5.0 MULTIPOINT ((-80 40), (-80 ...\n2                 5.6           11.1 MULTIPOINT ((-80 40), (-80 ...\n3                30.6            3.7 MULTIPOINT ((-80 41), (-80 ...\n4                15.5            9.0 MULTIPOINT ((-80 40), (-80 ...\n5                33.4            4.2 MULTIPOINT ((-80 41), (-80 ...\n6                 8.9           11.0 MULTIPOINT ((-80 40), (-80 ...\n7                 8.3           15.0 MULTIPOINT ((-80 41), (-80 ...\n8                13.0            8.3 MULTIPOINT ((-80 40), (-80 ...\n9                 5.0            9.5 MULTIPOINT ((-80 40), (-80 ...\n10               15.6            6.0 MULTIPOINT ((-80 41), (-80 ...\n\n\nThis is a basic plot of all the transit stops in the county:\n\ndf_route_stats %&gt;% \n  ggplot() +\n    geom_sf(size = .1, alpha = .5) +\n    theme_void()\n\n\n\n\n\n\n\n\nThe immediate question that comes to mind is “how many residents and jobs does a transit line serve?”. Keep in mind that more than one transit line can serve a given resident or job. This shows that the “Key Corridor” lines serve the most jobs.\n\ndf_route_stats %&gt;% \n  ggplot(aes(jobs, residents, fill = service_type)) +\n    geom_label(aes(label = route_id), alpha = .5) +\n    scale_x_comma() +\n    scale_y_comma() +\n    scale_fill_discrete(\"Service type\") +\n    labs(x = \"Jobs served\",\n         y = \"Residents served\")\n\n\n\n\n\n\n\n\nThis graph compares how many residents/jobs a transit line serves to how long the line is. The 28X and P10 are the least efficient in both cases. However, the 28X serves the Pittsburgh International Airport, and that utility is not captured in this analysis.\n\ndf_route_stats %&gt;% \n  filter(!is.na(route_id)) %&gt;% \n  select(route_id, service_type, route_length_miles, residents, jobs) %&gt;% \n  pivot_longer(cols = c(residents, jobs), names_to = \"variable\", values_to = \"value\") %&gt;% \n  ggplot(aes(route_length_miles, value, fill = service_type)) +\n    geom_label(aes(label = route_id), alpha = .5) +\n    facet_wrap(~str_to_title(str_c(variable, \"served\", sep = \" \")),\n               scales = \"free_y\",\n               ncol = 1,\n               strip.position = \"left\") +\n    scale_y_comma() +\n    scale_fill_discrete(\"Service Type\") +\n    labs(x = \"Route length (miles)\",\n         y = NULL) +\n    theme(strip.text = element_text(hjust = .5),\n          strip.background = element_rect(color = \"grey\"),\n          strip.placement = \"outside\")\n\n\n\n\n\n\n\n\nThis performs a similar comparison, but uses the number of stops per line instead of line distance. The 71/61 lines are very efficient in terms of jobs/stops, and the 59 appears to be the most inefficient.\n\ndf_route_stats %&gt;% \n  filter(!is.na(route_id)) %&gt;% \n  select(route_id, service_type, stop_count, residents, jobs) %&gt;% \n  pivot_longer(cols = c(residents, jobs), names_to = \"variable\", values_to = \"value\") %&gt;% \n  ggplot(aes(stop_count, value, fill = service_type)) +\n    geom_label(aes(label = route_id), alpha = .5) +\n    facet_wrap(~str_to_title(str_c(variable, \"served\", sep = \" \")),\n               scales = \"free_y\",\n               ncol = 1,\n               strip.position = \"left\") +\n    scale_y_comma() +\n    scale_fill_discrete(\"Service type\") +\n    labs(x = \"Number of stops\",\n         y = NULL) +\n    theme(strip.text = element_text(hjust = .5),\n          strip.background = element_rect(color = \"grey\"),\n          strip.placement = \"outside\")\n\n\n\n\n\n\n\n\nIn terms of stops per mile, the Express lines are most efficient. The incline lines are least efficient, but that is an artifact of their unique geography.\n\ndf_route_stats %&gt;% \n  filter(!is.na(route_id)) %&gt;% \n  select(route_id, service_type, stops_per_mile, residents, jobs) %&gt;% \n  pivot_longer(cols = c(residents, jobs), names_to = \"variable\", values_to = \"value\") %&gt;% \n  ggplot(aes(stops_per_mile, value, fill = service_type)) +\n    geom_label(aes(label = route_id), alpha = .5) +\n    facet_wrap(~str_to_title(str_c(variable, \"served\", sep = \" \")), \n               scales = \"free_y\",\n               ncol = 1,\n               strip.position = \"left\") +\n    scale_y_comma() +\n    scale_fill_discrete(\"Service type\") +\n    labs(x = \"Stops per mile\",\n         y = NULL) +\n    theme(strip.text = element_text(hjust = .5),\n          strip.background = element_rect(color = \"grey\"),\n          strip.placement = \"outside\")\n\n\n\n\n\n\n\n\nThis graph attempts to summarize everything by adding residents + jobs and comparing that to stops per mile. The “Express” and “Key Corridor” lines do the best here.\n\nplot &lt;- df_route_stats %&gt;% \n  filter(!is.na(route_id)) %&gt;% \n  select(route_id, service_type, stops_per_mile, residents, jobs) %&gt;% \n  mutate(residents_plus_jobs = residents + jobs) %&gt;% \n  ggplot(aes(stops_per_mile, residents_plus_jobs, fill = service_type, label = route_id)) +\n    geom_label(alpha = .5) +\n    labs(x = \"Stops per mile\",\n         y = \"Residents plus jobs served\",\n         caption = \"'Served' means the line came within 200 meters of the center of a census tract\") +\n    scale_x_continuous(expand = c(.1, .1)) +\n    scale_y_comma(expand = c(.1, .1)) +\n    scale_fill_discrete(\"Service type\") +\n    facet_wrap(vars(service_type), ncol = 1) +\n  theme(legend.position = \"bottom\")\n\nplot"
  },
  {
    "objectID": "posts/bivariate_transit_map/index.html",
    "href": "posts/bivariate_transit_map/index.html",
    "title": "Driving Alone vs. Public Transportation in Pittsburgh",
    "section": "",
    "text": "Intro\nThe clash between public transportation and single passenger vehicles is a heated topic of discussion nationally and in the Pittsburgh area. Public transit ridership has been heavily reduced by COVID-19 in many countries. These two commuting modes compete for the same riders, and investment dollars, and space. Car drivers are frustrated when a bus stops during rush hour to pick up passengers, while bus passengers are frustrated sitting in traffic caused by single passenger vehicles because transit doesn’t have right-of-way.\nFrom my point of view, Pittsburgh’s geography lends itself to a focus on public transit, at the expense of the single passenger vehicle. Most of the jobs in the county are in a single census tract Downtown, which is reflected in the spoke (and no wheel) design of the transit system. Downtown is surrounded by rivers and mountains, which drastically narrows the geography suited to infrastructure. You pretty much have to use a tunnel or bridge to commute Downtown, unless you are coming from directly east. It would make sense to give public transit priority access to those tunnels and bridges, since their throughput is many times higher than roads designated for single passenger vehicles.\n\n\n\n\n\n\n\n\n\nThe historical priority towards single passenger vehicles is reflected in the Census statistics about commuting modes in the area. Most people in the area commute to work by themselves in cars. In the Wexford-area census tract, 78% (5,141) of commuters drive to work alone (and sit in traffic on the parkway together). Public transit use is limited to areas where the government invested in transit, but even there transit is not typically the majority mode.\nIn this post I will use {tidycensus} to pull data about how many people commute by driving alone or taking public transit Allegheny County. I chose these two modes because they are the two most popular modes in the county, and are the most different in terms of style. I then graph the data with {ggplot2} and {biscale}. I hack the {biscale} legend a bit to get it to show the % of commuters, which may be of interest to other R users.\n\n\nCode and graphs\nLoad libraries and set up the environment:\n\nlibrary(tidyverse)\nlibrary(tidycensus)\nlibrary(sf)\nlibrary(tigris)\nlibrary(janitor)\nlibrary(biscale)\nlibrary(patchwork)\nlibrary(hrbrthemes)\nlibrary(kableExtra)\n\noptions(scipen = 999, digits = 4)\n\ntheme_set(theme_ipsum(base_size = 25))\n\nThese are the variables about commuter mode that the Census has for the 2019 American Community Survey (ACS):\n\nacs1_vars &lt;- load_variables(2019, 'acs1') %&gt;% \n  mutate(across(c(label, concept), str_to_lower))\n\nacs1_vars %&gt;%\n  filter(str_detect(name, \"^B08301_\")) %&gt;% \n  kbl() %&gt;% \n  scroll_box(height = \"400px\") %&gt;% \n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"),\n                position = \"left\")\n\n\n\n\n\nname\nlabel\nconcept\n\n\n\n\nB08301_001\nestimate!!total:\nmeans of transportation to work\n\n\nB08301_002\nestimate!!total:!!car, truck, or van:\nmeans of transportation to work\n\n\nB08301_003\nestimate!!total:!!car, truck, or van:!!drove alone\nmeans of transportation to work\n\n\nB08301_004\nestimate!!total:!!car, truck, or van:!!carpooled:\nmeans of transportation to work\n\n\nB08301_005\nestimate!!total:!!car, truck, or van:!!carpooled:!!in 2-person carpool\nmeans of transportation to work\n\n\nB08301_006\nestimate!!total:!!car, truck, or van:!!carpooled:!!in 3-person carpool\nmeans of transportation to work\n\n\nB08301_007\nestimate!!total:!!car, truck, or van:!!carpooled:!!in 4-person carpool\nmeans of transportation to work\n\n\nB08301_008\nestimate!!total:!!car, truck, or van:!!carpooled:!!in 5- or 6-person carpool\nmeans of transportation to work\n\n\nB08301_009\nestimate!!total:!!car, truck, or van:!!carpooled:!!in 7-or-more-person carpool\nmeans of transportation to work\n\n\nB08301_010\nestimate!!total:!!public transportation (excluding taxicab):\nmeans of transportation to work\n\n\nB08301_011\nestimate!!total:!!public transportation (excluding taxicab):!!bus\nmeans of transportation to work\n\n\nB08301_012\nestimate!!total:!!public transportation (excluding taxicab):!!subway or elevated rail\nmeans of transportation to work\n\n\nB08301_013\nestimate!!total:!!public transportation (excluding taxicab):!!long-distance train or commuter rail\nmeans of transportation to work\n\n\nB08301_014\nestimate!!total:!!public transportation (excluding taxicab):!!light rail, streetcar or trolley (carro público in puerto rico)\nmeans of transportation to work\n\n\nB08301_015\nestimate!!total:!!public transportation (excluding taxicab):!!ferryboat\nmeans of transportation to work\n\n\nB08301_016\nestimate!!total:!!taxicab\nmeans of transportation to work\n\n\nB08301_017\nestimate!!total:!!motorcycle\nmeans of transportation to work\n\n\nB08301_018\nestimate!!total:!!bicycle\nmeans of transportation to work\n\n\nB08301_019\nestimate!!total:!!walked\nmeans of transportation to work\n\n\nB08301_020\nestimate!!total:!!other means\nmeans of transportation to work\n\n\nB08301_021\nestimate!!total:!!worked from home\nmeans of transportation to work\n\n\n\n\n\n\n\nDriving alone in a single-passenger vehicle is by far the dominant commuting mode in the county.\n\nall_transit_vars &lt;- c(\"B08301_003\", \n                      \"B08301_004\", \n                      \"B08301_010\", \n                      \"B08301_016\", \n                      \"B08301_017\", \n                      \"B08301_018\", \n                      \"B08301_019\", \n                      \"B08301_020\",\n                      \"B08301_021\")\n\nall_transit_modes &lt;- get_acs(geography = \"county\", \n                             variables = acs1_vars %&gt;%\n                               filter(name %in% all_transit_vars) %&gt;% \n                               pull(name, label),\n                             summary_var = \"B08301_001\",\n                             year = 2019, state = \"PA\", county = \"Allegheny\",\n                             geometry = F)\n\nall_transit_modes %&gt;% \n  mutate(variable = str_remove(variable, \"^estimate!!total:\"),\n         variable = str_remove(variable, \"\\\\(excluding taxicab\\\\)\"),\n         variable = str_remove_all(variable, \"\\\\!\"),\n         variable = str_remove(variable, \":$\"),\n         variable = str_replace(variable, \":\", \" : \"),\n         variable = str_trim(variable),\n         variable = str_to_title(variable)) %&gt;% \n  group_by(variable) %&gt;% \n  summarize(estimate = sum(estimate),) %&gt;% \n  mutate(variable = fct_reorder(variable, estimate),\n         pct = estimate / sum(estimate)) %&gt;% \n  ggplot(aes(estimate, variable)) +\n  geom_col() +\n  geom_text(aes(x = estimate + 26000, label = scales::percent(pct, 1)),\n            size = 4) +\n  labs(title = \"Allegheny County Commuter Modes\",\n       subtitle = \"2019 American Community Survey\",\n       x = \"Commuters\",\n       y = NULL) +\n  scale_x_comma(limits = c(0, 500000),\n                labels = c(\"0\", \"1k\", \"2k\", \"3k\", \"4k\", \"5k\")) +\n  theme_ipsum(axis_text_size = 15)\n\n\n\n\n\n\n\n\nI will use these two variables to directly compare the use of single-passenger vehicles and public transit in the county.\n\nvars &lt;- c(\"Drove alone\" = \"B08301_003\",\n          \"Public transportation\" = \"B08301_010\")\n\nacs1_vars %&gt;%\n  filter(name %in% vars) %&gt;% \n  pull(label)\n\n[1] \"estimate!!total:!!car, truck, or van:!!drove alone\"          \n[2] \"estimate!!total:!!public transportation (excluding taxicab):\"\n\n\nThis downloads the commuter mode data and subtracts the rivers from the census tract polygons so it looks nice on a map:\n\ntract_transit %&gt;% \n  glimpse()\n\nRows: 804\nColumns: 5\n$ GEOID       &lt;chr&gt; \"42003408002\", \"42003408002\", \"42003210700\", \"42003210700\"…\n$ variable    &lt;chr&gt; \"Drove alone\", \"Public transportation\", \"Drove alone\", \"Pu…\n$ estimate    &lt;dbl&gt; 2815, 8, 589, 189, 566, 146, 1224, 260, 466, 100, 1063, 21…\n$ summary_est &lt;dbl&gt; 3165, 3165, 1231, 1231, 1110, 1110, 1992, 1992, 702, 702, …\n$ geometry    &lt;POLYGON [°]&gt; POLYGON ((-79.99 40.61, -79..., POLYGON ((-79.99 4…\n\n\nAs discussed earlier, public transit is not the majority commuting mode in most areas:\n\ntract_transit %&gt;% \n  st_drop_geometry() %&gt;% \n  group_by(GEOID) %&gt;%\n  mutate(pct_tract_commuters = estimate / sum(estimate),\n         combined_commuters = sum(estimate)) %&gt;%\n  ungroup() %&gt;%\n  mutate(GEOID = fct_reorder(GEOID, summary_est)) %&gt;%\n  arrange(desc(GEOID), desc(summary_est)) %&gt;% \n  mutate(is_downtown_label = case_when(GEOID == \"42003020100\" & variable == \"Drove alone\" ~ \"Downtown*\",\n                                       TRUE ~ NA_character_)) %&gt;% \n  slice(1:60) %&gt;% \n  ggplot(aes(estimate, GEOID, fill = variable)) +\n  geom_col(color = \"black\") +\n  geom_text(aes(x = estimate + 3000, label = is_downtown_label)) +\n  labs(title = \"Top 30 census tracts\",\n       subtitle = \"Total commuter population from all modes\",\n       x = \"Commuters\",\n       y = \"Census tracts\",\n       fill = \"Commute mode\") +\n  scale_x_comma() +\n  theme_ipsum(base_size = 15) +\n  theme(axis.text.y = element_blank(),\n        panel.grid.major = element_blank())\n\n\n\n\n\n\n\n\n*Most commuters that live in Downtown walk to work.\nThis shows that in absolute numbers, driving alone swamps public transit across the county.\n\nscatter_graph &lt;- tract_transit %&gt;% \n  st_drop_geometry() %&gt;% \n  select(GEOID, variable, estimate) %&gt;% \n  pivot_wider(names_from = variable, values_from = estimate) %&gt;% \n  clean_names() %&gt;% \n  ggplot(aes(drove_alone, public_transportation)) +\n  geom_point(alpha = .7, size = 1) +\n  labs(title = \"Commuter modes in Allegheny County\",\n       x = \"Driving Alone\",\n       y = \"Using Public Transportation\") +\n  scale_x_comma() +\n  scale_y_comma() +\n  tune::coord_obs_pred() +\n  theme_ipsum(base_size = 15)\n\nscatter_graph\n\n\n\n\n\n\n\n\nI made the X and Y axes symmetric to emphasize the difference in scale between the two variables.\nThis uses the bi_class function to divide the data into discrete bins based on how many people drive alone vs. use public transit. This turns two continuous variables into one categorical variable. I had to play around with the style argument to find an option that worked for the unbalanced data.\n\ntransit_bivariate_geo &lt;- tract_transit %&gt;% \n  st_drop_geometry() %&gt;% \n  drop_na(estimate) %&gt;% \n  select(GEOID, variable, estimate) %&gt;% \n  pivot_wider(names_from = variable, values_from = estimate) %&gt;% \n  clean_names() %&gt;% \n  replace_na(list(drove_alone = 0, public_transportation = 0)) %&gt;% \n  bi_class(x = drove_alone, \n           y = public_transportation, \n           style = \"fisher\", \n           dim = 3) %&gt;% \n  left_join(tracts, \n            by = c(\"geoid\" = \"GEOID\")) %&gt;% \n  st_sf()\n\nglimpse(transit_bivariate_geo)\n\nRows: 402\nColumns: 9\n$ geoid                 &lt;chr&gt; \"42003408002\", \"42003210700\", \"42003220600\", \"42…\n$ drove_alone           &lt;dbl&gt; 2815, 589, 566, 1224, 466, 1063, 887, 826, 551, …\n$ public_transportation &lt;dbl&gt; 8, 189, 146, 260, 100, 215, 262, 342, 670, 61, 3…\n$ bi_class              &lt;chr&gt; \"3-1\", \"1-2\", \"1-1\", \"2-2\", \"1-1\", \"2-2\", \"1-2\",…\n$ NAME                  &lt;chr&gt; \"Census Tract 4080.02, Allegheny County, Pennsyl…\n$ variable              &lt;chr&gt; \"B08301_001\", \"B08301_001\", \"B08301_001\", \"B0830…\n$ estimate              &lt;dbl&gt; 3165, 1231, 1110, 1992, 702, 1487, 1317, 1712, 1…\n$ moe                   &lt;dbl&gt; 231, 199, 116, 189, 80, 233, 163, 185, 257, 104,…\n$ geometry              &lt;POLYGON [°]&gt; POLYGON ((-79.99 40.61, -79..., POLYGON …\n\n\n\ntable(transit_bivariate_geo$bi_class) %&gt;% \n  enframe(name = \"bi_class\", value = \"count_tracts\") %&gt;% \n  kbl()\n\n\n\n\nbi_class\ncount_tracts\n\n\n\n\n1-1\n124\n\n\n1-2\n72\n\n\n1-3\n9\n\n\n2-1\n84\n\n\n2-2\n66\n\n\n2-3\n9\n\n\n3-1\n34\n\n\n3-2\n4\n\n\n\n\n\n\n\nThis graph overlays the discrete biscale bins on the previous data to show how the function discretized the data.\n\ntransit_bivariate_geo %&gt;% \n  ggplot(aes(drove_alone, public_transportation, color = bi_class)) +\n  geom_point(alpha = .75, size = 1) +\n  scale_x_comma() +\n  labs(x = \"Drove Alone\",\n       y = \"Used Public Transit\") +\n  guides(color = FALSE) +\n  theme_ipsum(base_size = 15)\n\n\n\n\n\n\n\n\nNote that the X and Y axes are independent in this graph.\nThis creates the biscale legend I will put next to the map.\n\nbi_var_legend &lt;- bi_legend(pal = \"DkBlue\",\n                           dim = 3,\n                           xlab = \" More drove alone\",\n                           ylab = \"More used public transit\",\n                           size = 26) +\n  theme(plot.background = element_rect(fill = alpha(\"white\", 0)),\n        panel.background = element_rect(fill = alpha(\"white\", 0)))\n\nbi_var_legend\n\n\n\n\n\n\n\n\nI would like to show the % of commuters that each bin represents, so I extract the color palette from the ggplot2 object and make my own legend with geom_tile.\n\nbuilt_legend &lt;- ggplot_build(bi_var_legend)\n\nlegend_palette &lt;- built_legend$data[[1]] %&gt;%\n  mutate(bi_class = str_c(x, y, sep = \"-\")) %&gt;% \n  select(fill, bi_class)\n\nlegend_palette %&gt;% \n  kbl()\n\n\n\n\nfill\nbi_class\n\n\n\n\n#e8e8e8\n1-1\n\n\n#ace4e4\n2-1\n\n\n#5ac8c8\n3-1\n\n\n#dfb0d6\n1-2\n\n\n#a5add3\n2-2\n\n\n#5698b9\n3-2\n\n\n#be64ac\n1-3\n\n\n#8c62aa\n2-3\n\n\n#3b4994\n3-3\n\n\n\n\n\n\n\n\ntransit_bivariate &lt;- transit_bivariate_geo %&gt;% \n  st_drop_geometry() %&gt;% \n  select(geoid, bi_class, drove_alone, public_transportation) %&gt;% \n  separate(bi_class, \n           into = c(\"drove_alone_bi\", \"public_transportation_bi\"), \n           sep = \"-\",\n           remove = FALSE) %&gt;% \n  complete(drove_alone_bi, public_transportation_bi, fill = list(drove_alone = 0, public_transportation = 0)) %&gt;% \n  mutate(bi_class = str_c(drove_alone_bi, public_transportation_bi, sep = \"-\"),\n         total = drove_alone + public_transportation,\n         pct_commuters = total / sum(total)) %&gt;%\n  group_by(bi_class, drove_alone_bi, public_transportation_bi) %&gt;% \n  summarize(count_tract = n(),\n            pct_commuters = sum(pct_commuters)) %&gt;% \n  ungroup()\n\nglimpse(transit_bivariate)\n\nRows: 9\nColumns: 5\n$ bi_class                 &lt;chr&gt; \"1-1\", \"1-2\", \"1-3\", \"2-1\", \"2-2\", \"2-3\", \"3-…\n$ drove_alone_bi           &lt;chr&gt; \"1\", \"1\", \"1\", \"2\", \"2\", \"2\", \"3\", \"3\", \"3\"\n$ public_transportation_bi &lt;chr&gt; \"1\", \"2\", \"3\", \"1\", \"2\", \"3\", \"1\", \"2\", \"3\"\n$ count_tract              &lt;int&gt; 124, 72, 9, 84, 66, 9, 34, 4, 1\n$ pct_commuters            &lt;dbl&gt; 0.13661, 0.11754, 0.02073, 0.25273, 0.22252, …\n\n\n\nlegend_palette &lt;- transit_bivariate %&gt;% \n  distinct(bi_class) %&gt;% \n  left_join(legend_palette, by = \"bi_class\")\n\nlegend_palette %&gt;% \n  kbl()\n\n\n\n\nbi_class\nfill\n\n\n\n\n1-1\n#e8e8e8\n\n\n1-2\n#dfb0d6\n\n\n1-3\n#be64ac\n\n\n2-1\n#ace4e4\n\n\n2-2\n#a5add3\n\n\n2-3\n#8c62aa\n\n\n3-1\n#5ac8c8\n\n\n3-2\n#5698b9\n\n\n3-3\n#3b4994\n\n\n\n\n\n\n\nNote that scale_fill_manual uses the palette I extracted from the ggplot2 object.\n\nbi_var_legend_new &lt;- transit_bivariate %&gt;% \n  mutate(pct_commuters = scales::percent(pct_commuters, accuracy = 1)) %&gt;% \n  ggplot(aes(x = drove_alone_bi, y = public_transportation_bi, fill = bi_class)) +\n  geom_tile() +\n  geom_label(fill = \"white\", alpha = .75, size = 12, label = \"    \") +\n  geom_text(aes(label = pct_commuters), alpha = 1, size = 7) +\n  coord_fixed(ratio = 1) +\n  labs(x = substitute(paste(\"More drove alone\", \"\" %-&gt;% \"\")),\n       y = substitute(paste(\"More used public transit\", \"\" %-&gt;% \"\"))) +\n  guides(fill = FALSE) +\n  scale_fill_manual(values = pull(legend_palette, fill)) +\n  theme_ipsum(plot_title_size = 30,\n              axis_title_size = 30) +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        panel.background = element_blank(),\n        axis.text.x = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks = element_blank())\n\nbi_var_legend_new +\n  labs(title = 'Percent of \"drive alone\" + \"public transit\" commuters')\n\n\n\n\n\n\n\n\nThis creates the map of commuter mode by census tract, filled by the discretized biscale bin.\n\ntransit_bi_var_plot &lt;- transit_bivariate_geo %&gt;% \n  ggplot(aes(fill = bi_class)) +\n  geom_sf(show.legend = FALSE, lwd = 0) +\n  geom_sf(data = rivers, fill = \"black\", color = \"black\") +\n  bi_scale_fill(pal = \"DkBlue\", dim = 3) +\n  bi_theme() +\n  theme_ipsum(base_size = 15) +\n  theme(axis.text.x = element_blank(),\n        axis.text.y = element_blank(),\n        panel.background = element_blank(),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank())\n\ntransit_bi_var_plot\n\n\n\n\n\n\n\n\nNow that I have my legend and map, I use patchwork to stitch them together.\n\ndesign = c(area(t = 2, l = 4, b = 20, r = 20),\n           area(t = 1, l = 1, b = 6, r = 6))\n\nplot(design)\n\n\n\n\n\n\n\n\n\ncombined_bi_var_plot &lt;- transit_bi_var_plot + bi_var_legend_new +\n  plot_layout(design = design) +\n  plot_annotation(title = \"Allegheny County Commuter Patterns\",\n                  subtitle = \"Legend: % of commuters that drove alone or use public transit\",\n                  caption = \"2019 American Community Survey\",\n                  theme = theme(panel.background = element_rect(fill = \"black\"),\n                                plot.title = element_text(size = 30),\n                                plot.subtitle = element_text(size = 25),\n                                plot.caption = element_text(size = 25)))\n\n\n\n\n\n\n\n\n\n\n\nLinks:\n\nhttps://www.pghcitypaper.com/pittsburgh/low-income-pittsburghers-are-becoming-increasingly-reliant-on-public-transit-bikes-walking-and-alternative-transportation/Content?oid=19059768\nhttps://www.pghcitypaper.com/pittsburgh/new-commutes-analyzing-the-changing-ways-pittsburghers-get-to-work/Content?oid=6405396\nhttps://www.pghcitypaper.com/pittsburgh/pittsburgh-is-the-7th-least-car-dependent-metro-in-america-study-says/Content?oid=16755873\nhttps://www.nytimes.com/2020/07/09/opinion/sunday/ban-cars-manhattan-cities.html\nhttps://www.nytimes.com/2021/03/25/climate/buses-trains-ridership-climate-change.html\nhttps://nacto.org/publication/transit-street-design-guide/introduction/why/designing-move-people/\nhttps://rweekly.org/"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/how-many-pittsburghers-cross-the-river-to-get-to-work/index.html",
    "href": "posts/how-many-pittsburghers-cross-the-river-to-get-to-work/index.html",
    "title": "How Many Pittsburghers Cross the River to Get to Work",
    "section": "",
    "text": "This post focuses on how many rivers Pittsburghers cross to get to work. I use the U.S. Census Bureau LEHD Origin-Destination Employment Statistics (LODES) dataset to draw lines between “home” census tracts and “work” census tracts, and then count how many “commuter lines” intersect with the 3 main rivers in Pittsburgh. This calculation is done in straight lines “as the crow flies”, not accounting for actual road routes."
  },
  {
    "objectID": "posts/how-many-pittsburghers-cross-the-river-to-get-to-work/index.html#tldr",
    "href": "posts/how-many-pittsburghers-cross-the-river-to-get-to-work/index.html#tldr",
    "title": "How Many Pittsburghers Cross the River to Get to Work",
    "section": "TLDR",
    "text": "TLDR\nA plurality of commuters don’t cross any rivers, and none cross three. \n\nMany commuters in the Golden Triangle and neighborhoods to the east don’t cross rivers to get to work. Commuters from the North and South Hills areas usually cross one river. Commuters from Sewickley, Coraopolis, and those that live close to the airport are most likely to cross two rivers."
  },
  {
    "objectID": "posts/how-many-pittsburghers-cross-the-river-to-get-to-work/index.html#data-munging-and-analysis",
    "href": "posts/how-many-pittsburghers-cross-the-river-to-get-to-work/index.html#data-munging-and-analysis",
    "title": "How Many Pittsburghers Cross the River to Get to Work",
    "section": "Data munging and analysis",
    "text": "Data munging and analysis\nI use the “pa_od_aux_JT00_2017.csv” file as shown here: \nIn my analysis I use many of the standard {tidyverse} packages, {sf}, {tidycensus}, {tidygraph}, and {ggraph}:\n\nlibrary(tidyverse)\nlibrary(vroom)\nlibrary(sf)\nlibrary(tigris)\nlibrary(tidycensus)\nlibrary(tidygraph)\nlibrary(ggraph)\n\nThe first step is to read in the geographies crosswalk:\n\ngeo_crosswalk &lt;- vroom(\"post_data/pa_xwalk.csv.gz\", col_types = cols(.default = \"c\"))\n\ngeo_crosswalk\n\n# A tibble: 421,545 × 43\n   tabblk2010    st    stusps stname cty   ctyname trct  trctname bgrp  bgrpname\n   &lt;chr&gt;         &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;   \n 1 420912030002… 42    PA     Penns… 42091 Montgo… 4209… 2030 (M… 4209… 2 (Trac…\n 2 420912070013… 42    PA     Penns… 42091 Montgo… 4209… 2070.01… 4209… 3 (Trac…\n 3 420912070013… 42    PA     Penns… 42091 Montgo… 4209… 2070.01… 4209… 3 (Trac…\n 4 420710144022… 42    PA     Penns… 42071 Lancas… 4207… 144.02 … 4207… 2 (Trac…\n 5 420710144022… 42    PA     Penns… 42071 Lancas… 4207… 144.02 … 4207… 2 (Trac…\n 6 420710134002… 42    PA     Penns… 42071 Lancas… 4207… 134 (La… 4207… 2 (Trac…\n 7 420710144022… 42    PA     Penns… 42071 Lancas… 4207… 144.02 … 4207… 2 (Trac…\n 8 420710144022… 42    PA     Penns… 42071 Lancas… 4207… 144.02 … 4207… 2 (Trac…\n 9 420710144022… 42    PA     Penns… 42071 Lancas… 4207… 144.02 … 4207… 2 (Trac…\n10 420710144022… 42    PA     Penns… 42071 Lancas… 4207… 144.02 … 4207… 2 (Trac…\n# ℹ 421,535 more rows\n# ℹ 33 more variables: cbsa &lt;chr&gt;, cbsaname &lt;chr&gt;, zcta &lt;chr&gt;, zctaname &lt;chr&gt;,\n#   stplc &lt;chr&gt;, stplcname &lt;chr&gt;, ctycsub &lt;chr&gt;, ctycsubname &lt;chr&gt;,\n#   stcd116 &lt;chr&gt;, stcd116name &lt;chr&gt;, stsldl &lt;chr&gt;, stsldlname &lt;chr&gt;,\n#   stsldu &lt;chr&gt;, stslduname &lt;chr&gt;, stschool &lt;chr&gt;, stschoolname &lt;chr&gt;,\n#   stsecon &lt;chr&gt;, stseconname &lt;chr&gt;, trib &lt;chr&gt;, tribname &lt;chr&gt;, tsub &lt;chr&gt;,\n#   tsubname &lt;chr&gt;, stanrc &lt;chr&gt;, stanrcname &lt;chr&gt;, necta &lt;chr&gt;, …\n\n\nThis downloads the census tract shapefiles:\n\nallegheny_tracts &lt;- get_decennial(geography = \"tract\",\n                           variables = c(total_pop = \"P001001\"),\n                           state = \"PA\",\n                           county = \"Allegheny County\",\n                           geometry = TRUE,\n                           output = \"wide\",\n                           year = 2010)\n\nallegheny_tracts\n\nSimple feature collection with 402 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -80.36087 ymin: 40.19435 xmax: -79.68885 ymax: 40.67494\nGeodetic CRS:  NAD83\n# A tibble: 402 × 4\n   GEOID       NAME                          total_pop                  geometry\n   &lt;chr&gt;       &lt;chr&gt;                             &lt;dbl&gt;        &lt;MULTIPOLYGON [°]&gt;\n 1 42003412002 Census Tract 4120.02, Allegh…      4865 (((-80.07936 40.58043, -…\n 2 42003413100 Census Tract 4131, Allegheny…      6609 (((-80.06788 40.60846, -…\n 3 42003413300 Census Tract 4133, Allegheny…      4742 (((-80.03822 40.55349, -…\n 4 42003416000 Census Tract 4160, Allegheny…      1636 (((-79.77054 40.56028, -…\n 5 42003417200 Census Tract 4172, Allegheny…      1260 (((-79.78122 40.54113, -…\n 6 42003423000 Census Tract 4230, Allegheny…      2801 (((-79.90692 40.4871, -7…\n 7 42003426800 Census Tract 4268, Allegheny…      5369 (((-79.94408 40.53137, -…\n 8 42003428100 Census Tract 4281, Allegheny…      1242 (((-79.97941 40.47738, -…\n 9 42003429500 Census Tract 4295, Allegheny…      4212 (((-80.01937 40.55063, -…\n10 42003431100 Census Tract 4311, Allegheny…      3380 (((-80.05242 40.49402, -…\n# ℹ 392 more rows\n\n\nThis is the shapefile of the rivers:\n\nrivers &lt;- st_read(\"post_data/Allegheny_County_Major_Rivers/Allegheny_County_Major_Rivers.shp\") %&gt;% \n  group_by(NAME) %&gt;% \n  summarize() %&gt;% \n  filter(!is.na(NAME))\n\nReading layer `Allegheny_County_Major_Rivers' from data source \n  `/Users/conorotompkins/Documents/github_repos/ctompkins_quarto_blog/posts/how-many-pittsburghers-cross-the-river-to-get-to-work/post_data/Allegheny_County_Major_Rivers/Allegheny_County_Major_Rivers.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 4 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -80.23017 ymin: 40.19435 xmax: -79.68877 ymax: 40.66965\nGeodetic CRS:  WGS 84\n\nrivers\n\nSimple feature collection with 3 features and 1 field\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -80.23017 ymin: 40.19435 xmax: -79.68877 ymax: 40.66965\nGeodetic CRS:  WGS 84\n# A tibble: 3 × 2\n  NAME                                                                  geometry\n* &lt;chr&gt;                                                            &lt;POLYGON [°]&gt;\n1 Allegheny River   ((-80.01324 40.44201, -80.01323 40.44203, -80.01316 40.4421…\n2 Monongahela River ((-80.01541 40.43983, -80.01531 40.43984, -80.01519 40.4396…\n3 Ohio River        ((-80.01329 40.44192, -80.01423 40.4447, -80.01433 40.4447,…\n\n\nThese are the rivers:\n\nrivers %&gt;% \n  ggplot() +\n    geom_sf(aes(color = NAME), show.legend = FALSE) +\n    geom_sf_label(aes(label = NAME, fill = NAME), show.legend = FALSE) +\n    theme_graph()\n\n\n\n\n\n\n\n\nThis shows the outlines of the tracts used in the analysis.\n\nallegheny_tracts %&gt;% \n  ggplot() +\n    geom_sf() +\n    theme_minimal()\n\n\n\n\n\n\n\n\nNext I read in the main LODES data. This is a big file, so it takes a moment.\n\ndf &lt;- vroom(\"post_data/pa_od_main_JT00_2017.csv.gz\", col_types = cols(.default = \"c\")) %&gt;% \n  mutate(S000 = as.numeric(S000)) %&gt;% \n  select(h_geocode, w_geocode, S000)\n\ndf\n\n# A tibble: 5,034,208 × 3\n   h_geocode       w_geocode        S000\n   &lt;chr&gt;           &lt;chr&gt;           &lt;dbl&gt;\n 1 420010301022005 420010301011003     1\n 2 420010303002001 420010301011003     1\n 3 420010301023038 420010301011012     1\n 4 420010314011078 420010301011012     1\n 5 420010301011027 420010301011016     1\n 6 420010301011033 420010301011016     1\n 7 420010301011038 420010301011016     1\n 8 420010301011116 420010301011016     1\n 9 420010301011123 420010301011016     1\n10 420010302001018 420010301011016     1\n# ℹ 5,034,198 more rows\n\n\nNext I summarize the number of commuters per home-work tract combination. The original file uses census block codes, which are too granular for this analysis. I link the blocks to census tracts and aggregate to that level.\n\ndf_tracts_summarized &lt;- df %&gt;% \n  group_by(h_geocode, w_geocode) %&gt;% \n  summarize(commuters = sum(S000)) %&gt;% \n  ungroup() %&gt;% \n  arrange(desc(commuters))\n\ndf_tracts_summarized &lt;- df_tracts_summarized %&gt;% \n  left_join(geo_crosswalk %&gt;% select(tabblk2010, trct), by = c(\"h_geocode\" = \"tabblk2010\")) %&gt;% \n  rename(h_tract = trct) %&gt;% \n  left_join(geo_crosswalk %&gt;% select(tabblk2010, trct), by = c(\"w_geocode\" = \"tabblk2010\")) %&gt;% \n  rename(w_tract = trct)\n\ndf_tracts_summarized &lt;- df_tracts_summarized %&gt;% \n  group_by(h_tract, w_tract) %&gt;% \n  summarize(commuters = sum(commuters)) %&gt;% \n  ungroup() %&gt;% \n  arrange(desc(commuters))\n\ndf_tracts_summarized &lt;- df_tracts_summarized %&gt;% \n  semi_join(allegheny_tracts, by = c(\"h_tract\" = \"GEOID\")) %&gt;% \n  semi_join(allegheny_tracts, by = c(\"w_tract\" = \"GEOID\"))\n\n# df_tracts_summarized %&gt;% \n#    summarize(jobs = sum(commuters))\n# 479006 total commuters\n\nThis code finds the center of each tract, which I use as the nodes in the network plots:\n\nallegheny_tracts &lt;- allegheny_tracts %&gt;% \n  arrange(GEOID)\n\nallegheny_tracts_centroids &lt;- cbind(allegheny_tracts,\n                                    st_coordinates(st_centroid(allegheny_tracts))) %&gt;% \n  st_set_geometry(NULL) %&gt;% \n  as_tibble() %&gt;% \n  rename(x = X,\n         y = Y) %&gt;% \n  select(GEOID, x, y)\n\nThis shows that the centroids correctly appear in the center of each tract:\n\nallegheny_tracts %&gt;% \n  ggplot() +\n    geom_sf() +\n    geom_point(data = allegheny_tracts_centroids, aes(x, y), size = .2) +\n    geom_sf(data = rivers, aes(color = NAME), show.legend = FALSE) +\n    geom_sf_label(data = rivers, aes(color = NAME, label = NAME),\n                  show.legend = FALSE) +\n    theme_void()\n\n\n\n\n\n\n\n\nHere I filter on commuter lines that have at least 25 commuters.\n\ng &lt;- df_tracts_summarized %&gt;% \n  as_tbl_graph(directed = TRUE) %&gt;% \n  activate(edges) %&gt;% \n  filter(commuters &gt; 25)\n\ng\n\n# A tbl_graph: 402 nodes and 2969 edges\n#\n# A directed multigraph with 17 components\n#\n# Edge Data: 2,969 × 3 (active)\n    from    to commuters\n   &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;\n 1     1     1       723\n 2     2     1       620\n 3     2     2       488\n 4     3     1       487\n 5     4     1       442\n 6     5     1       399\n 7     6     1       371\n 8     7     1       364\n 9     8     1       358\n10     9     1       355\n# ℹ 2,959 more rows\n#\n# Node Data: 402 × 1\n  name       \n  &lt;chr&gt;      \n1 42003020100\n2 42003409000\n3 42003191800\n# ℹ 399 more rows\n\n# df_tracts_summarized %&gt;%\n#   as_tbl_graph(directed = TRUE) %&gt;%\n#   activate(edges) %&gt;%\n#   filter(commuters &gt; 25) %&gt;%\n#   as_tibble() %&gt;%\n#   summarize(jobs = sum(commuters))\n# 184404 total commuters\n\nHere I set a manual layout for the ggraph object. I use the centroids of the census tracts as the nodes in the network graph.\n\nnode_pos &lt;- allegheny_tracts_centroids\n\nmanual_layout &lt;- create_layout(graph = g,\n                               layout = node_pos)\n\nmanual_layout %&gt;% \n  as_tibble()\n\n# A tibble: 402 × 7\n   GEOID           x     y name        .ggraph.orig_index .ggraph.index circular\n   &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;                    &lt;int&gt;         &lt;int&gt; &lt;lgl&gt;   \n 1 42003010300 -80.0  40.4 42003020100                  1             1 FALSE   \n 2 42003020100 -80.0  40.4 42003409000                  2             2 FALSE   \n 3 42003020300 -80.0  40.5 42003191800                  3             3 FALSE   \n 4 42003030500 -80.0  40.4 42003412001                  4             4 FALSE   \n 5 42003040200 -80.0  40.4 42003411000                  5             5 FALSE   \n 6 42003040400 -79.9  40.4 42003456003                  6             6 FALSE   \n 7 42003040500 -80.0  40.4 42003191700                  7             7 FALSE   \n 8 42003040600 -80.0  40.4 42003413100                  8             8 FALSE   \n 9 42003040900 -80.0  40.4 42003473100                  9             9 FALSE   \n10 42003050100 -80.0  40.4 42003426300                 10            10 FALSE   \n# ℹ 392 more rows\n\n\nThis graphs the commuter lines on top of the census tracts and rivers:\n\nggraph(manual_layout) +\n  geom_sf(data = allegheny_tracts, color = \"dark grey\", fill = NA) +\n  geom_sf(data = rivers, aes(color = NAME), show.legend = FALSE) +\n  geom_edge_fan(aes(edge_width = log10(commuters), \n                    edge_alpha = log10(commuters)),\n                arrow = arrow(length = unit(.5, 'lines')), \n                start_cap = circle(.1, 'lines'),\n                end_cap = circle(.2, 'lines'),\n                color = \"white\",\n                strength = .5) +\n  scale_edge_width_continuous(range = c(.1, 1)) +\n  scale_edge_alpha_continuous(range = c(.01, .4)) +\n  labs(x = NULL,\n       y = NULL,\n       title = \"Where do people commute from/to for work?\",\n       subtitle = \"Excludes within-tract commuters\",\n       caption = \"Based on 2017 US Census LODES dataset | @conor_tompkins\") +\n  theme_graph() +\n  theme(legend.background = element_rect(fill = \"black\"),\n        legend.text = element_text(color = \"white\"),\n        legend.title = element_text(color = \"white\"),\n        panel.background = element_rect(fill = \"black\"))\n\n\n\n\n\n\n\n\nThis calculates the centroids I will use to draw lines later on:\n\nallegheny_lines &lt;- cbind(allegheny_tracts, st_coordinates(st_centroid(allegheny_tracts))) %&gt;% \n  select(-c(NAME, total_pop)) %&gt;% \n  st_drop_geometry()\n\nallegheny_lines %&gt;% \n  ggplot() +\n    geom_point(aes(X, Y)) +\n    theme_minimal()\n\n\n\n\n\n\n\n\nHere I calculate the edges and nodes for the network graph:\n\ndf_edges &lt;- g %&gt;% \n  activate(edges) %&gt;% \n  as_tibble()\n\n# df_edges %&gt;%\n#   summarize(commuters = sum(commuters))\n# 184404 total commuters\n\ndf_nodes &lt;- g %&gt;% \n  activate(nodes) %&gt;% \n  as_tibble() %&gt;% \n  mutate(id = row_number())\n\nThe df_lines is pivoted long so there is a “to” and “from” row for each commuter line:\n\ndf_lines &lt;- df_edges %&gt;% \n  mutate(line_id = row_number()) %&gt;% \n  pivot_longer(c(from, to), names_to = \"point_type\", values_to = \"edge_id\")\n\ndf_lines\n\n# A tibble: 5,938 × 4\n   commuters line_id point_type edge_id\n       &lt;dbl&gt;   &lt;int&gt; &lt;chr&gt;        &lt;int&gt;\n 1       723       1 from             1\n 2       723       1 to               1\n 3       620       2 from             2\n 4       620       2 to               1\n 5       488       3 from             2\n 6       488       3 to               2\n 7       487       4 from             3\n 8       487       4 to               1\n 9       442       5 from             4\n10       442       5 to               1\n# ℹ 5,928 more rows\n\n# df_lines %&gt;%\n#   distinct(line_id, commuters) %&gt;%\n#   summarize(jobs = sum(commuters))\n# 184404 total commuters\n\nSince some commuter “lines” are really just points that start and end at the same centroid, I separate the commuter “lines” from the “points” for purposes of manipulating the geometries.\n\ndf_line_types &lt;- df_lines %&gt;% \n  pivot_wider(names_from = point_type, values_from = edge_id) %&gt;% \n  mutate(line_type = case_when(from == to ~ \"point\",\n                               from != to ~ \"linestring\")) %&gt;% \n  pivot_longer(cols = c(from, to), names_to = \"edge_type\", values_to = \"edge_id\")\n\ndf_line_types\n\n# A tibble: 5,938 × 5\n   commuters line_id line_type  edge_type edge_id\n       &lt;dbl&gt;   &lt;int&gt; &lt;chr&gt;      &lt;chr&gt;       &lt;int&gt;\n 1       723       1 point      from            1\n 2       723       1 point      to              1\n 3       620       2 linestring from            2\n 4       620       2 linestring to              1\n 5       488       3 point      from            2\n 6       488       3 point      to              2\n 7       487       4 linestring from            3\n 8       487       4 linestring to              1\n 9       442       5 linestring from            4\n10       442       5 linestring to              1\n# ℹ 5,928 more rows\n\n# df_line_types %&gt;%\n#   distinct(line_id, commuters) %&gt;%\n#   summarize(commuters = sum(commuters))\n# # 184404 total commuters\n\n\ndf_linestrings &lt;- df_line_types %&gt;% \n  filter(line_type == \"linestring\")\n\ndf_points &lt;- df_line_types %&gt;% \n  filter(line_type == \"point\")\n\nThis creates the geometries for the lines, points, and rivers. Then I set them all to the same coordinate system with st_set_crs.\n\ndf_linestrings &lt;- df_linestrings %&gt;% \n  left_join(df_nodes, by = c(\"edge_id\" = \"id\")) %&gt;% \n  left_join(allegheny_lines, by = c(\"name\" = \"GEOID\")) %&gt;% \n  st_as_sf(coords = c(\"X\", \"Y\")) %&gt;% \n  group_by(line_id, commuters) %&gt;%\n  summarise() %&gt;% # union points into lines using our created lineid\n  st_cast(\"LINESTRING\") %&gt;% \n  st_set_crs(4326)\n\n# df_linestrings %&gt;% \n#   ungroup() %&gt;% \n#   st_drop_geometry() %&gt;% \n#   summarize(commuters = sum(commuters))\n\n# 167409 commuters that change tracts\n\ndf_points &lt;- df_points %&gt;% \n  left_join(df_nodes, by = c(\"edge_id\" = \"id\")) %&gt;% \n  left_join(allegheny_lines, by = c(\"name\" = \"GEOID\")) %&gt;% \n  st_as_sf(coords = c(\"X\", \"Y\")) %&gt;% \n  group_by(line_id, commuters) %&gt;%\n  summarise() %&gt;%\n  st_cast(\"POINT\") %&gt;% \n  st_set_crs(4326)\n\n# df_points %&gt;% \n#   ungroup() %&gt;% \n#   st_drop_geometry() %&gt;% \n#   summarize(commuters = sum(commuters))\n\n# 16995 commuters that stay within a tract\n\n\nrivers &lt;- rivers %&gt;% \n  st_set_crs(4326)\n\nHere I calculate which commuter lines intersect with which rivers using st_intersects:\n\ndf_linestrings_intersect &lt;- df_linestrings %&gt;% \n  ungroup() %&gt;% \n  mutate(intersects_ohio = st_intersects(., rivers %&gt;% \n                                            filter(NAME == \"Ohio River\")) %&gt;% as.logical(),\n         intersects_allegheny = st_intersects(., rivers %&gt;% \n                                                filter(NAME == \"Allegheny River\")) %&gt;% as.logical(),\n         intersects_monongahela = st_intersects(., rivers %&gt;% \n                                                  filter(NAME == \"Monongahela River\")) %&gt;% as.logical())\n\ndf_commuter_rivers &lt;- df_linestrings_intersect %&gt;% \n  pivot_longer(c(contains(\"intersects\")), names_to = \"river_intersected\", values_to = \"value\") %&gt;% \n  mutate(value = case_when(is.na(value) ~ FALSE,\n                           !is.na(value) ~ value))\n \n# df_commuter_rivers %&gt;%\n#   distinct(line_id, commuters) %&gt;%\n#   summarize(jobs = sum(commuters))\n\n# 167409 commuters that change tracts\n\nThis shows that the intersection calculation was successful:\n\ndf_commuter_rivers %&gt;% \n  filter(value == TRUE) %&gt;% \n  ggplot() +\n    geom_sf(data = allegheny_tracts, color = NA, show.legend = FALSE) +\n    geom_sf(data = rivers, \n            aes(color = NAME),\n            show.legend = FALSE) +\n    geom_sf(aes(geometry = geometry,\n                size = commuters),\n            show.legend = TRUE) +\n    facet_wrap(~river_intersected,\n               ncol = 1) +\n    guides(color = FALSE,\n           size = FALSE) +\n    theme_graph() +\n    scale_size_continuous(range = c(.1, .5))\n\n\n\n\n\n\n\n\nThis combines the dataframes with the lines and points, and then summarizes to count how many of the geometries intersected with a river:\n\n#this was double counting\ndf_commuter_rivers_combined &lt;- df_commuter_rivers %&gt;% \n  bind_rows(df_points %&gt;% \n              mutate(value = FALSE))\n\n#use for lookup\ndf_lines_that_cross_rivers &lt;- df_commuter_rivers_combined %&gt;% \n  group_by(line_id) %&gt;% \n  summarize(rivers_crossed = sum(value)) %&gt;% \n  ungroup()\n\n#find the distinct line_ids and then summarize \ndf_commuter_rivers_summary &lt;- df_commuter_rivers_combined %&gt;% \n  distinct(line_id, commuters) %&gt;% \n  left_join(df_lines_that_cross_rivers) %&gt;% \n  group_by(rivers_crossed) %&gt;% \n  summarize(commuters = sum(commuters)) %&gt;% \n  ungroup()\n  \n\n# df_commuter_rivers_summary %&gt;% \n#   summarize(commuters = sum(commuters))\n# 184404 total commuters\n\ndf_commuter_rivers_summary %&gt;% \n  ggplot(aes(rivers_crossed, commuters)) +\n    geom_col(color = \"black\") +\n    geom_text(aes(y = commuters + 5000, label = scales::comma(commuters))) +\n    scale_y_continuous(labels = scales::comma) +\n    labs(title = \"Commuter travel patterns\",\n         subtitle = \"2017 U.S. Census LODES dataset\",\n         x = \"Rivers crossed\",\n         y = \"Number of commuters\",\n         caption = \"@conor_tompkins\") +\n    theme_bw()\n\n\n\n\n\n\n\n\nIt is always reassuring when your analysis doesn’t stray to far from conventional wisdom. Very few Pittsburghers in the dataset cross two rivers to get to work, and none cross three."
  },
  {
    "objectID": "posts/how-many-pittsburghers-cross-the-river-to-get-to-work/index.html#mapping-commuter-patterns",
    "href": "posts/how-many-pittsburghers-cross-the-river-to-get-to-work/index.html#mapping-commuter-patterns",
    "title": "How Many Pittsburghers Cross the River to Get to Work",
    "section": "Mapping commuter patterns",
    "text": "Mapping commuter patterns\nThe next step is to put this data on a map, since it is obviously spatial. The goal is to calculate the percentage of each census tract’s “from” commuters that crossed zero, one, two, and 3 rivers.\nThis prepares the edge data to be used to make a chloropleth map:\n\ndf_lines_chloro &lt;- df_edges %&gt;% \n  mutate(line_id = row_number()) %&gt;% \n  pivot_longer(c(from, to), names_to = \"point_type\", values_to = \"edge_id\")\n\ndf_lines_chloro\n\n# A tibble: 5,938 × 4\n   commuters line_id point_type edge_id\n       &lt;dbl&gt;   &lt;int&gt; &lt;chr&gt;        &lt;int&gt;\n 1       723       1 from             1\n 2       723       1 to               1\n 3       620       2 from             2\n 4       620       2 to               1\n 5       488       3 from             2\n 6       488       3 to               2\n 7       487       4 from             3\n 8       487       4 to               1\n 9       442       5 from             4\n10       442       5 to               1\n# ℹ 5,928 more rows\n\n# df_lines_chloro %&gt;%\n#   distinct(line_id, commuters) %&gt;%\n#   summarize(commuters = sum(commuters))\n# 184404 total commuters\n\nThe next few steps are largely just coercing the geometry to do what I want it to do. Interested parties can read the code.\n\ndf_line_types_chloro &lt;- df_lines_chloro %&gt;% \n  pivot_wider(names_from = point_type, values_from = edge_id) %&gt;% \n  mutate(line_type = case_when(from == to ~ \"point\",\n                               from != to ~ \"linestring\")) %&gt;% \n  pivot_longer(cols = c(from, to), names_to = \"edge_type\", values_to = \"edge_id\")\n\n# df_line_types_chloro %&gt;%\n#    distinct(line_id, commuters) %&gt;%\n#    summarize(commuters = sum(commuters))\n#184404 total commuters\n\n\ndf_linestrings_chloro &lt;- df_line_types_chloro %&gt;% \n  filter(line_type == \"linestring\")\n\n# df_linestrings_chloro %&gt;%\n#   distinct(line_id, commuters) %&gt;%\n#   summarize(commuters = sum(commuters))\n#167409 commuters that change tracts\n\ndf_linestrings_chloro_lookup &lt;- df_linestrings_chloro %&gt;% \n  select(line_id, edge_type, edge_id) %&gt;% \n  pivot_wider(names_from = edge_type, values_from = edge_id)\n\ndf_points_chloro &lt;- df_line_types %&gt;% \n  filter(line_type == \"point\")\n\n# df_points_chloro %&gt;%\n#   distinct(line_id, commuters) %&gt;%\n#   summarize(commuters = sum(commuters))\n#16995 commuters that stay within a tract\n\n\ndf_linestrings_chloro &lt;- df_linestrings_chloro %&gt;% \n  left_join(df_nodes, by = c(\"edge_id\" = \"id\")) %&gt;% \n  left_join(allegheny_lines, by = c(\"name\" = \"GEOID\")) %&gt;% \n  st_as_sf(coords = c(\"X\", \"Y\")) %&gt;% \n  group_by(line_id, commuters) %&gt;%\n  summarise() %&gt;% # union points into lines using our created lineid\n  st_cast(\"LINESTRING\") %&gt;% \n  st_set_crs(4326) %&gt;% \n  left_join(df_linestrings_chloro_lookup, by = c(\"line_id\" = \"line_id\")) %&gt;% \n  left_join(df_nodes, by = c(\"from\" = \"id\")) %&gt;% \n  left_join(allegheny_lines, by = c(\"name\" = \"GEOID\"))\n\n# df_linestrings_chloro %&gt;%\n#   ungroup() %&gt;%\n#   st_drop_geometry() %&gt;%\n#   summarize(commuters = sum(commuters))\n#167409 commuters that change tracts\n\ndf_points_chloro &lt;- df_points_chloro %&gt;% \n  left_join(df_nodes, by = c(\"edge_id\" = \"id\")) %&gt;% \n  left_join(allegheny_lines, by = c(\"name\" = \"GEOID\")) %&gt;% \n  st_as_sf(coords = c(\"X\", \"Y\")) %&gt;% \n  group_by(line_id, name, commuters) %&gt;%\n  st_cast(\"POINT\") %&gt;% \n  st_set_crs(4326)\n\n# df_points_chloro %&gt;%\n#   ungroup() %&gt;%\n#   st_drop_geometry() %&gt;%\n#   distinct(line_id, commuters) %&gt;%\n#   summarize(commuters = sum(commuters))\n#16995 commuters that stay within a tract\n\n\ndf_linestrings_chloro_intersect &lt;- df_linestrings_chloro %&gt;% \n  ungroup() %&gt;% \n  mutate(intersects_ohio = st_intersects(., rivers %&gt;% \n                                            filter(NAME == \"Ohio River\")) %&gt;% as.logical(),\n         intersects_allegheny = st_intersects(., rivers %&gt;% \n                                                filter(NAME == \"Allegheny River\")) %&gt;% as.logical(),\n         intersects_monongahela = st_intersects(., rivers %&gt;% \n                                                  filter(NAME == \"Monongahela River\")) %&gt;% as.logical()) %&gt;% \n  st_set_geometry(NULL) %&gt;% \n  select(-c(from, to)) %&gt;% \n  rename(from = name)\n\n# df_linestrings_chloro_intersect %&gt;%\n#   summarize(commuters = sum(commuters))\n#167409 commuters that change tracts\n\n\ndf_linestrings_chloro_intersect &lt;- df_linestrings_chloro_intersect %&gt;% \n  mutate_at(vars(contains(\"intersect\")), ~case_when(is.na(.) ~ FALSE,\n                           !is.na(.) ~ .)) %&gt;%\n  mutate(no_intersect = case_when(intersects_allegheny == FALSE & intersects_monongahela == FALSE & intersects_ohio == FALSE ~ TRUE,\n                                  TRUE ~ FALSE)) %&gt;% \n  select(line_id, from, contains(\"intersect\"), commuters) %&gt;% \n  pivot_longer(contains(\"intersect\"), names_to = \"river_intersected\", values_to = \"value\")\n\n# df_linestrings_chloro_intersect %&gt;%\n#   distinct(line_id, commuters) %&gt;%\n#   summarize(commuters = sum(commuters))\n#167409 commuters that change tracts\n\n\ndf_linestrings_chloro_intersect &lt;- df_linestrings_chloro_intersect %&gt;% \n  mutate(commuters = case_when(value == FALSE ~ 0,\n                               TRUE ~ commuters))\n\n# df_linestrings_chloro_intersect %&gt;%\n#   distinct(line_id, commuters) %&gt;%\n#   summarize(commuters = sum(commuters))\n#167409 commuters that change tracts\n\n\ndf_linestrings_chloro_intersect &lt;- df_linestrings_chloro_intersect %&gt;% \n  filter(value == TRUE) %&gt;% \n  group_by(line_id, from, commuters) %&gt;% \n  summarize(count_rivers_intersected = sum(river_intersected != \"no_intersect\")) %&gt;% \n  ungroup()\n\n# df_linestrings_chloro_intersect %&gt;%\n#   summarize(commuters = sum(commuters))\n#167409 commuters that change tracts  \n\n\ndf_points_chloro &lt;- cbind(df_points_chloro, st_coordinates(st_centroid(df_points_chloro))) %&gt;% \n  st_drop_geometry() %&gt;% \n  rename(from = name) %&gt;% \n  distinct(from, line_id, commuters) %&gt;% \n  mutate(count_rivers_intersected = 0)\n  \n\n# df_points_chloro %&gt;%\n#    summarize(commuters = sum(commuters))\n#16995 commuters that stay within a tract\n\n\ndf_combined &lt;- bind_rows(df_linestrings_chloro_intersect, df_points_chloro)\n\n# df_combined %&gt;%\n#     filter(is.na(from))\n# \n# df_combined %&gt;%\n#    summarize(commuters = sum(commuters))\n#184404 total commuters\n\n\ndf_combined &lt;- df_combined %&gt;% \n  arrange(from, desc(count_rivers_intersected), desc(commuters))\n\nThese are the final steps to create the chloropleth:\n\ndf_combined\n\n# A tibble: 2,969 × 4\n   line_id from        commuters count_rivers_intersected\n     &lt;int&gt; &lt;chr&gt;           &lt;dbl&gt;                    &lt;dbl&gt;\n 1     748 42003010300        63                        0\n 2    2192 42003020100        31                        2\n 3    1372 42003020100        42                        1\n 4    2282 42003020100        30                        1\n 5       1 42003020100       723                        0\n 6     632 42003020100        70                        0\n 7     730 42003020100        64                        0\n 8    1102 42003020100        49                        0\n 9    1761 42003020100        36                        0\n10     350 42003020300       112                        0\n# ℹ 2,959 more rows\n\n\nThis counts the number of commuters per “from” tract and “count of rivers intersected”. Note that there are multiple rows per “from” tract.\n\ndf_chloro_map &lt;- df_combined %&gt;%\n  ungroup() %&gt;% \n  group_by(from, count_rivers_intersected) %&gt;% \n  summarize(total_commuters = sum(commuters)) %&gt;% \n  ungroup()\n\ndf_chloro_map\n\n# A tibble: 723 × 3\n   from        count_rivers_intersected total_commuters\n   &lt;chr&gt;                          &lt;dbl&gt;           &lt;dbl&gt;\n 1 42003010300                        0              63\n 2 42003020100                        0             942\n 3 42003020100                        1              72\n 4 42003020100                        2              31\n 5 42003020300                        0             258\n 6 42003030500                        0             231\n 7 42003040200                        0             143\n 8 42003040400                        0             108\n 9 42003040500                        0              87\n10 42003040600                        0              59\n# ℹ 713 more rows\n\n# df_chloro_map %&gt;%\n#   filter(is.na(count_rivers_intersected))\n# \n# df_chloro_map %&gt;% \n#     summarize(commuters = sum(total_commuters))\n#184404 total commuters\n\nThe next step is to calculate the percent of a “from” tract’s commuters that crossed a given number of rivers:\n\ndf_chloro_map &lt;- df_chloro_map %&gt;% \n  group_by(from) %&gt;% \n  mutate(pct_of_commuters = total_commuters / sum(total_commuters)) %&gt;% \n  ungroup()\n\ndf_chloro_map\n\n# A tibble: 723 × 4\n   from        count_rivers_intersected total_commuters pct_of_commuters\n   &lt;chr&gt;                          &lt;dbl&gt;           &lt;dbl&gt;            &lt;dbl&gt;\n 1 42003010300                        0              63           1     \n 2 42003020100                        0             942           0.901 \n 3 42003020100                        1              72           0.0689\n 4 42003020100                        2              31           0.0297\n 5 42003020300                        0             258           1     \n 6 42003030500                        0             231           1     \n 7 42003040200                        0             143           1     \n 8 42003040400                        0             108           1     \n 9 42003040500                        0              87           1     \n10 42003040600                        0              59           1     \n# ℹ 713 more rows\n\n# df_chloro_map %&gt;% \n#   summarize(commuters = sum(total_commuters))\n#184404 total commuters\n\nThen I join df_chloro_map against the census tract geometry to get a complete list of all the tracts. I use complete to add rows for combinations of “from” tracts and count_rivers_intersected that did not appear in the data. Those added rows are given 0 for pct_of_commuters and total_commuters.\n\ndf_chloro_map &lt;- df_chloro_map %&gt;% \n  right_join(allegheny_tracts %&gt;% select(GEOID) %&gt;% st_set_geometry(NULL), by = c(\"from\" = \"GEOID\")) %&gt;% \n  complete(from, count_rivers_intersected = c(0, 1, 2)) %&gt;%\n  filter(!is.na(count_rivers_intersected)) %&gt;% #exclude tracts brought in from the right_join\n  replace_na(list(pct_of_commuters = 0, total_commuters = 0))\n\n# df_chloro_map %&gt;% \n#   filter(is.na(count_rivers_intersected))\n# \n# df_chloro_map %&gt;% \n#   summarize(commuters = sum(total_commuters, na.rm = TRUE))\n#184404 total commuters\n\nThe final step is to right join againt the census tract data to bring over the geometry.\n\ndf_chloro_map &lt;- df_chloro_map %&gt;% \n  right_join(allegheny_tracts, by = c(\"from\" = \"GEOID\"))\n\nglimpse(df_chloro_map)\n\nRows: 1,206\nColumns: 7\n$ from                     &lt;chr&gt; \"42003010300\", \"42003010300\", \"42003010300\", …\n$ count_rivers_intersected &lt;dbl&gt; 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, …\n$ total_commuters          &lt;dbl&gt; 63, 0, 0, 942, 72, 31, 258, 0, 0, 231, 0, 0, …\n$ pct_of_commuters         &lt;dbl&gt; 1.00000000, 0.00000000, 0.00000000, 0.9014354…\n$ NAME                     &lt;chr&gt; \"Census Tract 103, Allegheny County, Pennsylv…\n$ total_pop                &lt;dbl&gt; 6600, 6600, 6600, 3629, 3629, 3629, 616, 616,…\n$ geometry                 &lt;MULTIPOLYGON [°]&gt; MULTIPOLYGON (((-79.98077 4..., …\n\n\n\ndf_chloro_map %&gt;% \n  mutate(count_rivers_intersected = str_c(\"Rivers intersected:\", count_rivers_intersected, sep = \" \")) %&gt;% \n  ggplot() +\n    geom_sf(aes(geometry = geometry,\n                fill = pct_of_commuters),\n            color = NA) +\n    geom_sf(data = rivers,\n            aes(color = NAME),\n            size = 1,\n            show.legend = FALSE) +\n    facet_wrap(~count_rivers_intersected,\n               nrow = 1) +\n    scale_fill_viridis_c(\"% of commuters\",\n                         labels = scales::percent) +\n    labs(title = \"Commuter travel patterns\",\n         subtitle = \"2017 U.S. Census LODES dataset\",\n         caption = \"@conor_tompkins\") +\n    theme_void()\n\n\n\n\n\n\n\n\n\nErrata\nThe previous version of the bar chart double counted the commuters. This is the old version:\n\ndf_commuter_rivers_summary &lt;- df_commuter_rivers %&gt;% \n  bind_rows(df_points %&gt;% \n              mutate(value = FALSE)) %&gt;% \n  group_by(line_id) %&gt;% \n  summarize(rivers_crossed = sum(value),\n            commuters = sum(commuters))\n\ndf_commuter_rivers_summary %&gt;% \n  st_drop_geometry() %&gt;%\n  group_by(rivers_crossed) %&gt;% \n  summarize(commuters = sum(commuters)) %&gt;% \n  ggplot(aes(rivers_crossed, commuters)) +\n    geom_col() +\n    scale_y_continuous(labels = scales::comma) +\n    labs(title = \"Commuter travel patterns\",\n         subtitle = \"2017 U.S. Census LODES dataset\",\n         x = \"Rivers crossed\",\n         y = \"Number of commuters\",\n         caption = \"@conor_tompkins\") +\n    theme_bw()\n\n\n\n\n\n\n\n\nA similar but less impactful bug affected the chloropleth chart. For completeness, this is the old verison of that graph: \n\n\nReferences\n\nhttps://lehd.ces.census.gov/data/\nhttps://lehd.ces.census.gov/data/lodes/LODES7/LODESTechDoc7.4.pdf\nhttps://lehd.ces.census.gov/doc/workshop/2017/Presentations/TheaEvans.pdf\nhttps://medium.com/@urban_institute/open-accessible-data-on-jobs-and-workers-tract-level-lodes-data-945fcac9e280"
  },
  {
    "objectID": "posts/shifting_political_winds/index.html",
    "href": "posts/shifting_political_winds/index.html",
    "title": "Shifting political winds",
    "section": "",
    "text": "The purpose of this post is to recreate the “Shift from 2016” arrow map that the New York Times used to show which counties became more Democratic or Republican-leaning from 2016 to 2020. This is a screenshot of the NYTimes figure:\nI will use county-level Presidential election data from the MIT Election Data + Science Lab to recreate the chart. Since 2020 results are not final yet, I will focus on data from 2000-2016. I ran into multiple issues with the dataset, which I explain in the Code section below. The most signifcant issue was with the data from Alaska, which I excluded from the charts below because of problems with the data."
  },
  {
    "objectID": "posts/shifting_political_winds/index.html#recreating-the-nytimes-figure",
    "href": "posts/shifting_political_winds/index.html#recreating-the-nytimes-figure",
    "title": "Shifting political winds",
    "section": "Recreating the NYTimes Figure",
    "text": "Recreating the NYTimes Figure\nMy approach is to use {ggplot2} and {sf} to map the data and draw arrows at angles to display shifts in the Democratic margin.\nThis is the dataframe I use to make the final map. It contains the year, state, county, FIPS code, county and state geometries, and election results per county.\n\nglimpse(shift_map)\n\nRows: 12,610\nColumns: 22\n$ year                          &lt;dbl&gt; 2004, 2008, 2012, 2016, 2004, 2008, 2012…\n$ state                         &lt;chr&gt; \"ALABAMA\", \"ALABAMA\", \"ALABAMA\", \"ALABAM…\n$ county_name                   &lt;chr&gt; \"AUTAUGA\", \"AUTAUGA\", \"AUTAUGA\", \"AUTAUG…\n$ county_fips                   &lt;chr&gt; \"01001\", \"01001\", \"01001\", \"01001\", \"010…\n$ candidatevotes_sum_democrat   &lt;dbl&gt; 4758, 6093, 6363, 5936, 15599, 19386, 18…\n$ candidatevotes_sum_republican &lt;dbl&gt; 15196, 17403, 17379, 18172, 52971, 61271…\n$ pct_vote_democrat             &lt;dbl&gt; 0.23845, 0.25932, 0.26801, 0.24623, 0.22…\n$ pct_vote_republican           &lt;dbl&gt; 0.7616, 0.7407, 0.7320, 0.7538, 0.7725, …\n$ dem_margin_pct                &lt;dbl&gt; -0.52310, -0.48136, -0.46399, -0.50755, …\n$ dem_margin_votes              &lt;dbl&gt; -10438, -11310, -11016, -12236, -37372, …\n$ shift_pct                     &lt;dbl&gt; -0.106746, 0.041745, 0.017371, -0.043561…\n$ shift_votes                   &lt;dbl&gt; -3387, -872, 294, -1220, -10497, -4513, …\n$ shift_pct_scaled              &lt;dbl&gt; 71.83, 91.08, 87.92, 80.02, 78.51, 89.01…\n$ shift_votes_scaled            &lt;dbl&gt; 16602, 11700, 10573, 12378, 30460, 18796…\n$ shift_pct_binary              &lt;chr&gt; \"Republican\", \"Democratic\", \"Democratic\"…\n$ shift_votes_binned            &lt;lgl&gt; FALSE, TRUE, TRUE, TRUE, FALSE, FALSE, F…\n$ geometry                      &lt;POLYGON [m]&gt; POLYGON ((1269841 -1303980,..., …\n$ center                        &lt;list&gt; &lt;POINT (1253837 -1285138)&gt;, &lt;POINT (125…\n$ lng0                          &lt;dbl&gt; 1253837, 1253837, 1253837, 1253837, 1177…\n$ lat0                          &lt;dbl&gt; -1285138, -1285138, -1285138, -1285138, …\n$ lng1                          &lt;dbl&gt; 1259015, 1253616, 1254221, 1255982, 1183…\n$ lat1                          &lt;dbl&gt; -1269365, -1273441, -1274572, -1272948, …\n\n\n\nshift_map_filtered &lt;- shift_map %&gt;% \n  filter(state != \"ALASKA\") %&gt;%\n  filter(year == 2016) %&gt;% \n  mutate(shift_pct_binary = case_when(sign(shift_pct) == 1 ~ \"Democratic\",\n                                      sign(shift_pct) == -1 ~ \"Republican\"),\n         shift_pct_binary = as.factor(shift_pct_binary)) %&gt;% \n  mutate(shift_votes_binned = abs(shift_votes) &lt;= 3000)\n\nggplot() +\n  geom_sf(data = filter(state_geo, !str_detect(NAME, \"ALASKA\")),\n          linewidth = .2,\n          fill = NA) +\n  geom_point(data = filter(shift_map_filtered, abs(shift_votes) &lt;= 1500),\n             aes(x = lng0, y = lat0,\n                 color = shift_pct_binary),\n             size = .75,\n             alpha = .3) +\n  geom_segment(data = filter(shift_map_filtered, abs(shift_votes) &gt; 1500),\n               aes(x = lng0, xend = lng1,\n                   y = lat0, yend = lat1,\n                   color = shift_pct_binary,\n                   linewidth = shift_votes,\n                   alpha = shift_votes_binned),\n               linejoin = \"mitre\",\n               arrow = arrow(length = unit(0.08, \"inches\"))) +\n  scale_color_manual(values = c(\"#1375B7\", \"#C93135\"), guide = guide_legend(title.position = \"top\")) +\n  scale_linewidth_continuous(range = c(.001, 2), guide = \"none\") +\n  scale_alpha_manual(values = c(1, .3), guide = \"none\") +\n  labs(color = \"Shift in election margin\") +\n  facet_wrap(~year) +\n  theme_void(base_size = 25) +\n  theme(legend.direction = \"horizontal\",\n        legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nThe starting point of the line is the centroid of the county. The length and width of the lines are scaled to the shift in terms of number of votes. The NYTimes figure treats the shift as a binary variable when it rescales to degrees of the angle. In their graph, a Democratic shift is about 45 degrees (diagonal left) and a Republican shift is about 135 degrees (diagonal right). My figure maintains the continuous nature of the shift in %. I use the range 0-180 in degrees to indicate the shift. 0 degrees (all the way left) indicates a 100% shift towards Democrats, 90 degrees (pointing upwards) indicates no change, and 180 degrees (all the way to the right) indicates a 100% shift towards Republicans.\nThe end point of the line is calculated using the sine and cosine of the margin shift in % (re-scaled to be interpreted as degrees of an angle) multiplied by the margin shift in votes (re-scaled to be interpreted as meters), which is added to the origin point.\nI lower the opacity of the lines in counties where the vote totals did not shift much. I use points instead of lines for counties where there was a very small shift in votes. This prevents overplotting in geographically dense areas with small populations.\nThis animation shows the shift in Presidential election margin from 2004-2016.\n\npolitical_winds_anim &lt;- shift_map %&gt;% \n  filter(state != \"ALASKA\") %&gt;% \n  mutate(id = str_c(state, county_name, county_fips)) %&gt;% \n  mutate(year = as.integer(year)) %&gt;% \n  mutate(shift_pct_binary = case_when(sign(shift_pct) == 1 ~ \"Democratic\",\n                                      sign(shift_pct) == -1 ~ \"Republican\"),\n         shift_pct_binary = as.factor(shift_pct_binary)) %&gt;% \n  mutate(shift_votes_binned = abs(shift_votes) &lt;= 3000) %&gt;% \n  ggplot() +\n  geom_sf(data = filter(state_geo, NAME != \"ALASKA\"),\n          linewidth = .2,\n          fill = NA) +\n  geom_segment(aes(x = lng0, xend = lng1,\n                   y = lat0, yend = lat1,\n                   color = shift_pct_binary,\n                   linewidth = shift_votes,\n                   alpha = shift_votes_binned,\n                   group = id),\n               linejoin = \"mitre\",\n               arrow = arrow(length = unit(0.09, \"inches\"))) +\n  scale_color_manual(values = c(\"#1375B7\", \"#C93135\"), guide = guide_legend(title.position = \"top\")) +\n  scale_linewidth_continuous(range = c(.001, 1.3), guide = \"none\") +\n  scale_alpha_manual(values = c(1, .3), guide = \"none\") +\n  theme_void(base_size = 25) +\n  theme(legend.direction = \"horizontal\",\n        legend.position = \"bottom\") +\n  transition_states(year) +\n  labs(title = \"Shift in Presidential election Democratic margin\",\n       subtitle = \"Year: {closest_state}\",\n       color = \"Shift in Democratic margin\")\n\npolitical_winds_anim\n\n\n\n\n\n\n\n\nIn the animation there is less overplotting, so I do not replace lines with dots for counties where there was a very small shift in votes."
  },
  {
    "objectID": "posts/shifting_political_winds/index.html#code",
    "href": "posts/shifting_political_winds/index.html#code",
    "title": "Shifting political winds",
    "section": "Code",
    "text": "Code\n\nIngest\n\n#election shift\n#script to clean data\n\n#data from https://electionlab.mit.edu/data\n\n#fips info\n#https://en.wikipedia.org/wiki/Federal_Information_Processing_Standard_state_code#FIPS_state_codes\n#https://en.wikipedia.org/wiki/List_of_United_States_FIPS_codes_by_county\n#changes https://www.census.gov/programs-surveys/geography/technical-documentation/county-changes.2010.html\n\n#read in data\ndata &lt;- read_csv(\"post_data/countypres_2000-2020.csv\",\n                 col_types = cols(\n                   year = col_double(),\n                   state = col_character(),\n                   state_po = col_character(),\n                   county_name = col_character(),\n                   county_fips = col_character(),\n                   office = col_character(),\n                   candidate = col_character(),\n                   party = col_character(),\n                   candidatevotes = col_double(),\n                   totalvotes = col_double(),\n                   version = col_double()\n                 )) %&gt;% \n  clean_names() |&gt;\n  filter(year &lt;= 2016,\n         mode == \"TOTAL\") |&gt; \n  select(-mode)\n\nglimpse(data)\n\nRows: 50,524\nColumns: 11\n$ year           &lt;dbl&gt; 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2…\n$ state          &lt;chr&gt; \"ALABAMA\", \"ALABAMA\", \"ALABAMA\", \"ALABAMA\", \"ALABAMA\", …\n$ state_po       &lt;chr&gt; \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"…\n$ county_name    &lt;chr&gt; \"AUTAUGA\", \"AUTAUGA\", \"AUTAUGA\", \"AUTAUGA\", \"BALDWIN\", …\n$ county_fips    &lt;chr&gt; \"01001\", \"01001\", \"01001\", \"01001\", \"01003\", \"01003\", \"…\n$ office         &lt;chr&gt; \"US PRESIDENT\", \"US PRESIDENT\", \"US PRESIDENT\", \"US PRE…\n$ candidate      &lt;chr&gt; \"AL GORE\", \"GEORGE W. BUSH\", \"RALPH NADER\", \"OTHER\", \"A…\n$ party          &lt;chr&gt; \"DEMOCRAT\", \"REPUBLICAN\", \"GREEN\", \"OTHER\", \"DEMOCRAT\",…\n$ candidatevotes &lt;dbl&gt; 4942, 11993, 160, 113, 13997, 40872, 1033, 578, 5188, 5…\n$ totalvotes     &lt;dbl&gt; 17208, 17208, 17208, 17208, 56480, 56480, 56480, 56480,…\n$ version        &lt;dbl&gt; 20220315, 20220315, 20220315, 20220315, 20220315, 20220…\n\n\n\n\nClean\nThis code filters out state-wide vote tabulations and then filters only on the two-party Presidential vote.\n\ndata &lt;- data %&gt;% \n  rename(fips_raw = county_fips) %&gt;% \n  #filter out state-wide ballot collection\n  filter(!(state == \"CONNECTICUT\" & county_name == \"STATEWIDE WRITEIN\")) %&gt;% \n  filter(!(state == \"MAINE\" & county_name == \"MAINE UOCAVA\")) %&gt;% \n  filter(!(state == \"RHODE ISLAND\" & county_name == \"FEDERAL PRECINCT\"))\n\n#filter for only 2-party vote in presidential elections\ndata &lt;- data %&gt;% \n  filter(office == \"US PRESIDENT\",\n         party == \"DEMOCRAT\" | party == \"REPUBLICAN\") %&gt;% \n  arrange(state, county_name, fips_raw, year) %&gt;% \n  replace_na(list(candidatevotes = 0))\n\nMany of the FIPS codes from the source data dropped leading zeroes, which makes them unuseable for joining with Census data. This code adds the leading zeroes back.\nThese problems were fixed in a later update by MIT, so this code is not strictly necessary anymore\n\n#clean fips data\nstates_with_bad_fips &lt;- str_to_title(c(\"ALABAMA\", \"ALASKA\", \"ARIZONA\", \n                                      \"ARKANSAS\", \"CALIFORNIA\",\n                                      \"COLORADO\", \"CONNECTICUT\"))\ndata %&gt;% \n  filter(state %in% states_with_bad_fips) %&gt;% \n  mutate(county_fips = paste0(\"0\", fips_raw)) %&gt;% \n  distinct(fips_raw, county_fips)\n\n# A tibble: 0 × 2\n# ℹ 2 variables: fips_raw &lt;chr&gt;, county_fips &lt;chr&gt;\n\ndata &lt;- data %&gt;% \n  #add \"0\" to front of states where leading \"0\" was dropped\n  mutate(county_fips = case_when(state %in% states_with_bad_fips ~ paste0(\"0\", fips_raw),\n                          !(state %in% states_with_bad_fips) ~ fips_raw))\n\nI had to make a variety of decisions about how to clean up the data with regards to county geometries. The MIT data does not reflect cases where counties changed names or FIPS codes, or where counties merged. This code manually makes the changes necessary to join the data with Census geometry data. Note that I do not attempt to fix the data for Alaska, which was extremely different than the Census data. I was not confident that I could make accurate adjustments in this case, so I excluded Alaska entirely. These changes are not optimal, but I think it is close enough.\nThese problems were fixed in a later update by MIT, so this code is not strictly necessary anymore\n\n#decisions to make with wonky geometry\n#merge records for Shannnon and Oglala Lakota counties in SD\n#merge Kansas City Missouri and Jackson County Missouri\n#merge Bedford (city) fips 51515 with Bedford county 51019\n\ndata &lt;- data %&gt;% \n  #update Oglala Lakota SD fips\n  #changed in 2015 https://www.census.gov/programs-surveys/geography/technical-documentation/county-changes.2010.html\n  mutate(county_fips = case_when(state == \"SOUTH DAKOTA\" & county_name == \"OGLALA LAKOTA\" ~ \"46102\",\n                          TRUE ~ county_fips)) %&gt;% \n  #merge Kansas City Missouri with Jackson County Missouri\n  mutate(county_name = case_when(state == \"MISSOURI\" & county_name == \"KANSAS CITY\" ~ \"JACKSON\",\n                            TRUE ~ county_name),\n         county_fips = case_when(state == \"MISSOURI\" & county_name == \"JACKSON\" ~ \"29095\",\n                          TRUE ~ county_fips)) %&gt;% \n  #merge Bedford (city) fips 51515 with Bedford county 51019\n  mutate(county_fips = case_when(state == \"VIRGINIA\" & county_name == \"BEDFORD\" & county_fips == \"51515\" ~ \"51019\",\n                          TRUE ~ county_fips))\n\nThis compares the counties in the MIT data vs. what is in the Census API. Besides Alaska, this shows that my manual changes accounted for the issues I identified.\n\ncounties &lt;- get_acs(variables = \"B19013_001\",\n                      geography = \"county\",\n                      geometry = FALSE) %&gt;% \n  #mutate(census_geo_year = 2010) %&gt;% \n  select(NAME, GEOID)\n\nGetting data from the 2017-2021 5-year ACS\n\n\n\n#alaska falls out: this is expected\n#Broomfield County CO falls out for year 2000: was part of Boulder County in 2000\n#Oglala Lakota County SD falls out for year 2000: was Shannon County in 2000\n#\ndata %&gt;% \n  select(year, state, county_name, county_fips) %&gt;% \n  filter(state != \"ALASKA\") %&gt;% \n  anti_join(counties, by = c(\"county_fips\" = \"GEOID\")) %&gt;% \n  count(state, county_name)\n\n# A tibble: 1 × 3\n  state        county_name     n\n  &lt;chr&gt;        &lt;chr&gt;       &lt;int&gt;\n1 SOUTH DAKOTA SHANNON         8\n\n\nThe process of merging some counties meant that I had to summarize the election results to the level of my new “adjusted” counties. This code performs that process.\n\n#some counties have 4 records because of merging process\ndata %&gt;%\n  select(state, county_name, county_fips, year) %&gt;% \n  add_count(state, county_name, county_fips, year) %&gt;% \n  distinct(n)\n\n# A tibble: 2 × 1\n      n\n  &lt;int&gt;\n1     2\n2     4\n\n\n\n#summarize candidatevotes to account for merged counties\ndata %&gt;% \n  select(state, county_name, county_fips, year, office, party, candidate, candidatevotes) %&gt;% \n  group_by(state, county_name, county_fips, year, office, party, candidate) %&gt;% \n  summarize(candidatevotes_sum = sum(candidatevotes)) %&gt;% \n  ungroup() %&gt;% \n  add_count(state, county_name, county_fips, year) %&gt;% \n  #confirm that each county only has 2 records\n  distinct(n)\n\n`summarise()` has grouped output by 'state', 'county_name', 'county_fips',\n'year', 'office', 'party'. You can override using the `.groups` argument.\n\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1     2\n\n\n\ndata &lt;- data %&gt;% \n  select(state, county_name, county_fips, year, office, party, candidate, candidatevotes) %&gt;% \n  group_by(state, county_name, county_fips, year, office, party, candidate) %&gt;% \n  summarize(candidatevotes_sum = sum(candidatevotes)) %&gt;% \n  ungroup()\n\n`summarise()` has grouped output by 'state', 'county_name', 'county_fips',\n'year', 'office', 'party'. You can override using the `.groups` argument.\n\n\n\n\nMunge\nThis part performs the more straightfoward tasks of calculating a candidate’s % of the vote and the election-to-election shift in %.\n\npresidential_votes &lt;- data %&gt;% \n  group_by(year, state, county_name, county_fips) %&gt;% \n  mutate(pct_vote = candidatevotes_sum / sum(candidatevotes_sum)) %&gt;% \n  ungroup() %&gt;% \n  select(year, state, county_name, county_fips, party, candidatevotes_sum, pct_vote)\n\n\npresidential_votes_shift &lt;- presidential_votes %&gt;% \n  mutate(party = str_to_lower(party)) %&gt;%\n  pivot_wider(names_from = party, values_from = c(candidatevotes_sum, pct_vote)) %&gt;%\n  mutate(dem_margin_pct = pct_vote_democrat - pct_vote_republican,\n         dem_margin_votes = candidatevotes_sum_democrat - candidatevotes_sum_republican) %&gt;% \n  arrange(state, county_name, county_fips, year) %&gt;% \n  group_by(state, county_name, county_fips) %&gt;% \n  mutate(shift_pct = dem_margin_pct - lag(dem_margin_pct),\n         shift_votes = dem_margin_votes - lag(dem_margin_votes)) %&gt;% \n  filter(row_number() &gt; 1) %&gt;% \n  ungroup()\n\nFinally, this creates new variables that rescale the shift in % and votes to degrees and meters, respectively. I also create variations of shift_pct and shift_votes to use in the graph.\n\npresidential_votes_shift &lt;- presidential_votes_shift %&gt;% \n  mutate(shift_pct_scaled = rescale(shift_pct, to = c(0, 180)), #republican 0, democrat 180\n         shift_votes_scaled = rescale(abs(shift_votes), to = c(10^4, 10^6))) %&gt;% \n  mutate(shift_pct_binary = case_when(sign(shift_pct) == 1 ~ \"Democratic\",\n                                      sign(shift_pct) == -1 ~ \"Republican\"),\n         shift_pct_binary = as.factor(shift_pct_binary)) %&gt;% \n  mutate(shift_votes_binned = abs(shift_votes) &lt;= 3000)\n\n\n#create shift map object\nshift_map &lt;- presidential_votes_shift %&gt;% \n  left_join(county_geo, by = c(\"county_fips\" = \"GEOID\")) %&gt;% \n  st_sf() %&gt;% \n  rename(lng0 = center_lon_x,\n         lat0 = center_lat_y) %&gt;% \n  mutate(lng1 = lng0 + (shift_votes_scaled * cos(NISTdegTOradian(shift_pct_scaled))),\n         lat1 = lat0 + (shift_votes_scaled * sin(NISTdegTOradian(shift_pct_scaled))))\n\n\nshift_map_filtered &lt;- shift_map %&gt;% \n  filter(state != \"ALASKA\") %&gt;%\n  filter(year == 2016) %&gt;% \n  mutate(shift_pct_binary = case_when(sign(shift_pct) == 1 ~ \"Democratic\",\n                                      sign(shift_pct) == -1 ~ \"Republican\"),\n         shift_pct_binary = as.factor(shift_pct_binary))\n\nggplot() +\n  geom_sf(data = filter(state_geo, !str_detect(NAME, \"ALASKA\")),\n          linewidth = .2,\n          fill = NA) +\n  geom_point(data = filter(shift_map_filtered, abs(shift_votes) &lt;= 1500),\n             aes(x = lng0, y = lat0,\n                 color = shift_pct_binary),\n             size = .75,\n             alpha = .3) +\n  geom_segment(data = filter(shift_map_filtered, abs(shift_votes) &gt; 1500),\n               aes(x = lng0, xend = lng1,\n                   y = lat0, yend = lat1,\n                   color = shift_pct_binary,\n                   linewidth = shift_votes,\n                   alpha = shift_votes_binned),\n               linejoin = \"mitre\",\n               arrow = arrow(length = unit(0.08, \"inches\"))) +\n  scale_color_manual(values = c(\"#1375B7\", \"#C93135\"), guide = guide_legend(title.position = \"top\")) +\n  scale_linewidth_continuous(range = c(.001, 2), guide = \"none\") +\n  scale_alpha_manual(values = c(1, .3), guide = \"none\") +\n  labs(color = \"Shift in election margin\") +\n  facet_wrap(~year) +\n  theme_void(base_size = 25) +\n  theme(legend.direction = \"horizontal\",\n        legend.position = \"bottom\")\n\n\npolitical_winds_anim &lt;- shift_map %&gt;% \n  filter(state != \"Alaska\") %&gt;% \n  mutate(id = str_c(state, county_name, county_fips)) %&gt;% \n  mutate(year = as.integer(year)) %&gt;% \n  mutate(shift_votes_binned = abs(shift_votes) &lt;= 3000) %&gt;% \n  ggplot() +\n  geom_sf(data = filter(state_geo, NAME != \"Alaska\"),\n          linewidth = .2,\n          fill = NA) +\n  geom_segment(aes(x = lng0, xend = lng1,\n                   y = lat0, yend = lat1,\n                   color = shift_pct_binary,\n                   linewidth = shift_votes,\n                   alpha = shift_votes_binned,\n                   group = id),\n               linejoin = \"mitre\",\n               arrow = arrow(length = unit(0.09, \"inches\"))) +\n  scale_color_manual(values = c(\"#1375B7\", \"#C93135\"), guide = guide_legend(title.position = \"top\")) +\n  scale_size_continuous(range = c(.001, 1.3), guide = \"none\") +\n  scale_alpha_manual(values = c(1, .3), guide = \"none\") +\n  theme_void(base_size = 25) +\n  theme(legend.direction = \"horizontal\",\n        legend.position = \"bottom\") +\n  transition_states(year) +\n  labs(title = \"Shift in Presidential election Democratic margin\",\n       subtitle = \"Year: {closest_state}\",\n       color = \"Shift in Democratic margin\")\n\npolitical_winds_anim"
  },
  {
    "objectID": "posts/residential-zoning-in-pittsburgh/index.html",
    "href": "posts/residential-zoning-in-pittsburgh/index.html",
    "title": "Residential Zoning in Pittsburgh",
    "section": "",
    "text": "The New York Times recently published an article about zoning in U.S. cities, particularly single-unit detached residential housing. The article did not include Pittsburgh, so I downloaded the zone shapefile from the WPRDC and made my own map.\nThis blog quickly goes through the steps to make the map and other graphs about the data.\nFirst, load the required libraries and set up the environment:\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(ggmap)\nlibrary(janitor)\nlibrary(hrbrthemes)\n\noptions(scipen = 999)\n\nRead in the shapefile with st_read and inspect the data with glimpse:\n\nshapefile &lt;- st_read(\"post_data/Zoning-shp/Zoning.shp\")\n\nReading layer `Zoning' from data source \n  `/Users/conorotompkins/Documents/github_repos/ctompkins_quarto_blog/posts/residential-zoning-in-pittsburgh/post_data/Zoning-shp/Zoning.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1061 features and 19 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 1315934 ymin: 381925.6 xmax: 1379759 ymax: 433399.4\nProjected CRS: NAD83 / Pennsylvania South (ftUS)\n\nglimpse(shapefile)\n\nRows: 1,061\nColumns: 20\n$ OBJECTID   &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ perimeter  &lt;dbl&gt; 4020.2318, 3522.5948, 2748.8339, 715.3339, 4499.1899, 2498.…\n$ zoning_    &lt;int&gt; 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 18, 19,…\n$ zoning_id  &lt;int&gt; 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 18, 19,…\n$ zon_new    &lt;chr&gt; \"P\", \"P\", \"LNC\", \"LNC\", \"P\", \"HC\", \"LNC\", \"R1D-M\", \"R1D-M\",…\n$ shape_leng &lt;dbl&gt; 4020.2318, 3522.5948, 2748.8339, 715.3339, 4499.1899, 2498.…\n$ correction &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ full_zonin &lt;chr&gt; \"PARKS AND OPEN SPACE\", \"PARKS AND OPEN SPACE\", \"LOCAL NEIG…\n$ legendtype &lt;chr&gt; \"Parks\", \"Parks\", \"Local Neighborhood Commercial\", \"Local N…\n$ municode   &lt;chr&gt; \"http://library.municode.com/HTML/13525/level4/PIZOCO_TITNI…\n$ status     &lt;chr&gt; \"Approved\", \"Approved\", \"Approved\", \"Approved\", \"Approved\",…\n$ created_us &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ created_da &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ last_edite &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ last_edi_1 &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ pghdb_sde_ &lt;dbl&gt; 404170.093, 332097.787, 192093.378, 22816.707, 962349.849, …\n$ GlobalID   &lt;chr&gt; \"b54df7d1-57d2-4175-8a34-5334046e889b\", \"1ea53324-e92d-4394…\n$ Shape__Are &lt;dbl&gt; 404170.093, 332097.787, 192093.378, 22816.707, 962349.849, …\n$ Shape__Len &lt;dbl&gt; 4020.2318, 3522.5948, 2748.8339, 715.3339, 4499.1899, 2498.…\n$ geometry   &lt;MULTIPOLYGON [US_survey_foot]&gt; MULTIPOLYGON (((1367528 381..., M…\n\n\nWe need to munge the data to get it in shape for analysis. This makes some simple TRUE|FALSE flags for basic zone information and uses case_when to create type, which represents aggregated zone types.\n\ndf &lt;- shapefile %&gt;% \n  mutate(residential = str_detect(full_zonin, \"RESIDENT\"),\n         single_unit = str_detect(full_zonin, \"SINGLE-UNIT\"),\n         attached = str_detect(full_zonin, \"ATTACHED\"),\n         type = case_when(residential == TRUE & single_unit == TRUE & attached == FALSE ~ \"Single-unit detached residential\",\n                          residential == TRUE & single_unit == FALSE | attached == TRUE ~ \"Other residential\",\n                          full_zonin == \"EDUCATIONAL/MEDICAL INSTITUTION\" ~ \"Educational/Medical\",\n                          residential == FALSE ~ \"Other non-residential\"),\n         type = factor(type, levels = c(\"Single-unit detached residential\", \n                                        \"Other residential\",\n                                        \"Educational/Medical\",\n                                        \"Other non-residential\")),\n         alpha_flag = type == \"Single-unit detached residential\") |&gt; \n  rename(area = Shape__Are)\n\n\n\nRows: 1,061\nColumns: 24\n$ OBJECTID    &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ perimeter   &lt;dbl&gt; 4020.2318, 3522.5948, 2748.8339, 715.3339, 4499.1899, 2498…\n$ zoning_     &lt;int&gt; 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 18, 19…\n$ zoning_id   &lt;int&gt; 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 18, 19…\n$ zon_new     &lt;chr&gt; \"P\", \"P\", \"LNC\", \"LNC\", \"P\", \"HC\", \"LNC\", \"R1D-M\", \"R1D-M\"…\n$ shape_leng  &lt;dbl&gt; 4020.2318, 3522.5948, 2748.8339, 715.3339, 4499.1899, 2498…\n$ correction  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ full_zonin  &lt;chr&gt; \"PARKS AND OPEN SPACE\", \"PARKS AND OPEN SPACE\", \"LOCAL NEI…\n$ legendtype  &lt;chr&gt; \"Parks\", \"Parks\", \"Local Neighborhood Commercial\", \"Local …\n$ municode    &lt;chr&gt; \"http://library.municode.com/HTML/13525/level4/PIZOCO_TITN…\n$ status      &lt;chr&gt; \"Approved\", \"Approved\", \"Approved\", \"Approved\", \"Approved\"…\n$ created_us  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ created_da  &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ last_edite  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ last_edi_1  &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ pghdb_sde_  &lt;dbl&gt; 404170.093, 332097.787, 192093.378, 22816.707, 962349.849,…\n$ GlobalID    &lt;chr&gt; \"b54df7d1-57d2-4175-8a34-5334046e889b\", \"1ea53324-e92d-439…\n$ area        &lt;dbl&gt; 404170.093, 332097.787, 192093.378, 22816.707, 962349.849,…\n$ Shape__Len  &lt;dbl&gt; 4020.2318, 3522.5948, 2748.8339, 715.3339, 4499.1899, 2498…\n$ residential &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, TRU…\n$ single_unit &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, TRU…\n$ attached    &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA…\n$ type        &lt;fct&gt; Other non-residential, Other non-residential, Other non-re…\n$ alpha_flag  &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, TRU…\n\n\nThis counts the number of rows per full zone description (full_zonin) and type:\n\ndf_zones &lt;- df %&gt;% \n  count(full_zonin, type, sort = TRUE) %&gt;% \n  st_drop_geometry()\n\n\n\nRows: 59\nColumns: 3\n$ full_zonin &lt;chr&gt; \"PARKS AND OPEN SPACE\", \"LOCAL NEIGHBORHOOD COMMERCIAL\", \"S…\n$ type       &lt;fct&gt; Other non-residential, Other non-residential, Single-unit d…\n$ n          &lt;int&gt; 153, 125, 75, 70, 65, 54, 52, 44, 42, 42, 41, 38, 36, 19, 1…\n\n\nCreate a basic bar chart to show the distribution of type:\n\ndf %&gt;% \n  st_drop_geometry() %&gt;% \n  group_by(type, residential) %&gt;% \n  summarize(area = sum(area)) %&gt;% \n  ungroup() %&gt;% \n  mutate(type = fct_reorder(type, area)) %&gt;% \n  ggplot(aes(type, area / 1000000, fill = residential)) +\n    geom_col() +\n    scale_y_comma() +\n    scale_fill_discrete(\"Is the zone residential?\") +\n    labs(x = \"Zone type\",\n         y = \"Land area in millions of feet\") +\n    coord_flip() +\n    theme_ipsum()\n\n\n\n\n\n\n\n\nUse a bar chart to show the distribution of full_zonin:\n\ndf %&gt;% \n  st_drop_geometry() %&gt;% \n  group_by(full_zonin, residential) %&gt;% \n  summarize(area = sum(area)) %&gt;% \n  ungroup() %&gt;% \n  mutate(full_zonin = fct_reorder(full_zonin, area)) %&gt;% \n  ggplot(aes(full_zonin, area / 1000000, fill = residential)) +\n    geom_col() +\n    scale_y_comma() +\n    scale_fill_discrete(\"Is the zone residential?\") +\n    labs(x = \"Full zone description\",\n         y = \"Land area in millions of feet\") +\n    coord_flip() +\n    theme_ipsum()\n\n\n\n\n\n\n\n\nThis calculates the total land area zoned for any type of residential housing:\n\ndf %&gt;% \n  st_drop_geometry() %&gt;% \n  mutate(single_unit_flag = type == \"Single-unit detached residential\") %&gt;% \n  filter(residential == TRUE) %&gt;% \n  summarize(total_area = sum(area))\n\n  total_area\n1  751652465\n\n\nThis calculates the % of residential zoning that is zoned for single-unit detached residential housing units:\n\ndf %&gt;% \n  st_drop_geometry() %&gt;% \n  filter(residential == TRUE) %&gt;% \n  mutate(single_unit_flag = (type == \"Single-unit detached residential\")) %&gt;% \n  group_by(single_unit_flag) %&gt;% \n  summarize(zone_area = sum(area)) %&gt;% \n  mutate(pct_area = zone_area / sum(zone_area))\n\n# A tibble: 2 × 3\n  single_unit_flag  zone_area pct_area\n  &lt;lgl&gt;                 &lt;dbl&gt;    &lt;dbl&gt;\n1 FALSE            332150155.    0.442\n2 TRUE             419502310.    0.558\n\n\nThis creates a map of the zones, fills them by type, and overlays it on a GoogleMaps basemap. I also insert the boundaries of the City of Pittsburgh.\n\ncity_boundary &lt;- st_read(\"post_data/Pittsburgh_City_Boundary-shp/City_Boundary.shp\")\n\nReading layer `City_Boundary' from data source \n  `/Users/conorotompkins/Documents/github_repos/ctompkins_quarto_blog/posts/residential-zoning-in-pittsburgh/post_data/Pittsburgh_City_Boundary-shp/City_Boundary.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 8 features and 8 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 1315934 ymin: 381925.6 xmax: 1379780 ymax: 433399.4\nProjected CRS: NAD83 / Pennsylvania South (ftUS)\n\nggplot() +\n  geom_sf(data = df %&gt;% filter(type != \"Other non-residential\"), aes(fill = type), inherit.aes = FALSE, size = .5, alpha = 1, color = NA) +\n  geom_sf(data = city_boundary, inherit.aes = FALSE, alpha = 0, size = 2) +\n  coord_sf(crs = st_crs(4326)) +\n  scale_fill_manual(\"Zone type\",\n                      values = c(\"#ea60b9\", \"#4cafc5\", \"yellow\", \"light grey\")) +\n  labs(title = \"56% of residential zoned land area is single-family detached residential\",\n         subtitle = \"City of Pittsburgh zoning\",\n         caption = \"@conor_tompkins, data from WPRDC\") +\n  theme_void()\n\n\n\n\n\n\n\n\nI used scale_fill_manual to manually set the color palette to match the NYTimes article."
  },
  {
    "objectID": "posts/premier-league-538-spi-ratings/index.html",
    "href": "posts/premier-league-538-spi-ratings/index.html",
    "title": "Premier League 538 SPI Ratings",
    "section": "",
    "text": "538’s Soccer Power Index (SPI) rates the quality of soccer teams from a variety of leagues around the world. In this post I’ll use gganimate to animate team SPI over the past 3 seasons.\nThe SPI data is available on 538’s GitHub repo.\nSet up the environment:\n\nlibrary(tidyverse)\nlibrary(purrr)\nlibrary(gganimate)\nlibrary(ggrepel)\nlibrary(broom)\nlibrary(lubridate)\n\ntheme_set(theme_minimal(base_size = 18))\n\nLoad the data and make the data long, instead of having different columns for home and away results:\n\ndata &lt;- read_csv(\"https://projects.fivethirtyeight.com/soccer-api/club/spi_matches.csv\")\n\n\ndf_home &lt;- data %&gt;% \n  select(team1, date, league, spi1) %&gt;% \n  rename(team = team1,\n         spi = spi1) %&gt;% \n  mutate(venue = \"home\")\n\ndf_away &lt;- data %&gt;% \n  select(team2, date, league, spi2) %&gt;% \n  rename(team = team2,\n         spi = spi2) %&gt;% \n  mutate(venue = \"away\")\n\ndf_all &lt;- bind_rows(df_home, df_away) %&gt;% \n  filter(date &lt; \"2019-04-07\") %&gt;% \n  arrange(league, team, date) %&gt;% \n  group_by(league, team) %&gt;% \n  mutate(team_game_number = dense_rank(date)) %&gt;% \n  ungroup()\n\nFilter to EPL teams and add a season column:\n\ndf_epl &lt;- df_all %&gt;% \n  filter(date &lt; Sys.Date(),\n         league == \"Barclays Premier League\")\n\nseason1 &lt;- tibble(date = seq(ymd('2016-08-13'), ymd('2017-05-21'), by='days'),\n                  season = 1)\n\nseason2 &lt;- tibble(date = seq(ymd('2017-08-11'), ymd('2018-05-13'), by='days'),\n                  season = 2)\n\nseason3 &lt;- tibble(date = seq(ymd('2018-08-10'), ymd('2019-04-06'), by='days'),\n                  season = 3)\n\nseasons &lt;- bind_rows(season1, season2, season3)\n\ndf_epl_smooth &lt;- df_epl %&gt;%\n  left_join(seasons)\n\nCalculate the smoothed SPI per team per season using loess:\n\ndf_epl_smooth &lt;- df_epl_smooth %&gt;% \n  nest(-c(team, season)) %&gt;% \n  mutate(m = map(data, loess,\n                          formula = spi ~ team_game_number, span = .5),\n         spi_smooth = purrr::map(m, `[[`, \"fitted\"))\n\ndf_epl_smooth &lt;- df_epl_smooth %&gt;% \n  select(-m) %&gt;% \n  unnest()\n\ndf_epl_last &lt;- df_epl %&gt;% \n  group_by(team) %&gt;% \n  summarize(date = last(date),\n            spi = last(spi))\n\nCreate the animation:\n\nspi_smooth_gif &lt;- df_epl_smooth %&gt;% \n  ggplot(aes(date, spi_smooth, color = team, group = team)) +\n  geom_line() +\n  geom_point(size = 2) +\n  geom_segment(aes(xend = ymd(\"2019-04-05\"), yend = spi_smooth), linetype = 2, colour = 'grey') +\n  geom_label(aes(x = ymd(\"2019-04-05\"), label = team),\n             hjust = -.1,\n             vjust = 0) +\n  geom_rect(xmin = ymd(\"2017-05-25\"), xmax = ymd(\"2017-08-12\"),\n            ymin = -Inf, ymax = Inf,\n            fill = \"white\", color = \"white\") +\n  geom_rect(xmin = ymd(\"2018-05-18\"), xmax = ymd(\"2018-08-10\"),\n                ymin = -Inf, ymax = Inf, fill = \"white\", color = \"white\") +\n  guides(color = FALSE) +\n  labs(title = \"Premier League\",\n       subtitle = \"538 Soccer Power Index\",\n       x = NULL,\n       y = \"538 Soccer Power Index\",\n       caption = \"@conor_tompkins\") +\n  transition_reveal(date) +\n  coord_cartesian(clip = 'off') +\n  theme(plot.margin = margin(5.5, 110, 5.5, 5.5))\n\nanimate(spi_smooth_gif, height = 9, width = 9, duration = 15, nframes = 300)\n\n\n\n\n\n\n\n\nObservers of the EPL will know that in any given season there are 2-3 tiers of teams, given the economics and relegation structure of the league. In 2018 the difference between the top 6 and the rest of the league was particularly stark. This is partly due to the difficulties that Everton experienced after they sold Lukaku and signed older and less skilled players. This graph highlights Everton’s SPI:\n\neverton_gif &lt;- df_epl_smooth %&gt;% \n  mutate(everton_flag = case_when(team == \"Everton\" ~ \"Everton\",\n                                  team != \"Everton\" ~ \"\")) %&gt;% \n  ggplot(aes(date, spi_smooth, color = everton_flag, group = team)) +\n  geom_line() +\n  geom_point(size = 2) +\n  geom_segment(aes(xend = ymd(\"2019-04-05\"), yend = spi_smooth), linetype = 2, colour = 'grey') +\n  geom_label(aes(x = ymd(\"2019-04-05\"), label = team),\n             hjust = -.1,\n             vjust = 0) +\n  geom_rect(xmin = ymd(\"2017-05-25\"), xmax = ymd(\"2017-08-12\"),\n            ymin = -Inf, ymax = Inf,\n            fill = \"white\", color = \"white\") +\n  geom_rect(xmin = ymd(\"2018-05-18\"), xmax = ymd(\"2018-08-10\"),\n                ymin = -Inf, ymax = Inf, fill = \"white\", color = \"white\") +\n  scale_color_manual(values = c(\"light grey\", \"blue\")) +\n  guides(color = FALSE) +\n  labs(title = \"Premier League\",\n       subtitle = \"538 Soccer Power Index\",\n       x = NULL,\n       y = \"538 Soccer Power Index\",\n       caption = \"@conor_tompkins\") +\n  transition_reveal(date) +\n  coord_cartesian(clip = 'off') +\n  theme(plot.margin = margin(5.5, 110, 5.5, 5.5))\n\nanimate(everton_gif, height = 9, width = 9, duration = 15, nframes = 300)"
  },
  {
    "objectID": "posts/usl-in-the-538-global-club-soccer-rankings/index.html",
    "href": "posts/usl-in-the-538-global-club-soccer-rankings/index.html",
    "title": "USL in the 538 Global Club Soccer Rankings",
    "section": "",
    "text": "This post was originally run with data from August 2018. 538 does not provide historical rankings, so I had to rerun the code with June 2023 data when I migrated my blog.\n538 recently added the United Soccer League to their Soccer Power Index ratings. I’m a Riverhounds fan, so I wanted to see how the team compared to teams from leagues around the world.\n\nlibrary(tidyverse)\nlibrary(ggrepel)\n\ntheme_set(theme_bw())\n\n\ndf &lt;- read_csv(\"https://projects.fivethirtyeight.com/soccer-api/club/spi_global_rankings.csv\", progress = FALSE) %&gt;% \n  group_by(league) %&gt;% \n  mutate(league_spi = median(spi)) %&gt;% \n  ungroup() %&gt;% \n  mutate(league = fct_reorder(league, league_spi))\n\ndf\n\n# A tibble: 641 × 8\n    rank prev_rank name                     league    off   def   spi league_spi\n   &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;                    &lt;fct&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;\n 1     1         1 Manchester City          Barcla…  2.79  0.28  92         72.8\n 2     2         2 Bayern Munich            German…  3.04  0.68  87.7       67.6\n 3     3         3 Barcelona                Spanis…  2.45  0.43  86.4       67.0\n 4     4         4 Real Madrid              Spanis…  2.56  0.6   84.4       67.0\n 5     5         5 Liverpool                Barcla…  2.63  0.67  83.9       72.8\n 6     6         6 Arsenal                  Barcla…  2.53  0.61  83.9       72.8\n 7     7         7 Newcastle                Barcla…  2.38  0.53  83.7       72.8\n 8     8         8 Napoli                   Italy …  2.3   0.51  83.2       63.4\n 9     9         9 Borussia Dortmund        German…  2.83  0.84  82.9       67.6\n10    10        10 Brighton and Hove Albion Barcla…  2.47  0.73  80.9       72.8\n# ℹ 631 more rows\n\n\n\ndf %&gt;% \n  ggplot(aes(spi, league)) +\n  geom_jitter(aes(color = league), show.legend = FALSE,\n              height = .2,\n              alpha = .7) +\n  geom_jitter(data = df %&gt;% filter(name == \"Pittsburgh Riverhounds\"),\n              show.legend = FALSE,\n              height = .2,\n              alpha = .7) +\n  geom_label_repel(data = df %&gt;% filter(name == \"Pittsburgh Riverhounds\"), \n                   aes(label = name), \n                   size = 3,\n                   show.legend = FALSE,\n                   force = 6) +\n  labs(title = \"538 Soccer Power Index\",\n       subtitle = \"One dot = one team\",\n       y = NULL,\n       x = \"Soccer Power Index\",\n       caption = \"538 data, @conor_tompkins\")\n\n\n\n\n\n\n\n\n\ndf %&gt;% \n  ggplot(aes(spi, league)) +\n  geom_jitter(aes(color = league), show.legend = FALSE,\n              height = .2,\n              alpha = .7) +\n  labs(title = \"538 Soccer Power Index\",\n       subtitle = \"One dot = one team\",\n       y = NULL,\n       x = \"Soccer Power Index\",\n       caption = \"538 data, @conor_tompkins\")\n\n\n\n\n\n\n\n\nThis shows the offensive and defensive ratings of each USL team. The Riverhounds are squarely in the #LilleyBall quadrant.\n\ndf %&gt;% \n  filter(league == \"United Soccer League\") %&gt;% \n  ggplot(aes(off, def, label = name)) +\n  geom_point() +\n  geom_label_repel(size = 4,\n                   force = 4) +\n  scale_y_reverse() +\n  labs(title = \"538 Soccer Power Index\",\n       y = \"Defensive rating (scale reversed)\",\n       x = \"Offensive rating\",\n       caption = \"538 data, @conor_tompkins\")"
  },
  {
    "objectID": "posts/time-series-clustering-covid-19-cases/index.html",
    "href": "posts/time-series-clustering-covid-19-cases/index.html",
    "title": "Time series clustering COVID-19 case data",
    "section": "",
    "text": "Interactive Tableau visualization of the clusters\nThe goal of this post is to group states into clusters based on the shape of the curve of a state’s cumulative sum of COVID-19 cases. This type of clustering is useful when the variance in absolute values of a time series obscures the underlying pattern in the data. Since states experienced plateaus and peaks at different times, my hope is that the clustering is able to identify those differences.\nThis loads the packages I will use in the analysis and set up the environment:\nlibrary(tidyverse)\nlibrary(tsibble)\nlibrary(dtwclust)\nlibrary(tidymodels)\nlibrary(hrbrthemes)\nlibrary(tidycensus)\nlibrary(sf)\n\noptions(scipen = 999, digits = 4)\n\ntheme_set(theme_ipsum())\n\nset.seed(1234)\nI will adjust the cases to per 100,000, which requires information from the U.S. Census. This code pulls state-level population data from the Census API via tidycensus:\ncensus_data &lt;- get_acs(geography = \"state\", variables = \"B01003_001\", geometry = FALSE, year = 2020) %&gt;% \n  select(state = NAME, population = estimate)\nThis pulls the COVID-19 data from the NYTimes GitHub page:\ncovid &lt;- read_csv(\"https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-states.csv\") %&gt;% \n  arrange(state, date) %&gt;% \n  semi_join(census_data) %&gt;% \n  filter(date &lt;= \"2020-07-18\")\nI use the tsibble package to check if there are implicit gaps in the data. For example, if there was data for 2020-06-01 and 2020-06-03, there is an implicit gap because there is not data for 2020-06-02.\ncovid %&gt;% \n  as_tsibble(index = date, key = state) %&gt;% \n  count_gaps()\n\n# A tibble: 0 × 4\n# ℹ 4 variables: state &lt;chr&gt;, .from &lt;date&gt;, .to &lt;date&gt;, .n &lt;int&gt;\nThankfully, there are not any such gaps. If there were, I would have to impute values for the missing days.\nSince states experienced onset of COVID-19 at different times, I find the day each state hit 10 cases, and calculate days_since_10th_case, which I will use instead of date.\ncovid_10th_case &lt;- covid %&gt;% \n  filter(cases &gt;= 10) %&gt;% \n  group_by(state) %&gt;% \n  slice(1) %&gt;% \n  ungroup() %&gt;% \n  select(state, date_of_10th_case = date)\n\ncovid &lt;- covid %&gt;% \n  left_join(covid_10th_case, by = c(\"state\" = \"state\")) %&gt;% \n  group_by(state) %&gt;% \n  mutate(days_since_10th_case = date - date_of_10th_case) %&gt;% \n  ungroup() %&gt;% \n  filter(days_since_10th_case &gt;= 0)\n\ncovid &lt;- covid %&gt;% \n  select(state, days_since_10th_case, cases)\nNext I calculate cases_per_capita:\ncovid &lt;- covid %&gt;% \n  left_join(census_data) %&gt;% \n  mutate(cases_per_capita = (cases / population) * 100000) %&gt;% \n  select(-population)\nNext I scale the cases so that the mean is 0 and the standard deviation is 1. Each state has its own mean and standard deviation.\ncovid &lt;- covid %&gt;% \n  group_by(state) %&gt;% \n  mutate(cases_per_capita_scaled = scale(cases_per_capita, center = TRUE, scale = TRUE)) %&gt;% \n  ungroup()\nThe result of this is that the clustering algorithm will focus on the shape of the line for each state instead of absolute values. This graph shows the difference:\ncovid %&gt;% \n  pivot_longer(cols = contains(\"cases\"), names_to = \"metric\", values_to = \"value\") %&gt;% \n  ggplot(aes(days_since_10th_case, value, group = state)) +\n  geom_hline(yintercept = 0, linetype = 2) +\n  geom_line(alpha = .1) +\n  facet_wrap(~metric, ncol = 1, scales = \"free_y\") +\n  scale_y_comma()\ntsclust requires that the input data be a series of lists, not a dataframe. unstack takes a key and value as arguments and turns the dataframe into a list of lists.\ncovid_list &lt;- covid %&gt;% \n  select(state, cases_per_capita_scaled) %&gt;% \n  unstack(cases_per_capita_scaled ~ state)\nThis loops through the clustering function 20 times and saves each output to a list. The first object groups the data into 2 clusters, the second object has 3 clusters, and it continues in that pattern.\ncluster_dtw_h &lt;- list()\n\nkclust &lt;- 20\n\nfor (i in 2:kclust){\n  cluster_dtw_h[[i]] &lt;- tsclust(covid_list, \n                                type = \"h\", \n                                k = i,\n                                distance = \"dtw\", \n                                control = hierarchical_control(method = \"complete\"), \n                                seed = 390, \n                                preproc = NULL, \n                                args = tsclust_args(dist = list(window.size = 21L)))\n  \n  print(i)\n}\n\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n[1] 11\n[1] 12\n[1] 13\n[1] 14\n[1] 15\n[1] 16\n[1] 17\n[1] 18\n[1] 19\n[1] 20\nThe object that tsclust outputs has a complex structure that makes it difficult to work with at scale. The data I need to pull out is stored in various slots. The next step is to write functions that pulls out the data and tidies it up."
  },
  {
    "objectID": "posts/time-series-clustering-covid-19-cases/index.html#mapping",
    "href": "posts/time-series-clustering-covid-19-cases/index.html#mapping",
    "title": "Time series clustering COVID-19 case data",
    "section": "Mapping",
    "text": "Mapping\nThe data is aggregated at the state level, which can easily be graphed with ggplot2 and tidycensus.\n\nmap &lt;- get_acs(geography = \"state\", variables = \"B01003_001\", geometry = TRUE, shift_geo = TRUE)\n\nmap %&gt;% \n  ggplot() +\n  geom_sf() +\n  theme_void()\n\n\n\n\n\n\n\n\nThis joins the cluster assignments to the map object and summarizes the state polygons by region. This dissolves the state boundaries and creates polygons for each cluster.\n\nmap_cluster &lt;- map %&gt;% \n  left_join(cluster_assignments %&gt;% \n             filter(kclust == best_kclust), by = c(\"NAME\" = \"state\")) %&gt;% \n  add_count(cluster_assignment) %&gt;% \n  mutate(cluster_assignment = as.character(cluster_assignment),\n         cluster_assignment = fct_reorder(cluster_assignment, desc(n))) %&gt;% \n  group_by(cluster_assignment) %&gt;% \n  summarize()\n\nstate_clustered &lt;- map %&gt;% \n  left_join(cluster_assignments %&gt;% \n              filter(kclust == best_kclust), by = c(\"NAME\" = \"state\")) %&gt;% \n  add_count(cluster_assignment) %&gt;% \n  mutate(cluster_assignment = as.character(cluster_assignment),\n         cluster_assignment = fct_reorder(cluster_assignment, desc(n)))\n\nThis code creates the map, and overlays the state boundaries on the cluster polygons.\n\nmap_cluster %&gt;% \n  ggplot() +\n  geom_sf(aes(fill = cluster_assignment, color = cluster_assignment),\n          size = 1) +\n  geom_sf_text(data = state_clustered, aes(label = cluster_assignment)) +\n  labs(fill = \"Cluster assignment\",\n       color = \"Cluster assigmment\") +\n  guides(color = FALSE) +\n  theme_void() +\n  theme(legend.position = \"bottom\",\n        legend.direction = \"horizontal\")\n\n\n\n\n\n\n\n\nCluster 3 stands out as the group of states that are currently struggling with COVID-19 the most. Interestingly, these states are consistently clustered together regardless of the value of kclust, which means that these states are very similar.\nCluster 5 represents the states that had the earliest and worst outbreaks, but have beaten back the virus for now. Cluster 6 are the neighbors of New York and New Jersey. They experienced less peaky curves later than Cluster 5. Cluster 6 is an “echo” of Cluster 5.\nThe singleton clusters for kclust of 12 are Vermont, Nebraska, and Hawaii. Nebraska had a long period with almost no new cases at the beginning, but then had a very steep increase after that. Vermont’s curve started steeply almost immediately after its 10th case, which distinguishes it from the other states. Hawaii has had two periods of very steep increases sperated by a long period with few new cases. This is very likely due to the difficulty of traveling to the state with travel lockdowns in place.\n\nSources\n\nhttps://rpubs.com/esobolewska/dtw-time-series\nhttp://www.rdatamining.com/examples/time-series-clustering-classification\nhttp://rstudio-pubs-static.s3.amazonaws.com/398402_abe1a0343a4e4e03977de8f3791e96bb.html"
  },
  {
    "objectID": "posts/ebirding-in-allegheny-county/index.html",
    "href": "posts/ebirding-in-allegheny-county/index.html",
    "title": "eBirding in Allegheny County",
    "section": "",
    "text": "In this post I will do some exploratory analysis on eBird data. I’ve picked up birdwatching as a hobby during quarantine, and eBird has a ton of cool data on bird sightings."
  },
  {
    "objectID": "posts/ebirding-in-allegheny-county/index.html#setup",
    "href": "posts/ebirding-in-allegheny-county/index.html#setup",
    "title": "eBirding in Allegheny County",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(vroom)\nlibrary(janitor)\nlibrary(rebird)\nlibrary(hrbrthemes)\nlibrary(ggrepel)\nlibrary(gganimate)\nlibrary(widyr)\nlibrary(tidygraph)\nlibrary(ggraph)\nlibrary(tidytext)\n\noptions(scipen = 999, digits = 2)\n\ntheme_set(theme_ipsum())\n\nset.seed(1234)"
  },
  {
    "objectID": "posts/ebirding-in-allegheny-county/index.html#load-and-filter-data",
    "href": "posts/ebirding-in-allegheny-county/index.html#load-and-filter-data",
    "title": "eBirding in Allegheny County",
    "section": "Load and filter data",
    "text": "Load and filter data\nI downloaded data for bird sightings in Allegheny County from the eBird data portal. This code loads the data in R and prepares it for analysis.\n\ndf &lt;- vroom(\"post_data/ebd_US-PA-003_201001_202003_relFeb-2020.zip\", delim = \"\\t\") %&gt;% \n  clean_names() %&gt;% \n  mutate_at(vars(observer_id, locality, observation_date, time_observations_started, protocol_type), str_replace_na, \"NA\") %&gt;% \n  mutate(observation_date = ymd(observation_date),\n         observation_count_old = observation_count,\n         observation_count = as.numeric(str_replace(observation_count_old, \"X\", as.character(NA))),\n         observation_event_id = str_c(observer_id, locality, observation_date, time_observations_started)) %&gt;% \n  filter(all_species_reported == 1)\n\nI will focus on the two major “types” of observation protocols. The others are related to specific birding events, and might not be representative of the overall data.\n\ndf_top_protocols &lt;- df %&gt;% \n  count(protocol_type, sort = TRUE) %&gt;% \n  slice(1:2)\n\ndf_top_protocols\n\n# A tibble: 2 × 2\n  protocol_type      n\n  &lt;chr&gt;          &lt;int&gt;\n1 Traveling     545872\n2 Stationary    293991\n\n\n\ndf &lt;- df %&gt;% \n  semi_join(df_top_protocols)\n\nThis shows that Allegheny County birders began submitting data regularly in 2016. I will focus my analysis on recent observations.\n\ndf %&gt;% \n  count(observation_date) %&gt;% \n  mutate(recent_observation = year(observation_date) &gt;= 2016,\n         observation_date = ymd(observation_date)) %&gt;% \n  ggplot(aes(observation_date, n, color = recent_observation)) +\n    geom_line() +\n    scale_y_comma() +\n    scale_color_discrete(\"Recent Observation\") +\n    labs(y = \"Number of observations\",\n         x = \"Observation date\")\n\n\n\n\n\n\n\n\n\ndf &lt;- df %&gt;% \n  filter(year(observation_date) &gt;= 2016)\n\nThere is wide variation in the number of times a species was sighted in a given observation.\n\ndf %&gt;% \n  ggplot(aes(observation_count, group = common_name)) +\n    geom_density() +\n    scale_x_log10() +\n    labs(x = \"Species count (log10)\")\n\n\n\n\n\n\n\n\nThis calculates the 99th percentile of bird count per observation, and highlights the birds that are seen in large flocks.\n\ndf %&gt;% \n  group_by(common_name) %&gt;% \n  summarize(observation_count_99 = quantile(observation_count, probs = c(.99), na.rm = TRUE)) %&gt;% \n  arrange(desc(observation_count_99))\n\n# A tibble: 306 × 2\n   common_name       observation_count_99\n   &lt;chr&gt;                            &lt;dbl&gt;\n 1 Ring-billed Gull                 3400 \n 2 crow sp.                         2000 \n 3 gull sp.                          500 \n 4 Herring Gull                      248.\n 5 Tundra Swan                       230.\n 6 European Starling                 176.\n 7 Horned Lark                       175 \n 8 Bufflehead                        170.\n 9 Canada Goose                      148 \n10 Larus sp.                         143.\n# ℹ 296 more rows\n\n\nThis shows that the high count of Ring-billed Gulls is explained by groups of birders seeing flocks of thousands of the species. This also highlights that the same bird sighting can be counted twice because of simultaneous observation.\n\ndf %&gt;% \n  filter(common_name == \"Ring-billed Gull\") %&gt;% \n  arrange(desc(observation_count)) %&gt;% \n  slice(1:10) %&gt;% \n  select(observer_id, group_identifier, common_name, observation_date, duration_minutes, observation_count, locality)\n\n# A tibble: 10 × 7\n   observer_id group_identifier common_name    observation_date duration_minutes\n   &lt;chr&gt;       &lt;chr&gt;            &lt;chr&gt;          &lt;date&gt;                      &lt;dbl&gt;\n 1 obsr160352  G2943178         Ring-billed G… 2018-02-13                     38\n 2 obsr40545   G2943178         Ring-billed G… 2018-02-13                     38\n 3 obsr40545   G2930894         Ring-billed G… 2018-02-09                     50\n 4 obsr160352  G2930894         Ring-billed G… 2018-02-09                     75\n 5 obsr160352  &lt;NA&gt;             Ring-billed G… 2016-01-29                     25\n 6 obsr160352  G1578214         Ring-billed G… 2016-01-30                     71\n 7 obsr101818  G1578214         Ring-billed G… 2016-01-30                     71\n 8 obsr40545   G2940041         Ring-billed G… 2018-02-12                     55\n 9 obsr160352  &lt;NA&gt;             Ring-billed G… 2018-02-10                     55\n10 obsr620338  G2940041         Ring-billed G… 2018-02-12                     55\n# ℹ 2 more variables: observation_count &lt;dbl&gt;, locality &lt;chr&gt;"
  },
  {
    "objectID": "posts/ebirding-in-allegheny-county/index.html#species-counts",
    "href": "posts/ebirding-in-allegheny-county/index.html#species-counts",
    "title": "eBirding in Allegheny County",
    "section": "Species counts",
    "text": "Species counts\nThis does a basic count of the major species, not accounting for simultaneous observation:\n\ndf_counts &lt;- df %&gt;% \n  group_by(common_name) %&gt;% \n  summarize(species_count = sum(observation_count, na.rm = TRUE)) %&gt;% \n  arrange(desc(species_count))\n\ndf_counts\n\n# A tibble: 306 × 2\n   common_name       species_count\n   &lt;chr&gt;                     &lt;dbl&gt;\n 1 Ring-billed Gull         253373\n 2 American Crow            189804\n 3 European Starling        180456\n 4 American Robin           180148\n 5 Canada Goose             159040\n 6 Mallard                  103066\n 7 Northern Cardinal         84604\n 8 Mourning Dove             70975\n 9 Blue Jay                  64894\n10 Song Sparrow              63571\n# ℹ 296 more rows\n\n\n\ndf_counts %&gt;% \n  mutate(common_name = fct_reorder(common_name, species_count)) %&gt;% \n  slice(1:20) %&gt;% \n  ggplot(aes(species_count, common_name)) +\n    geom_col() +\n    scale_x_comma() +\n    labs(x = \"Observations\",\n         y = NULL)"
  },
  {
    "objectID": "posts/ebirding-in-allegheny-county/index.html#species-correlations",
    "href": "posts/ebirding-in-allegheny-county/index.html#species-correlations",
    "title": "eBirding in Allegheny County",
    "section": "Species correlations",
    "text": "Species correlations\nI was interested in which birds are correlated most with two common and popular birds, the Cardinal and the Blue Jay. This calculates the pairwise correlation and plots the top 10 birds correlated with the two target species:\n\nspecies_list &lt;- c(\"Northern Cardinal\", \"Blue Jay\")\n\n\ndf_pair_corr &lt;- df %&gt;% \n  pairwise_cor(common_name, observation_event_id, diag = FALSE, upper = FALSE)\n\n\ndf_pair_corr %&gt;% \n  filter(item1 %in% species_list) %&gt;% \n  drop_na(correlation) %&gt;% \n  arrange(item1, desc(correlation)) %&gt;% \n  group_by(item1) %&gt;% \n  slice(1:10) %&gt;% \n  ungroup() %&gt;% \n  mutate(item2 = reorder_within(x = item2, by = correlation, within = item1)) %&gt;% \n  ggplot(aes(correlation, item2, fill = item1)) +\n    geom_col(alpha = .9) +\n    facet_wrap(~item1, scales = \"free_y\") +\n    scale_y_reordered() +\n    scale_fill_manual(values = c(\"blue\", \"red\")) +\n    guides(fill = FALSE) +\n    labs(x = \"Correlation\",\n         y = NULL)\n\n\n\n\n\n\n\n\nCardinals and Blue Jays share many birds in common, but there are some distinctions, and the level of correlation can vary. I use a slopegraph to show the difference in how much a bird is correlated with Blue Jays vs. Cardinals.\nThis grabs the data for the two target species and gets the top 20 correlated birds for each species:\n\ndf_slopegraph &lt;- df_pair_corr %&gt;% \n  filter(item1 %in% species_list) %&gt;% \n  drop_na(correlation) %&gt;% \n  arrange(item1, desc(correlation)) %&gt;% \n  group_by(item1) %&gt;% \n  slice(1:20) %&gt;% \n  ungroup()\n\nThis calculates the difference in correlation for each bird:\n\ndf_corr_diff &lt;- df_slopegraph %&gt;% \n  pivot_wider(names_from = item1, values_from = correlation, names_prefix = \"corr_\") %&gt;% \n  clean_names() %&gt;% \n  mutate(corr_diff = abs(corr_blue_jay - corr_northern_cardinal)) %&gt;% \n  select(item2, corr_diff)\n\ndf_corr_diff\n\n# A tibble: 31 × 2\n   item2                   corr_diff\n   &lt;chr&gt;                       &lt;dbl&gt;\n 1 Northern Cardinal        NA      \n 2 Red-bellied Woodpecker    0.0347 \n 3 Tufted Titmouse           0.0142 \n 4 Carolina Wren            NA      \n 5 Downy Woodpecker         NA      \n 6 White-breasted Nuthatch   0.00495\n 7 Song Sparrow              0.118  \n 8 Northern Flicker          0.0199 \n 9 Mourning Dove            NA      \n10 Eastern Towhee           NA      \n# ℹ 21 more rows\n\n\n\ndf_slopegraph %&gt;% \n  left_join(df_corr_diff) %&gt;% \n  ggplot(aes(item1, correlation)) +\n    geom_line(aes(group = item2, color = corr_diff), size = 2) +\n    geom_point(size = 2) +\n    geom_text_repel(data = filter(df_slopegraph, item1 == species_list[2]),\n                    aes(y = correlation, label = item2), direction = \"both\", nudge_x = -.3, segment.alpha = .2) +\n    geom_text_repel(data = filter(df_slopegraph, item1 == species_list[1]),\n                    aes(y = correlation, label = item2), direction = \"both\", nudge_x = .3, segment.alpha = .2) +\n    scale_color_viridis_c(\"Absolute difference in correlation\") +\n    labs(x = NULL,\n         y = \"Correlation\") +\n    theme(panel.grid.minor.y = element_blank(),\n          panel.grid.major.y = element_blank(),\n          axis.title.x = element_blank())"
  },
  {
    "objectID": "posts/ebirding-in-allegheny-county/index.html#network-graph",
    "href": "posts/ebirding-in-allegheny-county/index.html#network-graph",
    "title": "eBirding in Allegheny County",
    "section": "Network Graph",
    "text": "Network Graph\nNetwork graphs are fun ways to show counts or correlations between groups. Since birds often exist in distinct environments and vary by season, graph analysis should be able to visualize connections.\nThis takes the top 100 birds in terms of total count and makes a network graph object consisting of nodes and edges.\n\ngraph_object_corr &lt;- df_pair_corr %&gt;% \n  semi_join(df_counts %&gt;% slice(1:75), by = c(\"item1\" = \"common_name\")) %&gt;% \n  semi_join(df_counts %&gt;% slice(1:75), by = c(\"item2\" = \"common_name\")) %&gt;% \n  as_tbl_graph(directed = FALSE) %&gt;% \n  activate(nodes) %&gt;% \n  filter(!node_is_isolated()) %&gt;% \n  activate(edges) %&gt;% \n  filter(abs(correlation) &gt;= .05) %&gt;% \n  activate(nodes) %&gt;% \n  filter(!node_is_isolated()) %&gt;% \n  left_join(df_counts, by = c(\"name\" = \"common_name\"))\n\ngraph_object_corr\n\n# A tbl_graph: 74 nodes and 1925 edges\n#\n# An undirected simple graph with 1 component\n#\n# Node Data: 74 × 2 (active)\n   name                   species_count\n   &lt;chr&gt;                          &lt;dbl&gt;\n 1 American Crow                 189804\n 2 American Goldfinch             60297\n 3 American Robin                180148\n 4 Belted Kingfisher               2754\n 5 Black-capped Chickadee         12379\n 6 Blue Jay                       64894\n 7 Brown-headed Cowbird            8877\n 8 Bufflehead                      5758\n 9 Canada Goose                  159040\n10 Carolina Chickadee             21862\n# ℹ 64 more rows\n#\n# Edge Data: 1,925 × 3\n   from    to correlation\n  &lt;int&gt; &lt;int&gt;       &lt;dbl&gt;\n1     1     2       0.262\n2     1     3       0.259\n3     2     3       0.371\n# ℹ 1,922 more rows\n\n\nThis plots the network graph to show all the connections that fit the criteria in the code above:\n\nplot &lt;- graph_object_corr %&gt;% \n    ggraph() +\n    geom_edge_link(aes(width = correlation, alpha = correlation)) +\n    geom_node_point(aes(size = species_count, alpha = species_count)) +\n    scale_size_continuous(\"Total observations\", labels = scales::comma) +\n    scale_alpha_continuous(\"Total observations\", labels = scales::comma) +\n    scale_edge_alpha(\"Observations together\", range = c(.01, .3)) +\n    scale_edge_width(\"Observations together\", range = c(.1, 2)) +\n    theme_void()\n\nplot\n\n\n\n\n\n\n\n\nThere is a dense group of birds that are highly correlated with each other. This high correlation could be caused by a shared environment or seasonal migration patterns.\n\nHighlight species\nI also wanted to be able to see where a given species fits in the network graph. This code prepares a network graph object and filters out species that are not highly connected to the main group.\n\ngraph_object_corr &lt;- df_pair_corr %&gt;% \n  as_tbl_graph(directed = FALSE) %&gt;% \n  activate(edges) %&gt;% \n  filter(abs(correlation) &gt; .2) %&gt;% \n  activate(nodes) %&gt;% \n  mutate(centrality = centrality_authority()) %&gt;% \n  filter(centrality &gt; .01) %&gt;% \n  filter(!node_is_isolated())\n\nThis code identifies the node ID for the Northern Cardinal and makes a dataframe I will use to filter with in the next code chunk.\n\nspecies_list &lt;- c(\"Northern Cardinal\")\n\ndf_species_corr_nodes &lt;- graph_object_corr %&gt;% \n  activate(nodes) %&gt;% \n  as_tibble() %&gt;% \n  mutate(node_id = row_number()) %&gt;% \n  filter(name %in% species_list)\n\nThis code identifes which node is the Northern Cardinal, and which edges connect to the Northern Cardinal. This identifies all the species that are connected to the bird, given the criteria I set above.\n\nplot_corr &lt;- graph_object_corr %&gt;% \n  activate(nodes) %&gt;% \n  mutate(name_label = case_when(name %in% species_list ~ name,\n                                TRUE ~ as.character(NA))) %&gt;% \n  activate(edges) %&gt;% \n  left_join(df_species_corr_nodes, by = c(\"from\" = \"node_id\")) %&gt;% \n  left_join(df_species_corr_nodes, by = c(\"to\" = \"node_id\")) %&gt;% \n  mutate(name = coalesce(name.x, name.y),\n         centrality = coalesce(name.x, name.y)) %&gt;% \n  select(-matches(\".x|.y\")) %&gt;% \n  mutate(species_flag = case_when(from %in% df_species_corr_nodes$node_id | to %in% df_species_corr_nodes$node_id ~ TRUE,\n                                  TRUE ~ FALSE),\n         name = case_when(is.na(name) ~ \"Other species\",\n                          TRUE ~ name)) %&gt;%\n  ggraph() +\n    geom_edge_link(aes(alpha = species_flag, width = species_flag)) +\n    geom_node_point(aes(shape = !is.na(name_label), size = !is.na(name_label), color = name_label)) +\n    geom_node_label(aes(label = name_label, color = name_label), repel =  TRUE) +\n    scale_edge_width_manual(values = c(.3, 1)) +\n    scale_edge_alpha_discrete(range = c(0.1, .5)) +\n    scale_shape_manual(values = c(1, 19)) +\n    scale_size_manual(values = c(2, 3)) +\n    guides(edge_alpha = FALSE,\n           edge_width = FALSE,\n           size = FALSE,\n           shape = FALSE,\n           color = FALSE) +\n    theme_void()\n\nplot_corr"
  },
  {
    "objectID": "posts/shiny-venn-diagram/index.html",
    "href": "posts/shiny-venn-diagram/index.html",
    "title": "Making a Venn diagram in Shiny",
    "section": "",
    "text": "Introduction\nThis blog post is about making Venn diagrams work in Shiny, and the issues I ran into with shiny::nearPoints(). I show how this impacted my initial approach, and discuss the underlying issue.\nTLDR; shiny::nearPoints() doesn’t work with dataframes containing list-columns the way I expected\n\n\nBackground\nI have been working on a Shiny app that I will use to plan birdwatching trips. It uses the {ebirdst} package to pull abundance data for hundreds of species of birds in 27x27km tiles in North America. A major feature of the app will be the ability to compare how similar two areas (tiles) are. This compares the abundance for a species in a given tile in a given month. I wanted to include a Venn diagram that shows which species are exclusive to each tile. The user can click on the Venn diagram to see the species associated with each segment of the Venn diagram.\nThis involves making a venn diagram in ggplot2 and extracting the segment that the user clicks on with nearPoints(). This was more challenging than I had anticipated.\n\n\nVenn diagram data\nnearPoints() requires:\n\ndf: a data frame with x and y coordinates it can interpret\ncoordinfo: the user click coordinates as captured from the ui\n\nI use the ggVennDiagram package to make the venn diagram plot. This package uses ggplot2, but does a lot of pre-processing of the data beforehand. This made it difficult to get access to the df for nearPoints().\nThis is an example of a ggVennDiagram plot. It takes a list object, turns that into a dataframe, and then uses sf to draw the circles.\n\nlibrary(tidyverse)\nlibrary(ggVennDiagram)\n\ngenes &lt;- paste(\"gene\",1:100,sep=\"\")\nset.seed(20210419)\nx &lt;- list(A=sample(genes,30),\n          B=sample(genes,50))\n\nggVennDiagram(x)\n\n\n\n\n\n\n\n\nLooking under the hood of ggVennDiagram() shows the pre-processing steps:\n\nvenn &lt;- Venn(x)\ndata &lt;- process_data(venn)\n\nVenn() creates an object with slots representing the two sets A and B\n\nVenn(x)\n\nAn object of class \"Venn\"\nSlot \"sets\":\n$A\n [1] \"gene27\" \"gene76\" \"gene57\" \"gene33\" \"gene78\" \"gene39\" \"gene63\" \"gene41\"\n [9] \"gene66\" \"gene17\" \"gene16\" \"gene69\" \"gene75\" \"gene9\"  \"gene68\" \"gene3\" \n[17] \"gene34\" \"gene54\" \"gene19\" \"gene83\" \"gene2\"  \"gene40\" \"gene87\" \"gene60\"\n[25] \"gene61\" \"gene24\" \"gene44\" \"gene93\" \"gene53\" \"gene7\" \n\n$B\n [1] \"gene84\"  \"gene36\"  \"gene37\"  \"gene47\"  \"gene91\"  \"gene46\"  \"gene92\" \n [8] \"gene33\"  \"gene67\"  \"gene73\"  \"gene25\"  \"gene5\"   \"gene63\"  \"gene2\"  \n[15] \"gene83\"  \"gene56\"  \"gene77\"  \"gene10\"  \"gene12\"  \"gene95\"  \"gene76\" \n[22] \"gene53\"  \"gene99\"  \"gene19\"  \"gene31\"  \"gene86\"  \"gene80\"  \"gene65\" \n[29] \"gene48\"  \"gene100\" \"gene89\"  \"gene58\"  \"gene35\"  \"gene30\"  \"gene21\" \n[36] \"gene44\"  \"gene72\"  \"gene18\"  \"gene45\"  \"gene42\"  \"gene1\"   \"gene27\" \n[43] \"gene90\"  \"gene14\"  \"gene43\"  \"gene26\"  \"gene96\"  \"gene17\"  \"gene16\" \n[50] \"gene29\" \n\n\nSlot \"names\":\n[1] \"A\" \"B\"\n\n\nprocess_data() turns those slots into dataframes with sf columns representing the segment polygons.\n\nvenn &lt;- Venn(x)\nprocess_data(venn)\n\nAn object of class \"VennPlotData\"\nSlot \"setEdge\":\nSimple feature collection with 2 features and 5 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 125 ymin: 250 xmax: 875 ymax: 750\nCRS:           NA\n# A tibble: 2 × 6\n  id                                        geometry component item  count name \n  &lt;chr&gt;                                 &lt;LINESTRING&gt; &lt;chr&gt;     &lt;nam&gt; &lt;int&gt; &lt;chr&gt;\n1 1     (500 716, 493.065 720.007, 485.954 723.777,… setEdge   &lt;chr&gt;    30 A    \n2 2     (500 284, 506.935 279.998, 514.046 276.243,… setEdge   &lt;chr&gt;    50 B    \n\nSlot \"setLabel\":\nSimple feature collection with 2 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 250 ymin: 780 xmax: 750 ymax: 780\nCRS:           NA\n# A tibble: 2 × 4\n  id     geometry component name \n  &lt;chr&gt;   &lt;POINT&gt; &lt;chr&gt;     &lt;chr&gt;\n1 1     (250 780) setLabel  A    \n2 2     (750 780) setLabel  B    \n\nSlot \"region\":\nSimple feature collection with 3 features and 5 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 125 ymin: 250 xmax: 875 ymax: 750\nCRS:           NA\n# A tibble: 3 × 6\n  id                                        geometry component item  count name \n  &lt;chr&gt;                                    &lt;POLYGON&gt; &lt;chr&gt;     &lt;lis&gt; &lt;int&gt; &lt;chr&gt;\n1 1     ((500 716, 492.317 711.329, 484.878 706.459… region    &lt;chr&gt;    19 A    \n2 2     ((500 284, 507.683 288.649, 515.122 293.497… region    &lt;chr&gt;    39 B    \n3 12    ((507.683 711.328, 515.122 706.458, 522.317… region    &lt;chr&gt;    11 A..B \n\n\nThe region slot is most important for my purposes. It contains the sf polygons for the segments and the distinct counts exclusive to each segment.\n\nprocess_data(venn) %&gt;% \n  .@region\n\nSimple feature collection with 3 features and 5 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 125 ymin: 250 xmax: 875 ymax: 750\nCRS:           NA\n# A tibble: 3 × 6\n  id                                        geometry component item  count name \n  &lt;chr&gt;                                    &lt;POLYGON&gt; &lt;chr&gt;     &lt;lis&gt; &lt;int&gt; &lt;chr&gt;\n1 1     ((500 716, 492.317 711.329, 484.878 706.459… region    &lt;chr&gt;    19 A    \n2 2     ((500 284, 507.683 288.649, 515.122 293.497… region    &lt;chr&gt;    39 B    \n3 12    ((507.683 711.328, 515.122 706.458, 522.317… region    &lt;chr&gt;    11 A..B \n\nprocess_data(venn) %&gt;% \n  .@region %&gt;% \n  ggplot(aes(fill = name)) +\n  geom_sf()\n\n\n\n\n\n\n\n\nI thought using nearPoints() would be pretty easy once I intercepted the region object from the preprocessing steps. I was wrong.\n\n\nShiny app error\nThis basic Shiny app will reproduce the error that nearPoints() generates:\n\nlibrary(shiny)\nlibrary(tidyverse)\nlibrary(ggVennDiagram)\nlibrary(sf)\n\n#ui\nui &lt;- fluidPage(\n  \n  titlePanel(\"Shiny Venn Diagram\"),\n  \n  mainPanel(\n    plotOutput(\"venn_diagram\", click = \"plot_click\"),\n    tableOutput(\"venn_table\")\n  )\n)\n\ngenes &lt;- paste(\"gene\",1:1000,sep=\"\")\nset.seed(20210419)\nx &lt;- list(A=sample(genes,300),\n          B=sample(genes,525))\n\nvenn &lt;- Venn(x)\nvenn_data &lt;- process_data(venn)@region %&gt;% \n  mutate(centroid = st_point_on_surface(geometry),\n         x = map_dbl(centroid, 1),\n         y = map_dbl(centroid, 2)) %&gt;% \n  select(x, y, name, geometry)\n\n#server\nserver &lt;- function(input, output){\n  \n  output$venn_diagram &lt;- renderPlot({\n    \n    venn_data %&gt;% \n      ggplot(aes(x, y, fill = name, label = name)) +\n      geom_sf() +\n      geom_label()\n    \n  })\n  \n  output$venn_table &lt;- renderTable({\n    \n    req(input$plot_click)\n    \n    nearPoints(venn_data, #this is the issue\n               input$plot_click,\n               threshold = 100)\n    \n  })\n  \n}\n\nThis is the error:\n\nWarning: Error in &lt;-: number of items to replace is not a multiple of replacement length\n104: print.xtable\n98: transform\n97: func\n95: f\n94: Reduce\n85: do\n84: hybrid_chain\n83: renderFunc\n82: output$venn_table\n1: shiny::runApp\n\n\n\nThe fix\nWrapping the venn_data object in st_drop_geometry() drops the sf list-column and turns it back into a regular dataframe.\n\nlibrary(shiny)\nlibrary(tidyverse)\nlibrary(ggVennDiagram)\nlibrary(sf)\n\n#ui\nui &lt;- fluidPage(\n  \n  titlePanel(\"Shiny Venn Diagram\"),\n  \n  mainPanel(\n    plotOutput(\"venn_diagram\", click = \"plot_click\"),\n    tableOutput(\"venn_table\")\n  )\n)\n\ngenes &lt;- paste(\"gene\",1:1000,sep=\"\")\nset.seed(20210419)\nx &lt;- list(A=sample(genes,300),\n          B=sample(genes,525))\n\nvenn &lt;- Venn(x)\nvenn_data &lt;- process_data(venn)@region %&gt;% \n  mutate(centroid = st_point_on_surface(geometry),\n         x = map_dbl(centroid, 1),\n         y = map_dbl(centroid, 2)) %&gt;% \n  select(x, y, name, geometry)\n\n#server\nserver &lt;- function(input, output){\n  \n  output$venn_diagram &lt;- renderPlot({\n    \n    venn_data %&gt;% \n      ggplot(aes(x, y, fill = name, label = name)) +\n      geom_sf() +\n      geom_label()\n    \n  })\n  \n  output$venn_table &lt;- renderTable({\n    \n    req(input$plot_click)\n    \n    nearPoints(st_drop_geometry(venn_data), #the fix\n               input$plot_click,\n               threshold = 100)\n    \n  })\n  \n}\n\n\n\nWorking Shiny App\nThis is a working example of a Venn diagram in Shiny. input$plot_click captures the coordinates of the click and nearPoints() returns a dataframe of the information about the segment the user clicked on. The ID of the segment is in the name column."
  },
  {
    "objectID": "posts/model-results-leaflet-map/index.html",
    "href": "posts/model-results-leaflet-map/index.html",
    "title": "Pittsburgh City Boundary Model Leaflet Map",
    "section": "",
    "text": "Last Friday I posted a classification model that attempted to identify which census tracts in Allegheny County fall within the border of the City of Pittsburgh. I used a variety of data from the 2010 US Census via {tidycensus}, and I intentionally did not train the model with any geographic data. I am more interested in which census tracts are more “city-like”, regardless of their distance from the geographic center of the city.\nAs with most models, I think you can learn the most when you investigate the cases where the model failed. I made an interactive Leaflet map to help me to interrogate the model’s results. For each tract it includes the average city classification % and the model’s correct classification %, as well as the census data that was used to train the model. Make sure to click the “View Fullscreen” button in the top left corner to see more of the map."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ctompkins_quarto_blog",
    "section": "",
    "text": "Post With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nOct 8, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nOct 5, 2023\n\n\nTristan O’Malley\n\n\n\n\n\n\n\n\n\n\n\n\nSuburbanization of Allegheny County\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 18, 2022\n\n\nR package build\n\n\n\n\n\n\n\n\n\n\n\n\nMaking a Venn diagram in Shiny\n\n\n\n\n\n\nshiny\n\n\n\n\n\n\n\n\n\nMar 12, 2022\n\n\nConor Tompkins\n\n\n\n\n\n\n\n\n\n\n\n\nEffect of Geographic Resolution on ebirdst Abundance\n\n\n\n\n\n\nR\n\n\neBird\n\n\n\n\n\n\n\n\n\nNov 23, 2021\n\n\nConor Tompkins\n\n\n\n\n\n\n\n\n\n\n\n\nPittsburgh Riverhounds under Coach Lilley\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 5, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nDriving Alone vs. Public Transportation in Pittsburgh\n\n\nMapping commuter modes with bivariate discretized bins\n\n\n\n\n\n\n\n\n\n\n\nMar 29, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nHouse Price Estimator Dashboard\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJan 28, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nBike rental access in Pittsburgh\n\n\n\n\n\n\nR\n\n\nPittsburgh\n\n\nHealthy Ride\n\n\nWPRDC\n\n\n\n\n\n\n\n\n\nDec 4, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nShifting political winds\n\n\nOr, drawing arrows on maps\n\n\n\nR\n\n\nPolitics\n\n\n\n\n\n\n\n\n\nNov 13, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing major commuter routes in Allegheny County\n\n\n\n\n\n\nR\n\n\nAllegheny County\n\n\nCommuters\n\n\nCensus\n\n\nMapbox\n\n\n\n\n\n\n\n\n\nOct 25, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nGraphing volatile home values in U.S. metro areas\n\n\n\n\n\n\nHousing\n\n\nZillow\n\n\n\n\n\n\n\n\n\nOct 21, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nEffect of COVID-19 on Pittsburgh parking transactions\n\n\n\n\n\n\nPittsburgh\n\n\nParking\n\n\nCOVID-19\n\n\n\n\n\n\n\n\n\nOct 5, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nMapping BosWash commuter patterns with Flowmap.blue\n\n\n\n\n\n\nCensus\n\n\nCommuter patterns\n\n\n\n\n\n\n\n\n\nSep 25, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nPittsburgh City Boundary Model Leaflet Map\n\n\n\n\n\n\nR\n\n\nPittsburgh\n\n\nAllegheny County\n\n\n\n\n\n\n\n\n\nAug 23, 2020\n\n\nConor Tompkins\n\n\n\n\n\n\n\n\n\n\n\n\nModeling the Pittsburgh City Boundary\n\n\n\n\n\n\n\n\n\n\n\nAug 14, 2020\n\n\nConor Tompkins\n\n\n\n\n\n\n\n\n\n\n\n\nComparing Healthy Ride Usage Pre And “Post” COVID-19\n\n\n\n\n\n\n\n\n\n\n\nAug 14, 2020\n\n\nConor Tompkins\n\n\n\n\n\n\n\n\n\n\n\n\nTime series clustering COVID-19 case data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJul 20, 2020\n\n\nConor Tompkins\n\n\n\n\n\n\n\n\n\n\n\n\nWorking with Paycheck Protection Program data in R\n\n\n\n\n\n\nR\n\n\npolitics\n\n\n\n\n\n\n\n\n\nJul 8, 2020\n\n\nConor Tompkins\n\n\n\n\n\n\n\n\n\n\n\n\nGraphing Allegheny County COVID-19 data\n\n\n\n\n\n\nR\n\n\nCOVID-19\n\n\n\n\n\n\n\n\n\nJul 7, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n(re)Modeling the Office\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nJun 21, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nCleaning Creatively Formatted US Census Bureau Migration Data\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nMay 25, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nClustering Bird Species with Seasonality\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nMay 3, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nCumulative eBird Sightings in Allegheny County\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nApr 29, 2020\n\n\nConor Tompkins\n\n\n\n\n\n\n\n\n\n\n\n\neBirding in Allegheny County\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nApr 18, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nRoughly Calculating Allegheny County Transit Efficiency\n\n\n\n\n\n\nR\n\n\nWPRDC\n\n\nAllegheny County\n\n\nTransit\n\n\n\n\n\n\n\n\n\nApr 4, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing Transit Connections Between Pittsburgh Census Tracts\n\n\n\n\n\n\nR\n\n\nPittsburgh\n\n\nAllegheny County\n\n\nTransit\n\n\nWPRDC\n\n\n\n\n\n\n\n\n\nMar 1, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nHow Many Pittsburghers Cross the River to Get to Work\n\n\n\n\n\n\nR\n\n\nPittsburgh\n\n\nAllegheny County\n\n\nCensus\n\n\nWPRDC\n\n\n\n\n\n\n\n\n\nFeb 9, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nForecasting Healthy Ride Ridership With Prophet\n\n\n\n\n\n\nR\n\n\nPittsburgh\n\n\nHealthy Ride\n\n\n\n\n\n\n\n\n\nAug 3, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nResidential Zoning in Pittsburgh\n\n\n\n\n\n\nR\n\n\nPittsburgh\n\n\n\n\n\n\n\n\n\nJun 22, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nMap Census Data With R\n\n\n\n\n\n\nR\n\n\nPotholes\n\n\nCensus\n\n\n\n\n\n\n\n\n\nMay 28, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nPremier League 538 SPI Ratings\n\n\n\n\n\n\nR\n\n\nPittsburgh\n\n\n538\n\n\nSoccer\n\n\n\n\n\n\n\n\n\nApr 7, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nAnimating Growth of Allegheny County\n\n\n\n\n\n\nR\n\n\nAllegheny County\n\n\nWPRDC\n\n\nHousing\n\n\n\n\n\n\n\n\n\nMar 31, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nModeling Pittsburgh House Sales Linear\n\n\n\n\n\n\nR\n\n\nHousing\n\n\nAllegheny County\n\n\n\n\n\n\n\n\n\nJan 13, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nActblue Interstate Political Campaign Donations\n\n\n\n\n\n\nR\n\n\nPolitics\n\n\n\n\n\n\n\n\n\nNov 11, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nNetworking USL Club Similarity With Euclidean Distance\n\n\n\n\n\n\nR\n\n\nSoccer\n\n\n538\n\n\n\n\n\n\n\n\n\nSep 14, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nClustering Allegheny County Census Tracts With PCA and k-means\n\n\n\n\n\n\nR\n\n\nCensus\n\n\nAllegheny County\n\n\n\n\n\n\n\n\n\nSep 8, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nUSL in the 538 Global Club Soccer Rankings\n\n\n\n\n\n\nR\n\n\nUSL\n\n\nRiverhounds\n\n\n\n\n\n\n\n\n\nAug 14, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Allegheny County With Census Data\n\n\n\n\n\n\nR\n\n\nCensus\n\n\nAllegheny County\n\n\n\n\n\n\n\n\n\nAug 6, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nExploring 311 Data With PCA\n\n\n\n\n\n\nCensus\n\n\nPittsburgh\n\n\nWPRDC\n\n\n\n\n\n\n\n\n\nJul 19, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nCar Crashes in Allegheny County\n\n\n\n\n\n\nWPDRC\n\n\nAllegheny County\n\n\n\n\n\n\n\n\n\nJun 27, 2018\n\n\n\n\n\n\nNo matching items"
  }
]