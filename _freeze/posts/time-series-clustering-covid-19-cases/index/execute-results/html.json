{
  "hash": "5df3bfcabd00c0b14144a586ea00a69a",
  "result": {
    "markdown": "---\n  # Documentation: https://sourcethemes.com/academic/docs/managing-content/\ntitle: \"Time series clustering COVID-19 case data\"\nslug: time-series-clustering-covid-19-cases\nauthor: \"Conor Tompkins\"\ntags: []\ncategories: []\ndate: 2020-07-20\nlastmod: 2020-08-07T09:32:51-04:00\nfeatured: false\ndraft: false\neditor_options: \n  chunk_output_type: console\nimage: preview.png\nexecute: \n  message: false\n  warning: false\n---\n\n  \n[Interactive Tableau visualization of the clusters](https://public.tableau.com/app/profile/conorotompkins/viz/COVID-19TimeSeriesClustering/Dashboard)\n                                                      \nThe goal of this post is to group states into clusters based on the shape of the curve of a state's cumulative sum of COVID-19 cases. This type of clustering is useful when the variance in absolute values of a time series obscures the underlying pattern in the data. Since states experienced plateaus and peaks at different times, my hope is that the clustering is able to identify those differences.\n\nThis loads the packages I will use in the analysis and set up the environment:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tsibble)\nlibrary(dtwclust)\nlibrary(tidymodels)\nlibrary(hrbrthemes)\nlibrary(tidycensus)\nlibrary(sf)\n\noptions(scipen = 999, digits = 4)\n\ntheme_set(theme_ipsum())\n\nset.seed(1234)\n```\n:::\n\n\n\nI will adjust the cases to per 100,000, which requires information from the U.S. Census. This code pulls state-level population data from the Census API via `tidycensus`:\n\n::: {.cell}\n\n```{.r .cell-code}\ncensus_data <- get_acs(geography = \"state\", variables = \"B01003_001\", geometry = FALSE, year = 2020) %>% \n  select(state = NAME, population = estimate)\n```\n:::\n\n\n\nThis pulls the COVID-19 data from the [NYTimes GitHub page]():\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncovid <- read_csv(\"https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-states.csv\") %>% \n  arrange(state, date) %>% \n  semi_join(census_data) %>% \n  filter(date <= \"2020-07-18\")\n```\n:::\n\n\nI use the `tsibble` package to check if there are implicit gaps in the data. For example, if there was data for 2020-06-01 and 2020-06-03, there is an implicit gap because there is not data for 2020-06-02.\n\n::: {.cell}\n\n```{.r .cell-code}\ncovid %>% \n  as_tsibble(index = date, key = state) %>% \n  count_gaps()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 0 × 4\n# ℹ 4 variables: state <chr>, .from <date>, .to <date>, .n <int>\n```\n:::\n:::\n\n\nThankfully, there are not any such gaps. If  there were, I would have to impute values for the missing days.\n\nSince states experienced onset of COVID-19 at different times, I find the day each state hit 10 cases, and calculate `days_since_10th_case`, which I will use instead of `date`.\n\n::: {.cell}\n\n```{.r .cell-code}\ncovid_10th_case <- covid %>% \n  filter(cases >= 10) %>% \n  group_by(state) %>% \n  slice(1) %>% \n  ungroup() %>% \n  select(state, date_of_10th_case = date)\n\ncovid <- covid %>% \n  left_join(covid_10th_case, by = c(\"state\" = \"state\")) %>% \n  group_by(state) %>% \n  mutate(days_since_10th_case = date - date_of_10th_case) %>% \n  ungroup() %>% \n  filter(days_since_10th_case >= 0)\n\ncovid <- covid %>% \n  select(state, days_since_10th_case, cases)\n```\n:::\n\n\nNext I calculate `cases_per_capita`:\n\n::: {.cell}\n\n```{.r .cell-code}\ncovid <- covid %>% \n  left_join(census_data) %>% \n  mutate(cases_per_capita = (cases / population) * 100000) %>% \n  select(-population)\n```\n:::\n\n\nNext I scale the cases so that the mean is 0 and the standard deviation is 1. Each state has its own mean and standard deviation.\n\n::: {.cell}\n\n```{.r .cell-code}\ncovid <- covid %>% \n  group_by(state) %>% \n  mutate(cases_per_capita_scaled = scale(cases_per_capita, center = TRUE, scale = TRUE)) %>% \n  ungroup()\n```\n:::\n\n\nThe result of this is that the clustering algorithm will focus on the *shape* of the line for each state instead of absolute values. This graph shows the difference:\n\n::: {.cell}\n\n```{.r .cell-code}\ncovid %>% \n  pivot_longer(cols = contains(\"cases\"), names_to = \"metric\", values_to = \"value\") %>% \n  ggplot(aes(days_since_10th_case, value, group = state)) +\n  geom_hline(yintercept = 0, linetype = 2) +\n  geom_line(alpha = .1) +\n  facet_wrap(~metric, ncol = 1, scales = \"free_y\") +\n  scale_y_comma()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n`tsclust` requires that the input data be a series of lists, not a dataframe. `unstack` takes a `key` and `value` as arguments and turns the dataframe into a list of lists.\n\n::: {.cell}\n\n```{.r .cell-code}\ncovid_list <- covid %>% \n  select(state, cases_per_capita_scaled) %>% \n  unstack(cases_per_capita_scaled ~ state)\n```\n:::\n\n\nThis loops through the clustering function 20 times and saves each output to a list. The first object groups the data into 2 clusters, the second object has 3 clusters, and it continues in that pattern.\n\n::: {.cell}\n\n```{.r .cell-code}\ncluster_dtw_h <- list()\n\nkclust <- 20\n\nfor (i in 2:kclust){\n  cluster_dtw_h[[i]] <- tsclust(covid_list, \n                                type = \"h\", \n                                k = i,\n                                distance = \"dtw\", \n                                control = hierarchical_control(method = \"complete\"), \n                                seed = 390, \n                                preproc = NULL, \n                                args = tsclust_args(dist = list(window.size = 21L)))\n  \n  print(i)\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n[1] 11\n[1] 12\n[1] 13\n[1] 14\n[1] 15\n[1] 16\n[1] 17\n[1] 18\n[1] 19\n[1] 20\n```\n:::\n:::\n\n\nThe object that `tsclust` outputs has a complex structure that makes it difficult to work with at scale. The data I need to pull out is stored in various slots. The next step is to write functions that pulls out the data and tidies it up.\n\n### Cluster assigments\n\nThis function pulls which cluster each state was assigned to, for each `kclust`.\n\n::: {.cell}\n\n```{.r .cell-code}\nget_cluster_assignments <- function(object, cluster_number){\n  \n  df <- slot(object[[cluster_number]], \"cluster\")\n\n  return(df)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncluster_assignments <- 2:kclust %>%\n  set_names() %>% \n  map_df(~get_cluster_assignments(cluster_dtw_h, cluster_number = .x), .id = \"kclust\") %>% \n  pivot_longer(cols = -kclust, names_to = \"state\", values_to = \"cluster_assignment\") %>% \n  mutate(kclust = as.numeric(kclust)) %>% \n  arrange(state, kclust)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(cluster_assignments)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 988\nColumns: 3\n$ kclust             <dbl> 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,…\n$ state              <chr> \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"Alabam…\n$ cluster_assignment <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n```\n:::\n:::\n\n\nThese graphs shows which states are more likely to be assigned to a different cluster, depending on the number of clusters.\n\n::: {.cell}\n\n```{.r .cell-code}\nstate_variance <- cluster_assignments %>% \n  distinct(state, cluster_assignment) %>% \n  count(state, sort = TRUE)\n\ncluster_assignments %>%\n  left_join(state_variance) %>% \n  mutate(state = fct_reorder(state, n)) %>% \n  ggplot(aes(kclust, state, fill = as.factor(cluster_assignment))) +\n  geom_tile() +\n  scale_fill_viridis_d() +\n  labs(fill = \"Cluster assignment\",\n       y = NULL) +\n  theme(legend.position = \"bottom\", legend.direction = \"horizontal\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncluster_assignments %>% \n  distinct(state, cluster_assignment) %>% \n  count(state) %>% \n  mutate(state = fct_reorder(state, n)) %>% \n  ggplot(aes(n, state)) +\n  geom_col() +\n  labs(title = \"How much each state reacts to an increase in kclust\",\n       x = \"Number of clusters a state appears in\",\n       y = NULL)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\nThe number of singelton clusters (clusters with only one state) is an important metric for determining the optimal number of clusters. If a state is truly unique, a singleton cluster may be appropriate. Having 50 singleton clusters, however, would obviously be overfit.\n\nThis shows that the number of singleton clusters increases as `kclust` increases.\n\n::: {.cell}\n\n```{.r .cell-code}\ncluster_singletons <- cluster_assignments %>% \n  count(kclust, cluster_assignment) %>% \n  group_by(kclust) %>% \n  mutate(min_cluster_population = min(n)) %>% \n  filter(n == min_cluster_population) %>% \n  ungroup() %>% \n  select(kclust, min_cluster_population, n) %>% \n  group_by(kclust, min_cluster_population) %>% \n  summarize(n = sum(n)) %>% \n  ungroup()\n\ncluster_singletons %>% \n  filter(min_cluster_population == 1) %>% \n  ggplot(aes(kclust, n)) +\n  geom_line() +\n  scale_x_comma() +\n  labs(x = \"kclust\",\n       y = \"Number of singleton clusters\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\nThere is not a singleton cluster until `kclust` is 7.\n\n::: {.cell}\n\n```{.r .cell-code}\ncluster_singletons %>% \n  mutate(first_singleton = cumsum(min_cluster_population == 1) == 1) %>%\n  filter(first_singleton == TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 4\n  kclust min_cluster_population     n first_singleton\n   <dbl>                  <int> <int> <lgl>          \n1      7                      1     1 TRUE           \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncluster_assignments %>% \n  count(kclust, cluster_assignment) %>% \n  ggplot(aes(kclust, n, color = as.factor(kclust))) +\n  geom_jitter(show.legend = FALSE) +\n  geom_vline(xintercept = 8, linetype = 2) +\n  labs(y = \"Cluster population\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\n\n### Cluster distance\n\nThis function pulls the average distance of each cluster, for each value of `kclust`. Clusters with lower average distance are more similar, and those with higher average distance are less similar.\n\n::: {.cell}\n\n```{.r .cell-code}\nget_cluster_metrics <- function(object, cluster_number){\n  \n  object %>%\n    pluck(cluster_number) %>%\n    slot(\"clusinfo\") %>%\n    as_tibble()\n  \n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncluster_metrics <- map(2:kclust, ~get_cluster_metrics(cluster_dtw_h, .x)) |> \n  list_rbind(names_to = \"kclust\")\n\ncluster_metrics\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 209 × 3\n   kclust  size av_dist\n    <int> <int>   <dbl>\n 1      1    29    5.91\n 2      1    23    6.83\n 3      2    29    5.91\n 4      2    17    5.44\n 5      2     6    3.99\n 6      3    16    4.55\n 7      3    17    5.44\n 8      3    13    3.70\n 9      3     6    3.99\n10      4    16    4.55\n# ℹ 199 more rows\n```\n:::\n:::\n\n\nThis shows that 12 clusters may be the optimal number for `kclust`. Values greater than that begin to see diminishing returns.\n\n::: {.cell}\n\n```{.r .cell-code}\ncluster_metrics %>% \n  ggplot(aes(kclust, av_dist)) +\n  geom_jitter(aes(color = as.factor(kclust), size = size), show.legend = FALSE) +\n  geom_smooth(group = 1) +\n  geom_vline(xintercept = 12, linetype = 2) +\n  scale_size_continuous(range = c(.5, 4)) +\n  labs(y = \"Average distance\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\n\n\nThis shows what the individual state time series data looks like when it is grouped into 12 clusters:\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_kclust <- 12\n\ncovid %>% \n  left_join(filter(cluster_assignments, kclust == best_kclust)) %>% \n  add_count(cluster_assignment) %>% \n  mutate(cluster_assignment = str_c(\"Cluster\", cluster_assignment, sep = \" \"),\n         cluster_assignment = fct_reorder(as.character(cluster_assignment), n),\n         cluster_assignment = fct_rev(cluster_assignment)) %>% \n  ggplot(aes(days_since_10th_case, cases_per_capita_scaled, \n             color = cluster_assignment, group = state)) +\n  geom_hline(yintercept = 0, linetype = 2) +\n  geom_vline(xintercept = 60, linetype = 2) +\n  geom_line(size = 1, alpha = .5) +\n  facet_wrap(~cluster_assignment, ncol = 4) +\n  guides(color = FALSE) +\n  labs(title = \"kclust == 12\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n:::\n\n\nI think 13 clusters is appropriate, but there are obviously cases where individual assignments can be disputed. This is an unsupervised clustering problem, so I generally pick the `kclust` with the least diminishing returns and go with it.\n\n## Mapping\n\nThe data is aggregated at the state level, which can easily be graphed with `ggplot2` and `tidycensus`.\n\n::: {.cell}\n\n```{.r .cell-code}\nmap <- get_acs(geography = \"state\", variables = \"B01003_001\", geometry = TRUE, shift_geo = TRUE)\n\nmap %>% \n  ggplot() +\n  geom_sf() +\n  theme_void()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n:::\n\n\nThis joins the cluster assignments to the map object and summarizes the state polygons by region. This dissolves the state boundaries and creates polygons for each cluster.\n\n::: {.cell}\n\n```{.r .cell-code}\nmap_cluster <- map %>% \n  left_join(cluster_assignments %>% \n             filter(kclust == best_kclust), by = c(\"NAME\" = \"state\")) %>% \n  add_count(cluster_assignment) %>% \n  mutate(cluster_assignment = as.character(cluster_assignment),\n         cluster_assignment = fct_reorder(cluster_assignment, desc(n))) %>% \n  group_by(cluster_assignment) %>% \n  summarize()\n\nstate_clustered <- map %>% \n  left_join(cluster_assignments %>% \n              filter(kclust == best_kclust), by = c(\"NAME\" = \"state\")) %>% \n  add_count(cluster_assignment) %>% \n  mutate(cluster_assignment = as.character(cluster_assignment),\n         cluster_assignment = fct_reorder(cluster_assignment, desc(n)))\n```\n:::\n\n\nThis code creates the map, and overlays the state boundaries on the cluster polygons.\n\n::: {.cell}\n\n```{.r .cell-code}\nmap_cluster %>% \n  ggplot() +\n  geom_sf(aes(fill = cluster_assignment, color = cluster_assignment),\n          size = 1) +\n  geom_sf_text(data = state_clustered, aes(label = cluster_assignment)) +\n  labs(fill = \"Cluster assignment\",\n       color = \"Cluster assigmment\") +\n  guides(color = FALSE) +\n  theme_void() +\n  theme(legend.position = \"bottom\",\n        legend.direction = \"horizontal\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n:::\n\n\n\n\nCluster 3 stands out as the group of states that are currently struggling with COVID-19 the most. Interestingly, these states are consistently clustered together regardless of the value of `kclust`, which means that these states are very similar.\n\nCluster 5 represents the states that had the earliest and worst outbreaks, but have beaten back the virus for now. Cluster 6 are the neighbors of New York and New Jersey. They experienced less peaky curves later than Cluster 5. Cluster 6 is an \"echo\" of Cluster 5.\n\nThe singleton clusters for kclust of 12 are Vermont, Nebraska, and Hawaii. Nebraska had a long period with almost no new cases at the beginning, but then had a very steep increase after that. Vermont's curve started steeply almost immediately after its 10th case, which distinguishes it from the other states. Hawaii has had two periods of very steep increases sperated by a long period with few new cases. This is very likely due to the difficulty of traveling to the state with travel lockdowns in place.\n\n### Sources\n\n* https://rpubs.com/esobolewska/dtw-time-series\n* http://www.rdatamining.com/examples/time-series-clustering-classification\n* http://rstudio-pubs-static.s3.amazonaws.com/398402_abe1a0343a4e4e03977de8f3791e96bb.html\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}