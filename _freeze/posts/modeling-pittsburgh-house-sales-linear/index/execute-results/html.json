{
  "hash": "1b0b1f36f69f253dfff291a0a6b6ea9f",
  "result": {
    "engine": "knitr",
    "markdown": "---\n# Documentation: https://sourcethemes.com/academic/docs/managing-content/\ntitle: \"Modeling Pittsburgh House Sales Linear\"\nsubtitle: \"\"\nsummary: \"\"\nauthors: [Conor Tompkins]\ntags: [R, Housing, Allegheny County]\ncategories: [R, Housing, Allegheny County]\ndate: 2019-01-13\nlastmod: 2020-09-06\nfeatured: false\ndraft: false\nimage: featured.png\nprojects: []\neditor_options: \n  chunk_output_type: console\nexecute:\n  echo: true\n  warning: false\n  message: false\n---\n\n\nIn this post I will be modeling house (land parcel) sales in Pittsburgh. The data is from the WPRDC's [Parcels n'at](http://tools.wprdc.org/parcels-n-at/#) dashboard or [here](https://data.wprdc.org/dataset/property-assessments/resource/f2b8d575-e256-4718-94ad-1e12239ddb92).\n\nThe goal is to use linear modeling to predict the sale price of a house using features of the house and the property.\n\nThis code sets up the environment and loads the libraries I will use.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#load libraries\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(caret)\nlibrary(broom)\nlibrary(modelr)\nlibrary(rsample)\nlibrary(janitor)\nlibrary(vroom)\n\n#set up environment\noptions(scipen = 999, digits = 5)\n\ntheme_set(theme_bw())\n```\n:::\n\n\nThis reads the data and engineers some features.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#read in data\ndf <- vroom(\"post_data/allegheny_county_master_file.csv\", col_types = cols(.default = \"c\")) %>% \n  clean_names() %>% \n  mutate(across(.cols = c(saleprice, finishedlivingarea, lotarea, yearblt,\n                          bedrooms, fullbaths, halfbaths), parse_number))\n\n#glimpse(df)\n\n\n# df %>% \n#   select(saleprice, finishedlivingarea, lotarea, yearblt, bedrooms, fullbaths, halfbaths) %>% \n#   glimpse()\n# \n# df %>% \n#   select(contains(\"muni\"))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- df %>% \n  mutate(munidesc = str_replace(munidesc, \" - PITTSBURGH\", \"\")) %>% \n  mutate(finishedlivingarea_log10 = log10(finishedlivingarea),\n         lotarea_log10 = log10(lotarea),\n         saleprice_log10 = log10(saleprice)) %>% \n  select(parid, classdesc, munidesc, schooldesc, neighdesc, taxdesc,\n         usedesc, homesteadflag, farmsteadflag, styledesc,\n         yearblt, extfinish_desc, roofdesc,  basementdesc,\n         gradedesc, conditiondesc, stories, totalrooms, bedrooms,\n         fullbaths, halfbaths, heatingcoolingdesc, fireplaces, \n         bsmtgarage, finishedlivingarea, finishedlivingarea_log10,\n         lotarea, lotarea_log10, saledate,\n         saleprice, saleprice_log10)\n\n#create grade vectors\ngrades_standard <- c(\"average -\", \"average\", \"average +\",\n                     \"good -\", \"good\", \"good +\",\n                     \"very good -\", \"very good\", \"very good +\")\n\ngrades_below_average_or_worse <- c(\"poor -\", \"poor\", \"poor +\",\n                                   \"below average -\", \"below average\", \"below average +\")\n\ngrades_excellent_or_better <- c(\"excellent -\", \"excellent\", \"excellent +\",\n                                \"highest cost -\", \"highest cost\", \"highest cost +\")\n\n#subset data and engineer features\ndf <- df %>% \n  filter(classdesc == \"RESIDENTIAL\",\n         saleprice > 100,\n         str_detect(munidesc, \"Ward\"),\n         finishedlivingarea > 0,\n         lotarea > 0) %>% \n  select(parid, munidesc, schooldesc, neighdesc, taxdesc,\n         usedesc, homesteadflag, farmsteadflag, styledesc,\n         yearblt, extfinish_desc, roofdesc,  basementdesc, \n         heatingcoolingdesc, gradedesc, conditiondesc, stories, \n         totalrooms, bedrooms, fullbaths, halfbaths, fireplaces, \n         bsmtgarage, finishedlivingarea_log10, lotarea_log10, \n         saleprice_log10, saledate) %>% \n  mutate(usedesc = fct_lump(usedesc, n = 5),\n         styledesc = fct_lump(styledesc, n = 10),\n         #clean up and condense gradedesc\n         gradedesc = str_to_lower(gradedesc),\n         gradedesc = case_when(gradedesc %in% grades_below_average_or_worse ~ \"below average + or worse\",\n                                    gradedesc %in% grades_excellent_or_better ~ \"excellent - or better\",\n                                    gradedesc %in% grades_standard ~ gradedesc),\n         gradedesc = fct_relevel(gradedesc, c(\"below average + or worse\", \"average -\", \"average\", \"average +\",\n                                                        \"good -\", \"good\", \"good +\",\n                                                        \"very good -\", \"very good\", \"very good +\", \"excellent - or better\")))\n\n#replace missing character rows with \"missing\", change character columns to factor\ndf <- df %>% \n  mutate_if(is.character, replace_na, \"missing\") %>% \n  mutate_if(is.character, as.factor)\n\n#select response and features\ndf <- df %>% \n  select(munidesc, usedesc, styledesc, conditiondesc, gradedesc,\n         finishedlivingarea_log10, lotarea_log10, yearblt, bedrooms, \n         fullbaths, halfbaths, saleprice_log10) %>% \n  na.omit()\n\n#muni_desc_levels <- levels(df$munidesc)\n\n#view data\nglimpse(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 74,687\nColumns: 12\n$ munidesc                 <fct> 1st Ward , 1st Ward , 1st Ward , 1st Ward , 1…\n$ usedesc                  <fct> Other, SINGLE FAMILY, Other, Other, Other, Ot…\n$ styledesc                <fct> Other, TOWNHOUSE, Other, Other, Other, Other,…\n$ conditiondesc            <fct> AVERAGE, EXCELLENT, GOOD, AVERAGE, AVERAGE, G…\n$ gradedesc                <fct> very good +, excellent - or better, very good…\n$ finishedlivingarea_log10 <dbl> 3.0993, 3.6170, 3.1844, 3.2057, 3.1173, 3.159…\n$ lotarea_log10            <dbl> 3.0993, 3.0461, 3.2355, 3.1584, 3.1173, 3.159…\n$ yearblt                  <dbl> 2007, 2012, 2007, 2007, 2015, 1905, 1905, 190…\n$ bedrooms                 <dbl> 2, 3, 2, 2, 2, 1, 2, 2, 1, 2, 4, 3, 3, 4, 4, …\n$ fullbaths                <dbl> 2, 3, 2, 1, 2, 1, 2, 2, 1, 2, 2, 2, 2, 3, 2, …\n$ halfbaths                <dbl> 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, …\n$ saleprice_log10          <dbl> 6.0414, 6.2544, 5.8543, 5.8876, 5.6767, 5.531…\n```\n\n\n:::\n:::\n\n\nAs shown in the data above, the model uses the following features to predict sale price:\n\n-   municipality name\n-   primary use of the parcel\n-   style of building\n-   condition of the structure\n-   grade of construction\n-   living area in square feet\n-   lot area in square feet\n-   year the house was built\n-   number of bedrooms\n-   number of full baths\n-   number of half-baths\n\nThis code sets up the data for cross validation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#create initial split object\ndf_split <- initial_split(df, prop = .75)\n\n#extract training dataframe\ntraining_data_full <- training(df_split)\n\n#extract testing dataframe\ntesting_data <- testing(df_split)\n\ndistinct(training_data_full, munidesc)|> \n  anti_join(distinct(testing_data, munidesc))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n  munidesc              \n  <fct>                 \n1 1st Ward  - McKEESPORT\n```\n\n\n:::\n\n```{.r .cell-code}\n#find dimensions of training_data_full and testing_data\ndim(training_data_full)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 56015    12\n```\n\n\n:::\n\n```{.r .cell-code}\ndim(testing_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 18672    12\n```\n\n\n:::\n:::\n\n\nThis code divides the data into training and testing sets.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\n\n#prep the df with the cross validation partitions\ncv_split <- vfold_cv(training_data_full, v = 5)\n\ncv_data <- cv_split %>% \n  mutate(\n    #extract train dataframe for each split\n    train = map(splits, ~training(.x)), \n    #extract validate dataframe for each split\n    validate = map(splits, ~testing(.x))\n  )\n\n#view df\ncv_data\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#  5-fold cross-validation \n# A tibble: 5 × 4\n  splits                id    train                  validate              \n  <list>                <chr> <list>                 <list>                \n1 <split [44812/11203]> Fold1 <tibble [44,812 × 12]> <tibble [11,203 × 12]>\n2 <split [44812/11203]> Fold2 <tibble [44,812 × 12]> <tibble [11,203 × 12]>\n3 <split [44812/11203]> Fold3 <tibble [44,812 × 12]> <tibble [11,203 × 12]>\n4 <split [44812/11203]> Fold4 <tibble [44,812 × 12]> <tibble [11,203 × 12]>\n5 <split [44812/11203]> Fold5 <tibble [44,812 × 12]> <tibble [11,203 × 12]>\n```\n\n\n:::\n:::\n\n\nThis builds the model to predict house sale price.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#build model using the train data for each fold of the cross validation\ncv_models_lm <- cv_data %>% \n  mutate(model = map(train, ~lm(formula = saleprice_log10 ~ ., data = .x)))\n\ncv_models_lm\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#  5-fold cross-validation \n# A tibble: 5 × 5\n  splits                id    train                  validate model \n  <list>                <chr> <list>                 <list>   <list>\n1 <split [44812/11203]> Fold1 <tibble [44,812 × 12]> <tibble> <lm>  \n2 <split [44812/11203]> Fold2 <tibble [44,812 × 12]> <tibble> <lm>  \n3 <split [44812/11203]> Fold3 <tibble [44,812 × 12]> <tibble> <lm>  \n4 <split [44812/11203]> Fold4 <tibble [44,812 × 12]> <tibble> <lm>  \n5 <split [44812/11203]> Fold5 <tibble [44,812 × 12]> <tibble> <lm>  \n```\n\n\n:::\n\n```{.r .cell-code}\n#problem with factors split across training/validation\n#https://stats.stackexchange.com/questions/235764/new-factors-levels-not-present-in-training-data\n```\n:::\n\n\nThis is where I begin to calculate metrics to judge how well my model is doing.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncv_prep_lm <- cv_models_lm %>% \n  mutate(\n    #extract actual sale price for the records in the validate dataframes\n    validate_actual = map(validate, ~.x$saleprice_log10),\n    #predict response variable for each validate set using its corresponding model\n    validate_predicted = map2(.x = model, .y = validate, ~predict(.x, .y))\n  )\n\n#View data\ncv_prep_lm\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#  5-fold cross-validation \n# A tibble: 5 × 7\n  splits                id    train    validate model  validate_actual\n  <list>                <chr> <list>   <list>   <list> <list>         \n1 <split [44812/11203]> Fold1 <tibble> <tibble> <lm>   <dbl [11,203]> \n2 <split [44812/11203]> Fold2 <tibble> <tibble> <lm>   <dbl [11,203]> \n3 <split [44812/11203]> Fold3 <tibble> <tibble> <lm>   <dbl [11,203]> \n4 <split [44812/11203]> Fold4 <tibble> <tibble> <lm>   <dbl [11,203]> \n5 <split [44812/11203]> Fold5 <tibble> <tibble> <lm>   <dbl [11,203]> \n# ℹ 1 more variable: validate_predicted <list>\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#calculate fit metrics for each validate fold       \ncv_eval_lm <- cv_prep_lm %>% \n  mutate(validate_rmse = map2_dbl(model, validate, modelr::rmse),\n         validate_mae = map2_dbl(model, validate, modelr::mae))\n\ncv_eval_lm <- cv_eval_lm %>% \n  mutate(fit = map(model, ~glance(.x))) %>% \n  unnest(fit)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#view data\ncv_eval_lm %>% \n  select(id, validate_mae, validate_rmse, adj.r.squared)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 4\n  id    validate_mae validate_rmse adj.r.squared\n  <chr>        <dbl>         <dbl>         <dbl>\n1 Fold1        0.328         0.447         0.428\n2 Fold2        0.330         0.450         0.428\n3 Fold3        0.329         0.448         0.430\n4 Fold4        0.328         0.448         0.431\n5 Fold5        0.326         0.442         0.428\n```\n\n\n:::\n:::\n\n\nFinally, this calculates how well the model did on the validation set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#summarize fit metrics on cross-validated dfs\ncv_eval_lm %>% \n  select(validate_mae, validate_rmse, adj.r.squared) %>% \n  summarize_all(mean)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  validate_mae validate_rmse adj.r.squared\n         <dbl>         <dbl>         <dbl>\n1        0.328         0.447         0.429\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#fit model on full training set\ntrain_df <- cv_data %>% \n  select(train) %>% \n  unnest(train)\n\nmodel_train <- lm(formula = saleprice_log10 ~ ., data = train_df)\n\nmodel_train %>% \n  glance()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df   logLik     AIC     BIC\n      <dbl>         <dbl> <dbl>     <dbl>   <dbl> <dbl>    <dbl>   <dbl>   <dbl>\n1     0.430         0.430 0.446     1941.       0    87 -137142. 274462. 275381.\n# ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n```\n\n\n:::\n:::\n\n\nThis is the RMSE on the training set\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#calculate rmse on training set\nrmse(model_train, train_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.44626\n```\n\n\n:::\n:::\n\n\n\n\nThis shows the impact each term of the model has on the response variable. This is for the training data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#visualize estimates for terms\nmodel_train %>% \n  tidy() %>% \n  filter(term != \"(Intercept)\") %>% \n  mutate(term = fct_reorder(term, estimate)) %>% \n  ggplot(aes(term, estimate)) +\n  geom_hline(yintercept = 0, linetype = 2, color = \"red\") +\n  geom_point() +\n  coord_flip()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-1.png){width=576}\n:::\n:::\n\n\nNext, I apply the model to the testing data to see how the model does out-of-sample.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#create dfs for train_data_small and validate_data\n#train_data_small <- cv_prep_lm %>% \n#  unnest(train) %>% \n#  select(-id)\n\nvalidate_df <- cv_prep_lm %>% \n  select(validate) %>% \n  unnest()\n```\n:::\n\n\nThis creates the augmented dataframe and plots the actual price vs. the fitted price.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#visualize model on validate data\naugment_validate <- augment(model_train, newdata = validate_df) %>% \n  mutate(.resid = saleprice_log10 - .fitted)\n\n#actual vs. fitted\ncv_prep_lm %>% \n  unnest(validate_actual, validate_predicted) %>% \n  ggplot(aes(validate_actual, validate_predicted)) +\n  geom_abline() +\n  stat_density_2d(aes(fill = stat(level)), geom = \"polygon\") +\n  geom_smooth(method = \"lm\") +\n  scale_x_continuous(limits = c(2, 7)) +\n  scale_y_continuous(limits = c(2, 7)) +\n  coord_equal() +\n  scale_fill_viridis_c()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\nThis distribution shows that the model overestimates the prices on many houses.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#distribution of residuals\naugment_validate %>% \n  ggplot(aes(.resid)) +\n  geom_density() +\n  geom_vline(xintercept = 0, color = \"red\", linetype = 2)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\nThis shows that the residuals are correlated with the actual price, which indicates that the model is failing to account for some dynamic in the sale process.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#sale price vs. residuals\naugment_validate %>% \n  ggplot(aes(.resid, saleprice_log10)) +\n  stat_density_2d(aes(fill = stat(level)), geom = \"polygon\") +\n  geom_vline(xintercept = 0, color = \"red\", linetype = 2) +\n  scale_fill_viridis_c()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\nThis calculates how well the model predicted sale price on out-of-sample testing data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#calculate fit of model on test data\nrmse(model_train, validate_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.44626\n```\n\n\n:::\n\n```{.r .cell-code}\nmae(model_train, validate_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.32743\n```\n\n\n:::\n\n```{.r .cell-code}\nrsquare(model_train, validate_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.42988\n```\n\n\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}